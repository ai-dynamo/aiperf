# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

from typing import Annotated

from cyclopts import Parameter
from pydantic import Field

from aiperf.common.config.base_config import BaseConfig
from aiperf.common.config.config_defaults import LoadGeneratorDefaults
from aiperf.common.config.groups import Groups
from aiperf.common.enums import RequestRateMode


class LoadGeneratorConfig(BaseConfig):
    """
    A configuration class for defining top-level load generator settings.
    """

    _CLI_GROUP = Groups.LOAD_GENERATOR

    # TODO: Potentially add a validator to ensure that the concurrency is not greater than the request count
    concurrency: Annotated[
        int,
        Field(
            ge=1,
            description="The concurrency value to benchmark.",
        ),
        Parameter(
            name=(
                "--concurrency",  # GenAI-Perf
            ),
            group=_CLI_GROUP,
        ),
    ] = LoadGeneratorDefaults.CONCURRENCY

    request_rate: Annotated[
        float | None,
        Field(
            gt=0,
            description="Sets the request rate for the load generated by AIPerf. Unit: requests/second",
        ),
        Parameter(
            name=(
                "--request-rate",  # GenAI-Perf
            ),
            group=_CLI_GROUP,
        ),
    ] = LoadGeneratorDefaults.REQUEST_RATE

    # NEW AIPerf Option
    request_rate_mode: Annotated[
        RequestRateMode,
        Field(
            description="Sets the request rate mode for the load generated by AIPerf. Valid values: constant, poisson.\n"
            "constant: Generate requests at a fixed rate.\n"
            "poisson: Generate requests using a poisson distribution.\n"
            f"The default is {LoadGeneratorDefaults.REQUEST_RATE_MODE}.",
        ),
        Parameter(
            name=("--request-rate-mode"),
            group=_CLI_GROUP,
        ),
    ] = LoadGeneratorDefaults.REQUEST_RATE_MODE

    request_count: Annotated[
        int,
        Field(
            ge=1,
            description="The number of requests to use for measurement.",
        ),
        Parameter(
            name=(
                "--request-count",  # GenAI-Perf
                "--num-requests",  # GenAI-Perf
            ),
            group=_CLI_GROUP,
        ),
    ] = LoadGeneratorDefaults.REQUEST_COUNT

    warmup_request_count: Annotated[
        int,
        Field(
            ge=0,
            description="The number of warmup requests to send before benchmarking.",
        ),
        Parameter(
            name=(
                "--warmup-request-count",  # GenAI-Perf
                "--num-warmup-requests",  # GenAI-Perf
            ),
            group=_CLI_GROUP,
        ),
    ] = LoadGeneratorDefaults.WARMUP_REQUEST_COUNT

    # # TODO: Not implemented yet
    # # TODO: Technically this ramp should be considered as a warmup phase. We are handling
    # #       this via the CreditPhase concept.
    # concurrency_ramp_up_time: Annotated[
    #     float | None,
    #     Field(
    #         gt=0,
    #         description="The time it takes to ramp up the concurrency from 0 to the target concurrency. Unit: seconds",
    #     ),
    #     Parameter(name=("--concurrency-ramp-up-time")),
    # ] = LoadGeneratorDefaults.CONCURRENCY_RAMP_UP_TIME
