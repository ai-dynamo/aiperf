{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-aiperf-documentation","title":"Welcome to AIPerf Documentation","text":"<p>AIPerf is a package for performance testing of AI models.</p>"},{"location":"#overview","title":"Overview","text":"<ul> <li>Explore the documentation using the navigation menu.</li> <li>See the Development page for contributing and setup instructions.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies</li> <li>Run the CLI or use the Python API</li> </ol> <p>For more details, see the rest of the documentation.</p>"},{"location":"Development/","title":"Development","text":""},{"location":"Development/#aiperf-developers-guide","title":"AIPerf Developers Guide","text":"<p>This guide will help you set up your development environment and understand the AIPerf system architecture.</p>"},{"location":"Development/#quick-start","title":"Quick Start","text":""},{"location":"Development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Linux environment (preferred)</li> </ul>"},{"location":"Development/#development-environment-setup","title":"Development Environment Setup","text":"<p>Choose ONE option below!</p>"},{"location":"Development/#option-a-devcontainer-preferred","title":"Option A: Devcontainer (Preferred)","text":"<p>Supported by: - VS Code - Cursor - Pycharm</p> <p>Use your IDE to re-open the project folder inside the devcontainer. Usually available as a popup when you first load the project. This will automatically build and install all required dependencies</p>"},{"location":"Development/#option-b-using-make-native-alternative","title":"Option B: Using Make (Native Alternative)","text":"<pre><code>make first-time-setup\n</code></pre>   This command will: - Install uv if not present - Create a virtual environment - Install dependencies - Install pre-commit hooks"},{"location":"Development/#option-c-manual-native-alternative","title":"Option C: Manual (Native Alternative)","text":"<pre><code># Install uv package manager\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment\nuv venv --python 3.12\n\n# Activate virtual environment\nsource .venv/bin/activate\n\n# Install package in editable development mode\nuv pip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install --install-hooks\n</code></pre>"},{"location":"Development/#running-aiperf","title":"Running AIPerf","text":"<ul> <li>Default mode:   ```bash   aiperf</li> </ul> <p># also same as running   aiperf --run-type process   ```</p> <ul> <li> <p>Kubernetes mode (not yet supported):   <code>bash   aiperf --run-type k8s</code></p> </li> <li> <p>With debug logging:   <code>bash   aiperf --log-level DEBUG</code></p> </li> <li> <p>View all options:   <code>bash   aiperf --help</code></p> </li> </ul> <p>Press <code>Ctrl-C</code> to stop the process normally.</p> <p>Note: Press <code>Ctrl-Z</code> followed by <code>disown</code>, then <code>pkill -9 aiperf</code> if the process gets \"stuck\".</p> <p>Each process is named after the service_id. To see the running processes, run:</p> <pre><code>pgrep -a \"aiperf\n\n2879138 aiperf system_controller_e3291509\n2879170 aiperf dataset_manager_d821a161\n2879173 aiperf timing_manager_5e8caf4b\n2879175 aiperf worker_manager_77393965\n2879177 aiperf records_manager_c16acdb0\n2879179 aiperf post_processor_manager_d595537b\n</code></pre>"},{"location":"Development/#development-commands","title":"Development Commands","text":"<p>The project includes an optional Makefile for common development tasks:</p> <pre><code>make help              # Show all available commands\nmake test              # Run tests\nmake test-verbose      # Run tests with debug logging\nmake coverage          # Run tests with coverage report\nmake lint              # Run linting with ruff\nmake lint-fix          # Auto-fix linting issues\nmake format            # Format code with ruff\nmake check-format      # Check code formatting\nmake clean             # Clean up cache files\nmake docker            # Build docker image\nmake install           # Install in editable mode\n</code></pre> <p>You can also look inside at the commands it runs if you prefer to run things manually.</p>"},{"location":"Development/#project-architecture","title":"Project Architecture","text":""},{"location":"Development/#directory-structure","title":"Directory Structure","text":"<pre><code>aiperf/\n\u251c\u2500\u2500 aiperf/                      # Main Python package\n\u2502   \u251c\u2500\u2500 cli.py                   # Command line interface\n\u2502   \u251c\u2500\u2500 common/                  # Shared utilities and components\n\u2502   \u2502   \u251c\u2500\u2500 bootstrap.py         # Service bootstrapping\n\u2502   \u2502   \u251c\u2500\u2500 comms/               # Communication system\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base.py          # Base communication classes\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 client_enums.py  # Client type definitions\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zmq/             # ZeroMQ implementation\n\u2502   \u2502   \u251c\u2500\u2500 config/              # Configuration management\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service_config.py # Service configuration models\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 loader.py        # Configuration loading utilities\n\u2502   \u2502   \u251c\u2500\u2500 service/             # Base service framework\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_service.py              # Core service implementation\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_service_interface.py    # Service interface definition\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_component_service.py    # Component service base\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 base_controller_service.py   # Controller service base\n\u2502   \u2502   \u251c\u2500\u2500 enums.py             # System-wide enumerations\n\u2502   \u2502   \u251c\u2500\u2500 models.py            # Pydantic data models\n\u2502   \u2502   \u251c\u2500\u2500 exceptions.py        # Custom exception classes\n\u2502   \u2502   \u251c\u2500\u2500 factories.py         # Service factory patterns\n\u2502   \u2502   \u251c\u2500\u2500 hooks.py             # Service lifecycle hooks\n\u2502   \u2502   \u251c\u2500\u2500 tokenizer.py         # Text tokenization utilities\n\u2502   \u2502   \u2514\u2500\u2500 utils.py             # General utilities\n\u2502   \u251c\u2500\u2500 services/                # Microservices implementation\n\u2502   \u2502   \u251c\u2500\u2500 system_controller/   # System orchestration service\n\u2502   \u2502   \u251c\u2500\u2500 dataset/             # Dataset management service\n\u2502   \u2502   \u251c\u2500\u2500 timing_manager/      # Request timing and credit management\n\u2502   \u2502   \u251c\u2500\u2500 worker_manager/      # Worker process management\n\u2502   \u2502   \u251c\u2500\u2500 worker/              # Request execution workers\n\u2502   \u2502   \u251c\u2500\u2500 records_manager/     # Result storage and management\n\u2502   \u2502   \u251c\u2500\u2500 post_processor_manager/ # Result processing and metrics\n\u2502   \u2502   \u2514\u2500\u2500 service_manager/     # Service lifecycle management\n\u2502   \u2514\u2500\u2500 tests/                   # Test suite\n\u2502       \u251c\u2500\u2500 conftest.py          # Pytest configuration\n\u2502       \u251c\u2500\u2500 services/            # Service-specific tests\n\u2502       \u251c\u2500\u2500 comms/               # Communication tests\n\u2502       \u2514\u2500\u2500 utils/               # Utility tests\n\u251c\u2500\u2500 docs/                        # Documentation\n\u251c\u2500\u2500 tools/                       # Development tools\n\u251c\u2500\u2500 .devcontainer/               # VS Code dev container config\n\u251c\u2500\u2500 .github/                     # GitHub workflows\n\u251c\u2500\u2500 pyproject.toml               # Project configuration\n\u251c\u2500\u2500 Makefile                     # Development automation\n\u251c\u2500\u2500 Dockerfile                   # Container configuration\n\u2514\u2500\u2500 README.md                    # Project overview\n</code></pre>"},{"location":"Development/#system-architecture","title":"System Architecture","text":"<p>AIPerf implements a distributed microservices architecture built around a message-passing system using ZeroMQ.</p>"},{"location":"Development/#core-services","title":"Core Services","text":"<ol> <li>System Controller (<code>system_controller</code>)</li> <li>Is bootstrapped by the CLI</li> <li>Orchestrates the entire system</li> <li>Manages service lifecycle and health monitoring</li> <li> <p>Handles service registration and coordination</p> </li> <li> <p>Dataset Manager (<code>dataset</code>)</p> </li> <li>Manages data generation and acquisition</li> <li>Provides synthetic prompt/token generation</li> <li> <p>Handles remote dataset retrieval</p> </li> <li> <p>Worker Manager (<code>worker_manager</code>)</p> </li> <li>Coordinates request distribution to workers</li> <li>Manages timing credits from the timing manager</li> <li> <p>Handles worker process lifecycle</p> </li> <li> <p>Worker (<code>worker</code>)</p> </li> <li>Executes actual inference requests</li> <li>Formats data for specific API interfaces</li> <li> <p>Manages multi-turn conversations</p> </li> <li> <p>Timing Manager (<code>timing_manager</code>)</p> </li> <li>Generates request schedules</li> <li>Issues timing credits for request throttling</li> <li> <p>Controls system load patterns</p> </li> <li> <p>Records Manager (<code>records_manager</code>)</p> </li> <li>Stores and manages test results</li> <li> <p>Provides result persistence and retrieval</p> </li> <li> <p>Post-Processor Manager (<code>post_processor_manager</code>)</p> </li> <li>Processes collected results</li> <li>Generates metrics and performance reports</li> <li>Provides analysis and conclusions</li> </ol>"},{"location":"Development/#service-bootstrapping-flow","title":"Service Bootstrapping Flow","text":"<p>The following diagram shows how services are bootstrapped in the AIPerf system:</p> <pre><code>%%{init: {'theme':'dark'}}%%\nflowchart LR\n    CLI[\"`**CLI**\n    aiperf`\"]\n    CLIConfig[\"`**CLI Options**\n    --run-type\n    --log-level\n    --config`\"]\n    ConfigFile[\"`**Config File**\n    YAML Config`\"]\n\n    SC[\"`**System Controller**\n    Orchestrates System`\"]\n\n    DS[\"`**Dataset Manager**\n    Data Generation`\"]\n    TM[\"`**Timing Manager**\n    Request Scheduling`\"]\n    WM[\"`**Worker Manager**\n    Worker Queue`\"]\n    RM[\"`**Records Manager**\n    Result Storage`\"]\n    PM[\"`**Post-Processor Manager**\n    Metrics &amp; Analysis`\"]\n\n    W1[\"`**Worker 1**`\"]\n    W2[\"`**Worker 2**`\"]\n    WN[\"`**Worker N**`\"]\n\n    CLI --&gt;|Bootstraps| SC\n    CLIConfig --&gt; CLI\n    ConfigFile --&gt; CLI\n\n    SC --&gt;|Configure &amp; Start| DS\n    SC --&gt;|Configure &amp; Start| TM\n    SC --&gt;|Configure &amp; Start| WM\n    SC --&gt;|Configure &amp; Start| RM\n    SC --&gt;|Configure &amp; Start| PM\n\n    WM --&gt;|Spawn &amp; Configure| W1\n    WM --&gt;|Spawn &amp; Configure| W2\n    WM --&gt;|Spawn &amp; Configure| WN\n\n    style CLI fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style ConfigFile fill:#7c2d12,stroke:#f97316,stroke-width:2px,color:#ffffff\n    style CLIConfig fill:#7c2d12,stroke:#f97316,stroke-width:2px,color:#ffffff\n    style SC fill:#166534,stroke:#22c55e,stroke-width:3px,color:#ffffff\n    style DS fill:#4338ca,stroke:#8b5cf6,stroke-width:2px,color:#ffffff\n    style TM fill:#4338ca,stroke:#8b5cf6,stroke-width:2px,color:#ffffff\n    style WM fill:#dc2626,stroke:#f87171,stroke-width:2px,color:#ffffff\n    style RM fill:#4338ca,stroke:#8b5cf6,stroke-width:2px,color:#ffffff\n    style PM fill:#4338ca,stroke:#8b5cf6,stroke-width:2px,color:#ffffff\n    style W1 fill:#be185d,stroke:#f472b6,stroke-width:2px,color:#ffffff\n    style W2 fill:#be185d,stroke:#f472b6,stroke-width:2px,color:#ffffff\n    style WN fill:#be185d,stroke:#f472b6,stroke-width:2px,color:#ffffff\n</code></pre>"},{"location":"Development/#communication-system","title":"Communication System","text":"<p>Services communicate using a strongly-typed message system:</p> <ul> <li>Topics: Categorized message channels (commands, status, data, heartbeat, etc.)</li> <li>Messages: Pydantic models ensuring type safety</li> <li>Service States: Well-defined lifecycle states with transitions</li> </ul> <p>Key communication patterns: - Command/Response: System controller to services - Status Updates: Services to system controller - Heartbeat: Automatic health monitoring - Credit System: Resource allocation and throttling</p>"},{"location":"Development/#service-framework","title":"Service Framework","text":"<p>All services inherit from <code>BaseService</code> which provides:</p>"},{"location":"Development/#automatic-features","title":"Automatic Features","text":"<ul> <li>Lifecycle Management: <code>initialize()</code> \u2192 <code>run()</code> \u2192 <code>configure()</code> \u2192 <code>start()</code> \u2192 <code>stop()</code> \u2192 <code>cleanup()</code></li> <li>State Management: Automatic state transitions with validation</li> <li>Communication: Message publishing/subscribing with type safety</li> <li>Health Monitoring: Automatic heartbeat generation</li> <li>Error Handling: Structured exception handling and recovery</li> </ul>"},{"location":"Development/#implementation-requirements","title":"Implementation Requirements","text":""},{"location":"Development/#implement-base-service-requirements","title":"Implement base service requirements","text":"<pre><code>    def __init__(\n        self, service_config: ServiceConfig, service_id: str | None = None\n    ) -&gt; None:\n        super().__init__(service_config=service_config, service_id=service_id)\n        self.logger.debug(\"Initializing records manager\")\n\n    @property\n    def service_type(self) -&gt; ServiceType:\n        \"\"\"The type of service.\"\"\"\n        return ServiceType.RECORDS_MANAGER\n</code></pre>"},{"location":"Development/#service-factory-registration","title":"Service Factory Registration","text":"<p>Register the service with the ServiceFactory:</p> <pre><code>@ServiceFactory.register(ServiceType.RECORDS_MANAGER)\nclass RecordsManager(BaseComponentService):\n    \"\"\"\n    The RecordsManager service is primarily responsible for holding the\n    results returned from the workers.\n    \"\"\"\n</code></pre>"},{"location":"Development/#service-lifecycle-hooks","title":"Service Lifecycle Hooks","text":"<p>Use decorators to hook into lifecycle events:</p> <pre><code>    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize records manager-specific components.\"\"\"\n        self.logger.debug(\"Initializing records manager\")\n        # TODO: Implement records manager initialization\n\n    @on_start\n    async def _start(self) -&gt; None:\n        \"\"\"Start the records manager.\"\"\"\n        self.logger.debug(\"Starting records manager\")\n        # TODO: Implement records manager start\n\n    @on_stop\n    async def _stop(self) -&gt; None:\n        \"\"\"Stop the records manager.\"\"\"\n        self.logger.debug(\"Stopping records manager\")\n        # TODO: Implement records manager stop\n\n    @on_cleanup\n    async def _cleanup(self) -&gt; None:\n        \"\"\"Clean up records manager-specific components.\"\"\"\n        self.logger.debug(\"Cleaning up records manager\")\n        # TODO: Implement records manager cleanup\n\n    @on_configure\n    async def _configure(self, message: Message) -&gt; None:\n        \"\"\"Configure the records manager.\"\"\"\n        self.logger.debug(f\"Configuring records manager with message: {message}\")\n        # TODO: Implement records manager configuration\n</code></pre>"},{"location":"Development/#running-services","title":"Running Services","text":""},{"location":"Development/#bootstrapping","title":"Bootstrapping","text":"<p>Services are launched using the bootstrap system:</p> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the records manager.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(RecordsManager)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>The bootstrap system: - Sets up uvloop for high-performance async I/O - Loads service configuration - Manages service lifecycle - Handles graceful shutdown</p>"},{"location":"Development/#configuration-management","title":"Configuration Management","text":"<p>Services are configured using Pydantic models:</p> <p>(TBD) Coming SOON</p> <pre><code>from aiperf.common.config import ServiceConfig\nfrom aiperf.common.enums import ServiceRunType\n\nconfig = ServiceConfig(\n    service_run_type=ServiceRunType.MULTIPROCESSING,\n    # Additional configuration options\n)\n</code></pre>"},{"location":"Development/#testing","title":"Testing","text":""},{"location":"Development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests (provide basic coverage, no logging)\nmake test\n\n# Run with verbose output and debug logging, basic coverage\nmake test-verbose\n\n# Run with HTML coverage report, and no logging\nmake coverage\n\n# Run specific test file\npytest aiperf/tests/test_specific_file.py -v\n</code></pre>"},{"location":"Development/#test-structure","title":"Test Structure","text":"<ul> <li>Unit Tests: Test individual components in isolation</li> <li>Integration Tests: Test service interactions</li> <li>Base Test Classes: Shared test utilities and fixtures</li> </ul>"},{"location":"Development/#test-utilities","title":"Test Utilities","text":"<p>The test suite provides base classes for testing services:</p> <ul> <li><code>BaseTestService</code>: Base class for testing services</li> <li><code>BaseTestControllerService</code>: For testing controller services</li> <li><code>BaseTestComponentService</code>: For testing component-specific functionality</li> </ul>"},{"location":"Development/#code-quality","title":"Code Quality","text":""},{"location":"Development/#linting-and-formatting","title":"Linting and Formatting","text":"<p>The project uses <code>ruff</code> for both linting and formatting:</p> <pre><code># Check code style\nruff check .\n\n# Auto-fix issues\nruff check . --fix\n\n# Format code\nruff format .\n\n# Check formatting\nruff format . --check\n</code></pre>"},{"location":"Development/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks are automatically installed and run: - Code formatting with ruff - Import sorting - Basic syntax checking - Test execution on changed files</p>"},{"location":"Development/#configuration","title":"Configuration","text":"<p>Quality tools are configured in <code>pyproject.toml</code>: - Ruff: Line length 88, comprehensive rule set - Pytest: Async mode, coverage reporting - Coverage: HTML reports, skip covered lines</p>"},{"location":"Development/#development-best-practices","title":"Development Best Practices","text":""},{"location":"Development/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use Pydantic models for data structures</li> <li>Prefer enums for string choices (using our custom <code>CaseInsensitiveStrEnum</code>)</li> <li>Implement DRY principles</li> <li>Use type hints throughout</li> </ul>"},{"location":"Development/#testing_1","title":"Testing","text":"<ul> <li>Write tests for all new functionality</li> <li>Use pytest fixtures for shared setup</li> <li>Parameterize tests when appropriate</li> <li>Aim for high test coverage</li> <li>Test both success and failure cases</li> </ul>"},{"location":"Development/#service-development","title":"Service Development","text":"<ul> <li>Always inherit from appropriate base service class</li> <li>Use lifecycle hooks for setup/teardown</li> <li>Implement proper error handling</li> <li>Follow message-passing patterns</li> <li>Document service responsibilities clearly</li> </ul>"},{"location":"Development/#configuration_1","title":"Configuration","text":"<ul> <li>Use Pydantic models for configuration</li> <li>Validate configuration at startup</li> <li>Provide sensible defaults</li> <li>Support environment variable overrides</li> </ul>"},{"location":"Development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Development/#common-issues","title":"Common Issues","text":"<ol> <li>Services not starting: Check logs for initialization errors</li> <li>Communication failures: Verify ZeroMQ ports are available</li> <li>Test failures: Ensure virtual environment is activated</li> <li>Import errors: Run <code>pip install -e \".[dev]\"</code> to update dependencies</li> </ol>"},{"location":"Development/#debugging","title":"Debugging","text":"<ul> <li>Use <code>--log-level DEBUG</code> for detailed logging</li> <li>Check service states in system controller logs</li> <li>Monitor heartbeat messages for service health</li> <li>Use <code>make test-verbose</code> for detailed test output</li> </ul>"},{"location":"Development/#performance","title":"Performance","text":"<ul> <li>Services use uvloop for high-performance async I/O</li> <li>ZeroMQ provides efficient inter-service communication</li> <li>Multiprocessing mode distributes load across CPU cores</li> <li>Kubernetes mode enables horizontal scaling</li> </ul>"},{"location":"Development/#contributing","title":"Contributing","text":"<ol> <li>Follow the development setup instructions</li> <li>Create feature branches from main</li> <li>Write tests for new functionality</li> <li>Ensure all quality checks pass</li> <li>Submit pull requests with clear descriptions following proper naming</li> </ol> <p>For questions or issues, refer to the project documentation or open an issue in the repository.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page contains the API documentation for all Python modules in the codebase (excluding init.py files).</p>"},{"location":"api/#aiperfcli","title":"aiperf.cli","text":"<p>Main CLI entry point for the AIPerf system.</p>"},{"location":"api/#aiperf.cli.analyze","title":"<code>analyze(user_config, service_config=None)</code>","text":"<p>Sweep through one or more parameters.</p> Source code in <code>aiperf/cli.py</code> <pre><code>@app.command(name=\"analyze\")\ndef analyze(\n    user_config: UserConfig,\n    service_config: ServiceConfig | None = None,\n) -&gt; None:\n    \"\"\"Sweep through one or more parameters.\"\"\"\n    # TODO: Implement this\n    from aiperf.cli_runner import warn_command_not_implemented\n\n    warn_command_not_implemented(\"analyze\")\n</code></pre>"},{"location":"api/#aiperf.cli.create_template","title":"<code>create_template(template_filename=CLIDefaults.TEMPLATE_FILENAME)</code>","text":"<p>Create a template configuration file.</p> Source code in <code>aiperf/cli.py</code> <pre><code>@app.command(name=\"create-template\")\ndef create_template(\n    template_filename: Annotated[\n        str,\n        Field(\n            description=f\"Path to the template file. Defaults to {CLIDefaults.TEMPLATE_FILENAME}.\"\n        ),\n        Parameter(\n            name=(\"--template-filename\", \"-t\"),\n        ),\n    ] = CLIDefaults.TEMPLATE_FILENAME,\n) -&gt; None:\n    \"\"\"Create a template configuration file.\"\"\"\n    # TODO: Implement this\n    from aiperf.cli_runner import warn_command_not_implemented\n\n    warn_command_not_implemented(\"create-template\")\n</code></pre>"},{"location":"api/#aiperf.cli.profile","title":"<code>profile(user_config, service_config=None)</code>","text":"<p>Run the Profile subcommand.</p> <p>Parameters:</p> Name Type Description Default <code>user_config</code> <code>UserConfig</code> <p>User configuration for the benchmark</p> required <code>service_config</code> <code>ServiceConfig | None</code> <p>Service configuration options</p> <code>None</code> Source code in <code>aiperf/cli.py</code> <pre><code>@app.command(name=\"profile\")\ndef profile(\n    user_config: UserConfig,\n    service_config: ServiceConfig | None = None,\n) -&gt; None:\n    \"\"\"Run the Profile subcommand.\n\n    Args:\n        user_config: User configuration for the benchmark\n        service_config: Service configuration options\n    \"\"\"\n    from aiperf.cli_runner import run_system_controller\n    from aiperf.common.config import load_service_config\n\n    service_config = service_config or load_service_config()\n\n    run_system_controller(user_config, service_config)\n</code></pre>"},{"location":"api/#aiperf.cli.validate_config","title":"<code>validate_config(user_config=None, service_config=None)</code>","text":"<p>Validate the configuration file.</p> Source code in <code>aiperf/cli.py</code> <pre><code>@app.command(name=\"validate-config\")\ndef validate_config(\n    user_config: UserConfig | None = None,\n    service_config: ServiceConfig | None = None,\n) -&gt; None:\n    \"\"\"Validate the configuration file.\"\"\"\n    # TODO: Implement this\n    from aiperf.cli_runner import warn_command_not_implemented\n\n    warn_command_not_implemented(\"validate-config\")\n</code></pre>"},{"location":"api/#aiperfcli_runner","title":"aiperf.cli_runner","text":""},{"location":"api/#aiperf.cli_runner.run_system_controller","title":"<code>run_system_controller(user_config, service_config)</code>","text":"<p>Run the system controller with the given configuration.</p> Source code in <code>aiperf/cli_runner.py</code> <pre><code>def run_system_controller(\n    user_config: UserConfig,\n    service_config: ServiceConfig,\n) -&gt; None:\n    \"\"\"Run the system controller with the given configuration.\"\"\"\n\n    from aiperf.common.aiperf_logger import AIPerfLogger\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n    from aiperf.services import SystemController\n\n    logger = AIPerfLogger(__name__)\n\n    log_queue = None\n    if service_config.disable_ui:\n        from aiperf.common.logging import setup_rich_logging\n\n        setup_rich_logging(user_config, service_config)\n\n    # Create and start the system controller\n    logger.info(\"Starting AIPerf System\")\n\n    try:\n        bootstrap_and_run_service(\n            SystemController,\n            service_id=\"system_controller\",\n            service_config=service_config,\n            user_config=user_config,\n            log_queue=log_queue,\n        )\n    except Exception:\n        logger.exception(\"Error running AIPerf System\")\n        raise\n    finally:\n        logger.debug(\"AIPerf System exited\")\n</code></pre>"},{"location":"api/#aiperf.cli_runner.warn_command_not_implemented","title":"<code>warn_command_not_implemented(command)</code>","text":"<p>Warn the user that the subcommand is not implemented.</p> Source code in <code>aiperf/cli_runner.py</code> <pre><code>def warn_command_not_implemented(command: str) -&gt; None:\n    \"\"\"Warn the user that the subcommand is not implemented.\"\"\"\n    import sys\n\n    from rich.console import Console\n    from rich.panel import Panel\n\n    console = Console()\n    console.print(\n        Panel(\n            f\"Command [bold red]{command}[/bold red] is not yet implemented\",\n            title=\"Error\",\n            title_align=\"left\",\n            border_style=\"red\",\n        )\n    )\n\n    sys.exit(1)\n</code></pre>"},{"location":"api/#aiperfclientshttpaiohttp_client","title":"aiperf.clients.http.aiohttp_client","text":""},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin","title":"<code>AioHttpClientMixin</code>","text":"<p>A high-performance HTTP client for communicating with HTTP based REST APIs using aiohttp.</p> <p>This class is optimized for maximum performance and accurate timing measurements, making it ideal for benchmarking scenarios.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>class AioHttpClientMixin:\n    \"\"\"A high-performance HTTP client for communicating with HTTP based REST APIs using aiohttp.\n\n    This class is optimized for maximum performance and accurate timing measurements,\n    making it ideal for benchmarking scenarios.\n    \"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfo) -&gt; None:\n        super().__init__()\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.model_endpoint = model_endpoint\n        self.tcp_connector = create_tcp_connector()\n\n        # For now, just set all timeouts to the same value.\n        # TODO: Add support for different timeouts for different parts of the request.\n        self.timeout = aiohttp.ClientTimeout(\n            total=self.model_endpoint.endpoint.timeout,\n            connect=self.model_endpoint.endpoint.timeout,\n            sock_connect=self.model_endpoint.endpoint.timeout,\n            sock_read=self.model_endpoint.endpoint.timeout,\n            ceil_threshold=self.model_endpoint.endpoint.timeout,\n        )\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the client.\"\"\"\n        if self.tcp_connector:\n            await self.tcp_connector.close()\n            self.tcp_connector = None\n\n    async def post_request(\n        self,\n        url: str,\n        payload: str,\n        headers: dict[str, str],\n        **kwargs: Any,\n    ) -&gt; RequestRecord:\n        \"\"\"Send a streaming or non-streaming POST request to the specified URL with the given payload and headers.\n\n        If the response is an SSE stream, the response will be parsed into a list of SSE messages.\n        Otherwise, the response will be parsed into a TextResponse object.\n        \"\"\"\n\n        self.logger.debug(\"Sending POST request to %s\", url)\n\n        record: RequestRecord = RequestRecord(\n            start_perf_ns=time.perf_counter_ns(),\n        )\n\n        try:\n            # Make raw HTTP request with precise timing using aiohttp\n            async with aiohttp.ClientSession(\n                connector=self.tcp_connector,\n                timeout=self.timeout,\n                headers=headers,\n                skip_auto_headers=[\n                    *list(headers.keys()),\n                    \"User-Agent\",\n                    \"Accept-Encoding\",\n                ],\n                connector_owner=False,\n            ) as session:\n                record.start_perf_ns = time.perf_counter_ns()\n                async with session.post(\n                    url, data=payload, headers=headers, **kwargs\n                ) as response:\n                    record.status = response.status\n                    # Check for HTTP errors\n                    if response.status != 200:\n                        error_text = await response.text()\n                        record.error = ErrorDetails(\n                            code=response.status,\n                            type=response.reason,\n                            message=error_text,\n                        )\n                        return record\n\n                    record.recv_start_perf_ns = time.perf_counter_ns()\n\n                    if response.content_type == \"text/event-stream\":\n                        # Parse SSE stream with optimal performance\n                        messages = await AioHttpSSEStreamReader(\n                            response\n                        ).read_complete_stream()\n                        record.responses.extend(messages)\n                    else:\n                        raw_response = await response.text()\n                        record.end_perf_ns = time.perf_counter_ns()\n                        record.responses.append(\n                            TextResponse(\n                                perf_ns=record.end_perf_ns,\n                                content_type=response.content_type,\n                                text=raw_response,\n                            )\n                        )\n                    record.end_perf_ns = time.perf_counter_ns()\n\n        except Exception as e:\n            record.end_perf_ns = time.perf_counter_ns()\n            self.logger.error(\"Error in aiohttp request: %s\", str(e))\n            record.error = ErrorDetails(type=e.__class__.__name__, message=str(e))\n\n        return record\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the client.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the client.\"\"\"\n    if self.tcp_connector:\n        await self.tcp_connector.close()\n        self.tcp_connector = None\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin.post_request","title":"<code>post_request(url, payload, headers, **kwargs)</code>  <code>async</code>","text":"<p>Send a streaming or non-streaming POST request to the specified URL with the given payload and headers.</p> <p>If the response is an SSE stream, the response will be parsed into a list of SSE messages. Otherwise, the response will be parsed into a TextResponse object.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def post_request(\n    self,\n    url: str,\n    payload: str,\n    headers: dict[str, str],\n    **kwargs: Any,\n) -&gt; RequestRecord:\n    \"\"\"Send a streaming or non-streaming POST request to the specified URL with the given payload and headers.\n\n    If the response is an SSE stream, the response will be parsed into a list of SSE messages.\n    Otherwise, the response will be parsed into a TextResponse object.\n    \"\"\"\n\n    self.logger.debug(\"Sending POST request to %s\", url)\n\n    record: RequestRecord = RequestRecord(\n        start_perf_ns=time.perf_counter_ns(),\n    )\n\n    try:\n        # Make raw HTTP request with precise timing using aiohttp\n        async with aiohttp.ClientSession(\n            connector=self.tcp_connector,\n            timeout=self.timeout,\n            headers=headers,\n            skip_auto_headers=[\n                *list(headers.keys()),\n                \"User-Agent\",\n                \"Accept-Encoding\",\n            ],\n            connector_owner=False,\n        ) as session:\n            record.start_perf_ns = time.perf_counter_ns()\n            async with session.post(\n                url, data=payload, headers=headers, **kwargs\n            ) as response:\n                record.status = response.status\n                # Check for HTTP errors\n                if response.status != 200:\n                    error_text = await response.text()\n                    record.error = ErrorDetails(\n                        code=response.status,\n                        type=response.reason,\n                        message=error_text,\n                    )\n                    return record\n\n                record.recv_start_perf_ns = time.perf_counter_ns()\n\n                if response.content_type == \"text/event-stream\":\n                    # Parse SSE stream with optimal performance\n                    messages = await AioHttpSSEStreamReader(\n                        response\n                    ).read_complete_stream()\n                    record.responses.extend(messages)\n                else:\n                    raw_response = await response.text()\n                    record.end_perf_ns = time.perf_counter_ns()\n                    record.responses.append(\n                        TextResponse(\n                            perf_ns=record.end_perf_ns,\n                            content_type=response.content_type,\n                            text=raw_response,\n                        )\n                    )\n                record.end_perf_ns = time.perf_counter_ns()\n\n    except Exception as e:\n        record.end_perf_ns = time.perf_counter_ns()\n        self.logger.error(\"Error in aiohttp request: %s\", str(e))\n        record.error = ErrorDetails(type=e.__class__.__name__, message=str(e))\n\n    return record\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpSSEStreamReader","title":"<code>AioHttpSSEStreamReader</code>","text":"<p>A helper class for reading an SSE stream from an aiohttp.ClientResponse object.</p> <p>This class is optimized for maximum performance and accurate timing measurements, making it ideal for benchmarking scenarios.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>class AioHttpSSEStreamReader:\n    \"\"\"A helper class for reading an SSE stream from an aiohttp.ClientResponse object.\n\n    This class is optimized for maximum performance and accurate timing measurements,\n    making it ideal for benchmarking scenarios.\n    \"\"\"\n\n    def __init__(self, response: aiohttp.ClientResponse):\n        self.response = response\n\n    async def read_complete_stream(self) -&gt; list[SSEMessage]:\n        \"\"\"Read the complete SSE stream in a performant manner and return a list of\n        SSE messages that contain the most accurate timestamp data possible.\n\n        Returns:\n            A list of SSE messages.\n        \"\"\"\n        messages: list[SSEMessage] = []\n\n        async for raw_message, first_byte_ns in self.__aiter__():\n            # Parse the raw SSE message into a SSEMessage object\n            message = parse_sse_message(raw_message, first_byte_ns)\n            messages.append(message)\n\n        return messages\n\n    async def __aiter__(self) -&gt; typing.AsyncIterator[tuple[str, int]]:\n        \"\"\"Iterate over the SSE stream in a performant manner and return a tuple of the\n        raw SSE message, the perf_counter_ns of the first byte, and the perf_counter_ns of the last byte.\n        This provides the most accurate timing information possible without any delays due to the nature of\n        the aiohttp library. The first byte is read immediately to capture the timestamp of the first byte,\n        and the last byte is read after the rest of the chunk is read to capture the timestamp of the last byte.\n\n        Returns:\n            An async iterator of tuples of the raw SSE message, and the perf_counter_ns of the first byte\n        \"\"\"\n\n        while not self.response.content.at_eof():\n            # Read the first byte of the SSE stream\n            first_byte = await self.response.content.read(1)\n            chunk_ns_first_byte = time.perf_counter_ns()\n            if not first_byte:\n                break\n\n            chunk = await self.response.content.readuntil(b\"\\n\\n\")\n\n            if not chunk:\n                break\n            chunk = first_byte + chunk\n\n            try:\n                # Use the fastest available decoder\n                yield (\n                    chunk.decode(\"utf-8\").strip(),\n                    chunk_ns_first_byte,\n                )\n            except UnicodeDecodeError:\n                # Handle potential encoding issues gracefully\n                yield (\n                    chunk.decode(\"utf-8\", errors=\"replace\").strip(),\n                    chunk_ns_first_byte,\n                )\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpSSEStreamReader.__aiter__","title":"<code>__aiter__()</code>  <code>async</code>","text":"<p>Iterate over the SSE stream in a performant manner and return a tuple of the raw SSE message, the perf_counter_ns of the first byte, and the perf_counter_ns of the last byte. This provides the most accurate timing information possible without any delays due to the nature of the aiohttp library. The first byte is read immediately to capture the timestamp of the first byte, and the last byte is read after the rest of the chunk is read to capture the timestamp of the last byte.</p> <p>Returns:</p> Type Description <code>AsyncIterator[tuple[str, int]]</code> <p>An async iterator of tuples of the raw SSE message, and the perf_counter_ns of the first byte</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def __aiter__(self) -&gt; typing.AsyncIterator[tuple[str, int]]:\n    \"\"\"Iterate over the SSE stream in a performant manner and return a tuple of the\n    raw SSE message, the perf_counter_ns of the first byte, and the perf_counter_ns of the last byte.\n    This provides the most accurate timing information possible without any delays due to the nature of\n    the aiohttp library. The first byte is read immediately to capture the timestamp of the first byte,\n    and the last byte is read after the rest of the chunk is read to capture the timestamp of the last byte.\n\n    Returns:\n        An async iterator of tuples of the raw SSE message, and the perf_counter_ns of the first byte\n    \"\"\"\n\n    while not self.response.content.at_eof():\n        # Read the first byte of the SSE stream\n        first_byte = await self.response.content.read(1)\n        chunk_ns_first_byte = time.perf_counter_ns()\n        if not first_byte:\n            break\n\n        chunk = await self.response.content.readuntil(b\"\\n\\n\")\n\n        if not chunk:\n            break\n        chunk = first_byte + chunk\n\n        try:\n            # Use the fastest available decoder\n            yield (\n                chunk.decode(\"utf-8\").strip(),\n                chunk_ns_first_byte,\n            )\n        except UnicodeDecodeError:\n            # Handle potential encoding issues gracefully\n            yield (\n                chunk.decode(\"utf-8\", errors=\"replace\").strip(),\n                chunk_ns_first_byte,\n            )\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpSSEStreamReader.read_complete_stream","title":"<code>read_complete_stream()</code>  <code>async</code>","text":"<p>Read the complete SSE stream in a performant manner and return a list of SSE messages that contain the most accurate timestamp data possible.</p> <p>Returns:</p> Type Description <code>list[SSEMessage]</code> <p>A list of SSE messages.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def read_complete_stream(self) -&gt; list[SSEMessage]:\n    \"\"\"Read the complete SSE stream in a performant manner and return a list of\n    SSE messages that contain the most accurate timestamp data possible.\n\n    Returns:\n        A list of SSE messages.\n    \"\"\"\n    messages: list[SSEMessage] = []\n\n    async for raw_message, first_byte_ns in self.__aiter__():\n        # Parse the raw SSE message into a SSEMessage object\n        message = parse_sse_message(raw_message, first_byte_ns)\n        messages.append(message)\n\n    return messages\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.create_tcp_connector","title":"<code>create_tcp_connector(**kwargs)</code>","text":"<p>Create a new connector with the given configuration.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>def create_tcp_connector(**kwargs) -&gt; aiohttp.TCPConnector:\n    \"\"\"Create a new connector with the given configuration.\"\"\"\n\n    def socket_factory(addr_info):\n        \"\"\"Custom socket factory optimized for SSE streaming performance.\"\"\"\n        family, sock_type, proto, _, _ = addr_info\n        sock = socket.socket(family=family, type=sock_type, proto=proto)\n        SocketDefaults.apply_to_socket(sock)\n        return sock\n\n    default_kwargs: dict[str, Any] = {\n        \"limit\": AioHttpDefaults.LIMIT,\n        \"limit_per_host\": AioHttpDefaults.LIMIT_PER_HOST,\n        \"ttl_dns_cache\": AioHttpDefaults.TTL_DNS_CACHE,\n        \"use_dns_cache\": AioHttpDefaults.USE_DNS_CACHE,\n        \"enable_cleanup_closed\": AioHttpDefaults.ENABLE_CLEANUP_CLOSED,\n        \"force_close\": AioHttpDefaults.FORCE_CLOSE,\n        \"keepalive_timeout\": AioHttpDefaults.KEEPALIVE_TIMEOUT,\n        \"happy_eyeballs_delay\": AioHttpDefaults.HAPPY_EYEBALLS_DELAY,\n        \"family\": AioHttpDefaults.SOCKET_FAMILY,\n        \"socket_factory\": socket_factory,\n    }\n\n    default_kwargs.update(kwargs)\n\n    return aiohttp.TCPConnector(\n        **default_kwargs,\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.parse_sse_message","title":"<code>parse_sse_message(raw_message, perf_ns)</code>","text":"<p>Parse a raw SSE message into an SSEMessage object.</p> <p>Parsing logic based on official HTML SSE Living Standard: https://html.spec.whatwg.org/multipage/server-sent-events.html#parsing-an-event-stream</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>def parse_sse_message(raw_message: str, perf_ns: int) -&gt; SSEMessage:\n    \"\"\"Parse a raw SSE message into an SSEMessage object.\n\n    Parsing logic based on official HTML SSE Living Standard:\n    https://html.spec.whatwg.org/multipage/server-sent-events.html#parsing-an-event-stream\n    \"\"\"\n\n    message = SSEMessage(perf_ns=perf_ns)\n    for line in raw_message.split(\"\\n\"):\n        if not (line := line.strip()):\n            continue\n\n        parts = line.split(\":\", 1)\n        if len(parts) &lt; 2:\n            # Fields without a colon have no value, so the whole line is the field name\n            message.packets.append(SSEField(name=parts[0].strip(), value=None))\n            continue\n\n        field_name, value = parts\n\n        if field_name == \"\":\n            # Field name is empty, so this is a comment\n            field_name = SSEFieldType.COMMENT\n\n        message.packets.append(SSEField(name=field_name.strip(), value=value.strip()))\n\n    return message\n</code></pre>"},{"location":"api/#aiperfclientshttpdefaults","title":"aiperf.clients.http.defaults","text":""},{"location":"api/#aiperf.clients.http.defaults.AioHttpDefaults","title":"<code>AioHttpDefaults</code>  <code>dataclass</code>","text":"<p>Default values for aiohttp.ClientSession.</p> Source code in <code>aiperf/clients/http/defaults.py</code> <pre><code>@dataclass(frozen=True)\nclass AioHttpDefaults:\n    \"\"\"Default values for aiohttp.ClientSession.\"\"\"\n\n    LIMIT = 2500  # Maximum number of concurrent connections\n    LIMIT_PER_HOST = 2500  # Maximum number of concurrent connections per host\n    TTL_DNS_CACHE = 300  # Time to live for DNS cache\n    USE_DNS_CACHE = True  # Enable DNS cache\n    ENABLE_CLEANUP_CLOSED = False  # Disable cleanup of closed connections\n    FORCE_CLOSE = False  # Disable force close connections\n    KEEPALIVE_TIMEOUT = 300  # Keepalive timeout\n    HAPPY_EYEBALLS_DELAY = None  # Happy eyeballs delay (None = disabled)\n    SOCKET_FAMILY = socket.AF_INET  # Family of the socket (IPv4)\n</code></pre>"},{"location":"api/#aiperf.clients.http.defaults.SocketDefaults","title":"<code>SocketDefaults</code>  <code>dataclass</code>","text":"<p>Default values for socket options.</p> Source code in <code>aiperf/clients/http/defaults.py</code> <pre><code>@dataclass(frozen=True)\nclass SocketDefaults:\n    \"\"\"\n    Default values for socket options.\n    \"\"\"\n\n    TCP_NODELAY = 1  # Disable Nagle's algorithm\n    TCP_QUICKACK = 1  # Quick ACK mode\n\n    SO_KEEPALIVE = 1  # Enable keepalive\n    TCP_KEEPIDLE = 60  # Start keepalive after 1 min idle\n    TCP_KEEPINTVL = 30  # Keepalive interval: 30 seconds\n    TCP_KEEPCNT = 1  # 1 failed keepalive probes = dead\n\n    SO_LINGER = 0  # Disable linger\n    SO_REUSEADDR = 1  # Enable reuse address\n    SO_REUSEPORT = 1  # Enable reuse port\n\n    SO_RCVBUF = 1024 * 1024 * 10  # 10MB receive buffer\n    SO_SNDBUF = 1024 * 1024 * 10  # 10MB send buffer\n\n    SO_RCVTIMEO = 30  # 30 second receive timeout\n    SO_SNDTIMEO = 30  # 30 second send timeout\n    TCP_USER_TIMEOUT = 30000  # 30 sec user timeout\n\n    @classmethod\n    def apply_to_socket(cls, sock: socket.socket) -&gt; None:\n        \"\"\"Apply the default socket options to the given socket.\"\"\"\n\n        # Low-latency optimizations for streaming\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, cls.TCP_NODELAY)\n\n        # Connection keepalive settings for long-lived SSE connections\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, cls.SO_KEEPALIVE)\n\n        # Fine-tune keepalive timing (Linux-specific)\n        if hasattr(socket, \"TCP_KEEPIDLE\"):\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPIDLE, cls.TCP_KEEPIDLE)\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPINTVL, cls.TCP_KEEPINTVL)\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPCNT, cls.TCP_KEEPCNT)\n\n        # Buffer size optimizations for streaming\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, cls.SO_RCVBUF)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, cls.SO_SNDBUF)\n\n        # Linux-specific TCP optimizations\n        if hasattr(socket, \"TCP_QUICKACK\"):\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_QUICKACK, cls.TCP_QUICKACK)\n\n        if hasattr(socket, \"TCP_USER_TIMEOUT\"):\n            sock.setsockopt(\n                socket.SOL_TCP, socket.TCP_USER_TIMEOUT, cls.TCP_USER_TIMEOUT\n            )\n</code></pre>"},{"location":"api/#aiperf.clients.http.defaults.SocketDefaults.apply_to_socket","title":"<code>apply_to_socket(sock)</code>  <code>classmethod</code>","text":"<p>Apply the default socket options to the given socket.</p> Source code in <code>aiperf/clients/http/defaults.py</code> <pre><code>@classmethod\ndef apply_to_socket(cls, sock: socket.socket) -&gt; None:\n    \"\"\"Apply the default socket options to the given socket.\"\"\"\n\n    # Low-latency optimizations for streaming\n    sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, cls.TCP_NODELAY)\n\n    # Connection keepalive settings for long-lived SSE connections\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, cls.SO_KEEPALIVE)\n\n    # Fine-tune keepalive timing (Linux-specific)\n    if hasattr(socket, \"TCP_KEEPIDLE\"):\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPIDLE, cls.TCP_KEEPIDLE)\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPINTVL, cls.TCP_KEEPINTVL)\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPCNT, cls.TCP_KEEPCNT)\n\n    # Buffer size optimizations for streaming\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, cls.SO_RCVBUF)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, cls.SO_SNDBUF)\n\n    # Linux-specific TCP optimizations\n    if hasattr(socket, \"TCP_QUICKACK\"):\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_QUICKACK, cls.TCP_QUICKACK)\n\n    if hasattr(socket, \"TCP_USER_TIMEOUT\"):\n        sock.setsockopt(\n            socket.SOL_TCP, socket.TCP_USER_TIMEOUT, cls.TCP_USER_TIMEOUT\n        )\n</code></pre>"},{"location":"api/#aiperfclientsmodel_endpoint_info","title":"aiperf.clients.model_endpoint_info","text":"<p>Model endpoint information.</p> <p>This module contains the pydantic models that encapsulate the information needed to send requests to an inference server, primarily around the model, endpoint, and additional request payload information.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.EndpointInfo","title":"<code>EndpointInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about an endpoint.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class EndpointInfo(AIPerfBaseModel):\n    \"\"\"Information about an endpoint.\"\"\"\n\n    type: EndpointType = Field(\n        default=EndpointType.OPENAI_CHAT_COMPLETIONS,\n        description=\"The type of request payload to use for the endpoint.\",\n    )\n    base_url: str | None = Field(\n        default=None,\n        description=\"URL of the endpoint.\",\n    )\n    custom_endpoint: str | None = Field(\n        default=None,\n        description=\"Custom endpoint to use for the models.\",\n    )\n    url_params: dict[str, Any] | None = Field(\n        default=None, description=\"Custom URL parameters to use for the endpoint.\"\n    )\n    streaming: bool = Field(\n        default=False,\n        description=\"Whether the endpoint supports streaming.\",\n    )\n    headers: dict[str, str] | None = Field(\n        default=None,\n        description=\"Custom URL headers to use for the endpoint.\",\n    )\n    api_key: str | None = Field(\n        default=None,\n        description=\"API key to use for the endpoint.\",\n    )\n    ssl_options: dict[str, Any] | None = Field(\n        default=None,\n        description=\"SSL options to use for the endpoint.\",\n    )\n    timeout: float = Field(\n        default=EndPointDefaults.TIMEOUT,\n        description=\"The timeout in seconds for each request to the endpoint.\",\n    )\n    extra: dict[str, Any] | None = Field(\n        default=None,\n        description=\"Additional inputs to include with every request. \"\n        \"You can repeat this flag for multiple inputs. Inputs should be in an 'input_name:value' format. \"\n        \"Alternatively, a string representing a json formatted dict can be provided.\",\n    )\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"EndpointInfo\":\n        \"\"\"Create an HttpEndpointInfo from a UserConfig.\"\"\"\n        return cls(\n            type=EndpointType(user_config.endpoint.type),\n            custom_endpoint=user_config.endpoint.custom_endpoint,\n            streaming=user_config.endpoint.streaming,\n            base_url=user_config.endpoint.url,\n            headers=user_config.input.headers,\n            extra=user_config.input.extra,\n            timeout=user_config.endpoint.timeout_seconds,\n            api_key=user_config.endpoint.api_key,\n        )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.EndpointInfo.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create an HttpEndpointInfo from a UserConfig.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"EndpointInfo\":\n    \"\"\"Create an HttpEndpointInfo from a UserConfig.\"\"\"\n    return cls(\n        type=EndpointType(user_config.endpoint.type),\n        custom_endpoint=user_config.endpoint.custom_endpoint,\n        streaming=user_config.endpoint.streaming,\n        base_url=user_config.endpoint.url,\n        headers=user_config.input.headers,\n        extra=user_config.input.extra,\n        timeout=user_config.endpoint.timeout_seconds,\n        api_key=user_config.endpoint.api_key,\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo","title":"<code>ModelEndpointInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a model endpoint.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class ModelEndpointInfo(AIPerfBaseModel):\n    \"\"\"Information about a model endpoint.\"\"\"\n\n    models: ModelListInfo = Field(\n        ...,\n        description=\"The models to use for the endpoint.\",\n    )\n    endpoint: EndpointInfo = Field(\n        ...,\n        description=\"The endpoint to use for the models.\",\n    )\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"ModelEndpointInfo\":\n        \"\"\"Create a ModelEndpointInfo from a UserConfig.\"\"\"\n        return cls(\n            models=ModelListInfo.from_user_config(user_config),\n            endpoint=EndpointInfo.from_user_config(user_config),\n        )\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"Get the full URL for the endpoint.\"\"\"\n        url = self.endpoint.base_url.rstrip(\"/\") if self.endpoint.base_url else \"\"\n        if self.endpoint.custom_endpoint:\n            url += \"/\" + self.endpoint.custom_endpoint.lstrip(\"/\")\n        elif path := self.endpoint.type.endpoint_path():\n            url += \"/\" + path.lstrip(\"/\")\n        return url\n\n    @property\n    def primary_model(self) -&gt; ModelInfo:\n        \"\"\"Get the primary model.\"\"\"\n        return self.models.models[0]\n\n    @property\n    def primary_model_name(self) -&gt; str:\n        \"\"\"Get the primary model name.\"\"\"\n        return self.primary_model.name\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.primary_model","title":"<code>primary_model</code>  <code>property</code>","text":"<p>Get the primary model.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.primary_model_name","title":"<code>primary_model_name</code>  <code>property</code>","text":"<p>Get the primary model name.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.url","title":"<code>url</code>  <code>property</code>","text":"<p>Get the full URL for the endpoint.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create a ModelEndpointInfo from a UserConfig.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"ModelEndpointInfo\":\n    \"\"\"Create a ModelEndpointInfo from a UserConfig.\"\"\"\n    return cls(\n        models=ModelListInfo.from_user_config(user_config),\n        endpoint=EndpointInfo.from_user_config(user_config),\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelInfo","title":"<code>ModelInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a model.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class ModelInfo(AIPerfBaseModel):\n    \"\"\"Information about a model.\"\"\"\n\n    name: str = Field(\n        ...,\n        min_length=1,\n        description=\"The name of the model. This is used to identify the model.\",\n    )\n    version: str | None = Field(\n        default=None,\n        description=\"The version of the model.\",\n    )\n    modality: Modality = Field(\n        default=Modality.TEXT,\n        description=\"The modality of the model. This is used to determine the type of request payload \"\n        \"to use for the endpoint. If CUSTOM, the model is not supported.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelListInfo","title":"<code>ModelListInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a list of models.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class ModelListInfo(AIPerfBaseModel):\n    \"\"\"Information about a list of models.\"\"\"\n\n    models: list[ModelInfo] = Field(\n        ...,\n        min_length=1,\n        description=\"The models to use for the endpoint.\",\n    )\n    model_selection_strategy: ModelSelectionStrategy = Field(\n        ...,\n        description=\"The strategy to use for selecting the model to use for the endpoint.\",\n    )\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"ModelListInfo\":\n        \"\"\"Create a ModelListInfo from a UserConfig.\"\"\"\n        return cls(\n            models=[ModelInfo(name=model) for model in user_config.model_names],\n            model_selection_strategy=user_config.endpoint.model_selection_strategy,\n        )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelListInfo.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create a ModelListInfo from a UserConfig.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"ModelListInfo\":\n    \"\"\"Create a ModelListInfo from a UserConfig.\"\"\"\n    return cls(\n        models=[ModelInfo(name=model) for model in user_config.model_names],\n        model_selection_strategy=user_config.endpoint.model_selection_strategy,\n    )\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_aiohttp","title":"aiperf.clients.openai.openai_aiohttp","text":""},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp","title":"<code>OpenAIClientAioHttp</code>","text":"<p>               Bases: <code>AioHttpClientMixin</code>, <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> <p>Inference client for OpenAI based requests using aiohttp.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>@InferenceClientFactory.register_all(\n    EndpointType.OPENAI_CHAT_COMPLETIONS,\n    EndpointType.OPENAI_COMPLETIONS,\n    EndpointType.OPENAI_EMBEDDINGS,\n    EndpointType.OPENAI_RESPONSES,\n)\nclass OpenAIClientAioHttp(AioHttpClientMixin, AIPerfLoggerMixin, ABC):\n    \"\"\"Inference client for OpenAI based requests using aiohttp.\"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfo, **kwargs) -&gt; None:\n        super().__init__(model_endpoint, **kwargs)\n        self.model_endpoint = model_endpoint\n\n    def get_headers(self, model_endpoint: ModelEndpointInfo) -&gt; dict[str, str]:\n        \"\"\"Get the headers for the given endpoint.\"\"\"\n\n        accept = (\n            \"text/event-stream\"\n            if model_endpoint.endpoint.streaming\n            else \"application/json\"\n        )\n\n        headers = {\n            \"User-Agent\": \"aiperf/1.0\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": accept,\n        }\n        if model_endpoint.endpoint.api_key:\n            headers[\"Authorization\"] = f\"Bearer {model_endpoint.endpoint.api_key}\"\n        if model_endpoint.endpoint.headers:\n            headers.update(model_endpoint.endpoint.headers)\n        return headers\n\n    def get_url(self, model_endpoint: ModelEndpointInfo) -&gt; str:\n        \"\"\"Get the URL for the given endpoint.\"\"\"\n        url = model_endpoint.url\n        if not url.startswith(\"http\"):\n            url = f\"http://{url}\"\n        return url\n\n    async def send_request(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        payload: dict[str, Any],\n    ) -&gt; RequestRecord:\n        \"\"\"Send OpenAI request using aiohttp.\"\"\"\n\n        # capture start time before request is sent in the case of an error\n        start_perf_ns = time.perf_counter_ns()\n        try:\n            self.debug(\n                lambda: f\"Sending OpenAI request to {model_endpoint.url}, payload: {payload}\"\n            )\n\n            record = await self.post_request(\n                self.get_url(model_endpoint),\n                json.dumps(payload),\n                self.get_headers(model_endpoint),\n            )\n            record.request = payload\n\n        except Exception as e:\n            record = RequestRecord(\n                request=payload,\n                start_perf_ns=start_perf_ns,\n                end_perf_ns=time.perf_counter_ns(),\n                error=ErrorDetails(type=e.__class__.__name__, message=str(e)),\n            )\n            self.exception(f\"Error in OpenAI request: {e.__class__.__name__} {str(e)}\")\n\n        return record\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp.get_headers","title":"<code>get_headers(model_endpoint)</code>","text":"<p>Get the headers for the given endpoint.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>def get_headers(self, model_endpoint: ModelEndpointInfo) -&gt; dict[str, str]:\n    \"\"\"Get the headers for the given endpoint.\"\"\"\n\n    accept = (\n        \"text/event-stream\"\n        if model_endpoint.endpoint.streaming\n        else \"application/json\"\n    )\n\n    headers = {\n        \"User-Agent\": \"aiperf/1.0\",\n        \"Content-Type\": \"application/json\",\n        \"Accept\": accept,\n    }\n    if model_endpoint.endpoint.api_key:\n        headers[\"Authorization\"] = f\"Bearer {model_endpoint.endpoint.api_key}\"\n    if model_endpoint.endpoint.headers:\n        headers.update(model_endpoint.endpoint.headers)\n    return headers\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp.get_url","title":"<code>get_url(model_endpoint)</code>","text":"<p>Get the URL for the given endpoint.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>def get_url(self, model_endpoint: ModelEndpointInfo) -&gt; str:\n    \"\"\"Get the URL for the given endpoint.\"\"\"\n    url = model_endpoint.url\n    if not url.startswith(\"http\"):\n        url = f\"http://{url}\"\n    return url\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp.send_request","title":"<code>send_request(model_endpoint, payload)</code>  <code>async</code>","text":"<p>Send OpenAI request using aiohttp.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>async def send_request(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    payload: dict[str, Any],\n) -&gt; RequestRecord:\n    \"\"\"Send OpenAI request using aiohttp.\"\"\"\n\n    # capture start time before request is sent in the case of an error\n    start_perf_ns = time.perf_counter_ns()\n    try:\n        self.debug(\n            lambda: f\"Sending OpenAI request to {model_endpoint.url}, payload: {payload}\"\n        )\n\n        record = await self.post_request(\n            self.get_url(model_endpoint),\n            json.dumps(payload),\n            self.get_headers(model_endpoint),\n        )\n        record.request = payload\n\n    except Exception as e:\n        record = RequestRecord(\n            request=payload,\n            start_perf_ns=start_perf_ns,\n            end_perf_ns=time.perf_counter_ns(),\n            error=ErrorDetails(type=e.__class__.__name__, message=str(e)),\n        )\n        self.exception(f\"Error in OpenAI request: {e.__class__.__name__} {str(e)}\")\n\n    return record\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_chat","title":"aiperf.clients.openai.openai_chat","text":""},{"location":"api/#aiperf.clients.openai.openai_chat.OpenAIChatCompletionRequestConverter","title":"<code>OpenAIChatCompletionRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI chat completion requests.</p> Source code in <code>aiperf/clients/openai/openai_chat.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_CHAT_COMPLETIONS)\nclass OpenAIChatCompletionRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI chat completion requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a chat completion request.\"\"\"\n\n        message = await self._create_message(turn)\n\n        payload = {\n            \"messages\": [message],\n            \"model\": model_endpoint.primary_model_name,\n            \"stream\": model_endpoint.endpoint.streaming,\n        }\n\n        if model_endpoint.endpoint.extra:\n            payload.update(model_endpoint.endpoint.extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n\n    def _create_message(self, turn: Turn) -&gt; dict[Any, Any]:\n        message = {\n            \"role\": turn.role or DEFAULT_ROLE,\n            \"content\": [],\n        }\n        for text in turn.texts:\n            for content in text.contents:\n                if not content:\n                    continue\n                message[\"content\"].append({\"type\": \"text\", \"text\": content})\n\n        for image in turn.images:\n            for content in image.contents:\n                if not content:\n                    continue\n                message[\"content\"].append(\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": content}}\n                )\n\n        for audio in turn.audios:\n            for content in audio.contents:\n                if not content:\n                    continue\n                if \",\" not in content:\n                    raise ValueError(\n                        \"Audio content must be in the format 'format,b64_audio'.\"\n                    )\n                format, b64_audio = content.split(\",\", 1)\n                message[\"content\"].append(\n                    {\n                        \"type\": \"input_audio\",\n                        \"input_audio\": {\n                            \"data\": b64_audio,\n                            \"format\": format,\n                        },\n                    }\n                )\n\n        return message\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_chat.OpenAIChatCompletionRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a chat completion request.</p> Source code in <code>aiperf/clients/openai/openai_chat.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a chat completion request.\"\"\"\n\n    message = await self._create_message(turn)\n\n    payload = {\n        \"messages\": [message],\n        \"model\": model_endpoint.primary_model_name,\n        \"stream\": model_endpoint.endpoint.streaming,\n    }\n\n    if model_endpoint.endpoint.extra:\n        payload.update(model_endpoint.endpoint.extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_completions","title":"aiperf.clients.openai.openai_completions","text":""},{"location":"api/#aiperf.clients.openai.openai_completions.OpenAICompletionRequestConverter","title":"<code>OpenAICompletionRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI completion requests.</p> Source code in <code>aiperf/clients/openai/openai_completions.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_COMPLETIONS)\nclass OpenAICompletionRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI completion requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a completion request.\"\"\"\n\n        prompts = [\n            content for text in turn.texts for content in text.contents if content\n        ]\n\n        extra = model_endpoint.endpoint.extra or {}\n\n        payload = {\n            \"prompt\": prompts,\n            \"model\": model_endpoint.primary_model_name,\n            \"stream\": model_endpoint.endpoint.streaming,\n        }\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_completions.OpenAICompletionRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a completion request.</p> Source code in <code>aiperf/clients/openai/openai_completions.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a completion request.\"\"\"\n\n    prompts = [\n        content for text in turn.texts for content in text.contents if content\n    ]\n\n    extra = model_endpoint.endpoint.extra or {}\n\n    payload = {\n        \"prompt\": prompts,\n        \"model\": model_endpoint.primary_model_name,\n        \"stream\": model_endpoint.endpoint.streaming,\n    }\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_embeddings","title":"aiperf.clients.openai.openai_embeddings","text":""},{"location":"api/#aiperf.clients.openai.openai_embeddings.OpenAIEmbeddingsRequestConverter","title":"<code>OpenAIEmbeddingsRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI embeddings requests.</p> Source code in <code>aiperf/clients/openai/openai_embeddings.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_EMBEDDINGS)\nclass OpenAIEmbeddingsRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI embeddings requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for an embeddings request.\"\"\"\n\n        prompts = [\n            content for text in turn.texts for content in text.contents if content\n        ]\n\n        extra = model_endpoint.endpoint.extra or {}\n\n        payload = {\n            \"model\": model_endpoint.primary_model_name,\n            \"input\": prompts,\n        }\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_embeddings.OpenAIEmbeddingsRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for an embeddings request.</p> Source code in <code>aiperf/clients/openai/openai_embeddings.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for an embeddings request.\"\"\"\n\n    prompts = [\n        content for text in turn.texts for content in text.contents if content\n    ]\n\n    extra = model_endpoint.endpoint.extra or {}\n\n    payload = {\n        \"model\": model_endpoint.primary_model_name,\n        \"input\": prompts,\n    }\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_responses","title":"aiperf.clients.openai.openai_responses","text":""},{"location":"api/#aiperf.clients.openai.openai_responses.OpenAIResponsesRequestConverter","title":"<code>OpenAIResponsesRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI Responses requests.</p> Source code in <code>aiperf/clients/openai/openai_responses.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_RESPONSES)\nclass OpenAIResponsesRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI Responses requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a responses request.\"\"\"\n\n        # TODO: Add support for image and audio inputs.\n        prompts = [\n            content for text in turn.texts for content in text.contents if content\n        ]\n\n        extra = model_endpoint.endpoint.extra or {}\n\n        payload = {\n            \"input\": prompts,\n            \"model\": model_endpoint.primary_model_name,\n            # TODO: How do we handle max_output_tokens? Should be provided by OSL logic\n            \"max_output_tokens\": extra.pop(\"max_output_tokens\", None),\n            \"stream\": model_endpoint.endpoint.streaming,\n        }\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_responses.OpenAIResponsesRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a responses request.</p> Source code in <code>aiperf/clients/openai/openai_responses.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a responses request.\"\"\"\n\n    # TODO: Add support for image and audio inputs.\n    prompts = [\n        content for text in turn.texts for content in text.contents if content\n    ]\n\n    extra = model_endpoint.endpoint.extra or {}\n\n    payload = {\n        \"input\": prompts,\n        \"model\": model_endpoint.primary_model_name,\n        # TODO: How do we handle max_output_tokens? Should be provided by OSL logic\n        \"max_output_tokens\": extra.pop(\"max_output_tokens\", None),\n        \"stream\": model_endpoint.endpoint.streaming,\n    }\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfcommonaiperf_logger","title":"aiperf.common.aiperf_logger","text":""},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger","title":"<code>AIPerfLogger</code>","text":"<p>Logger for AIPerf messages with lazy evaluation support for f-strings.</p> <p>This logger supports lazy evaluation of f-strings through lambdas to avoid expensive string formatting operations when the log level is not enabled.</p> It also extends the standard logging module with additional log levels <ul> <li>TRACE    (TRACE &lt; DEBUG)</li> <li>NOTICE   (INFO &lt; NOTICE &lt; WARNING)</li> <li>SUCCESS  (WARNING &lt; SUCCESS &lt; ERROR)</li> </ul> Usage <p>logger = AIPerfLogger(\"my_logger\") logger.debug(lambda: f\"Processing {item} with {count} items\") logger.info(\"Simple string message\") logger.notice(\"Notice message\") logger.success(\"Benchmark completed successfully\")</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>class AIPerfLogger:\n    \"\"\"Logger for AIPerf messages with lazy evaluation support for f-strings.\n\n    This logger supports lazy evaluation of f-strings through lambdas to avoid\n    expensive string formatting operations when the log level is not enabled.\n\n    It also extends the standard logging module with additional log levels:\n        - TRACE    (TRACE &lt; DEBUG)\n        - NOTICE   (INFO &lt; NOTICE &lt; WARNING)\n        - SUCCESS  (WARNING &lt; SUCCESS &lt; ERROR)\n\n    Usage:\n        logger = AIPerfLogger(\"my_logger\")\n        logger.debug(lambda: f\"Processing {item} with {count} items\")\n        logger.info(\"Simple string message\")\n        logger.notice(\"Notice message\")\n        logger.success(\"Benchmark completed successfully\")\n        # Need to pass local variables to the lambda to avoid them going out of scope\n        logger.debug(lambda i=i: f\"Binding loop variable: {i}\")\n        logger.exception(f\"Direct f-string usage: {e}\")\n    \"\"\"\n\n    def __init__(self, logger_name: str):\n        self.logger_name = logger_name\n        self._logger = logging.getLogger(logger_name)\n\n        # Cache the internal logging module's _log method\n        self._internal_log = self._logger._log\n\n        # Forward the internal findCaller method to our custom method\n        self._logger.findCaller = self.find_caller\n\n        # Python style method names\n        self.is_enabled_for = self._logger.isEnabledFor\n        self.set_level = self._logger.setLevel\n        self.get_effective_level = self._logger.getEffectiveLevel\n\n        # Legacy logging method compatibility / passthrough\n        self.isEnabledFor = self._logger.isEnabledFor\n        self.setLevel = self._logger.setLevel\n        self.getEffectiveLevel = self._logger.getEffectiveLevel\n        self.handlers = self._logger.handlers\n        self.addHandler = self._logger.addHandler\n        self.removeHandler = self._logger.removeHandler\n        self.hasHandlers = self._logger.hasHandlers\n        self.root = self._logger.root\n\n    @property\n    def is_debug_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_DEBUG)\n\n    @property\n    def is_trace_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_TRACE)\n\n    def _log(self, level: int, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Internal log method that handles lazy evaluation of f-strings.\"\"\"\n        if callable(msg):\n            # NOTE: Internal python logging _log method requires a tuple for the args, even if there are no args\n            self._internal_log(level, msg(*args), (), **kwargs)\n        else:\n            self._internal_log(level, msg, args, **kwargs)\n\n    @classmethod\n    def is_valid_level(cls, level: int | str) -&gt; bool:\n        \"\"\"Check if the given level is a valid level.\"\"\"\n        if isinstance(level, str):\n            return level in [\n                \"TRACE\",\n                \"DEBUG\",\n                \"INFO\",\n                \"NOTICE\",\n                \"WARNING\",\n                \"SUCCESS\",\n                \"ERROR\",\n                \"CRITICAL\",\n            ]\n        else:\n            return level in [\n                _TRACE,\n                _DEBUG,\n                _INFO,\n                _NOTICE,\n                _WARNING,\n                _SUCCESS,\n                _ERROR,\n                _CRITICAL,\n            ]\n\n    @classmethod\n    def get_level_number(cls, level: int | str) -&gt; int:\n        \"\"\"Get the numeric level for the given level.\"\"\"\n        if isinstance(level, str):\n            return getattr(cls, level.upper())\n        else:\n            return level\n\n    def find_caller(\n        self, stack_info=False, stacklevel=1\n    ) -&gt; tuple[str, int, str, str | None]:\n        \"\"\"\n        NOTE: This is a modified version of the findCaller method in the logging module,\n        in order to allow us to add custom ignored files.\n\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        # On some versions of IronPython, currentframe() returns None if\n        # IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        orig_f = f\n        while f and stacklevel &gt; 1:\n            f = f.f_back\n            stacklevel -= 1\n        if not f:\n            f = orig_f\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\n        while f and hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            # NOTE: The if-statement below was modified to use our own list of ignored files (_ignored_files).\n            # This is required to avoid it appearing as all logs are coming from this file.\n            if filename in _ignored_files:\n                f = f.f_back\n                continue\n            sinfo = None\n            if stack_info:\n                sio = io.StringIO()\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == \"\\n\":\n                    sinfo = sinfo[:-1]\n                sio.close()\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n            break\n        return rv\n\n    def log(self, level: int, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(level):\n            self._log(level, msg, args, **kwargs)\n\n    def trace(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a trace message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_TRACE):\n            self._log(_TRACE, msg, *args, **kwargs)\n\n    def debug(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a debug message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_DEBUG):\n            self._log(_DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an info message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_INFO):\n            self._log(_INFO, msg, *args, **kwargs)\n\n    def notice(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a notice message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_NOTICE):\n            self._log(_NOTICE, msg, *args, **kwargs)\n\n    def warning(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a warning message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_WARNING):\n            self._log(_WARNING, msg, *args, **kwargs)\n\n    def success(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a success message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_SUCCESS):\n            self._log(_SUCCESS, msg, *args, **kwargs)\n\n    def error(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an error message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an exception message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, msg, *args, exc_info=True, **kwargs)\n\n    def critical(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a critical message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_CRITICAL):\n            self._log(_CRITICAL, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger--need-to-pass-local-variables-to-the-lambda-to-avoid-them-going-out-of-scope","title":"Need to pass local variables to the lambda to avoid them going out of scope","text":"<p>logger.debug(lambda i=i: f\"Binding loop variable: {i}\") logger.exception(f\"Direct f-string usage: {e}\")</p>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.critical","title":"<code>critical(msg, *args, **kwargs)</code>","text":"<p>Log a critical message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def critical(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a critical message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_CRITICAL):\n        self._log(_CRITICAL, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.debug","title":"<code>debug(msg, *args, **kwargs)</code>","text":"<p>Log a debug message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def debug(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a debug message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_DEBUG):\n        self._log(_DEBUG, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.error","title":"<code>error(msg, *args, **kwargs)</code>","text":"<p>Log an error message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def error(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an error message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.exception","title":"<code>exception(msg, *args, **kwargs)</code>","text":"<p>Log an exception message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def exception(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an exception message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, msg, *args, exc_info=True, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.find_caller","title":"<code>find_caller(stack_info=False, stacklevel=1)</code>","text":"<p>NOTE: This is a modified version of the findCaller method in the logging module, in order to allow us to add custom ignored files.</p> <p>Find the stack frame of the caller so that we can note the source file name, line number and function name.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def find_caller(\n    self, stack_info=False, stacklevel=1\n) -&gt; tuple[str, int, str, str | None]:\n    \"\"\"\n    NOTE: This is a modified version of the findCaller method in the logging module,\n    in order to allow us to add custom ignored files.\n\n    Find the stack frame of the caller so that we can note the source\n    file name, line number and function name.\n    \"\"\"\n    f = currentframe()\n    # On some versions of IronPython, currentframe() returns None if\n    # IronPython isn't run with -X:Frames.\n    if f is not None:\n        f = f.f_back\n    orig_f = f\n    while f and stacklevel &gt; 1:\n        f = f.f_back\n        stacklevel -= 1\n    if not f:\n        f = orig_f\n    rv = \"(unknown file)\", 0, \"(unknown function)\", None\n    while f and hasattr(f, \"f_code\"):\n        co = f.f_code\n        filename = os.path.normcase(co.co_filename)\n        # NOTE: The if-statement below was modified to use our own list of ignored files (_ignored_files).\n        # This is required to avoid it appearing as all logs are coming from this file.\n        if filename in _ignored_files:\n            f = f.f_back\n            continue\n        sinfo = None\n        if stack_info:\n            sio = io.StringIO()\n            sio.write(\"Stack (most recent call last):\\n\")\n            traceback.print_stack(f, file=sio)\n            sinfo = sio.getvalue()\n            if sinfo[-1] == \"\\n\":\n                sinfo = sinfo[:-1]\n            sio.close()\n        rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n        break\n    return rv\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.get_level_number","title":"<code>get_level_number(level)</code>  <code>classmethod</code>","text":"<p>Get the numeric level for the given level.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>@classmethod\ndef get_level_number(cls, level: int | str) -&gt; int:\n    \"\"\"Get the numeric level for the given level.\"\"\"\n    if isinstance(level, str):\n        return getattr(cls, level.upper())\n    else:\n        return level\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.info","title":"<code>info(msg, *args, **kwargs)</code>","text":"<p>Log an info message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def info(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_INFO):\n        self._log(_INFO, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.is_valid_level","title":"<code>is_valid_level(level)</code>  <code>classmethod</code>","text":"<p>Check if the given level is a valid level.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>@classmethod\ndef is_valid_level(cls, level: int | str) -&gt; bool:\n    \"\"\"Check if the given level is a valid level.\"\"\"\n    if isinstance(level, str):\n        return level in [\n            \"TRACE\",\n            \"DEBUG\",\n            \"INFO\",\n            \"NOTICE\",\n            \"WARNING\",\n            \"SUCCESS\",\n            \"ERROR\",\n            \"CRITICAL\",\n        ]\n    else:\n        return level in [\n            _TRACE,\n            _DEBUG,\n            _INFO,\n            _NOTICE,\n            _WARNING,\n            _SUCCESS,\n            _ERROR,\n            _CRITICAL,\n        ]\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.log","title":"<code>log(level, msg, *args, **kwargs)</code>","text":"<p>Log a message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def log(self, level: int, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(level):\n        self._log(level, msg, args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.notice","title":"<code>notice(msg, *args, **kwargs)</code>","text":"<p>Log a notice message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def notice(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a notice message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_NOTICE):\n        self._log(_NOTICE, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.success","title":"<code>success(msg, *args, **kwargs)</code>","text":"<p>Log a success message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def success(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a success message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_SUCCESS):\n        self._log(_SUCCESS, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.trace","title":"<code>trace(msg, *args, **kwargs)</code>","text":"<p>Log a trace message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def trace(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a trace message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_TRACE):\n        self._log(_TRACE, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.warning","title":"<code>warning(msg, *args, **kwargs)</code>","text":"<p>Log a warning message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def warning(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a warning message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_WARNING):\n        self._log(_WARNING, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperfcommonbootstrap","title":"aiperf.common.bootstrap","text":""},{"location":"api/#aiperf.common.bootstrap.bootstrap_and_run_service","title":"<code>bootstrap_and_run_service(service_class, service_config=None, user_config=None, service_id=None, log_queue=None, **kwargs)</code>","text":"<p>Bootstrap the service and run it.</p> <p>This function will load the service configuration, create an instance of the service, and run it.</p> <p>Parameters:</p> Name Type Description Default <code>service_class</code> <code>type[ServiceProtocol]</code> <p>The python class of the service to run. This should be a subclass of BaseService. This should be a type and not an instance.</p> required <code>service_config</code> <code>ServiceConfig | None</code> <p>The service configuration to use. If not provided, the service configuration will be loaded from the environment variables.</p> <code>None</code> <code>user_config</code> <code>UserConfig | None</code> <p>The user configuration to use. If not provided, the user configuration will be loaded from the environment variables.</p> <code>None</code> <code>log_queue</code> <code>Queue | None</code> <p>Optional multiprocessing queue for child process logging. If provided, the child process logging will be set up.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the service constructor.</p> <code>{}</code> Source code in <code>aiperf/common/bootstrap.py</code> <pre><code>def bootstrap_and_run_service(\n    service_class: type[ServiceProtocol],\n    service_config: ServiceConfig | None = None,\n    user_config: UserConfig | None = None,\n    service_id: str | None = None,\n    log_queue: \"multiprocessing.Queue | None\" = None,\n    **kwargs,\n):\n    \"\"\"Bootstrap the service and run it.\n\n    This function will load the service configuration,\n    create an instance of the service, and run it.\n\n    Args:\n        service_class: The python class of the service to run. This should be a subclass of\n            BaseService. This should be a type and not an instance.\n        service_config: The service configuration to use. If not provided, the service\n            configuration will be loaded from the environment variables.\n        user_config: The user configuration to use. If not provided, the user configuration\n            will be loaded from the environment variables.\n        log_queue: Optional multiprocessing queue for child process logging. If provided,\n            the child process logging will be set up.\n        kwargs: Additional keyword arguments to pass to the service constructor.\n    \"\"\"\n\n    # Load the service configuration\n    if service_config is None:\n        from aiperf.common.config import load_service_config\n\n        service_config = load_service_config()\n\n    # Load the user configuration\n    if user_config is None:\n        from aiperf.common.config import load_user_config\n\n        # TODO: Add support for loading user config from a file/environment variables\n        user_config = load_user_config()\n\n    async def _run_service():\n        if service_config.enable_yappi:\n            _start_yappi_profiling()\n\n        service = service_class(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            **kwargs,\n        )\n\n        from aiperf.common.logging import setup_child_process_logging\n\n        setup_child_process_logging(\n            log_queue, service.service_id, service_config, user_config\n        )\n\n        if user_config.input.random_seed is not None:\n            random.seed(user_config.input.random_seed)\n            # Try and set the numpy random seed\n            # https://numpy.org/doc/stable/reference/random/index.html#random-quick-start\n            with contextlib.suppress(ImportError):\n                import numpy as np\n\n                np.random.seed(user_config.input.random_seed)\n\n        try:\n            await service.initialize()\n            await service.start()\n            await service.stopped_event.wait()\n        except Exception as e:\n            service.exception(f\"Unhandled exception in service: {e}\")\n\n        if service_config.enable_yappi:\n            _stop_yappi_profiling(service.service_id, user_config)\n\n    with contextlib.suppress(asyncio.CancelledError):\n        if service_config.enable_uvloop:\n            import uvloop\n\n            uvloop.run(_run_service())\n        else:\n            asyncio.run(_run_service())\n</code></pre>"},{"location":"api/#aiperfcommoncommsbase_comms","title":"aiperf.common.comms.base_comms","text":""},{"location":"api/#aiperf.common.comms.base_comms.BaseCommunication","title":"<code>BaseCommunication</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>Base class for specifying the base communication layer for AIPerf components.</p> Source code in <code>aiperf/common/comms/base_comms.py</code> <pre><code>@implements_protocol(CommunicationProtocol)\nclass BaseCommunication(AIPerfLifecycleMixin, ABC):\n    \"\"\"Base class for specifying the base communication layer for AIPerf components.\"\"\"\n\n    @abstractmethod\n    def get_address(self, address_type: CommAddressType) -&gt; str:\n        \"\"\"Get the address for a given address type.\n\n        Args:\n            address_type: The type of address to get the address for, or the address itself.\n\n        Returns:\n            The address for the given address type, or the address itself if it is a string.\n        \"\"\"\n\n    @abstractmethod\n    def create_client(\n        self,\n        client_type: CommClientType,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; CommunicationClientProtocol:\n        \"\"\"Create a communication client for a given client type and address.\n\n        Args:\n            client_type: The type of client to create.\n            address: The type of address to use when looking up in the communication config, or the address itself.\n            bind: Whether to bind or connect the socket.\n            socket_ops: Additional socket options to set.\n            max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n        \"\"\"\n\n    def create_pub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PubClientProtocol:\n        return cast(\n            PubClientProtocol,\n            self.create_client(CommClientType.PUB, address, bind, socket_ops),\n        )\n\n    def create_sub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; SubClientProtocol:\n        return cast(\n            SubClientProtocol,\n            self.create_client(CommClientType.SUB, address, bind, socket_ops),\n        )\n\n    def create_push_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PushClientProtocol:\n        return cast(\n            PushClientProtocol,\n            self.create_client(CommClientType.PUSH, address, bind, socket_ops),\n        )\n\n    def create_pull_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; PullClientProtocol:\n        return cast(\n            PullClientProtocol,\n            self.create_client(\n                CommClientType.PULL,\n                address,\n                bind,\n                socket_ops,\n                max_pull_concurrency=max_pull_concurrency,\n            ),\n        )\n\n    def create_request_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; RequestClientProtocol:\n        return cast(\n            RequestClientProtocol,\n            self.create_client(CommClientType.REQUEST, address, bind, socket_ops),\n        )\n\n    def create_reply_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; ReplyClientProtocol:\n        return cast(\n            ReplyClientProtocol,\n            self.create_client(CommClientType.REPLY, address, bind, socket_ops),\n        )\n</code></pre>"},{"location":"api/#aiperf.common.comms.base_comms.BaseCommunication.create_client","title":"<code>create_client(client_type, address, bind=False, socket_ops=None, max_pull_concurrency=None)</code>  <code>abstractmethod</code>","text":"<p>Create a communication client for a given client type and address.</p> <p>Parameters:</p> Name Type Description Default <code>client_type</code> <code>CommClientType</code> <p>The type of client to create.</p> required <code>address</code> <code>CommAddressType</code> <p>The type of address to use when looking up in the communication config, or the address itself.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> <code>False</code> <code>socket_ops</code> <code>dict | None</code> <p>Additional socket options to set.</p> <code>None</code> <code>max_pull_concurrency</code> <code>int | None</code> <p>The maximum number of concurrent pull requests to allow. (Only used for pull clients)</p> <code>None</code> Source code in <code>aiperf/common/comms/base_comms.py</code> <pre><code>@abstractmethod\ndef create_client(\n    self,\n    client_type: CommClientType,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n) -&gt; CommunicationClientProtocol:\n    \"\"\"Create a communication client for a given client type and address.\n\n    Args:\n        client_type: The type of client to create.\n        address: The type of address to use when looking up in the communication config, or the address itself.\n        bind: Whether to bind or connect the socket.\n        socket_ops: Additional socket options to set.\n        max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.comms.base_comms.BaseCommunication.get_address","title":"<code>get_address(address_type)</code>  <code>abstractmethod</code>","text":"<p>Get the address for a given address type.</p> <p>Parameters:</p> Name Type Description Default <code>address_type</code> <code>CommAddressType</code> <p>The type of address to get the address for, or the address itself.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The address for the given address type, or the address itself if it is a string.</p> Source code in <code>aiperf/common/comms/base_comms.py</code> <pre><code>@abstractmethod\ndef get_address(self, address_type: CommAddressType) -&gt; str:\n    \"\"\"Get the address for a given address type.\n\n    Args:\n        address_type: The type of address to get the address for, or the address itself.\n\n    Returns:\n        The address for the given address type, or the address itself if it is a string.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqdealer_request_client","title":"aiperf.common.comms.zmq.dealer_request_client","text":""},{"location":"api/#aiperf.common.comms.zmq.dealer_request_client.ZMQDealerRequestClient","title":"<code>ZMQDealerRequestClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code>, <code>TaskManagerMixin</code></p> <p>ZMQ DEALER socket client for asynchronous request-response communication.</p> <p>The DEALER socket connects to ROUTER sockets and can send requests asynchronously, receiving responses through callbacks or awaitable futures.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502    ROUTER    \u2502 \u2502   (Client)   \u2502                    \u2502  (Service)   \u2502 \u2502              \u2502&lt;\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - DEALER Clients send requests to ROUTER Services - Responses are routed back to the originating DEALER</p> <p>DEALER/ROUTER is a Many-to-One communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQDealerRouterProxy</code> for more details.</p> Source code in <code>aiperf/common/comms/zmq/dealer_request_client.py</code> <pre><code>@implements_protocol(RequestClientProtocol)\n@CommunicationClientFactory.register(CommClientType.REQUEST)\nclass ZMQDealerRequestClient(BaseZMQClient, TaskManagerMixin):\n    \"\"\"\n    ZMQ DEALER socket client for asynchronous request-response communication.\n\n    The DEALER socket connects to ROUTER sockets and can send requests asynchronously,\n    receiving responses through callbacks or awaitable futures.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502    ROUTER    \u2502\n    \u2502   (Client)   \u2502                    \u2502  (Service)   \u2502\n    \u2502              \u2502&lt;\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - DEALER Clients send requests to ROUTER Services\n    - Responses are routed back to the originating DEALER\n\n    DEALER/ROUTER is a Many-to-One communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQDealerRouterProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Dealer (Req) client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.DEALER, address, bind, socket_ops, **kwargs)\n\n        self.request_callbacks: dict[\n            str, Callable[[Message], Coroutine[Any, Any, None]]\n        ] = {}\n\n    @background_task(immediate=True, interval=None)\n    async def _request_async_task(self) -&gt; None:\n        \"\"\"Task to handle incoming requests.\"\"\"\n        while not self.stop_requested:\n            try:\n                message = await self.socket.recv_string()\n                self.trace(lambda msg=message: f\"Received response: {msg}\")\n                response_message = Message.from_json(message)\n\n                # Call the callback if it exists\n                if response_message.request_id in self.request_callbacks:\n                    callback = self.request_callbacks.pop(response_message.request_id)\n                    self.execute_async(callback(response_message))\n\n            except zmq.Again:\n                self.debug(\"No data on dealer socket received, yielding to event loop\")\n                await yield_to_event_loop()\n            except Exception as e:\n                self.exception(f\"Exception receiving responses: {e}\")\n                await yield_to_event_loop()\n            except asyncio.CancelledError:\n                self.debug(\"Dealer request client receiver task cancelled\")\n                raise  # re-raise the cancelled error\n\n    @on_stop\n    async def _stop_remaining_tasks(self) -&gt; None:\n        \"\"\"Wait for all tasks to complete.\"\"\"\n        await self.cancel_all_tasks()\n\n    async def request_async(\n        self,\n        message: Message,\n        callback: Callable[[Message], Coroutine[Any, Any, None]],\n    ) -&gt; None:\n        \"\"\"Send a request and be notified when the response is received.\"\"\"\n        await self._check_initialized()\n\n        if not isinstance(message, Message):\n            raise TypeError(\n                f\"message must be an instance of Message, got {type(message).__name__}\"\n            )\n\n        # Generate request ID if not provided so that responses can be matched\n        if not message.request_id:\n            message.request_id = str(uuid.uuid4())\n\n        self.request_callbacks[message.request_id] = callback\n\n        request_json = message.model_dump_json()\n        self.trace(lambda msg=request_json: f\"Sending request: {msg}\")\n\n        try:\n            await self.socket.send_string(request_json)\n\n        except Exception as e:\n            raise CommunicationError(\n                f\"Exception sending request: {e.__class__.__qualname__} {e}\",\n            ) from e\n\n    async def request(\n        self,\n        message: Message,\n        timeout: float = DEFAULT_COMMS_REQUEST_TIMEOUT,\n    ) -&gt; Message:\n        \"\"\"Send a request and wait for a response up to timeout seconds.\n\n        Args:\n            message (Message): The request message to send.\n            timeout (float): Maximum time to wait for a response in seconds.\n\n        Returns:\n            Message: The response message received.\n\n        Raises:\n            CommunicationError: if the request fails, or\n            asyncio.TimeoutError: if the response is not received in time.\n        \"\"\"\n        future = asyncio.Future[Message]()\n\n        async def callback(response_message: Message) -&gt; None:\n            future.set_result(response_message)\n\n        await self.request_async(message, callback)\n        return await asyncio.wait_for(future, timeout=timeout)\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.dealer_request_client.ZMQDealerRequestClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Dealer (Req) client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/dealer_request_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Dealer (Req) client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.DEALER, address, bind, socket_ops, **kwargs)\n\n    self.request_callbacks: dict[\n        str, Callable[[Message], Coroutine[Any, Any, None]]\n    ] = {}\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.dealer_request_client.ZMQDealerRequestClient.request","title":"<code>request(message, timeout=DEFAULT_COMMS_REQUEST_TIMEOUT)</code>  <code>async</code>","text":"<p>Send a request and wait for a response up to timeout seconds.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The request message to send.</p> required <code>timeout</code> <code>float</code> <p>Maximum time to wait for a response in seconds.</p> <code>DEFAULT_COMMS_REQUEST_TIMEOUT</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>The response message received.</p> <p>Raises:</p> Type Description <code>CommunicationError</code> <p>if the request fails, or</p> <code>TimeoutError</code> <p>if the response is not received in time.</p> Source code in <code>aiperf/common/comms/zmq/dealer_request_client.py</code> <pre><code>async def request(\n    self,\n    message: Message,\n    timeout: float = DEFAULT_COMMS_REQUEST_TIMEOUT,\n) -&gt; Message:\n    \"\"\"Send a request and wait for a response up to timeout seconds.\n\n    Args:\n        message (Message): The request message to send.\n        timeout (float): Maximum time to wait for a response in seconds.\n\n    Returns:\n        Message: The response message received.\n\n    Raises:\n        CommunicationError: if the request fails, or\n        asyncio.TimeoutError: if the response is not received in time.\n    \"\"\"\n    future = asyncio.Future[Message]()\n\n    async def callback(response_message: Message) -&gt; None:\n        future.set_result(response_message)\n\n    await self.request_async(message, callback)\n    return await asyncio.wait_for(future, timeout=timeout)\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.dealer_request_client.ZMQDealerRequestClient.request_async","title":"<code>request_async(message, callback)</code>  <code>async</code>","text":"<p>Send a request and be notified when the response is received.</p> Source code in <code>aiperf/common/comms/zmq/dealer_request_client.py</code> <pre><code>async def request_async(\n    self,\n    message: Message,\n    callback: Callable[[Message], Coroutine[Any, Any, None]],\n) -&gt; None:\n    \"\"\"Send a request and be notified when the response is received.\"\"\"\n    await self._check_initialized()\n\n    if not isinstance(message, Message):\n        raise TypeError(\n            f\"message must be an instance of Message, got {type(message).__name__}\"\n        )\n\n    # Generate request ID if not provided so that responses can be matched\n    if not message.request_id:\n        message.request_id = str(uuid.uuid4())\n\n    self.request_callbacks[message.request_id] = callback\n\n    request_json = message.model_dump_json()\n    self.trace(lambda msg=request_json: f\"Sending request: {msg}\")\n\n    try:\n        await self.socket.send_string(request_json)\n\n    except Exception as e:\n        raise CommunicationError(\n            f\"Exception sending request: {e.__class__.__qualname__} {e}\",\n        ) from e\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqpub_client","title":"aiperf.common.comms.zmq.pub_client","text":""},{"location":"api/#aiperf.common.comms.zmq.pub_client.ZMQPubClient","title":"<code>ZMQPubClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>The PUB socket broadcasts messages to all connected SUB sockets that have subscribed to the message topic/type.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 OR \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - Single PUB socket broadcasts messages to all subscribers (One-to-Many) OR - Multiple PUB sockets broadcast messages to a single SUB socket (Many-to-One)</p> <ul> <li>SUB sockets filter messages by topic/message_type</li> <li>Fire-and-forget messaging (no acknowledgments)</li> </ul> <p>PUB/SUB is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQXPubXSubProxy</code> for more details.</p> Source code in <code>aiperf/common/comms/zmq/pub_client.py</code> <pre><code>@implements_protocol(PubClientProtocol)\n@CommunicationClientFactory.register(CommClientType.PUB)\nclass ZMQPubClient(BaseZMQClient):\n    \"\"\"\n    The PUB socket broadcasts messages to all connected SUB sockets that have\n    subscribed to the message topic/type.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    OR\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - Single PUB socket broadcasts messages to all subscribers (One-to-Many)\n    OR\n    - Multiple PUB sockets broadcast messages to a single SUB socket (Many-to-One)\n\n    - SUB sockets filter messages by topic/message_type\n    - Fire-and-forget messaging (no acknowledgments)\n\n    PUB/SUB is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQXPubXSubProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Publisher client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.PUB, address, bind, socket_ops, **kwargs)\n\n    async def publish(self, message: Message) -&gt; None:\n        \"\"\"Publish a message. The topic will be set automatically based on the message type.\n\n        Args:\n            message: Message to publish (must be a Message object)\n        \"\"\"\n        await self._check_initialized()\n\n        try:\n            topic = self._determine_topic(message)\n            message_json = message.model_dump_json()\n            # Publish message\n            self.trace(lambda: f\"Publishing message {topic=} {message_json=}\")\n            await self.socket.send_multipart([topic.encode(), message_json.encode()])\n\n        except (asyncio.CancelledError, zmq.ContextTerminated):\n            self.debug(\n                lambda: f\"Pub client {self.client_id} cancelled or context terminated\"\n            )\n            return\n\n        except Exception as e:\n            raise CommunicationError(\n                f\"Failed to publish message {message.message_type}: {e}\",\n            ) from e\n\n    def _determine_topic(self, message: Message) -&gt; str:\n        \"\"\"Determine the topic based on the message.\"\"\"\n        # For targeted messages such as commands, we can set the topic to a specific service by id or type\n        # Note that target_service_id always takes precedence over target_service_type\n\n        if isinstance(message, TargetedServiceMessage):\n            if message.target_service_id:\n                return f\"{message.message_type}.{message.target_service_id}\"\n            if message.target_service_type:\n                return f\"{message.message_type}.{message.target_service_type}\"\n        return message.message_type\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.pub_client.ZMQPubClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Publisher client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/pub_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Publisher client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.PUB, address, bind, socket_ops, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.pub_client.ZMQPubClient.publish","title":"<code>publish(message)</code>  <code>async</code>","text":"<p>Publish a message. The topic will be set automatically based on the message type.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to publish (must be a Message object)</p> required Source code in <code>aiperf/common/comms/zmq/pub_client.py</code> <pre><code>async def publish(self, message: Message) -&gt; None:\n    \"\"\"Publish a message. The topic will be set automatically based on the message type.\n\n    Args:\n        message: Message to publish (must be a Message object)\n    \"\"\"\n    await self._check_initialized()\n\n    try:\n        topic = self._determine_topic(message)\n        message_json = message.model_dump_json()\n        # Publish message\n        self.trace(lambda: f\"Publishing message {topic=} {message_json=}\")\n        await self.socket.send_multipart([topic.encode(), message_json.encode()])\n\n    except (asyncio.CancelledError, zmq.ContextTerminated):\n        self.debug(\n            lambda: f\"Pub client {self.client_id} cancelled or context terminated\"\n        )\n        return\n\n    except Exception as e:\n        raise CommunicationError(\n            f\"Failed to publish message {message.message_type}: {e}\",\n        ) from e\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqpull_client","title":"aiperf.common.comms.zmq.pull_client","text":""},{"location":"api/#aiperf.common.comms.zmq.pull_client.ZMQPullClient","title":"<code>ZMQPullClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ PULL socket client for receiving work from PUSH sockets.</p> <p>The PULL socket receives messages from PUSH sockets in a pipeline pattern, distributing work fairly among multiple PULL workers.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502 \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502 \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   Tasks:    \u2502             \u25b2                     \u25b2 \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502 \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   - Task D  \u2502             \u25bc \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502    PULL     \u2502                      \u2502 (Worker N)  \u2502                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - PULL receives work from multiple PUSH producers - Work is fairly distributed among PULL workers - Pipeline pattern for distributed processing - Each message is delivered to exactly one PULL socket</p> <p>PULL/PUSH is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQPushPullProxy</code> for more details.</p> Source code in <code>aiperf/common/comms/zmq/pull_client.py</code> <pre><code>@implements_protocol(PullClientProtocol)\n@CommunicationClientFactory.register(CommClientType.PULL)\nclass ZMQPullClient(BaseZMQClient):\n    \"\"\"\n    ZMQ PULL socket client for receiving work from PUSH sockets.\n\n    The PULL socket receives messages from PUSH sockets in a pipeline pattern,\n    distributing work fairly among multiple PULL workers.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502\n    \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502\n    \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   Tasks:    \u2502             \u25b2                     \u25b2\n    \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n    \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   - Task D  \u2502             \u25bc\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502    PULL     \u2502\n                         \u2502 (Worker N)  \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - PULL receives work from multiple PUSH producers\n    - Work is fairly distributed among PULL workers\n    - Pipeline pattern for distributed processing\n    - Each message is delivered to exactly one PULL socket\n\n    PULL/PUSH is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQPushPullProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Puller class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n            max_pull_concurrency (int, optional): The maximum number of concurrent requests to allow.\n        \"\"\"\n        super().__init__(zmq.SocketType.PULL, address, bind, socket_ops, **kwargs)\n        self._pull_callbacks: dict[\n            MessageTypeT, Callable[[Message], Coroutine[Any, Any, None]]\n        ] = {}\n\n        if max_pull_concurrency is not None:\n            self.semaphore = asyncio.Semaphore(value=max_pull_concurrency)\n        else:\n            self.semaphore = asyncio.Semaphore(\n                value=int(os.getenv(\"AIPERF_WORKER_CONCURRENT_REQUESTS\", 500))\n            )\n\n    @background_task(immediate=True, interval=None)\n    async def _pull_receiver(self) -&gt; None:\n        \"\"\"Background task for receiving data from the pull socket.\n\n        This method is a coroutine that will run indefinitely until the client is\n        shutdown. It will wait for messages from the socket and handle them.\n        \"\"\"\n        while not self.stop_requested:\n            try:\n                # acquire the semaphore to limit the number of concurrent requests\n                # NOTE: This MUST be done BEFORE calling recv_string() to allow the zmq push/pull\n                # logic to properly load balance the requests.\n                await self.semaphore.acquire()\n\n                message_json = await self.socket.recv_string()\n                self.trace(\n                    lambda msg=message_json: f\"Received message from pull socket: {msg}\"\n                )\n                self.execute_async(self._process_message(message_json))\n\n            except zmq.Again:\n                self.debug(\"Pull client receiver task timed out\")\n                self.semaphore.release()  # release the semaphore as it was not used\n                await yield_to_event_loop()\n            except Exception as e:\n                self.exception(f\"Exception receiving data from pull socket: {e}\")\n                self.semaphore.release()  # release the semaphore as it was not used\n                await yield_to_event_loop()\n            except (asyncio.CancelledError, zmq.ContextTerminated):\n                self.debug(\"Pull client receiver task cancelled\")\n                self.semaphore.release()  # release the semaphore as it was not used\n                break\n\n    @on_stop\n    async def _stop(self) -&gt; None:\n        \"\"\"Wait for all tasks to complete.\"\"\"\n        await self.cancel_all_tasks()\n\n    async def _process_message(self, message_json: str) -&gt; None:\n        \"\"\"Process a message from the pull socket.\n\n        This method is called by the background task when a message is received from\n        the pull socket. It will deserialize the message and call the appropriate\n        callback function.\n        \"\"\"\n        try:\n            message = Message.from_json(message_json)\n\n            # Call callbacks with Message object\n            if message.message_type in self._pull_callbacks:\n                await self._pull_callbacks[message.message_type](message)\n            else:\n                self.warning(\n                    lambda message_type=message.message_type: f\"Pull message received for message type {message_type} without callback\"\n                )\n        finally:\n            # always release the semaphore to allow receiving more messages\n            self.semaphore.release()\n\n    def register_pull_callback(\n        self,\n        message_type: MessageTypeT,\n        callback: Callable[[Message], Coroutine[Any, Any, None]],\n    ) -&gt; None:\n        \"\"\"Register a ZMQ Pull data callback for a given message type.\n\n        Note that only one callback can be registered for a given message type.\n\n        Args:\n            message_type: The message type to register the callback for.\n            callback: The function to call when data is received.\n        Raises:\n            CommunicationError: If the client is not initialized\n        \"\"\"\n        # Register callback\n        if message_type not in self._pull_callbacks:\n            self._pull_callbacks[message_type] = callback\n        else:\n            raise ValueError(\n                f\"Callback already registered for message type {message_type}\"\n            )\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.pull_client.ZMQPullClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, max_pull_concurrency=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Puller class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> <code>max_pull_concurrency</code> <code>int</code> <p>The maximum number of concurrent requests to allow.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/pull_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Puller class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n        max_pull_concurrency (int, optional): The maximum number of concurrent requests to allow.\n    \"\"\"\n    super().__init__(zmq.SocketType.PULL, address, bind, socket_ops, **kwargs)\n    self._pull_callbacks: dict[\n        MessageTypeT, Callable[[Message], Coroutine[Any, Any, None]]\n    ] = {}\n\n    if max_pull_concurrency is not None:\n        self.semaphore = asyncio.Semaphore(value=max_pull_concurrency)\n    else:\n        self.semaphore = asyncio.Semaphore(\n            value=int(os.getenv(\"AIPERF_WORKER_CONCURRENT_REQUESTS\", 500))\n        )\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.pull_client.ZMQPullClient.register_pull_callback","title":"<code>register_pull_callback(message_type, callback)</code>","text":"<p>Register a ZMQ Pull data callback for a given message type.</p> <p>Note that only one callback can be registered for a given message type.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>MessageTypeT</code> <p>The message type to register the callback for.</p> required <code>callback</code> <code>Callable[[Message], Coroutine[Any, Any, None]]</code> <p>The function to call when data is received.</p> required <p>Raises:     CommunicationError: If the client is not initialized</p> Source code in <code>aiperf/common/comms/zmq/pull_client.py</code> <pre><code>def register_pull_callback(\n    self,\n    message_type: MessageTypeT,\n    callback: Callable[[Message], Coroutine[Any, Any, None]],\n) -&gt; None:\n    \"\"\"Register a ZMQ Pull data callback for a given message type.\n\n    Note that only one callback can be registered for a given message type.\n\n    Args:\n        message_type: The message type to register the callback for.\n        callback: The function to call when data is received.\n    Raises:\n        CommunicationError: If the client is not initialized\n    \"\"\"\n    # Register callback\n    if message_type not in self._pull_callbacks:\n        self._pull_callbacks[message_type] = callback\n    else:\n        raise ValueError(\n            f\"Callback already registered for message type {message_type}\"\n        )\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqpush_client","title":"aiperf.common.comms.zmq.push_client","text":""},{"location":"api/#aiperf.common.comms.zmq.push_client.MAX_PUSH_RETRIES","title":"<code>MAX_PUSH_RETRIES = 2</code>  <code>module-attribute</code>","text":"<p>Maximum number of retries for pushing a message.</p>"},{"location":"api/#aiperf.common.comms.zmq.push_client.RETRY_DELAY_INTERVAL_SEC","title":"<code>RETRY_DELAY_INTERVAL_SEC = 0.1</code>  <code>module-attribute</code>","text":"<p>The interval to wait before retrying to push a message.</p>"},{"location":"api/#aiperf.common.comms.zmq.push_client.ZMQPushClient","title":"<code>ZMQPushClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ PUSH socket client for sending work to PULL sockets.</p> <p>The PUSH socket sends messages to PULL sockets in a pipeline pattern, distributing work fairly among available PULL workers.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502 \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502 \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   Tasks:    \u2502             \u25b2                     \u25b2 \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502 \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   - Task D  \u2502             \u25bc \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502    PULL     \u2502                      \u2502 (Worker 3)  \u2502                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - Round-robin distribution of work tasks (One-to-Many) - Each message delivered to exactly one worker - Pipeline pattern for distributed processing - Automatic load balancing across available workers</p> <p>PUSH/PULL is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQPushPullProxy</code> for more details.</p> Source code in <code>aiperf/common/comms/zmq/push_client.py</code> <pre><code>@implements_protocol(PushClientProtocol)\n@CommunicationClientFactory.register(CommClientType.PUSH)\nclass ZMQPushClient(BaseZMQClient):\n    \"\"\"\n    ZMQ PUSH socket client for sending work to PULL sockets.\n\n    The PUSH socket sends messages to PULL sockets in a pipeline pattern,\n    distributing work fairly among available PULL workers.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502\n    \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502\n    \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   Tasks:    \u2502             \u25b2                     \u25b2\n    \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n    \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   - Task D  \u2502             \u25bc\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502    PULL     \u2502\n                         \u2502 (Worker 3)  \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - Round-robin distribution of work tasks (One-to-Many)\n    - Each message delivered to exactly one worker\n    - Pipeline pattern for distributed processing\n    - Automatic load balancing across available workers\n\n    PUSH/PULL is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQPushPullProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Push client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.PUSH, address, bind, socket_ops, **kwargs)\n\n    async def _push_message(\n        self,\n        message: Message,\n        retry_count: int = 0,\n        max_retries: int = MAX_PUSH_RETRIES,\n    ) -&gt; None:\n        \"\"\"Push a message to the socket. Will retry up to max_retries times.\n\n        Args:\n            message: Message to be sent must be a Message object\n            retry_count: Current retry count\n            max_retries: Maximum number of times to retry pushing the message\n        \"\"\"\n        try:\n            data_json = message.model_dump_json()\n            await self.socket.send_string(data_json)\n            self.trace(lambda msg=data_json: f\"Pushed json data: {msg}\")\n        except (asyncio.CancelledError, zmq.ContextTerminated):\n            self.debug(\"Push client cancelled or context terminated\")\n            return\n        except zmq.Again as e:\n            self.debug(\"Push client timed out\")\n            if retry_count &gt;= max_retries:\n                raise CommunicationError(\n                    f\"Failed to push data after {retry_count} retries: {e}\",\n                ) from e\n\n            await asyncio.sleep(RETRY_DELAY_INTERVAL_SEC)\n            return await self._push_message(message, retry_count + 1, max_retries)\n        except Exception as e:\n            raise CommunicationError(f\"Failed to push data: {e}\") from e\n\n    async def push(self, message: Message) -&gt; None:\n        \"\"\"Push data to a target. The message will be routed automatically\n        based on the message type.\n\n        Args:\n            message: Message to be sent must be a Message object\n        \"\"\"\n        await self._check_initialized()\n\n        await self._push_message(message)\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.push_client.ZMQPushClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Push client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/push_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Push client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.PUSH, address, bind, socket_ops, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.push_client.ZMQPushClient.push","title":"<code>push(message)</code>  <code>async</code>","text":"<p>Push data to a target. The message will be routed automatically based on the message type.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to be sent must be a Message object</p> required Source code in <code>aiperf/common/comms/zmq/push_client.py</code> <pre><code>async def push(self, message: Message) -&gt; None:\n    \"\"\"Push data to a target. The message will be routed automatically\n    based on the message type.\n\n    Args:\n        message: Message to be sent must be a Message object\n    \"\"\"\n    await self._check_initialized()\n\n    await self._push_message(message)\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqrouter_reply_client","title":"aiperf.common.comms.zmq.router_reply_client","text":""},{"location":"api/#aiperf.common.comms.zmq.router_reply_client.ZMQRouterReplyClient","title":"<code>ZMQRouterReplyClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ ROUTER socket client for handling requests from DEALER clients.</p> <p>The ROUTER socket receives requests from DEALER clients and sends responses back to the originating DEALER client using routing envelopes.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502    ROUTER    \u2502 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502  (Service)   \u2502 \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502              \u2502 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - ROUTER handles requests from multiple DEALER clients - Maintains routing envelopes to send responses back - Many-to-one request handling pattern - Supports concurrent request processing</p> <p>ROUTER/DEALER is a Many-to-One communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQDealerRouterProxy</code> for more details.</p> Source code in <code>aiperf/common/comms/zmq/router_reply_client.py</code> <pre><code>@implements_protocol(ReplyClientProtocol)\n@CommunicationClientFactory.register(CommClientType.REPLY)\nclass ZMQRouterReplyClient(BaseZMQClient):\n    \"\"\"\n    ZMQ ROUTER socket client for handling requests from DEALER clients.\n\n    The ROUTER socket receives requests from DEALER clients and sends responses\n    back to the originating DEALER client using routing envelopes.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502    ROUTER    \u2502\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502  (Service)   \u2502\n    \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502              \u2502\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - ROUTER handles requests from multiple DEALER clients\n    - Maintains routing envelopes to send responses back\n    - Many-to-one request handling pattern\n    - Supports concurrent request processing\n\n    ROUTER/DEALER is a Many-to-One communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQDealerRouterProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Router (Rep) client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.ROUTER, address, bind, socket_ops, **kwargs)\n\n        self._request_handlers: dict[\n            MessageTypeT,\n            tuple[str, Callable[[Message], Coroutine[Any, Any, Message | None]]],\n        ] = {}\n        self._response_futures: dict[str, asyncio.Future[Message | None]] = {}\n\n    @on_stop\n    async def _clear_request_handlers(self) -&gt; None:\n        self._request_handlers.clear()\n\n    def register_request_handler(\n        self,\n        service_id: str,\n        message_type: MessageTypeT,\n        handler: Callable[[Message], Coroutine[Any, Any, Message | None]],\n    ) -&gt; None:\n        \"\"\"Register a request handler. Anytime a request is received that matches the\n        message type, the handler will be called. The handler should return a response\n        message. If the handler returns None, the request will be ignored.\n\n        Note that there is a limit of 1 to 1 mapping between message type and handler.\n\n        Args:\n            service_id: The service ID to register the handler for\n            message_type: The message type to register the handler for\n            handler: The handler to register\n        \"\"\"\n        if message_type in self._request_handlers:\n            raise ValueError(\n                f\"Handler already registered for message type {message_type}\"\n            )\n\n        self.debug(\n            lambda service_id=service_id,\n            type=message_type: f\"Registering request handler for {service_id} with message type {type}\"\n        )\n        self._request_handlers[message_type] = (service_id, handler)\n\n    async def _handle_request(self, request_id: str, request: Message) -&gt; None:\n        \"\"\"Handle a request.\n\n        This method will:\n        - Parse the request JSON to create a Message object\n        - Call the handler for the message type\n        - Set the response future\n        \"\"\"\n        message_type = request.message_type\n\n        try:\n            _, handler = self._request_handlers[message_type]\n            response = await handler(request)\n\n        except Exception as e:\n            self.exception(f\"Exception calling handler for {message_type}: {e}\")\n            response = ErrorMessage(\n                request_id=request_id,\n                error=ErrorDetails.from_exception(e),\n            )\n\n        try:\n            self._response_futures[request_id].set_result(response)\n        except Exception as e:\n            self.exception(\n                f\"Exception setting response future for request {request_id}: {e}\"\n            )\n\n    async def _wait_for_response(\n        self, request_id: str, routing_envelope: tuple[bytes, ...]\n    ) -&gt; None:\n        \"\"\"Wait for a response to a request.\n\n        This method will wait for the response future to be set and then send the response\n        back to the client.\n        \"\"\"\n        try:\n            # Wait for the response asynchronously.\n            response = await self._response_futures[request_id]\n\n            if response is None:\n                self.warning(\n                    lambda req_id=request_id: f\"Got None as response for request {req_id}\"\n                )\n                response = ErrorMessage(\n                    request_id=request_id,\n                    error=ErrorDetails(\n                        type=\"NO_RESPONSE\",\n                        message=\"No response was generated for the request.\",\n                    ),\n                )\n\n            self._response_futures.pop(request_id, None)\n\n            # Send the response back to the client.\n            await self.socket.send_multipart(\n                [*routing_envelope, response.model_dump_json().encode()]\n            )\n        except Exception as e:\n            self.exception(\n                f\"Exception waiting for response for request {request_id}: {e}\"\n            )\n\n    @background_task(immediate=True, interval=None)\n    async def _rep_router_receiver(self) -&gt; None:\n        \"\"\"Background task for receiving requests and sending responses.\n\n        This method is a coroutine that will run indefinitely until the client is\n        shutdown. It will wait for requests from the socket and send responses in\n        an asynchronous manner.\n        \"\"\"\n        self.debug(\"Router reply client background task initialized\")\n\n        while not self.stop_requested:\n            try:\n                # Receive request\n                try:\n                    data = await self.socket.recv_multipart()\n                    self.trace(lambda msg=data: f\"Received request: {msg}\")\n\n                    request = Message.from_json(data[-1])\n                    if not request.request_id:\n                        self.exception(f\"Request ID is missing from request: {data}\")\n                        continue\n\n                    routing_envelope: tuple[bytes, ...] = (\n                        tuple(data[:-1])\n                        if len(data) &gt; 1\n                        else (request.request_id.encode(),)\n                    )\n                except zmq.Again:\n                    # This means we timed out waiting for a request.\n                    # We can continue to the next iteration of the loop.\n                    self.debug(\"Router reply client receiver task timed out\")\n                    await yield_to_event_loop()\n                    continue\n\n                # Create a new response future for this request that will be resolved\n                # when the handler returns a response.\n                self._response_futures[request.request_id] = asyncio.Future()\n                # Handle the request in a new task.\n                self.execute_async(self._handle_request(request.request_id, request))\n                self.execute_async(\n                    self._wait_for_response(request.request_id, routing_envelope)\n                )\n\n            except Exception as e:\n                self.exception(f\"Exception receiving request: {e}\")\n                await yield_to_event_loop()\n            except asyncio.CancelledError:\n                self.debug(\"Router reply client receiver task cancelled\")\n                break\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.router_reply_client.ZMQRouterReplyClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Router (Rep) client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/router_reply_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Router (Rep) client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.ROUTER, address, bind, socket_ops, **kwargs)\n\n    self._request_handlers: dict[\n        MessageTypeT,\n        tuple[str, Callable[[Message], Coroutine[Any, Any, Message | None]]],\n    ] = {}\n    self._response_futures: dict[str, asyncio.Future[Message | None]] = {}\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.router_reply_client.ZMQRouterReplyClient.register_request_handler","title":"<code>register_request_handler(service_id, message_type, handler)</code>","text":"<p>Register a request handler. Anytime a request is received that matches the message type, the handler will be called. The handler should return a response message. If the handler returns None, the request will be ignored.</p> <p>Note that there is a limit of 1 to 1 mapping between message type and handler.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>The service ID to register the handler for</p> required <code>message_type</code> <code>MessageTypeT</code> <p>The message type to register the handler for</p> required <code>handler</code> <code>Callable[[Message], Coroutine[Any, Any, Message | None]]</code> <p>The handler to register</p> required Source code in <code>aiperf/common/comms/zmq/router_reply_client.py</code> <pre><code>def register_request_handler(\n    self,\n    service_id: str,\n    message_type: MessageTypeT,\n    handler: Callable[[Message], Coroutine[Any, Any, Message | None]],\n) -&gt; None:\n    \"\"\"Register a request handler. Anytime a request is received that matches the\n    message type, the handler will be called. The handler should return a response\n    message. If the handler returns None, the request will be ignored.\n\n    Note that there is a limit of 1 to 1 mapping between message type and handler.\n\n    Args:\n        service_id: The service ID to register the handler for\n        message_type: The message type to register the handler for\n        handler: The handler to register\n    \"\"\"\n    if message_type in self._request_handlers:\n        raise ValueError(\n            f\"Handler already registered for message type {message_type}\"\n        )\n\n    self.debug(\n        lambda service_id=service_id,\n        type=message_type: f\"Registering request handler for {service_id} with message type {type}\"\n    )\n    self._request_handlers[message_type] = (service_id, handler)\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqsub_client","title":"aiperf.common.comms.zmq.sub_client","text":""},{"location":"api/#aiperf.common.comms.zmq.sub_client.ZMQSubClient","title":"<code>ZMQSubClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ SUB socket client for subscribing to messages from PUB sockets. One-to-Many or Many-to-One communication pattern.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 OR \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - Single SUB socket subscribes to multiple PUB publishers (One-to-Many) OR - Multiple SUB sockets subscribe to a single PUB publisher (Many-to-One)</p> <ul> <li>Subscribes to specific message topics/types</li> <li>Receives all messages matching subscriptions</li> </ul> <p>SUB/PUB is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQXPubXSubProxy</code> for more details.</p> Source code in <code>aiperf/common/comms/zmq/sub_client.py</code> <pre><code>@implements_protocol(SubClientProtocol)\n@CommunicationClientFactory.register(CommClientType.SUB)\nclass ZMQSubClient(BaseZMQClient):\n    \"\"\"\n    ZMQ SUB socket client for subscribing to messages from PUB sockets.\n    One-to-Many or Many-to-One communication pattern.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    OR\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n    Usage Pattern:\n    - Single SUB socket subscribes to multiple PUB publishers (One-to-Many)\n    OR\n    - Multiple SUB sockets subscribe to a single PUB publisher (Many-to-One)\n\n    - Subscribes to specific message topics/types\n    - Receives all messages matching subscriptions\n\n    SUB/PUB is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQXPubXSubProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Subscriber class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.SUB, address, bind, socket_ops, **kwargs)\n\n        self._subscribers: dict[MessageTypeT, list[Callable[[Message], Any]]] = {}\n\n    async def subscribe_all(\n        self,\n        message_callback_map: dict[\n            MessageTypeT,\n            Callable[[Message], Any] | list[Callable[[Message], Any]],\n        ],\n    ) -&gt; None:\n        \"\"\"Subscribe to all message_types in the map. For each MessageType, a single\n        callback or a list of callbacks can be provided.\"\"\"\n        await self._check_initialized()\n        for message_type, callbacks in message_callback_map.items():\n            if isinstance(callbacks, list):\n                for callback in callbacks:\n                    await self._subscribe_internal(message_type, callback)\n            else:\n                await self._subscribe_internal(message_type, callbacks)\n        # TODO: HACK: This is a hack to ensure that the subscriptions are registered\n        # since we do not have any confirmation from the server that the subscriptions\n        # are registered, yet.\n        await asyncio.sleep(0.1)\n\n    async def subscribe(\n        self, message_type: MessageTypeT, callback: Callable[[Message], Any]\n    ) -&gt; None:\n        \"\"\"Subscribe to a message_type.\n\n        Args:\n            message_type: MessageTypeT to subscribe to\n            callback: Function to call when a message is received (receives Message object)\n\n        Raises:\n            Exception if subscription was not successful, None otherwise\n        \"\"\"\n        await self._check_initialized()\n        await self._subscribe_internal(message_type, callback)\n        # TODO: HACK: This is a hack to ensure that the subscriptions are registered\n        # since we do not have any confirmation from the server that the subscriptions\n        # are registered, yet.\n        await asyncio.sleep(0.1)\n\n    async def _subscribe_internal(\n        self, message_type: MessageTypeT, callback: Callable[[Message], Any]\n    ) -&gt; None:\n        \"\"\"Subscribe to a message_type.\n\n        Args:\n            message_type: MessageTypeT to subscribe to\n            callback: Function to call when a message is received (receives Message object)\n        \"\"\"\n        try:\n            # Only subscribe to message_type if this is the first callback for this type\n            if message_type not in self._subscribers:\n                self.socket.subscribe(message_type.encode())\n                self._subscribers[message_type] = []\n\n            # Register callback\n            self._subscribers[message_type].append(callback)\n\n            self.trace(\n                lambda: f\"Subscribed to message_type: {message_type}, {self._subscribers[message_type]}\"\n            )\n\n        except Exception as e:\n            self.exception(f\"Exception subscribing to message_type {message_type}: {e}\")\n            raise CommunicationError(\n                f\"Failed to subscribe to message_type {message_type}: {e}\",\n            ) from e\n\n    async def _handle_message(self, topic_bytes: bytes, message_bytes: bytes) -&gt; None:\n        \"\"\"Handle a message from a subscribed message_type.\"\"\"\n        message_type = topic_bytes.decode()\n        message_json = message_bytes.decode()\n        self.trace(\n            lambda: f\"Received message from message_type: '{message_type}', message: {message_json}\"\n        )\n\n        # Targeted messages are in the format \"message_type.&lt;target&gt;\"\n        if \".\" in message_type:\n            # grab the first part which is the message type\n            message_type = message_type.split(\".\")[0]\n\n        if message_type == MessageType.COMMAND:\n            message = CommandMessage.from_json(message_json)\n        elif message_type == MessageType.COMMAND_RESPONSE:\n            message = CommandResponse.from_json(message_json)\n        else:\n            message = Message.from_json_with_type(message_type, message_json)\n\n        # Call callbacks with the parsed message object\n        if message_type in self._subscribers:\n            with contextlib.suppress(Exception):  # Ignore errors, they will get logged\n                await call_all_functions(self._subscribers[message_type], message)\n\n    @background_task(immediate=True, interval=None)\n    async def _sub_receiver(self) -&gt; None:\n        \"\"\"Background task for receiving messages from subscribed topics.\n\n        This method is a coroutine that will run indefinitely until the client is\n        shutdown. It will wait for messages from the socket and handle them.\n        \"\"\"\n        while not self.stop_requested:\n            try:\n                (\n                    topic_bytes,\n                    message_bytes,\n                ) = await self.socket.recv_multipart()\n\n                self.execute_async(self._handle_message(topic_bytes, message_bytes))\n\n            except zmq.Again:\n                self.debug(f\"Sub client {self.client_id} receiver task timed out\")\n                await yield_to_event_loop()\n            except Exception as e:\n                self.exception(\n                    f\"Exception receiving message from subscription: {e}, {type(e)}\"\n                )\n                await yield_to_event_loop()\n            except (asyncio.CancelledError, zmq.ContextTerminated):\n                self.debug(f\"Sub client {self.client_id} receiver task cancelled\")\n                break\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.sub_client.ZMQSubClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Subscriber class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/sub_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Subscriber class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.SUB, address, bind, socket_ops, **kwargs)\n\n    self._subscribers: dict[MessageTypeT, list[Callable[[Message], Any]]] = {}\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.sub_client.ZMQSubClient.subscribe","title":"<code>subscribe(message_type, callback)</code>  <code>async</code>","text":"<p>Subscribe to a message_type.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>MessageTypeT</code> <p>MessageTypeT to subscribe to</p> required <code>callback</code> <code>Callable[[Message], Any]</code> <p>Function to call when a message is received (receives Message object)</p> required Source code in <code>aiperf/common/comms/zmq/sub_client.py</code> <pre><code>async def subscribe(\n    self, message_type: MessageTypeT, callback: Callable[[Message], Any]\n) -&gt; None:\n    \"\"\"Subscribe to a message_type.\n\n    Args:\n        message_type: MessageTypeT to subscribe to\n        callback: Function to call when a message is received (receives Message object)\n\n    Raises:\n        Exception if subscription was not successful, None otherwise\n    \"\"\"\n    await self._check_initialized()\n    await self._subscribe_internal(message_type, callback)\n    # TODO: HACK: This is a hack to ensure that the subscriptions are registered\n    # since we do not have any confirmation from the server that the subscriptions\n    # are registered, yet.\n    await asyncio.sleep(0.1)\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.sub_client.ZMQSubClient.subscribe_all","title":"<code>subscribe_all(message_callback_map)</code>  <code>async</code>","text":"<p>Subscribe to all message_types in the map. For each MessageType, a single callback or a list of callbacks can be provided.</p> Source code in <code>aiperf/common/comms/zmq/sub_client.py</code> <pre><code>async def subscribe_all(\n    self,\n    message_callback_map: dict[\n        MessageTypeT,\n        Callable[[Message], Any] | list[Callable[[Message], Any]],\n    ],\n) -&gt; None:\n    \"\"\"Subscribe to all message_types in the map. For each MessageType, a single\n    callback or a list of callbacks can be provided.\"\"\"\n    await self._check_initialized()\n    for message_type, callbacks in message_callback_map.items():\n        if isinstance(callbacks, list):\n            for callback in callbacks:\n                await self._subscribe_internal(message_type, callback)\n        else:\n            await self._subscribe_internal(message_type, callbacks)\n    # TODO: HACK: This is a hack to ensure that the subscriptions are registered\n    # since we do not have any confirmation from the server that the subscriptions\n    # are registered, yet.\n    await asyncio.sleep(0.1)\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqzmq_base_client","title":"aiperf.common.comms.zmq.zmq_base_client","text":""},{"location":"api/#aiperf.common.comms.zmq.zmq_base_client.BaseZMQClient","title":"<code>BaseZMQClient</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code></p> <p>Base class for all ZMQ clients. It can be used as-is to create a new ZMQ client, or it can be subclassed to create specific ZMQ client functionality.</p> <p>It inherits from the :class:<code>AIPerfLifecycleMixin</code>, allowing derived classes to implement specific hooks.</p> Source code in <code>aiperf/common/comms/zmq/zmq_base_client.py</code> <pre><code>class BaseZMQClient(AIPerfLifecycleMixin):\n    \"\"\"Base class for all ZMQ clients. It can be used as-is to create a new ZMQ client,\n    or it can be subclassed to create specific ZMQ client functionality.\n\n    It inherits from the :class:`AIPerfLifecycleMixin`, allowing derived\n    classes to implement specific hooks.\n    \"\"\"\n\n    def __init__(\n        self,\n        socket_type: zmq.SocketType,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        client_id: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Base class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to BIND or CONNECT the socket.\n            socket_type (SocketType): The type of ZMQ socket (eg. PUB, SUB, ROUTER, DEALER, etc.).\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        self.context: zmq.asyncio.Context = zmq.asyncio.Context.instance()\n        self.socket_type: zmq.SocketType = socket_type\n        self.socket: zmq.asyncio.Socket = self.context.socket(self.socket_type)\n        self.address: str = address\n        self.bind: bool = bind\n        self.socket_ops: dict = socket_ops or {}\n        self.client_id: str = (\n            client_id\n            or f\"{self.socket_type.name.lower()}_client_{uuid.uuid4().hex[:8]}\"\n        )\n        super().__init__(id=self.client_id, **kwargs)\n        self.trace(lambda: f\"ZMQ client __init__: {self.client_id}\")\n\n    async def _check_initialized(self) -&gt; None:\n        \"\"\"Raise an exception if the socket is not initialized or closed.\"\"\"\n        if self.stop_requested:\n            raise asyncio.CancelledError(\"Socket was stopped\")\n        if not self.socket:\n            raise NotInitializedError(\"Socket not initialized or closed\")\n\n    @property\n    def socket_type_name(self) -&gt; str:\n        \"\"\"Get the name of the socket type.\"\"\"\n        return self.socket_type.name\n\n    @on_init\n    async def _initialize_socket(self) -&gt; None:\n        \"\"\"Initialize the communication.\n\n        This method will:\n        - Create the zmq socket\n        - Bind or connect the socket to the address\n        - Set the socket options\n        - Run the AIPerfHook.ON_INIT hooks\n        \"\"\"\n        try:\n            self.debug(\n                lambda: f\"ZMQ {self.socket_type_name} socket initialized, try {'BIND' if self.bind else 'CONNECT'} to {self.address} ({self.client_id})\"\n            )\n\n            if self.bind:\n                self.socket.bind(self.address)\n            else:\n                self.socket.connect(self.address)\n\n            # Set default timeouts\n            self.socket.setsockopt(zmq.RCVTIMEO, ZMQSocketDefaults.RCVTIMEO)\n            self.socket.setsockopt(zmq.SNDTIMEO, ZMQSocketDefaults.SNDTIMEO)\n\n            # Set performance-oriented socket options\n            self.socket.setsockopt(zmq.TCP_KEEPALIVE, ZMQSocketDefaults.TCP_KEEPALIVE)\n            self.socket.setsockopt(\n                zmq.TCP_KEEPALIVE_IDLE, ZMQSocketDefaults.TCP_KEEPALIVE_IDLE\n            )\n            self.socket.setsockopt(\n                zmq.TCP_KEEPALIVE_INTVL, ZMQSocketDefaults.TCP_KEEPALIVE_INTVL\n            )\n            self.socket.setsockopt(\n                zmq.TCP_KEEPALIVE_CNT, ZMQSocketDefaults.TCP_KEEPALIVE_CNT\n            )\n            self.socket.setsockopt(zmq.IMMEDIATE, ZMQSocketDefaults.IMMEDIATE)\n            self.socket.setsockopt(zmq.LINGER, ZMQSocketDefaults.LINGER)\n\n            # Set additional socket options requested by the caller\n            for key, val in self.socket_ops.items():\n                self.socket.setsockopt(key, val)\n\n            self.debug(\n                lambda: f\"ZMQ {self.socket_type_name} socket {'BOUND' if self.bind else 'CONNECTED'} to {self.address} ({self.client_id})\"\n            )\n\n        except Exception as e:\n            raise InitializationError(f\"Failed to initialize ZMQ socket: {e}\") from e\n\n    @on_stop\n    async def _shutdown_socket(self) -&gt; None:\n        \"\"\"Shutdown the socket.\"\"\"\n        try:\n            if self.socket:\n                self.socket.close()\n        except zmq.ContextTerminated:\n            self.debug(\n                lambda: f\"ZMQ context already terminated, skipping socket close ({self.client_id})\"\n            )\n            return\n        except Exception as e:\n            self.exception(\n                f\"Uncaught exception shutting down ZMQ socket: {e} ({self.client_id})\"\n            )\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_base_client.BaseZMQClient.socket_type_name","title":"<code>socket_type_name</code>  <code>property</code>","text":"<p>Get the name of the socket type.</p>"},{"location":"api/#aiperf.common.comms.zmq.zmq_base_client.BaseZMQClient.__init__","title":"<code>__init__(socket_type, address, bind, socket_ops=None, client_id=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Base class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to BIND or CONNECT the socket.</p> required <code>socket_type</code> <code>SocketType</code> <p>The type of ZMQ socket (eg. PUB, SUB, ROUTER, DEALER, etc.).</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/zmq_base_client.py</code> <pre><code>def __init__(\n    self,\n    socket_type: zmq.SocketType,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    client_id: str | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Base class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to BIND or CONNECT the socket.\n        socket_type (SocketType): The type of ZMQ socket (eg. PUB, SUB, ROUTER, DEALER, etc.).\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    self.context: zmq.asyncio.Context = zmq.asyncio.Context.instance()\n    self.socket_type: zmq.SocketType = socket_type\n    self.socket: zmq.asyncio.Socket = self.context.socket(self.socket_type)\n    self.address: str = address\n    self.bind: bool = bind\n    self.socket_ops: dict = socket_ops or {}\n    self.client_id: str = (\n        client_id\n        or f\"{self.socket_type.name.lower()}_client_{uuid.uuid4().hex[:8]}\"\n    )\n    super().__init__(id=self.client_id, **kwargs)\n    self.trace(lambda: f\"ZMQ client __init__: {self.client_id}\")\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqzmq_comms","title":"aiperf.common.comms.zmq.zmq_comms","text":""},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.BaseZMQCommunication","title":"<code>BaseZMQCommunication</code>","text":"<p>               Bases: <code>BaseCommunication</code>, <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> <p>ZeroMQ-based implementation of the CommunicationProtocol.</p> <p>Uses ZeroMQ for publish/subscribe, request/reply, and pull/push patterns to facilitate communication between AIPerf components.</p> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>@implements_protocol(CommunicationProtocol)\nclass BaseZMQCommunication(BaseCommunication, AIPerfLoggerMixin, ABC):\n    \"\"\"ZeroMQ-based implementation of the CommunicationProtocol.\n\n    Uses ZeroMQ for publish/subscribe, request/reply, and pull/push patterns to\n    facilitate communication between AIPerf components.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: BaseZMQCommunicationConfig,\n    ) -&gt; None:\n        super().__init__()\n        self.config = config\n\n        self.context = zmq.asyncio.Context.instance()\n        self._clients_cache: dict[\n            tuple[CommClientType, CommAddressType, bool], CommunicationClientProtocol\n        ] = {}\n\n        self.debug(f\"ZMQ communication using protocol: {type(self.config).__name__}\")\n\n    def get_address(self, address_type: CommAddressType) -&gt; str:\n        \"\"\"Get the actual address based on the address type from the config.\"\"\"\n        if isinstance(address_type, CommAddress):\n            return self.config.get_address(address_type)\n        return address_type\n\n    def create_client(\n        self,\n        client_type: CommClientType,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n        **kwargs,\n    ) -&gt; CommunicationClientProtocol:\n        \"\"\"Create a communication client for a given client type and address.\n\n        Args:\n            client_type: The type of client to create.\n            address: The type of address to use when looking up in the communication config, or the address itself.\n            bind: Whether to bind or connect the socket.\n            socket_ops: Additional socket options to set.\n            max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n        \"\"\"\n        if (client_type, address, bind) in self._clients_cache:\n            return self._clients_cache[(client_type, address, bind)]\n\n        if self.state != LifecycleState.CREATED:\n            # We require the clients to be created before the communication class is initialized.\n            # This is because this class manages the lifecycle of the clients of as well.\n            raise InvalidStateError(\n                f\"Communication clients must be created before the {self.__class__.__name__} \"\n                f\"class is initialized: {self.state!r}\"\n            )\n\n        client = CommunicationClientFactory.create_instance(\n            client_type,\n            address=self.get_address(address),\n            bind=bind,\n            socket_ops=socket_ops,\n            max_pull_concurrency=max_pull_concurrency,\n            **kwargs,\n        )\n\n        self._clients_cache[(client_type, address, bind)] = client\n        self.attach_child_lifecycle(client)\n        return client\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.BaseZMQCommunication.create_client","title":"<code>create_client(client_type, address, bind=False, socket_ops=None, max_pull_concurrency=None, **kwargs)</code>","text":"<p>Create a communication client for a given client type and address.</p> <p>Parameters:</p> Name Type Description Default <code>client_type</code> <code>CommClientType</code> <p>The type of client to create.</p> required <code>address</code> <code>CommAddressType</code> <p>The type of address to use when looking up in the communication config, or the address itself.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> <code>False</code> <code>socket_ops</code> <code>dict | None</code> <p>Additional socket options to set.</p> <code>None</code> <code>max_pull_concurrency</code> <code>int | None</code> <p>The maximum number of concurrent pull requests to allow. (Only used for pull clients)</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>def create_client(\n    self,\n    client_type: CommClientType,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n    **kwargs,\n) -&gt; CommunicationClientProtocol:\n    \"\"\"Create a communication client for a given client type and address.\n\n    Args:\n        client_type: The type of client to create.\n        address: The type of address to use when looking up in the communication config, or the address itself.\n        bind: Whether to bind or connect the socket.\n        socket_ops: Additional socket options to set.\n        max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n    \"\"\"\n    if (client_type, address, bind) in self._clients_cache:\n        return self._clients_cache[(client_type, address, bind)]\n\n    if self.state != LifecycleState.CREATED:\n        # We require the clients to be created before the communication class is initialized.\n        # This is because this class manages the lifecycle of the clients of as well.\n        raise InvalidStateError(\n            f\"Communication clients must be created before the {self.__class__.__name__} \"\n            f\"class is initialized: {self.state!r}\"\n        )\n\n    client = CommunicationClientFactory.create_instance(\n        client_type,\n        address=self.get_address(address),\n        bind=bind,\n        socket_ops=socket_ops,\n        max_pull_concurrency=max_pull_concurrency,\n        **kwargs,\n    )\n\n    self._clients_cache[(client_type, address, bind)] = client\n    self.attach_child_lifecycle(client)\n    return client\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.BaseZMQCommunication.get_address","title":"<code>get_address(address_type)</code>","text":"<p>Get the actual address based on the address type from the config.</p> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>def get_address(self, address_type: CommAddressType) -&gt; str:\n    \"\"\"Get the actual address based on the address type from the config.\"\"\"\n    if isinstance(address_type, CommAddress):\n        return self.config.get_address(address_type)\n    return address_type\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.ZMQIPCCommunication","title":"<code>ZMQIPCCommunication</code>","text":"<p>               Bases: <code>BaseZMQCommunication</code></p> <p>ZeroMQ-based implementation of the Communication interface using IPC transport.</p> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>@CommunicationFactory.register(CommunicationBackend.ZMQ_IPC)\n@implements_protocol(CommunicationProtocol)\nclass ZMQIPCCommunication(BaseZMQCommunication):\n    \"\"\"ZeroMQ-based implementation of the Communication interface using IPC transport.\"\"\"\n\n    def __init__(self, config: ZMQIPCConfig | None = None) -&gt; None:\n        \"\"\"Initialize ZMQ IPC communication.\n\n        Args:\n            config: ZMQIPCConfig object with configuration parameters\n        \"\"\"\n        super().__init__(config or ZMQIPCConfig())\n        # call after super init so that way self.config is set\n        self._setup_ipc_directory()\n\n    def _setup_ipc_directory(self) -&gt; None:\n        \"\"\"Create IPC socket directory if using IPC transport.\"\"\"\n        self._ipc_socket_dir = Path(self.config.path)\n        if not self._ipc_socket_dir.exists():\n            self.debug(\n                f\"IPC socket directory does not exist, creating: {self._ipc_socket_dir}\"\n            )\n            self._ipc_socket_dir.mkdir(parents=True, exist_ok=True)\n            self.debug(f\"Created IPC socket directory: {self._ipc_socket_dir}\")\n        else:\n            self.debug(f\"IPC socket directory already exists: {self._ipc_socket_dir}\")\n\n    @on_stop\n    def _cleanup_ipc_sockets(self) -&gt; None:\n        \"\"\"Clean up IPC socket files.\"\"\"\n        if self._ipc_socket_dir and self._ipc_socket_dir.exists():\n            # Remove all .ipc files in the directory\n            ipc_files = glob.glob(str(self._ipc_socket_dir / \"*.ipc\"))\n            for ipc_file in ipc_files:\n                try:\n                    if os.path.exists(ipc_file):\n                        os.unlink(ipc_file)\n                        self.debug(f\"Removed IPC socket file: {ipc_file}\")\n                except OSError as e:\n                    if e.errno != errno.ENOENT:\n                        self.warning(\n                            f\"Failed to remove IPC socket file {ipc_file}: {e}\"\n                        )\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.ZMQIPCCommunication.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize ZMQ IPC communication.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ZMQIPCConfig | None</code> <p>ZMQIPCConfig object with configuration parameters</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>def __init__(self, config: ZMQIPCConfig | None = None) -&gt; None:\n    \"\"\"Initialize ZMQ IPC communication.\n\n    Args:\n        config: ZMQIPCConfig object with configuration parameters\n    \"\"\"\n    super().__init__(config or ZMQIPCConfig())\n    # call after super init so that way self.config is set\n    self._setup_ipc_directory()\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.ZMQTCPCommunication","title":"<code>ZMQTCPCommunication</code>","text":"<p>               Bases: <code>BaseZMQCommunication</code></p> <p>ZeroMQ-based implementation of the Communication interface using TCP transport.</p> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>@CommunicationFactory.register(CommunicationBackend.ZMQ_TCP)\n@implements_protocol(CommunicationProtocol)\nclass ZMQTCPCommunication(BaseZMQCommunication):\n    \"\"\"ZeroMQ-based implementation of the Communication interface using TCP transport.\"\"\"\n\n    def __init__(self, config: ZMQTCPConfig | None = None) -&gt; None:\n        \"\"\"Initialize ZMQ TCP communication.\n\n        Args:\n            config: ZMQTCPTransportConfig object with configuration parameters\n        \"\"\"\n        super().__init__(config or ZMQTCPConfig())\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_comms.ZMQTCPCommunication.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize ZMQ TCP communication.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ZMQTCPConfig | None</code> <p>ZMQTCPTransportConfig object with configuration parameters</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/zmq_comms.py</code> <pre><code>def __init__(self, config: ZMQTCPConfig | None = None) -&gt; None:\n    \"\"\"Initialize ZMQ TCP communication.\n\n    Args:\n        config: ZMQTCPTransportConfig object with configuration parameters\n    \"\"\"\n    super().__init__(config or ZMQTCPConfig())\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqzmq_defaults","title":"aiperf.common.comms.zmq.zmq_defaults","text":""},{"location":"api/#aiperf.common.comms.zmq.zmq_defaults.ZMQSocketDefaults","title":"<code>ZMQSocketDefaults</code>","text":"<p>Default values for ZMQ sockets.</p> Source code in <code>aiperf/common/comms/zmq/zmq_defaults.py</code> <pre><code>class ZMQSocketDefaults:\n    \"\"\"Default values for ZMQ sockets.\"\"\"\n\n    # Socket Options\n    RCVTIMEO = 300000  # 5 minutes\n    SNDTIMEO = 300000  # 5 minutes\n    TCP_KEEPALIVE = 1\n    TCP_KEEPALIVE_IDLE = 60\n    TCP_KEEPALIVE_INTVL = 10\n    TCP_KEEPALIVE_CNT = 3\n    IMMEDIATE = 1  # Don't queue messages\n    LINGER = 0  # Don't wait on close\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqzmq_proxy_base","title":"aiperf.common.comms.zmq.zmq_proxy_base","text":""},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_base.BaseZMQProxy","title":"<code>BaseZMQProxy</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>A Base ZMQ Proxy class.</p> <ul> <li>Frontend and backend sockets forward messages bidirectionally<ul> <li>Frontend and Backend sockets both BIND</li> </ul> </li> <li>Multiple clients CONNECT to <code>frontend_address</code></li> <li>Multiple services CONNECT to <code>backend_address</code></li> <li>Control: Optional REP socket for proxy commands (start/stop/pause) - not implemented yet</li> <li>Monitoring: Optional PUB socket that broadcasts copies of all forwarded messages - not implemented yet</li> <li>Proxy runs in separate thread to avoid blocking main event loop</li> </ul> Source code in <code>aiperf/common/comms/zmq/zmq_proxy_base.py</code> <pre><code>class BaseZMQProxy(AIPerfLifecycleMixin, ABC):\n    \"\"\"\n    A Base ZMQ Proxy class.\n\n    - Frontend and backend sockets forward messages bidirectionally\n        - Frontend and Backend sockets both BIND\n    - Multiple clients CONNECT to `frontend_address`\n    - Multiple services CONNECT to `backend_address`\n    - Control: Optional REP socket for proxy commands (start/stop/pause) - not implemented yet\n    - Monitoring: Optional PUB socket that broadcasts copies of all forwarded messages - not implemented yet\n    - Proxy runs in separate thread to avoid blocking main event loop\n    \"\"\"\n\n    def __init__(\n        self,\n        frontend_socket_class: type[BaseZMQClient],\n        backend_socket_class: type[BaseZMQClient],\n        zmq_proxy_config: BaseZMQProxyConfig,\n        socket_ops: dict | None = None,\n        proxy_uuid: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the ZMQ Proxy. This is a base class for all ZMQ Proxies.\n\n        Args:\n            frontend_socket_class (type[BaseZMQClient]): The frontend socket class.\n            backend_socket_class (type[BaseZMQClient]): The backend socket class.\n            zmq_proxy_config (BaseZMQProxyConfig): The ZMQ proxy configuration.\n            socket_ops (dict, optional): Additional socket options to set.\n            proxy_uuid (str, optional): An optional UUID for the proxy instance. If not provided,\n                a new UUID will be generated. This is useful for tracing and debugging purposes.\n        \"\"\"\n\n        self.proxy_uuid = proxy_uuid or uuid.uuid4().hex[:8]\n        self.proxy_id = f\"{self.__class__.__name__.lower()}_{self.proxy_uuid}\"\n        super().__init__()\n        self.context = zmq.asyncio.Context.instance()\n        self.socket_ops = socket_ops\n\n        self.monitor_task: asyncio.Task | None = None\n        self.proxy_task: asyncio.Task | None = None\n        self.control_client: ProxySocketClient | None = None\n        self.capture_client: ProxySocketClient | None = None\n\n        self.frontend_address = zmq_proxy_config.frontend_address\n        self.backend_address = zmq_proxy_config.backend_address\n        self.control_address = zmq_proxy_config.control_address\n        self.capture_address = zmq_proxy_config.capture_address\n\n        self.debug(\n            lambda: f\"Proxy Initializing - Frontend: {self.frontend_address}, Backend: {self.backend_address}\"\n        )\n\n        self.backend_socket = backend_socket_class(\n            address=self.backend_address,\n            socket_ops=self.socket_ops,\n            proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n        )  # type: ignore\n\n        self.frontend_socket = frontend_socket_class(\n            address=self.frontend_address,\n            socket_ops=self.socket_ops,\n            proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n        )  # type: ignore\n\n        if self.control_address:\n            self.debug(lambda: f\"Proxy Control - Address: {self.control_address}\")\n            self.control_client = ProxySocketClient(\n                socket_type=SocketType.REP,\n                address=self.control_address,\n                socket_ops=self.socket_ops,\n                end_type=ProxyEndType.Control,\n                proxy_uuid=self.proxy_uuid,\n            )\n\n        if self.capture_address:\n            self.debug(lambda: f\"Proxy Capture - Address: {self.capture_address}\")\n            self.capture_client = ProxySocketClient(\n                socket_type=SocketType.PUB,\n                address=self.capture_address,\n                socket_ops=self.socket_ops,\n                end_type=ProxyEndType.Capture,\n                proxy_uuid=self.proxy_uuid,\n            )\n\n    @classmethod\n    @abstractmethod\n    def from_config(\n        cls,\n        config: BaseZMQProxyConfig | None,\n        socket_ops: dict | None = None,\n    ) -&gt; \"BaseZMQProxy | None\":\n        \"\"\"Create a BaseZMQProxy from a BaseZMQProxyConfig, or None if not provided.\"\"\"\n        ...\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize and start the BaseZMQProxy.\"\"\"\n        self.debug(\"Proxy Initializing Sockets...\")\n        self.debug(\n            lambda: f\"Frontend {self.frontend_socket.socket_type.name} socket binding to: {self.frontend_address} (for {self.backend_socket.socket_type.name} clients)\"\n        )\n        self.debug(\n            lambda: f\"Backend {self.backend_socket.socket_type.name} socket binding to: {self.backend_address} (for {self.frontend_socket.socket_type.name} services)\"\n        )\n        if hasattr(self.backend_socket, \"proxy_id\"):\n            self.debug(\n                lambda: f\"Backend socket identity: {self.backend_socket.proxy_id}\"\n            )\n\n        try:\n            exceptions = await asyncio.gather(\n                self.backend_socket.initialize(),\n                self.frontend_socket.initialize(),\n                *[\n                    client.initialize()\n                    for client in [self.control_client, self.capture_client]\n                    if client\n                ],\n                return_exceptions=True,\n            )\n            if any(exceptions):\n                self.exception(f\"Proxy Socket Initialization Failed: {exceptions}\")\n                raise\n\n            self.debug(\"Proxy Sockets Initialized Successfully\")\n\n            if self.control_client:\n                self.debug(lambda: f\"Control socket bound to: {self.control_address}\")\n            if self.capture_client:\n                self.debug(lambda: f\"Capture socket bound to: {self.capture_address}\")\n\n        except Exception as e:\n            self.exception(f\"Proxy Socket Initialization Failed: {e}\")\n            raise\n\n    @on_start\n    async def _start_proxy(self) -&gt; None:\n        \"\"\"Start the Base ZMQ Proxy.\n\n        This method starts the proxy and waits for it to complete asynchronously.\n        The proxy forwards messages between the frontend and backend sockets.\n\n        Raises:\n            ProxyError: If the proxy produces an error.\n        \"\"\"\n        self.debug(\"Starting Proxy...\")\n\n        self.proxy_task = asyncio.create_task(\n            asyncio.to_thread(\n                zmq.proxy_steerable,\n                self.frontend_socket.socket,\n                self.backend_socket.socket,\n                capture=self.capture_client.socket if self.capture_client else None,\n                control=self.control_client.socket if self.control_client else None,\n            )\n        )\n\n    @background_task(immediate=True, interval=None)\n    async def _monitor_messages(self) -&gt; None:\n        \"\"\"Monitor messages flowing through the proxy via the capture socket.\"\"\"\n        if not self.capture_client or not self.capture_address:\n            self.debug(\"Proxy Monitor Not Enabled\")\n            return\n\n        self.debug(\n            lambda: f\"Proxy Monitor Starting - Capture Address: {self.capture_address}\"\n        )\n\n        capture_socket = self.context.socket(SocketType.SUB)\n        capture_socket.connect(self.capture_address)\n        self.debug(\n            lambda: f\"Proxy Monitor Connected to Capture Address: {self.capture_address}\"\n        )\n        capture_socket.setsockopt(zmq.SUBSCRIBE, b\"\")  # Subscribe to all messages\n        self.debug(\"Proxy Monitor Subscribed to all messages\")\n\n        try:\n            while not self.stop_requested:\n                recv_msg = await capture_socket.recv_multipart()\n                self.debug(lambda msg=recv_msg: f\"Proxy Monitor Received: {msg}\")\n        except Exception as e:\n            self.exception(f\"Proxy Monitor Error - {e}\")\n            raise\n        except asyncio.CancelledError:\n            return\n        finally:\n            capture_socket.close()\n\n    @on_stop\n    async def _stop_proxy(self) -&gt; None:\n        \"\"\"Shutdown the BaseZMQProxy.\"\"\"\n        self.debug(\"Proxy Stopping...\")\n        if self.proxy_task:\n            self.proxy_task.cancel()\n            self.proxy_task = None\n        await self.frontend_socket.stop()\n        await self.backend_socket.stop()\n        if self.control_client:\n            await self.control_client.stop()\n        if self.capture_client:\n            await self.capture_client.stop()\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_base.BaseZMQProxy.__init__","title":"<code>__init__(frontend_socket_class, backend_socket_class, zmq_proxy_config, socket_ops=None, proxy_uuid=None)</code>","text":"<p>Initialize the ZMQ Proxy. This is a base class for all ZMQ Proxies.</p> <p>Parameters:</p> Name Type Description Default <code>frontend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The frontend socket class.</p> required <code>backend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The backend socket class.</p> required <code>zmq_proxy_config</code> <code>BaseZMQProxyConfig</code> <p>The ZMQ proxy configuration.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> <code>proxy_uuid</code> <code>str</code> <p>An optional UUID for the proxy instance. If not provided, a new UUID will be generated. This is useful for tracing and debugging purposes.</p> <code>None</code> Source code in <code>aiperf/common/comms/zmq/zmq_proxy_base.py</code> <pre><code>def __init__(\n    self,\n    frontend_socket_class: type[BaseZMQClient],\n    backend_socket_class: type[BaseZMQClient],\n    zmq_proxy_config: BaseZMQProxyConfig,\n    socket_ops: dict | None = None,\n    proxy_uuid: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the ZMQ Proxy. This is a base class for all ZMQ Proxies.\n\n    Args:\n        frontend_socket_class (type[BaseZMQClient]): The frontend socket class.\n        backend_socket_class (type[BaseZMQClient]): The backend socket class.\n        zmq_proxy_config (BaseZMQProxyConfig): The ZMQ proxy configuration.\n        socket_ops (dict, optional): Additional socket options to set.\n        proxy_uuid (str, optional): An optional UUID for the proxy instance. If not provided,\n            a new UUID will be generated. This is useful for tracing and debugging purposes.\n    \"\"\"\n\n    self.proxy_uuid = proxy_uuid or uuid.uuid4().hex[:8]\n    self.proxy_id = f\"{self.__class__.__name__.lower()}_{self.proxy_uuid}\"\n    super().__init__()\n    self.context = zmq.asyncio.Context.instance()\n    self.socket_ops = socket_ops\n\n    self.monitor_task: asyncio.Task | None = None\n    self.proxy_task: asyncio.Task | None = None\n    self.control_client: ProxySocketClient | None = None\n    self.capture_client: ProxySocketClient | None = None\n\n    self.frontend_address = zmq_proxy_config.frontend_address\n    self.backend_address = zmq_proxy_config.backend_address\n    self.control_address = zmq_proxy_config.control_address\n    self.capture_address = zmq_proxy_config.capture_address\n\n    self.debug(\n        lambda: f\"Proxy Initializing - Frontend: {self.frontend_address}, Backend: {self.backend_address}\"\n    )\n\n    self.backend_socket = backend_socket_class(\n        address=self.backend_address,\n        socket_ops=self.socket_ops,\n        proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n    )  # type: ignore\n\n    self.frontend_socket = frontend_socket_class(\n        address=self.frontend_address,\n        socket_ops=self.socket_ops,\n        proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n    )  # type: ignore\n\n    if self.control_address:\n        self.debug(lambda: f\"Proxy Control - Address: {self.control_address}\")\n        self.control_client = ProxySocketClient(\n            socket_type=SocketType.REP,\n            address=self.control_address,\n            socket_ops=self.socket_ops,\n            end_type=ProxyEndType.Control,\n            proxy_uuid=self.proxy_uuid,\n        )\n\n    if self.capture_address:\n        self.debug(lambda: f\"Proxy Capture - Address: {self.capture_address}\")\n        self.capture_client = ProxySocketClient(\n            socket_type=SocketType.PUB,\n            address=self.capture_address,\n            socket_ops=self.socket_ops,\n            end_type=ProxyEndType.Capture,\n            proxy_uuid=self.proxy_uuid,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_base.BaseZMQProxy.from_config","title":"<code>from_config(config, socket_ops=None)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create a BaseZMQProxy from a BaseZMQProxyConfig, or None if not provided.</p> Source code in <code>aiperf/common/comms/zmq/zmq_proxy_base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(\n    cls,\n    config: BaseZMQProxyConfig | None,\n    socket_ops: dict | None = None,\n) -&gt; \"BaseZMQProxy | None\":\n    \"\"\"Create a BaseZMQProxy from a BaseZMQProxyConfig, or None if not provided.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_base.ProxySocketClient","title":"<code>ProxySocketClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>A ZMQ Proxy socket client class that extends BaseZMQClient.</p> <p>This class is used to create proxy sockets for the frontend, backend, capture, and control endpoint types of a ZMQ Proxy.</p> Source code in <code>aiperf/common/comms/zmq/zmq_proxy_base.py</code> <pre><code>class ProxySocketClient(BaseZMQClient):\n    \"\"\"A ZMQ Proxy socket client class that extends BaseZMQClient.\n\n    This class is used to create proxy sockets for the frontend, backend, capture, and control\n    endpoint types of a ZMQ Proxy.\n    \"\"\"\n\n    def __init__(\n        self,\n        socket_type: SocketType,\n        address: str,\n        end_type: ProxyEndType,\n        socket_ops: dict | None = None,\n        proxy_uuid: str | None = None,\n    ) -&gt; None:\n        self.client_id = f\"proxy_{end_type}_{socket_type.name.lower()}_{proxy_uuid or uuid.uuid4().hex[:8]}\"\n        super().__init__(\n            socket_type,\n            address,\n            bind=True,\n            socket_ops=socket_ops,\n            client_id=self.client_id,\n        )\n        self.debug(\n            lambda: f\"ZMQ Proxy {end_type.name} {socket_type.name} - Address: {address}\"\n        )\n</code></pre>"},{"location":"api/#aiperfcommoncommszmqzmq_proxy_sockets","title":"aiperf.common.comms.zmq.zmq_proxy_sockets","text":""},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_sockets.ZMQDealerRouterProxy","title":"<code>ZMQDealerRouterProxy = define_proxy_class(ZMQProxyType.DEALER_ROUTER, create_proxy_socket_class(SocketType.ROUTER, ProxyEndType.Frontend), create_proxy_socket_class(SocketType.DEALER, ProxyEndType.Backend))</code>  <code>module-attribute</code>","text":"<p>A ROUTER socket for the proxy's frontend and a DEALER socket for the proxy's backend.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  DEALER   \u2502&lt;\u2500\u2500\u2500&gt;\u2502              PROXY               \u2502&lt;\u2500\u2500\u2500\u2500&gt;\u2502  ROUTER   \u2502 \u2502  Client 1 \u2502     \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502 Service 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 \u2502  ROUTER  \u2502&lt;\u2500\u2500\u2500\u2500\u2500&gt; \u2502  DEALER  \u2502 \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502 \u2502 Frontend \u2502        \u2502 Backend  \u2502 \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  DEALER   \u2502&lt;\u2500\u2500\u2500&gt;\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502&lt;\u2500\u2500\u2500\u2500&gt;\u2502  ROUTER   \u2502 \u2502  Client N \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 Service N \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>The ROUTER frontend socket receives messages from DEALER clients and forwards them through the proxy to ROUTER services. The ZMQ proxy handles the message routing automatically.</p> <p>The DEALER backend socket receives messages from ROUTER services and forwards them through the proxy to DEALER clients. The ZMQ proxy handles the message routing automatically.</p> <p>CRITICAL: This socket must NOT have an identity when used in a proxy configuration, as it needs to be transparent to preserve routing envelopes for proper response forwarding back to original DEALER clients.</p>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_sockets.ZMQPushPullProxy","title":"<code>ZMQPushPullProxy = define_proxy_class(ZMQProxyType.PUSH_PULL, create_proxy_socket_class(SocketType.PULL, ProxyEndType.Frontend), create_proxy_socket_class(SocketType.PUSH, ProxyEndType.Backend))</code>  <code>module-attribute</code>","text":"<p>A PULL socket for the proxy's frontend and a PUSH socket for the proxy's backend.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   PUSH    \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502              PROXY              \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502   PULL    \u2502 \u2502  Client 1 \u2502      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502 Service 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 \u2502   PULL   \u2502\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502   PUSH   \u2502 \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 \u2502 Frontend \u2502       \u2502 Backend  \u2502 \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   PUSH    \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502   PULL    \u2502 \u2502  Client N \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 Service N \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>The PULL frontend socket receives messages from PUSH clients and forwards them through the proxy to PUSH services. The ZMQ proxy handles the message routing automatically.</p> <p>The PUSH backend socket forwards messages from the proxy to PULL services. The ZMQ proxy handles the message routing automatically.</p>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_sockets.ZMQXPubXSubProxy","title":"<code>ZMQXPubXSubProxy = define_proxy_class(ZMQProxyType.XPUB_XSUB, create_proxy_socket_class(SocketType.XSUB, ProxyEndType.Frontend), create_proxy_socket_class(SocketType.XPUB, ProxyEndType.Backend))</code>  <code>module-attribute</code>","text":"<p>An XSUB socket for the proxy's frontend and an XPUB socket for the proxy's backend.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUB    \u2502\u2500\u2500\u2500&gt;\u2502              PROXY              \u2502\u2500\u2500\u2500&gt;\u2502    SUB    \u2502 \u2502  Client 1 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 Service 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u2502   XSUB   \u2502\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502   XPUB   \u2502 \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 \u2502 Frontend \u2502       \u2502 Backend  \u2502 \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUB    \u2502\u2500\u2500\u2500&gt;\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2500\u2500\u2500&gt;\u2502    SUB    \u2502 \u2502  Client N \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Service N \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>The XSUB frontend socket receives messages from PUB clients and forwards them through the proxy to XPUB services. The ZMQ proxy handles the message routing automatically.</p> <p>The XPUB backend socket forwards messages from the proxy to SUB services. The ZMQ proxy handles the message routing automatically.</p>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_sockets.create_proxy_socket_class","title":"<code>create_proxy_socket_class(socket_type, end_type)</code>","text":"<p>Create a proxy socket class using the specified socket type. This is used to reduce the boilerplate code required to create a ZMQ Proxy class.</p> Source code in <code>aiperf/common/comms/zmq/zmq_proxy_sockets.py</code> <pre><code>def create_proxy_socket_class(\n    socket_type: SocketType, end_type: ProxyEndType\n) -&gt; type[BaseZMQClient]:\n    \"\"\"Create a proxy socket class using the specified socket type. This is used to\n    reduce the boilerplate code required to create a ZMQ Proxy class.\n    \"\"\"\n\n    class_name = f\"ZMQProxy{end_type.name}Socket{socket_type.name}\"\n\n    class ProxySocket(ProxySocketClient):\n        \"\"\"A ZMQ Proxy socket class with a specific socket type.\"\"\"\n\n        def __init__(\n            self,\n            address: str,\n            socket_ops: dict | None = None,\n            proxy_uuid: str | None = None,\n        ):\n            \"\"\"Initialize the ZMQ Proxy socket class.\"\"\"\n\n            super().__init__(\n                socket_type,\n                address,\n                end_type=end_type,\n                socket_ops=socket_ops,\n                proxy_uuid=proxy_uuid,\n            )\n\n    # Dynamically set the class name and qualname based on the socket and end type\n    ProxySocket.__name__ = class_name\n    ProxySocket.__qualname__ = class_name\n    ProxySocket.__doc__ = f\"A ZMQ Proxy {end_type.name} socket implementation.\"\n    return ProxySocket\n</code></pre>"},{"location":"api/#aiperf.common.comms.zmq.zmq_proxy_sockets.define_proxy_class","title":"<code>define_proxy_class(proxy_type, frontend_socket_class, backend_socket_class)</code>","text":"<p>This function reduces the boilerplate code required to create a ZMQ Proxy class. It will generate a ZMQ Proxy class and register it with the ZMQProxyFactory.</p> <p>Parameters:</p> Name Type Description Default <code>proxy_type</code> <code>ZMQProxyType</code> <p>The type of proxy to generate.</p> required <code>frontend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The class of the frontend socket.</p> required <code>backend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The class of the backend socket.</p> required Source code in <code>aiperf/common/comms/zmq/zmq_proxy_sockets.py</code> <pre><code>def define_proxy_class(\n    proxy_type: ZMQProxyType,\n    frontend_socket_class: type[BaseZMQClient],\n    backend_socket_class: type[BaseZMQClient],\n) -&gt; type[BaseZMQProxy]:\n    \"\"\"This function reduces the boilerplate code required to create a ZMQ Proxy class.\n    It will generate a ZMQ Proxy class and register it with the ZMQProxyFactory.\n\n    Args:\n        proxy_type: The type of proxy to generate.\n        frontend_socket_class: The class of the frontend socket.\n        backend_socket_class: The class of the backend socket.\n    \"\"\"\n\n    class ZMQProxy(BaseZMQProxy):\n        \"\"\"\n        A Generated ZMQ Proxy class.\n\n        This class is responsible for creating the ZMQ proxy that forwards messages\n        between frontend and backend sockets.\n        \"\"\"\n\n        def __init__(\n            self,\n            zmq_proxy_config: BaseZMQProxyConfig,\n            socket_ops: dict | None = None,\n        ) -&gt; None:\n            super().__init__(\n                frontend_socket_class=frontend_socket_class,\n                backend_socket_class=backend_socket_class,\n                zmq_proxy_config=zmq_proxy_config,\n                socket_ops=socket_ops,\n            )\n\n        @classmethod\n        def from_config(\n            cls,\n            config: BaseZMQProxyConfig | None,\n            socket_ops: dict | None = None,\n        ) -&gt; \"ZMQProxy | None\":\n            if config is None:\n                return None\n            return cls(\n                zmq_proxy_config=config,\n                socket_ops=socket_ops,\n            )\n\n    # Dynamically set the class name and qualname based on the proxy type\n    ZMQProxy.__name__ = f\"ZMQ_{proxy_type.name}_Proxy\"\n    ZMQProxy.__qualname__ = ZMQProxy.__name__\n    ZMQProxy.__doc__ = f\"A ZMQ Proxy for {proxy_type.name} communication.\"\n    ZMQProxyFactory.register(proxy_type)(ZMQProxy)\n    return ZMQProxy\n</code></pre>"},{"location":"api/#aiperfcommonconfigaudio_config","title":"aiperf.common.config.audio_config","text":""},{"location":"api/#aiperf.common.config.audio_config.AudioConfig","title":"<code>AudioConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining audio related settings.</p> Source code in <code>aiperf/common/config/audio_config.py</code> <pre><code>class AudioConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining audio related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.AUDIO_INPUT\n\n    batch_size: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The batch size of audio requests AIPerf should send.\\n\"\n            \"This is currently supported with the OpenAI `multimodal` endpoint type\",\n        ),\n        Parameter(\n            name=(\n                \"--audio-batch-size\",\n                \"--batch-size-audio\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.BATCH_SIZE\n\n    length: AudioLengthConfig = AudioLengthConfig()\n\n    format: Annotated[\n        AudioFormat,\n        Field(\n            description=\"The format of the audio files (wav or mp3).\",\n        ),\n        Parameter(\n            name=(\n                \"--audio-format\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.FORMAT\n\n    depths: Annotated[\n        list[int],\n        Field(\n            min_length=1,\n            description=\"A list of audio bit depths to randomly select from in bits.\",\n        ),\n        BeforeValidator(parse_str_or_list_of_positive_values),\n        Parameter(\n            name=(\n                \"--audio-depths\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.DEPTHS\n\n    sample_rates: Annotated[\n        list[float],\n        Field(\n            min_length=1,\n            description=\"A list of audio sample rates to randomly select from in kHz.\\n\"\n            \"Common sample rates are 16, 44.1, 48, 96, etc.\",\n        ),\n        BeforeValidator(parse_str_or_list_of_positive_values),\n        Parameter(\n            name=(\n                \"--audio-sample-rates\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.SAMPLE_RATES\n\n    num_channels: Annotated[\n        int,\n        Field(\n            ge=1,\n            le=2,\n            description=\"The number of audio channels to use for the audio data generation.\",\n        ),\n        Parameter(\n            name=(\n                \"--audio-num-channels\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.NUM_CHANNELS\n</code></pre>"},{"location":"api/#aiperf.common.config.audio_config.AudioLengthConfig","title":"<code>AudioLengthConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining audio length related settings.</p> Source code in <code>aiperf/common/config/audio_config.py</code> <pre><code>class AudioLengthConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining audio length related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.AUDIO_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean length of the audio in seconds.\",\n        ),\n        Parameter(\n            name=(\n                \"--audio-length-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.LENGTH_MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the length of the audio in seconds.\",\n        ),\n        Parameter(\n            name=(\n                \"--audio-length-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.LENGTH_STDDEV\n</code></pre>"},{"location":"api/#aiperfcommonconfigbase_config","title":"aiperf.common.config.base_config","text":""},{"location":"api/#aiperf.common.config.base_config.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base configuration class for all configurations.</p> Source code in <code>aiperf/common/config/base_config.py</code> <pre><code>class BaseConfig(AIPerfBaseModel):\n    \"\"\"\n    Base configuration class for all configurations.\n    \"\"\"\n\n    def serialize_to_yaml(self, verbose: bool = False, indent: int = 4) -&gt; str:\n        \"\"\"\n        Serialize a Pydantic model to a YAML string.\n\n        Args:\n            verbose: Whether to include verbose comments in the YAML output.\n            indent: The per-level indentation to use.\n        \"\"\"\n        # Dump model to dict with context (flags propagate recursively)\n        context = {\n            \"verbose\": verbose,\n        }\n\n        data = self.model_dump(context=context)\n\n        # Attach comments recursively\n        commented_data = self._attach_comments(\n            data=data,\n            model=self,\n            context=context,\n            indent=indent,\n        )\n\n        # Dump to YAML\n        yaml = YAML(pure=True)\n        yaml.indent(mapping=indent, sequence=indent, offset=indent)\n\n        stream = io.StringIO()\n        yaml.dump(commented_data, stream)\n        return stream.getvalue()\n\n    @staticmethod\n    def _attach_comments(\n        data: Any,\n        model: AIPerfBaseModel,\n        context: dict,\n        indent: int,\n        indent_level: int = 0,\n    ) -&gt; Any:\n        \"\"\"\n        Recursively convert dicts to ruamel.yaml CommentedMap and attach comments from\n        Pydantic field descriptions, or based on context (e.g., verbose flag).\n\n        Args:\n            data: The raw data to convert to a CommentedMap.\n            model: The Pydantic model that contains the field descriptions.\n            context: The Pydantic serializer context which contains the serializer flags.\n            indent: The per-level indentation to use for the comments.\n            indent_level: The current level of indentation. The actual indentation is\n                `indent * indent_level`.\n\n        Returns:\n            The data with comments attached.\n        \"\"\"\n        if isinstance(data, dict):\n            # Create a CommentedMap to store the commented data. This is a special type of\n            # dict provided by the ruamel.yaml library that preserves the order of the keys and\n            # allows for comments to be attached to the keys.\n            commented_map = CommentedMap()\n\n            for field_name, value in data.items():\n                field = model.__class__.model_fields.get(field_name)\n\n                if not BaseConfig._should_add_field_to_template(field):\n                    continue\n\n                if BaseConfig._is_a_nested_config(field, value):\n                    # Recursively process nested models\n                    commented_map[field_name] = BaseConfig._attach_comments(\n                        value,\n                        getattr(model, field_name),\n                        context=context,\n                        indent=indent,\n                        indent_level=indent_level + 1,\n                    )\n\n                    commented_map.yaml_set_comment_before_after_key(\n                        field_name,\n                        before=\"\\n\",\n                        indent=indent * (indent_level + 1),\n                    )\n                else:\n                    # Attach the value to the commented map\n                    commented_map[field_name] = BaseConfig._preprocess_value(value)\n\n                # Attach comment if verbose and description exists\n                if context.get(\"verbose\") and field and field.description:\n                    # Set the comment before the key, with the specified indentation\n                    commented_map.yaml_set_comment_before_after_key(\n                        field_name,\n                        before=\"\\n\" + field.description,\n                        indent=indent * indent_level,\n                    )\n\n            return commented_map\n\n    @staticmethod\n    def _should_add_field_to_template(field: Any) -&gt; bool:\n        # Check if the field should be added to the template based on json_schema_extra\n        # and the add_to_template flag.\n        # If add_to_template is False, we skip adding the field to the template.\n        # If add_to_template is True or not present, we include the field in the template.\n        if field and field.json_schema_extra:\n            return field.json_schema_extra.get(ADD_TO_TEMPLATE, True)\n        else:\n            return True\n\n    @staticmethod\n    def _is_a_nested_config(field: Any, value: Any) -&gt; bool:\n        return (\n            isinstance(value, dict)\n            and field\n            and issubclass(field.annotation, AIPerfBaseModel)\n        )\n\n    @staticmethod\n    def _preprocess_value(value: Any) -&gt; Any:\n        \"\"\"\n        Preprocess the value before serialization.\n        \"\"\"\n\n        if isinstance(value, Enum):\n            return str(value.value).lower()\n        elif isinstance(value, Path):\n            return str(value)\n        else:\n            return value\n</code></pre>"},{"location":"api/#aiperf.common.config.base_config.BaseConfig.serialize_to_yaml","title":"<code>serialize_to_yaml(verbose=False, indent=4)</code>","text":"<p>Serialize a Pydantic model to a YAML string.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to include verbose comments in the YAML output.</p> <code>False</code> <code>indent</code> <code>int</code> <p>The per-level indentation to use.</p> <code>4</code> Source code in <code>aiperf/common/config/base_config.py</code> <pre><code>def serialize_to_yaml(self, verbose: bool = False, indent: int = 4) -&gt; str:\n    \"\"\"\n    Serialize a Pydantic model to a YAML string.\n\n    Args:\n        verbose: Whether to include verbose comments in the YAML output.\n        indent: The per-level indentation to use.\n    \"\"\"\n    # Dump model to dict with context (flags propagate recursively)\n    context = {\n        \"verbose\": verbose,\n    }\n\n    data = self.model_dump(context=context)\n\n    # Attach comments recursively\n    commented_data = self._attach_comments(\n        data=data,\n        model=self,\n        context=context,\n        indent=indent,\n    )\n\n    # Dump to YAML\n    yaml = YAML(pure=True)\n    yaml.indent(mapping=indent, sequence=indent, offset=indent)\n\n    stream = io.StringIO()\n    yaml.dump(commented_data, stream)\n    return stream.getvalue()\n</code></pre>"},{"location":"api/#aiperfcommonconfigconfig_defaults","title":"aiperf.common.config.config_defaults","text":""},{"location":"api/#aiperfcommonconfigconfig_validators","title":"aiperf.common.config.config_validators","text":""},{"location":"api/#aiperf.common.config.config_validators.parse_file","title":"<code>parse_file(value)</code>","text":"<p>Parses the given string value and returns a Path object if the value represents a valid file or directory. Returns None if the input value is empty. Args:     value (str): The string value to parse. Returns:     Optional[Path]: A Path object if the value is valid, or None if the value is empty. Raises:     ValueError: If the value is not a valid file or directory.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_file(value: str | None) -&gt; Path | None:\n    \"\"\"\n    Parses the given string value and returns a Path object if the value represents\n    a valid file or directory. Returns None if the input value is empty.\n    Args:\n        value (str): The string value to parse.\n    Returns:\n        Optional[Path]: A Path object if the value is valid, or None if the value is empty.\n    Raises:\n        ValueError: If the value is not a valid file or directory.\n    \"\"\"\n\n    if not value:\n        return None\n    elif not isinstance(value, str):\n        raise ValueError(f\"Expected a string, but got {type(value).__name__}\")\n    else:\n        path = Path(value)\n        if path.is_file() or path.is_dir():\n            return path\n        else:\n            raise ValueError(f\"'{value}' is not a valid file or directory\")\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_goodput","title":"<code>parse_goodput(goodputs)</code>","text":"<p>Parses and validates a dictionary of goodput values, ensuring that all values are non-negative integers or floats, and converts them to floats. Args:     goodputs (Dict[str, Any]): A dictionary where keys are target metric names         (strings) and values are the corresponding goodput values. Returns:     Dict[str, float]: A dictionary with the same keys as the input, but with         all values converted to floats. Raises:     ValueError: If any value in the input dictionary is not an integer or float,         or if any value is negative.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_goodput(goodputs: dict[str, Any]) -&gt; dict[str, float]:\n    \"\"\"\n    Parses and validates a dictionary of goodput values, ensuring that all values\n    are non-negative integers or floats, and converts them to floats.\n    Args:\n        goodputs (Dict[str, Any]): A dictionary where keys are target metric names\n            (strings) and values are the corresponding goodput values.\n    Returns:\n        Dict[str, float]: A dictionary with the same keys as the input, but with\n            all values converted to floats.\n    Raises:\n        ValueError: If any value in the input dictionary is not an integer or float,\n            or if any value is negative.\n    \"\"\"\n\n    constraints = {}\n    for target_metric, target_value in goodputs.items():\n        if isinstance(target_value, (int | float)):\n            if target_value &lt; 0:\n                raise ValueError(\n                    f\"User Config: Goodput values must be non-negative ({target_metric}: {target_value})\"\n                )\n\n            constraints[target_metric] = float(target_value)\n        else:\n            raise ValueError(\"User Config: Goodput values must be integers or floats\")\n\n    return constraints\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_service_types","title":"<code>parse_service_types(input)</code>","text":"<p>Parses the input to ensure it is a set of service types. Will replace hyphens with underscores for user convenience.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_service_types(input: Any | None) -&gt; set[ServiceType] | None:\n    \"\"\"Parses the input to ensure it is a set of service types.\n    Will replace hyphens with underscores for user convenience.\"\"\"\n    if input is None:\n        return None\n\n    return {\n        ServiceType(service_type.replace(\"-\", \"_\"))\n        for service_type in parse_str_or_csv_list(input)\n    }\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_csv_list","title":"<code>parse_str_or_csv_list(input)</code>","text":"<p>Parses the input to ensure it is either a string or a list. If the input is a string, it splits the string by commas and trims any whitespace around each element, returning the result as a list. If the input is already a list, it will split each item by commas and trim any whitespace around each element, returning the combined result as a list. If the input is neither a string nor a list, a ValueError is raised.</p> <p>[1, 2, 3] -&gt; [1, 2, 3] \"1,2,3\" -&gt; [\"1\", \"2\", \"3\"][\"1,2,3\", \"4,5,6\"] -&gt; [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"][\"1,2,3\", 4, 5] -&gt; [\"1\", \"2\", \"3\", 4, 5]</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_csv_list(input: Any) -&gt; list[Any]:\n    \"\"\"\n    Parses the input to ensure it is either a string or a list. If the input is a string,\n    it splits the string by commas and trims any whitespace around each element, returning\n    the result as a list. If the input is already a list, it will split each item by commas\n    and trim any whitespace around each element, returning the combined result as a list.\n    If the input is neither a string nor a list, a ValueError is raised.\n\n    [1, 2, 3] -&gt; [1, 2, 3]\n    \"1,2,3\" -&gt; [\"1\", \"2\", \"3\"]\n    [\"1,2,3\", \"4,5,6\"] -&gt; [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n    [\"1,2,3\", 4, 5] -&gt; [\"1\", \"2\", \"3\", 4, 5]\n    \"\"\"\n    if isinstance(input, str):\n        output = [item.strip() for item in input.split(\",\")]\n    elif isinstance(input, list):\n        output = []\n        for item in input:\n            if isinstance(item, str):\n                output.extend([token.strip() for token in item.split(\",\")])\n            else:\n                output.append(item)\n    else:\n        raise ValueError(f\"User Config: {input} - must be a string or list\")\n\n    return output\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_dict","title":"<code>parse_str_or_dict(input)</code>","text":"<p>Parses the input to ensure it is a dictionary.</p> <ul> <li>If the input is a string:<ul> <li>If the string starts with a '{', it is parsed as a JSON string.</li> <li>Otherwise, it splits the string by commas and then for each item, it splits the item by colons into key and value, trims any whitespace.</li> </ul> </li> <li>If the input is already a dictionary, it is returned as-is.</li> <li>If the input is a list, it is converted to a dictionary by splitting each string by colons into key and value, trims any whitespace.</li> <li>Otherwise, a ValueError is raised.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input to be parsed. Expected to be a string, list, or dictionary.</p> required <p>Returns:     dict[str, Any]: A dictionary derived from the input. Raises:     ValueError: If the input is neither a string, list, nor dictionary, or if the parsing fails.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_dict(input: Any | None) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Parses the input to ensure it is a dictionary.\n\n    - If the input is a string:\n        - If the string starts with a '{', it is parsed as a JSON string.\n        - Otherwise, it splits the string by commas and then for each item, it splits the item by colons\n        into key and value, trims any whitespace.\n    - If the input is already a dictionary, it is returned as-is.\n    - If the input is a list, it is converted to a dictionary by splitting each string by colons\n    into key and value, trims any whitespace.\n    - Otherwise, a ValueError is raised.\n\n    Args:\n        input (Any): The input to be parsed. Expected to be a string, list, or dictionary.\n    Returns:\n        dict[str, Any]: A dictionary derived from the input.\n    Raises:\n        ValueError: If the input is neither a string, list, nor dictionary, or if the parsing fails.\n    \"\"\"\n\n    if input is None:\n        return None\n\n    if isinstance(input, dict):\n        return input\n\n    if isinstance(input, list):\n        return {\n            key.strip(): value.strip()\n            for item in input\n            for key, value in [item.split(\":\")]\n        }\n\n    if isinstance(input, str):\n        if input.startswith(\"{\"):\n            try:\n                return json.loads(input)\n            except json.JSONDecodeError as e:\n                raise ValueError(\n                    f\"User Config: {input} - must be a valid JSON string\"\n                ) from e\n        else:\n            return {\n                key.strip(): value.strip()\n                for item in input.split(\",\")\n                for key, value in [item.split(\":\")]\n            }\n\n    raise ValueError(f\"User Config: {input} - must be a valid string, list, or dict\")\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_list","title":"<code>parse_str_or_list(input)</code>","text":"<p>Parses the input to ensure it is either a string or a list. If the input is a string, it splits the string by commas and trims any whitespace around each element, returning the result as a list. If the input is already a list, it is returned as-is. If the input is neither a string nor a list, a ValueError is raised. Args:     input (Any): The input to be parsed. Expected to be a string or a list. Returns:     list: A list of strings derived from the input. Raises:     ValueError: If the input is neither a string nor a list.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_list(input: Any) -&gt; list[Any]:\n    \"\"\"\n    Parses the input to ensure it is either a string or a list. If the input is a string,\n    it splits the string by commas and trims any whitespace around each element, returning\n    the result as a list. If the input is already a list, it is returned as-is. If the input\n    is neither a string nor a list, a ValueError is raised.\n    Args:\n        input (Any): The input to be parsed. Expected to be a string or a list.\n    Returns:\n        list: A list of strings derived from the input.\n    Raises:\n        ValueError: If the input is neither a string nor a list.\n    \"\"\"\n    if isinstance(input, str):\n        output = [item.strip() for item in input.split(\",\")]\n    elif isinstance(input, list):\n        # TODO: When using cyclopts, the values are already lists, so we have to split them by commas.\n        output = []\n        for item in input:\n            if isinstance(item, str):\n                output.extend([token.strip() for token in item.split(\",\")])\n            else:\n                output.append(item)\n    else:\n        raise ValueError(f\"User Config: {input} - must be a string or list\")\n\n    return output\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_list_of_positive_values","title":"<code>parse_str_or_list_of_positive_values(input)</code>","text":"<p>Parses the input to ensure it is a list of positive integers or floats. This function first converts the input into a list using <code>parse_str_or_list</code>. It then validates that each value in the list is either an integer or a float and that all values are strictly greater than zero. If any value fails this validation, a <code>ValueError</code> is raised. Args:     input (Any): The input to be parsed. It can be a string or a list. Returns:     List[Any]: A list of positive integers or floats. Raises:     ValueError: If any value in the parsed list is not a positive integer or float.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_list_of_positive_values(input: Any) -&gt; list[Any]:\n    \"\"\"\n    Parses the input to ensure it is a list of positive integers or floats.\n    This function first converts the input into a list using `parse_str_or_list`.\n    It then validates that each value in the list is either an integer or a float\n    and that all values are strictly greater than zero. If any value fails this\n    validation, a `ValueError` is raised.\n    Args:\n        input (Any): The input to be parsed. It can be a string or a list.\n    Returns:\n        List[Any]: A list of positive integers or floats.\n    Raises:\n        ValueError: If any value in the parsed list is not a positive integer or float.\n    \"\"\"\n\n    output = parse_str_or_list(input)\n\n    try:\n        output = [\n            float(x) if \".\" in str(x) or \"e\" in str(x).lower() else int(x)\n            for x in output\n        ]\n    except ValueError as e:\n        raise ValueError(f\"User Config: {output} - all values must be numeric\") from e\n\n    if not all(isinstance(x, (int | float)) and x &gt; 0 for x in output):\n        raise ValueError(f\"User Config: {output} - all values must be positive numbers\")\n\n    return output\n</code></pre>"},{"location":"api/#aiperfcommonconfigconversation_config","title":"aiperf.common.config.conversation_config","text":""},{"location":"api/#aiperf.common.config.conversation_config.ConversationConfig","title":"<code>ConversationConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining conversations related settings.</p> Source code in <code>aiperf/common/config/conversation_config.py</code> <pre><code>class ConversationConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining conversations related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.CONVERSATION_INPUT\n\n    num: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The total number of unique conversations to generate.\\n\"\n            \"Each conversation represents a single request session between client and server.\\n\"\n            \"Supported on synthetic mode only and conversations will be reused until benchmarking is complete.\",\n        ),\n        Parameter(\n            name=(\n                \"--conversation-num\",\n                \"--num-conversations\",\n                \"--num-sessions\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ConversationDefaults.NUM\n\n    turn: TurnConfig = TurnConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.conversation_config.TurnConfig","title":"<code>TurnConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining turn related settings in a conversation.</p> Source code in <code>aiperf/common/config/conversation_config.py</code> <pre><code>class TurnConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining turn related settings in a conversation.\n    \"\"\"\n\n    _CLI_GROUP = Groups.CONVERSATION_INPUT\n\n    mean: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The mean number of turns within a conversation.\",\n        ),\n        Parameter(\n            name=(\n                \"--conversation-turn-mean\",\n                \"--session-turns-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDefaults.MEAN\n\n    stddev: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the number of turns within a conversation.\",\n        ),\n        Parameter(\n            name=(\n                \"--conversation-turn-stddev\",\n                \"--session-turns-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDefaults.STDDEV\n\n    delay: TurnDelayConfig = TurnDelayConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.conversation_config.TurnDelayConfig","title":"<code>TurnDelayConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining turn delay related settings.</p> Source code in <code>aiperf/common/config/conversation_config.py</code> <pre><code>class TurnDelayConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining turn delay related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.CONVERSATION_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean delay between turns within a conversation in milliseconds.\",\n        ),\n        Parameter(\n            name=(\n                \"--conversation-turn-delay-mean\",\n                \"--session-turn-delay-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDelayDefaults.MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the delay between turns \\n\"\n            \"within a conversation in milliseconds.\",\n        ),\n        Parameter(\n            name=(\n                \"--conversation-turn-delay-stddev\",\n                \"--session-turn-delay-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDelayDefaults.STDDEV\n\n    ratio: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"A ratio to scale multi-turn delays.\",\n        ),\n        Parameter(\n            name=(\n                \"--conversation-turn-delay-ratio\",\n                \"--session-delay-ratio\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDelayDefaults.RATIO\n</code></pre>"},{"location":"api/#aiperfcommonconfigendpoint_config","title":"aiperf.common.config.endpoint_config","text":""},{"location":"api/#aiperf.common.config.endpoint_config.EndPointConfig","title":"<code>EndPointConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining endpoint related settings.</p> Source code in <code>aiperf/common/config/endpoint_config.py</code> <pre><code>class EndPointConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining endpoint related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.ENDPOINT\n\n    model_selection_strategy: Annotated[\n        ModelSelectionStrategy,\n        Field(\n            description=\"When multiple models are specified, this is how a specific model should be assigned to a prompt.\\n\"\n            \"round_robin: nth prompt in the list gets assigned to n-mod len(models).\\n\"\n            \"random: assignment is uniformly random\",\n        ),\n        Parameter(\n            name=(\n                \"--model-selection-strategy\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.MODEL_SELECTION_STRATEGY\n\n    custom_endpoint: Annotated[\n        str | None,\n        Field(\n            description=\"Set a custom endpoint that differs from the OpenAI defaults.\",\n        ),\n        Parameter(\n            name=(\n                \"--custom-endpoint\",\n                \"--endpoint\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.CUSTOM_ENDPOINT\n\n    type: Annotated[\n        EndpointType,\n        Field(\n            description=\"The type to send requests to on the server.\",\n        ),\n        Parameter(\n            name=(\n                \"--endpoint-type\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.TYPE\n\n    streaming: Annotated[\n        bool,\n        Field(\n            description=\"An option to enable the use of the streaming API.\",\n        ),\n        Parameter(\n            name=(\n                \"--streaming\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.STREAMING\n\n    server_metrics_urls: Annotated[\n        list[str],\n        Field(\n            description=\"The list of Triton server metrics URLs.\\n\"\n            \"These are used for Telemetry metric reporting with Triton.\",\n        ),\n        BeforeValidator(parse_str_or_list),\n        Parameter(\n            name=(\n                \"--server-metrics-urls\",  # GenAI-Perf\n                \"--server-metrics-url\",  # GenAI-Perf\n            ),\n            parse=False,  # TODO: Not yet supported\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.SERVER_METRICS_URLS\n\n    url: Annotated[\n        str,\n        Field(\n            description=\"URL of the endpoint to target for benchmarking.\",\n        ),\n        Parameter(\n            name=(\n                \"--url\",  # GenAI-Perf\n                \"-u\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.URL\n\n    grpc_method: Annotated[\n        str,\n        Field(\n            description=\"A fully-qualified gRPC method name in \"\n            \"'&lt;package&gt;.&lt;service&gt;/&lt;method&gt;' format.\\n\"\n            \"The option is only supported by dynamic gRPC service kind and is\\n\"\n            \"required to identify the RPC to use when sending requests to the server.\",\n        ),\n        Parameter(\n            name=(\n                \"--grpc-method\",  # GenAI-Perf\n            ),\n            parse=False,  # TODO: Not yet supported\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.GRPC_METHOD\n\n    # NEW AIPerf Option\n    timeout_seconds: Annotated[\n        float,\n        Field(\n            description=\"The timeout in floating points seconds for each request to the endpoint.\",\n        ),\n        Parameter(\n            name=(\"--request-timeout-seconds\"),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.TIMEOUT\n\n    # NEW AIPerf Option\n    api_key: Annotated[\n        str | None,\n        Field(\n            description=\"The API key to use for the endpoint. If provided, it will be sent with every request as \"\n            \"a header: `Authorization: Bearer &lt;api_key&gt;`.\",\n        ),\n        Parameter(\n            name=(\"--api-key\"),\n            group=_CLI_GROUP,\n        ),\n    ] = EndPointDefaults.API_KEY\n</code></pre>"},{"location":"api/#aiperfcommonconfiggroups","title":"aiperf.common.config.groups","text":""},{"location":"api/#aiperf.common.config.groups.Groups","title":"<code>Groups</code>","text":"<p>Groups for the CLI.</p> <p>NOTE: The order of these groups are the order they will be displayed in the help text.</p> Source code in <code>aiperf/common/config/groups.py</code> <pre><code>class Groups:\n    \"\"\"Groups for the CLI.\n\n    NOTE: The order of these groups are the order they will be displayed in the help text.\n    \"\"\"\n\n    ENDPOINT = Group.create_ordered(\"Endpoint\")\n    INPUT = Group.create_ordered(\"Input\")\n    OUTPUT = Group.create_ordered(\"Output\")\n    TOKENIZER = Group.create_ordered(\"Tokenizer\")\n    LOAD_GENERATOR = Group.create_ordered(\"Load Generator\")\n    CONVERSATION_INPUT = Group.create_ordered(\"Conversation Input\")\n    INPUT_SEQUENCE_LENGTH = Group.create_ordered(\"Input Sequence Length (ISL)\")\n    OUTPUT_SEQUENCE_LENGTH = Group.create_ordered(\"Output Sequence Length (OSL)\")\n    PROMPT = Group.create_ordered(\"Prompt\")\n    PREFIX_PROMPT = Group.create_ordered(\"Prefix Prompt\")\n    AUDIO_INPUT = Group.create_ordered(\"Audio Input\")\n    IMAGE_INPUT = Group.create_ordered(\"Image Input\")\n    MEASUREMENT = Group.create_ordered(\"Measurement\")\n    SERVICE = Group.create_ordered(\"Service\")\n    WORKERS = Group.create_ordered(\"Workers\")\n    DEVELOPER = Group.create_ordered(\"Developer\")\n</code></pre>"},{"location":"api/#aiperfcommonconfigimage_config","title":"aiperf.common.config.image_config","text":""},{"location":"api/#aiperf.common.config.image_config.ImageConfig","title":"<code>ImageConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining image related settings.</p> Source code in <code>aiperf/common/config/image_config.py</code> <pre><code>class ImageConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining image related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.IMAGE_INPUT\n\n    width: ImageWidthConfig = ImageWidthConfig()\n    height: ImageHeightConfig = ImageHeightConfig()\n    batch_size: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The image batch size of the requests AIPerf should send.\\n\"\n            \"This is currently supported with the image retrieval endpoint type.\",\n        ),\n        Parameter(\n            name=(\n                \"--image-batch-size\",\n                \"--batch-size-image\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.BATCH_SIZE\n\n    format: Annotated[\n        ImageFormat,\n        Field(\n            description=\"The compression format of the images.\",\n        ),\n        Parameter(\n            name=(\n                \"--image-format\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.FORMAT\n</code></pre>"},{"location":"api/#aiperf.common.config.image_config.ImageHeightConfig","title":"<code>ImageHeightConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining image height related settings.</p> Source code in <code>aiperf/common/config/image_config.py</code> <pre><code>class ImageHeightConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining image height related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.IMAGE_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean height of images when generating synthetic image data.\",\n        ),\n        Parameter(\n            name=(\n                \"--image-height-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.HEIGHT_MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of height of images when generating synthetic image data.\",\n        ),\n        Parameter(\n            name=(\n                \"--image-height-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.HEIGHT_STDDEV\n</code></pre>"},{"location":"api/#aiperf.common.config.image_config.ImageWidthConfig","title":"<code>ImageWidthConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining image width related settings.</p> Source code in <code>aiperf/common/config/image_config.py</code> <pre><code>class ImageWidthConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining image width related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.IMAGE_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean width of images when generating synthetic image data.\",\n        ),\n        Parameter(\n            name=(\n                \"--image-width-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.WIDTH_MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of width of images when generating synthetic image data.\",\n        ),\n        Parameter(\n            name=(\n                \"--image-width-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.WIDTH_STDDEV\n</code></pre>"},{"location":"api/#aiperfcommonconfiginput_config","title":"aiperf.common.config.input_config","text":""},{"location":"api/#aiperf.common.config.input_config.InputConfig","title":"<code>InputConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining input related settings.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>class InputConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining input related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.INPUT\n\n    @model_validator(mode=\"after\")\n    def validate_fixed_schedule(self) -&gt; Self:\n        \"\"\"Validate the fixed schedule configuration.\"\"\"\n        if self.fixed_schedule and self.file is None:\n            raise ValueError(\"Fixed schedule requires a file to be provided\")\n        if self.file is not None:\n            self.fixed_schedule = True\n            logger.debug(\"Fixed schedule is enabled because file is provided\")\n        return self\n\n    extra: Annotated[\n        dict[str, Any] | None,\n        Field(\n            description=\"Provide additional inputs to include with every request.\\n\"\n            \"Inputs should be in an 'input_name:value' format.\",\n        ),\n        Parameter(\n            name=(\n                \"--extra-inputs\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n        BeforeValidator(parse_str_or_dict),\n    ] = InputDefaults.EXTRA\n\n    goodput: Annotated[\n        dict[str, Any],\n        Field(\n            description=\"An option to provide constraints in order to compute goodput.\\n\"\n            \"Specify goodput constraints as 'key:value' pairs,\\n\"\n            \"where the key is a valid metric name, and the value is a number representing\\n\"\n            \"either milliseconds or a throughput value per second.\\n\"\n            \"For example: request_latency:300,output_token_throughput_per_user:600\",\n        ),\n        Parameter(\n            name=(\n                \"--goodput\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n        BeforeValidator(parse_goodput),\n    ] = InputDefaults.GOODPUT\n\n    headers: Annotated[\n        dict[str, str] | None,\n        Field(\n            description=\"Adds a custom header to the requests.\\n\"\n            \"Headers must be specified as 'Header:Value' pairs.\",\n        ),\n        BeforeValidator(parse_str_or_dict),\n        Parameter(\n            name=(\n                \"--header\",  # GenAI-Perf\n                \"-H\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.HEADERS\n\n    file: Annotated[\n        Any,\n        Field(\n            description=\"The file or directory path that contains the dataset to use for profiling.\\n\"\n            \"This parameter is used in conjunction with the `custom_dataset_type` parameter\\n\"\n            \"to support different types of user provided datasets.\",\n        ),\n        BeforeValidator(parse_file),\n        Parameter(\n            name=(\n                \"--input-file\",  # GenAI-Perf,\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FILE\n\n    fixed_schedule: Annotated[\n        bool,\n        Field(\n            description=\"Specifies to run a fixed schedule of requests. This is normally inferred from the --input-file parameter, but can be set manually here.\"\n        ),\n        Parameter(\n            name=(\n                \"--fixed-schedule\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FIXED_SCHEDULE\n\n    # NEW AIPerf Option\n    custom_dataset_type: Annotated[\n        CustomDatasetType,\n        Field(\n            description=\"The type of custom dataset to use.\\n\"\n            \"This parameter is used in conjunction with the --file parameter.\",\n        ),\n        Parameter(\n            name=(\"--custom-dataset-type\"),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.CUSTOM_DATASET_TYPE\n\n    random_seed: Annotated[\n        int | None,\n        Field(\n            default=None,\n            description=\"The seed used to generate random values.\\n\"\n            \"Set to some value to make the synthetic data generation deterministic.\\n\"\n            \"It will use system default if not provided.\",\n        ),\n        Parameter(\n            name=(\n                \"--random-seed\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.RANDOM_SEED\n\n    audio: AudioConfig = AudioConfig()\n    image: ImageConfig = ImageConfig()\n    prompt: PromptConfig = PromptConfig()\n    conversation: ConversationConfig = ConversationConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.input_config.InputConfig.validate_fixed_schedule","title":"<code>validate_fixed_schedule()</code>","text":"<p>Validate the fixed schedule configuration.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_fixed_schedule(self) -&gt; Self:\n    \"\"\"Validate the fixed schedule configuration.\"\"\"\n    if self.fixed_schedule and self.file is None:\n        raise ValueError(\"Fixed schedule requires a file to be provided\")\n    if self.file is not None:\n        self.fixed_schedule = True\n        logger.debug(\"Fixed schedule is enabled because file is provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperfcommonconfigloader","title":"aiperf.common.config.loader","text":""},{"location":"api/#aiperf.common.config.loader.load_service_config","title":"<code>load_service_config()</code>","text":"<p>Load the service configuration.</p> Source code in <code>aiperf/common/config/loader.py</code> <pre><code>def load_service_config() -&gt; ServiceConfig:\n    \"\"\"Load the service configuration.\"\"\"\n    # TODO: implement\n    return ServiceConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.loader.load_user_config","title":"<code>load_user_config()</code>","text":"<p>Load the user configuration.</p> Source code in <code>aiperf/common/config/loader.py</code> <pre><code>def load_user_config() -&gt; UserConfig:\n    \"\"\"Load the user configuration.\"\"\"\n    # TODO: implement\n    raise NotImplementedError(\"User configuration is not implemented\")\n</code></pre>"},{"location":"api/#aiperfcommonconfigloadgen_config","title":"aiperf.common.config.loadgen_config","text":""},{"location":"api/#aiperf.common.config.loadgen_config.LoadGeneratorConfig","title":"<code>LoadGeneratorConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining top-level load generator settings.</p> Source code in <code>aiperf/common/config/loadgen_config.py</code> <pre><code>class LoadGeneratorConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining top-level load generator settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.LOAD_GENERATOR\n\n    # TODO: Potentially add a validator to ensure that the concurrency is not greater than the request count\n    concurrency: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The concurrency value to benchmark.\",\n        ),\n        Parameter(\n            name=(\n                \"--concurrency\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.CONCURRENCY\n\n    request_rate: Annotated[\n        float | None,\n        Field(\n            gt=0,\n            description=\"Sets the request rate for the load generated by AIPerf. Unit: requests/second\",\n        ),\n        Parameter(\n            name=(\n                \"--request-rate\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.REQUEST_RATE\n\n    # NEW AIPerf Option\n    request_rate_mode: Annotated[\n        RequestRateMode,\n        Field(\n            description=\"Sets the request rate mode for the load generated by AIPerf. Valid values: constant, poisson.\\n\"\n            \"constant: Generate requests at a fixed rate.\\n\"\n            \"poisson: Generate requests using a poisson distribution.\\n\"\n            f\"The default is {LoadGeneratorDefaults.REQUEST_RATE_MODE}.\",\n        ),\n        Parameter(\n            name=(\"--request-rate-mode\"),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.REQUEST_RATE_MODE\n\n    request_count: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The number of requests to use for measurement.\",\n        ),\n        Parameter(\n            name=(\n                \"--request-count\",  # GenAI-Perf\n                \"--num-requests\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.REQUEST_COUNT\n\n    warmup_request_count: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The number of warmup requests to send before benchmarking.\",\n        ),\n        Parameter(\n            name=(\n                \"--warmup-request-count\",  # GenAI-Perf\n                \"--num-warmup-requests\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.WARMUP_REQUEST_COUNT\n</code></pre>"},{"location":"api/#aiperfcommonconfigmeasurement_config","title":"aiperf.common.config.measurement_config","text":""},{"location":"api/#aiperf.common.config.measurement_config.MeasurementConfig","title":"<code>MeasurementConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining top-level measurement settings.</p> Source code in <code>aiperf/common/config/measurement_config.py</code> <pre><code>class MeasurementConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining top-level measurement settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.MEASUREMENT\n\n    # TODO: Not implemented yet\n    measurement_interval: Annotated[\n        float,\n        Field(\n            ge=1,\n            le=1_000_000,\n            description=\"The time interval used for each measurement in milliseconds. \"\n            \"AIPerf will sample a time interval specified and take \"\n            \"measurement over the requests completed within that time interval. \"\n            \"When using the default stability percentage, AIPerf will benchmark  \"\n            \"for 3*(measurement_interval) milliseconds.\",\n        ),\n        Parameter(\n            name=(\n                \"--measurement-interval-ms\",\n                \"--measurement-interval\",  # GenAI-Perf\n                \"-p\",  # GenAI-Perf\n            ),\n            parse=False,  # TODO: Not yet supported\n            group=_CLI_GROUP,\n        ),\n    ] = MeasurementDefaults.MEASUREMENT_INTERVAL\n\n    # TODO: Not implemented yet\n    stability_percentage: Annotated[\n        float,\n        Field(\n            gt=0.0,\n            lt=1.0,\n            description=\"The allowed variation in latency measurements when determining if a result is stable.\\n\"\n            \"The measurement is considered as stable if the ratio of max / min\\n\"\n            \"from the recent 3 measurements is within (stability percentage)\\n\"\n            \"in terms of both infer per second and latency.\",\n        ),\n        Parameter(\n            name=(\n                \"--stability-percentage\",  # GenAI-Perf\n                \"-s\",  # GenAI-Perf\n            ),\n            parse=False,  # TODO: Not yet supported\n            group=_CLI_GROUP,\n        ),\n    ] = MeasurementDefaults.STABILITY_PERCENTAGE\n</code></pre>"},{"location":"api/#aiperfcommonconfigoutput_config","title":"aiperf.common.config.output_config","text":""},{"location":"api/#aiperf.common.config.output_config.OutputConfig","title":"<code>OutputConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining output related settings.</p> Source code in <code>aiperf/common/config/output_config.py</code> <pre><code>class OutputConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining output related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.OUTPUT\n\n    artifact_directory: Annotated[\n        Path,\n        Field(\n            description=\"The directory to store all the (output) artifacts generated by AIPerf.\",\n        ),\n        Parameter(\n            name=(\n                \"--output-artifact-dir\",\n                \"--artifact-dir\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = OutputDefaults.ARTIFACT_DIRECTORY\n</code></pre>"},{"location":"api/#aiperfcommonconfigprompt_config","title":"aiperf.common.config.prompt_config","text":""},{"location":"api/#aiperf.common.config.prompt_config.InputTokensConfig","title":"<code>InputTokensConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining input token related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class InputTokensConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining input token related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.INPUT_SEQUENCE_LENGTH\n\n    mean: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The mean of number of tokens in the generated prompts when using synthetic data.\",\n        ),\n        Parameter(\n            name=(\n                \"--prompt-input-tokens-mean\",\n                \"--synthetic-input-tokens-mean\",  # GenAI-Perf\n                \"--isl\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputTokensDefaults.MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of number of tokens in the generated prompts when using synthetic data.\",\n        ),\n        Parameter(\n            name=(\n                \"--prompt-input-tokens-stddev\",\n                \"--synthetic-input-tokens-stddev\",  # GenAI-Perf\n                \"--isl-stddev\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputTokensDefaults.STDDEV\n\n    # NEW AIPerf Option\n    block_size: Annotated[\n        int,\n        Field(\n            default=512,\n            description=\"The block size of the prompt.\",\n        ),\n        Parameter(\n            name=(\n                \"--prompt-input-tokens-block-size\",\n                \"--synthetic-input-tokens-block-size\",\n                \"--isl-block-size\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputTokensDefaults.BLOCK_SIZE\n</code></pre>"},{"location":"api/#aiperf.common.config.prompt_config.OutputTokensConfig","title":"<code>OutputTokensConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining output token related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class OutputTokensConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining output token related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.OUTPUT_SEQUENCE_LENGTH\n\n    mean: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The mean number of tokens in each output.\",\n        ),\n        Parameter(\n            name=(\n                \"--prompt-output-tokens-mean\",\n                \"--output-tokens-mean\",  # GenAI-Perf\n                \"--osl\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = OutputTokensDefaults.MEAN\n\n    deterministic: Annotated[\n        bool,\n        Field(\n            description=(\n                \"This can be set to improve the precision of the mean by setting the\\n\"\n                \"minimum number of tokens equal to the requested number of tokens.\\n\"\n                \"This is currently supported with Triton.\"\n            ),\n        ),\n        Parameter(\n            name=(\n                \"--prompt-output-tokens-deterministic\",\n                \"--output-tokens-mean-deterministic\",  # GenAI-Perf\n                \"--osl-deterministic\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = OutputTokensDefaults.DETERMINISTIC\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the number of tokens in each output.\",\n        ),\n        Parameter(\n            name=(\n                \"--prompt-output-tokens-stddev\",\n                \"--output-tokens-stddev\",  # GenAI-Perf\n                \"--osl-stddev\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = OutputTokensDefaults.STDDEV\n</code></pre>"},{"location":"api/#aiperf.common.config.prompt_config.PrefixPromptConfig","title":"<code>PrefixPromptConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining prefix prompt related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class PrefixPromptConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining prefix prompt related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.PREFIX_PROMPT\n\n    pool_size: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=(\n                \"The total size of the prefix prompt pool to select prefixes from.\\n\"\n                \"If this value is not zero, these are prompts that are prepended to input prompts.\\n\"\n                \"This is useful for benchmarking models that use a K-V cache.\"\n            ),\n        ),\n        Parameter(\n            name=(\n                \"--prompt-prefix-pool-size\",\n                \"--prefix-prompt-pool-size\",\n                \"--num-prefix-prompts\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = PrefixPromptDefaults.POOL_SIZE\n\n    length: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=(\n                \"The number of tokens in each prefix prompt.\\n\"\n                'This is only used if \"num\" is greater than zero.\\n'\n                \"Note that due to the prefix and user prompts being concatenated,\\n\"\n                \"the number of tokens in the final prompt may be off by one.\"\n            ),\n        ),\n        Parameter(\n            name=(\n                \"--prompt-prefix-length\",\n                \"--prefix-prompt-length\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = PrefixPromptDefaults.LENGTH\n</code></pre>"},{"location":"api/#aiperf.common.config.prompt_config.PromptConfig","title":"<code>PromptConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining prompt related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class PromptConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining prompt related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.PROMPT\n\n    batch_size: Annotated[\n        int,\n        Field(\n            description=\"The batch size of text requests AIPerf should send.\\n\"\n            \"This is currently supported with the embeddings and rankings endpoint types\",\n        ),\n        Parameter(\n            name=(\n                \"--prompt-batch-size\",\n                \"--batch-size-text\",  # GenAI-Perf\n                \"--batch-size\",  # GenAI-Perf\n                \"-b\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = PromptDefaults.BATCH_SIZE\n\n    input_tokens: InputTokensConfig = InputTokensConfig()\n    output_tokens: OutputTokensConfig = OutputTokensConfig()\n    prefix_prompt: PrefixPromptConfig = PrefixPromptConfig()\n</code></pre>"},{"location":"api/#aiperfcommonconfigservice_config","title":"aiperf.common.config.service_config","text":""},{"location":"api/#aiperf.common.config.service_config.ServiceConfig","title":"<code>ServiceConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Base configuration for all services. It will be provided to all services during their init function.</p> Source code in <code>aiperf/common/config/service_config.py</code> <pre><code>class ServiceConfig(BaseSettings):\n    \"\"\"Base configuration for all services. It will be provided to all services during their __init__ function.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"AIPERF_\",\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"allow\",\n    )\n\n    _CLI_GROUP = Groups.SERVICE\n\n    @model_validator(mode=\"after\")\n    def validate_log_level_from_verbose_flags(self) -&gt; Self:\n        \"\"\"Set log level based on verbose flags.\"\"\"\n        if self.extra_verbose:\n            self.log_level = AIPerfLogLevel.TRACE\n        elif self.verbose:\n            self.log_level = AIPerfLogLevel.DEBUG\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_comm_config(self) -&gt; Self:\n        \"\"\"Initialize the comm_config if it is not provided, based on the comm_backend.\"\"\"\n        if self.comm_config is None:\n            if self.comm_backend == CommunicationBackend.ZMQ_IPC:\n                self.comm_config = ZMQIPCConfig()\n            elif self.comm_backend == CommunicationBackend.ZMQ_TCP:\n                self.comm_config = ZMQTCPConfig()\n            else:\n                raise ValueError(f\"Invalid communication backend: {self.comm_backend}\")\n        return self\n\n    service_run_type: Annotated[\n        ServiceRunType,\n        Field(\n            description=\"Type of service run (process, k8s)\",\n        ),\n        Parameter(\n            name=(\"--service-run-type\", \"--run-type\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.SERVICE_RUN_TYPE\n\n    comm_backend: Annotated[\n        CommunicationBackend,\n        Field(\n            description=\"Communication backend to use\",\n        ),\n        Parameter(\n            name=(\"--comm-backend\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.COMM_BACKEND\n\n    comm_config: Annotated[\n        BaseZMQCommunicationConfig | None,\n        Field(\n            description=\"Communication configuration\",\n        ),\n        Parameter(\n            parse=False,  # This is not supported via CLI\n        ),\n    ] = ServiceDefaults.COMM_CONFIG\n\n    heartbeat_timeout: Annotated[\n        float,\n        Field(\n            description=\"Time in seconds after which a service is considered dead if no \"\n            \"heartbeat received\",\n        ),\n        Parameter(\n            name=(\"--heartbeat-timeout\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.HEARTBEAT_TIMEOUT\n\n    registration_timeout: Annotated[\n        float,\n        Field(\n            description=\"Time in seconds to wait for all required services to register\",\n        ),\n        Parameter(\n            name=(\"--registration-timeout\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.REGISTRATION_TIMEOUT\n\n    command_timeout: Annotated[\n        float,\n        Field(\n            description=\"Default timeout for command responses\",\n        ),\n        Parameter(\n            name=(\"--command-timeout\", \"--command-timeout-seconds\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.COMMAND_TIMEOUT\n\n    heartbeat_interval_seconds: Annotated[\n        float,\n        Field(\n            description=\"Interval in seconds between heartbeat messages\",\n        ),\n        Parameter(\n            name=(\"--heartbeat-interval-seconds\", \"--heartbeat-interval\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.HEARTBEAT_INTERVAL_SECONDS\n\n    workers: Annotated[\n        WorkersConfig,\n        Field(\n            description=\"Worker configuration\",\n        ),\n    ] = WorkersConfig()\n\n    log_level: Annotated[\n        AIPerfLogLevel,\n        Field(\n            description=\"Logging level\",\n        ),\n        Parameter(\n            name=(\"--log-level\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.LOG_LEVEL\n\n    verbose: Annotated[\n        bool,\n        Field(\n            description=\"Equivalent to --log-level DEBUG. Enables more verbose logging output, but lacks some raw message logging.\",\n            json_schema_extra={ADD_TO_TEMPLATE: False},\n        ),\n        Parameter(\n            name=(\"--verbose\", \"-v\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.VERBOSE\n\n    extra_verbose: Annotated[\n        bool,\n        Field(\n            description=\"Equivalent to --log-level TRACE. Enables the most verbose logging output possible.\",\n            json_schema_extra={ADD_TO_TEMPLATE: False},\n        ),\n        Parameter(\n            name=(\"--extra-verbose\", \"-vv\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.EXTRA_VERBOSE\n\n    disable_ui: Annotated[\n        bool,\n        Field(\n            description=\"Disable the UI (prints progress to the console as log messages). This is equivalent to --ui-type none.\",\n        ),\n        Parameter(\n            name=(\"--disable-ui\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.DISABLE_UI\n\n    enable_uvloop: Annotated[\n        bool,\n        Field(\n            description=\"Enable the use of uvloop instead of the default asyncio event loop\",\n        ),\n        Parameter(\n            name=(\"--enable-uvloop\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.ENABLE_UVLOOP\n\n    # TODO: Potentially auto-scale this in the future.\n    result_parser_service_count: Annotated[\n        int,\n        Field(\n            description=\"Number of services to spawn for parsing inference results. The higher the request rate, the more services should be spawned.\",\n        ),\n        Parameter(\n            name=(\"--result-parser-service-count\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.RESULT_PARSER_SERVICE_COUNT\n\n    progress_report_interval: Annotated[\n        float,\n        Field(\n            description=\"Interval in seconds to report progress. This is used to report the progress of the profile to the user.\",\n        ),\n        Parameter(\n            name=(\"--progress-report-interval-seconds\", \"--progress-report-interval\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.PROGRESS_REPORT_INTERVAL\n\n    enable_yappi: Annotated[\n        bool,\n        Field(\n            description=\"*[Developer use only]* Enable yappi profiling (Yet Another Python Profiler) to profile AIPerf's internal python code. \"\n            \"This can be used in the development of AIPerf in order to find performance bottlenecks across the various services. \"\n            \"The output '.prof' files can be viewed with snakeviz. Requires yappi and snakeviz to be installed. \"\n            \"Run 'pip install yappi snakeviz' to install them.\",\n        ),\n        Parameter(\n            name=(\"--enable-yappi-profiling\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.ENABLE_YAPPI\n\n    debug_services: Annotated[\n        set[ServiceType] | None,\n        Field(\n            description=\"List of services to enable debug logging for. Can be a comma-separated list, a single service type, \"\n            \"or the cli flag can be used multiple times.\",\n        ),\n        Parameter(\n            name=(\"--debug-service\", \"--debug-services\"),\n            group=_CLI_GROUP,\n        ),\n        BeforeValidator(parse_service_types),\n    ] = ServiceDefaults.DEBUG_SERVICES\n\n    trace_services: Annotated[\n        set[ServiceType] | None,\n        Field(\n            description=\"List of services to enable trace logging for. Can be a comma-separated list, a single service type, \"\n            \"or the cli flag can be used multiple times.\",\n        ),\n        Parameter(\n            name=(\"--trace-service\", \"--trace-services\"),\n            group=_CLI_GROUP,\n        ),\n        BeforeValidator(parse_service_types),\n    ] = ServiceDefaults.TRACE_SERVICES\n</code></pre>"},{"location":"api/#aiperf.common.config.service_config.ServiceConfig.validate_comm_config","title":"<code>validate_comm_config()</code>","text":"<p>Initialize the comm_config if it is not provided, based on the comm_backend.</p> Source code in <code>aiperf/common/config/service_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_comm_config(self) -&gt; Self:\n    \"\"\"Initialize the comm_config if it is not provided, based on the comm_backend.\"\"\"\n    if self.comm_config is None:\n        if self.comm_backend == CommunicationBackend.ZMQ_IPC:\n            self.comm_config = ZMQIPCConfig()\n        elif self.comm_backend == CommunicationBackend.ZMQ_TCP:\n            self.comm_config = ZMQTCPConfig()\n        else:\n            raise ValueError(f\"Invalid communication backend: {self.comm_backend}\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.service_config.ServiceConfig.validate_log_level_from_verbose_flags","title":"<code>validate_log_level_from_verbose_flags()</code>","text":"<p>Set log level based on verbose flags.</p> Source code in <code>aiperf/common/config/service_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_log_level_from_verbose_flags(self) -&gt; Self:\n    \"\"\"Set log level based on verbose flags.\"\"\"\n    if self.extra_verbose:\n        self.log_level = AIPerfLogLevel.TRACE\n    elif self.verbose:\n        self.log_level = AIPerfLogLevel.DEBUG\n    return self\n</code></pre>"},{"location":"api/#aiperfcommonconfigsweep_config","title":"aiperf.common.config.sweep_config","text":""},{"location":"api/#aiperf.common.config.sweep_config.SweepConfig","title":"<code>SweepConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A sweep of parameters.</p> Source code in <code>aiperf/common/config/sweep_config.py</code> <pre><code>class SweepConfig(BaseConfig):\n    \"\"\"A sweep of parameters.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.config.sweep_config.SweepParam","title":"<code>SweepParam</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A parameter to be swept.</p> Source code in <code>aiperf/common/config/sweep_config.py</code> <pre><code>class SweepParam(BaseConfig):\n    \"\"\"A parameter to be swept.\"\"\"\n</code></pre>"},{"location":"api/#aiperfcommonconfigtokenizer_config","title":"aiperf.common.config.tokenizer_config","text":""},{"location":"api/#aiperf.common.config.tokenizer_config.TokenizerConfig","title":"<code>TokenizerConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining tokenizer related settings.</p> Source code in <code>aiperf/common/config/tokenizer_config.py</code> <pre><code>class TokenizerConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining tokenizer related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.TOKENIZER\n\n    name: Annotated[\n        str | None,\n        Field(\n            description=(\n                \"The HuggingFace tokenizer to use to interpret token metrics \"\n                \"from prompts and responses.\\nThe value can be the \"\n                \"name of a tokenizer or the filepath of the tokenizer.\\n\"\n                \"The default value is the model name.\"\n            ),\n        ),\n        Parameter(\n            name=(\"--tokenizer\"),\n            group=_CLI_GROUP,\n        ),\n    ] = TokenizerDefaults.NAME\n\n    revision: Annotated[\n        str,\n        Field(\n            description=(\n                \"The specific model version to use.\\n\"\n                \"It can be a branch name, tag name, or commit ID.\"\n            ),\n        ),\n        Parameter(\n            name=(\"--tokenizer-revision\"),\n            group=_CLI_GROUP,\n        ),\n    ] = TokenizerDefaults.REVISION\n\n    trust_remote_code: Annotated[\n        bool,\n        Field(\n            description=(\n                \"Allows custom tokenizer to be downloaded and executed.\\n\"\n                \"This carries security risks and should only be used for repositories you trust.\\n\"\n                \"This is only necessary for custom tokenizers stored in HuggingFace Hub.\"\n            ),\n        ),\n        Parameter(\n            name=(\"--tokenizer-trust-remote-code\"),\n            group=_CLI_GROUP,\n        ),\n    ] = TokenizerDefaults.TRUST_REMOTE_CODE\n</code></pre>"},{"location":"api/#aiperfcommonconfiguser_config","title":"aiperf.common.config.user_config","text":""},{"location":"api/#aiperf.common.config.user_config.UserConfig","title":"<code>UserConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining top-level user settings.</p> Source code in <code>aiperf/common/config/user_config.py</code> <pre><code>class UserConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining top-level user settings.\n    \"\"\"\n\n    model_names: Annotated[\n        list[str],\n        Field(\n            ...,\n            description=\"Model name(s) to be benchmarked. Can be a comma-separated list or a single model name.\",\n        ),\n        BeforeValidator(parse_str_or_list),\n        Parameter(\n            name=(\n                \"--model-names\",\n                \"--model\",  # GenAI-Perf\n                \"-m\",  # GenAI-Perf\n            ),\n            group=Groups.ENDPOINT,\n        ),\n    ] = EndPointConfig()\n\n    endpoint: Annotated[\n        EndPointConfig,\n        Field(\n            description=\"Endpoint configuration\",\n        ),\n    ]\n\n    input: Annotated[\n        InputConfig,\n        Field(\n            description=\"Input configuration\",\n        ),\n    ] = InputConfig()\n\n    output: Annotated[\n        OutputConfig,\n        Field(\n            description=\"Output configuration\",\n        ),\n    ] = OutputConfig()\n\n    tokenizer: Annotated[\n        TokenizerConfig,\n        Field(\n            description=\"Tokenizer configuration\",\n        ),\n    ] = TokenizerConfig()\n\n    loadgen: Annotated[\n        LoadGeneratorConfig,\n        Field(\n            description=\"Load Generator configuration\",\n        ),\n    ] = LoadGeneratorConfig()\n\n    measurement: Annotated[\n        MeasurementConfig,\n        Field(\n            description=\"Measurement configuration\",\n        ),\n    ] = MeasurementConfig()\n</code></pre>"},{"location":"api/#aiperfcommonconfigworker_config","title":"aiperf.common.config.worker_config","text":""},{"location":"api/#aiperf.common.config.worker_config.WorkersConfig","title":"<code>WorkersConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Worker configuration.</p> Source code in <code>aiperf/common/config/worker_config.py</code> <pre><code>class WorkersConfig(BaseConfig):\n    \"\"\"Worker configuration.\"\"\"\n\n    _CLI_GROUP = Groups.WORKERS\n\n    min: Annotated[\n        int | None,\n        Field(\n            description=\"Minimum number of workers to maintain\",\n        ),\n        Parameter(\n            name=(\"--workers-min\", \"--min-workers\"),\n            group=_CLI_GROUP,\n        ),\n    ] = WorkersDefaults.MIN\n\n    max: Annotated[\n        int | None,\n        Field(\n            description=\"Maximum number of workers to create. If not specified, the number of\"\n            \" workers will be determined by the smaller of (concurrency + 1) and (num CPUs - 1).\",\n        ),\n        Parameter(\n            name=(\"--workers-max\", \"--max-workers\"),\n            group=_CLI_GROUP,\n        ),\n    ] = WorkersDefaults.MAX\n\n    health_check_interval: Annotated[\n        float,\n        Field(\n            description=\"Interval in seconds to for workers to publish their health status.\",\n        ),\n        Parameter(\n            name=(\"--workers-health-check-interval\"),\n            group=_CLI_GROUP,\n        ),\n    ] = WorkersDefaults.HEALTH_CHECK_INTERVAL\n</code></pre>"},{"location":"api/#aiperfcommonconfigzmq_config","title":"aiperf.common.config.zmq_config","text":""},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig","title":"<code>BaseZMQCommunicationConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Configuration for ZMQ communication.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class BaseZMQCommunicationConfig(BaseModel, ABC):\n    \"\"\"Configuration for ZMQ communication.\"\"\"\n\n    # Proxy config options to be overridden by subclasses\n    event_bus_proxy_config: ClassVar[BaseZMQProxyConfig]\n    dataset_manager_proxy_config: ClassVar[BaseZMQProxyConfig]\n    raw_inference_proxy_config: ClassVar[BaseZMQProxyConfig]\n\n    @property\n    @abstractmethod\n    def records_push_pull_address(self) -&gt; str:\n        \"\"\"Get the inference push/pull address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def credit_drop_address(self) -&gt; str:\n        \"\"\"Get the credit drop address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def credit_return_address(self) -&gt; str:\n        \"\"\"Get the credit return address based on protocol configuration.\"\"\"\n\n    def get_address(self, address_type: CommAddress) -&gt; str:\n        \"\"\"Get the actual address based on the address type.\"\"\"\n        address_map = {\n            CommAddress.EVENT_BUS_PROXY_FRONTEND: self.event_bus_proxy_config.frontend_address,\n            CommAddress.EVENT_BUS_PROXY_BACKEND: self.event_bus_proxy_config.backend_address,\n            CommAddress.DATASET_MANAGER_PROXY_FRONTEND: self.dataset_manager_proxy_config.frontend_address,\n            CommAddress.DATASET_MANAGER_PROXY_BACKEND: self.dataset_manager_proxy_config.backend_address,\n            CommAddress.CREDIT_DROP: self.credit_drop_address,\n            CommAddress.CREDIT_RETURN: self.credit_return_address,\n            CommAddress.RECORDS: self.records_push_pull_address,\n            CommAddress.RAW_INFERENCE_PROXY_FRONTEND: self.raw_inference_proxy_config.frontend_address,\n            CommAddress.RAW_INFERENCE_PROXY_BACKEND: self.raw_inference_proxy_config.backend_address,\n        }\n\n        if address_type not in address_map:\n            raise ValueError(f\"Invalid address type: {address_type}\")\n\n        return address_map[address_type]\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.credit_drop_address","title":"<code>credit_drop_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the credit drop address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.credit_return_address","title":"<code>credit_return_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the credit return address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.records_push_pull_address","title":"<code>records_push_pull_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the inference push/pull address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.get_address","title":"<code>get_address(address_type)</code>","text":"<p>Get the actual address based on the address type.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>def get_address(self, address_type: CommAddress) -&gt; str:\n    \"\"\"Get the actual address based on the address type.\"\"\"\n    address_map = {\n        CommAddress.EVENT_BUS_PROXY_FRONTEND: self.event_bus_proxy_config.frontend_address,\n        CommAddress.EVENT_BUS_PROXY_BACKEND: self.event_bus_proxy_config.backend_address,\n        CommAddress.DATASET_MANAGER_PROXY_FRONTEND: self.dataset_manager_proxy_config.frontend_address,\n        CommAddress.DATASET_MANAGER_PROXY_BACKEND: self.dataset_manager_proxy_config.backend_address,\n        CommAddress.CREDIT_DROP: self.credit_drop_address,\n        CommAddress.CREDIT_RETURN: self.credit_return_address,\n        CommAddress.RECORDS: self.records_push_pull_address,\n        CommAddress.RAW_INFERENCE_PROXY_FRONTEND: self.raw_inference_proxy_config.frontend_address,\n        CommAddress.RAW_INFERENCE_PROXY_BACKEND: self.raw_inference_proxy_config.backend_address,\n    }\n\n    if address_type not in address_map:\n        raise ValueError(f\"Invalid address type: {address_type}\")\n\n    return address_map[address_type]\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig","title":"<code>BaseZMQProxyConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Configuration Protocol for ZMQ Proxy.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class BaseZMQProxyConfig(BaseModel, ABC):\n    \"\"\"Configuration Protocol for ZMQ Proxy.\"\"\"\n\n    @property\n    @abstractmethod\n    def frontend_address(self) -&gt; str:\n        \"\"\"Get the frontend address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def backend_address(self) -&gt; str:\n        \"\"\"Get the backend address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def control_address(self) -&gt; str | None:\n        \"\"\"Get the control address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def capture_address(self) -&gt; str | None:\n        \"\"\"Get the capture address based on protocol configuration.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.backend_address","title":"<code>backend_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the backend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.capture_address","title":"<code>capture_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the capture address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.control_address","title":"<code>control_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the control address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.frontend_address","title":"<code>frontend_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the frontend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig","title":"<code>ZMQIPCConfig</code>","text":"<p>               Bases: <code>BaseZMQCommunicationConfig</code></p> <p>Configuration for IPC transport.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQIPCConfig(BaseZMQCommunicationConfig):\n    \"\"\"Configuration for IPC transport.\"\"\"\n\n    path: str = Field(default=\"/tmp/aiperf\", description=\"Path for IPC sockets\")\n    dataset_manager_proxy_config: ZMQIPCProxyConfig = Field(  # type: ignore\n        default=ZMQIPCProxyConfig(name=\"dataset_manager_proxy\"),\n        description=\"Configuration for the ZMQ Dealer Router Proxy. If provided, the proxy will be created and started.\",\n    )\n    event_bus_proxy_config: ZMQIPCProxyConfig = Field(  # type: ignore\n        default=ZMQIPCProxyConfig(name=\"event_bus_proxy\"),\n        description=\"Configuration for the ZMQ XPUB/XSUB Proxy. If provided, the proxy will be created and started.\",\n    )\n    raw_inference_proxy_config: ZMQIPCProxyConfig = Field(  # type: ignore\n        default=ZMQIPCProxyConfig(name=\"raw_inference_proxy\"),\n        description=\"Configuration for the ZMQ Push/Pull Proxy. If provided, the proxy will be created and started.\",\n    )\n\n    @property\n    def records_push_pull_address(self) -&gt; str:\n        \"\"\"Get the records push/pull address based on protocol configuration.\"\"\"\n        return f\"ipc://{self.path}/records_push_pull.ipc\"\n\n    @property\n    def credit_drop_address(self) -&gt; str:\n        \"\"\"Get the credit drop address based on protocol configuration.\"\"\"\n        return f\"ipc://{self.path}/credit_drop.ipc\"\n\n    @property\n    def credit_return_address(self) -&gt; str:\n        \"\"\"Get the credit return address based on protocol configuration.\"\"\"\n        return f\"ipc://{self.path}/credit_return.ipc\"\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.credit_drop_address","title":"<code>credit_drop_address</code>  <code>property</code>","text":"<p>Get the credit drop address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.credit_return_address","title":"<code>credit_return_address</code>  <code>property</code>","text":"<p>Get the credit return address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.records_push_pull_address","title":"<code>records_push_pull_address</code>  <code>property</code>","text":"<p>Get the records push/pull address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig","title":"<code>ZMQIPCProxyConfig</code>","text":"<p>               Bases: <code>BaseZMQProxyConfig</code></p> <p>Configuration for IPC proxy.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQIPCProxyConfig(BaseZMQProxyConfig):\n    \"\"\"Configuration for IPC proxy.\"\"\"\n\n    path: str = Field(default=\"/tmp/aiperf\", description=\"Path for IPC sockets\")\n    name: str = Field(default=\"proxy\", description=\"Name for IPC sockets\")\n    enable_control: bool = Field(default=False, description=\"Enable control socket\")\n    enable_capture: bool = Field(default=False, description=\"Enable capture socket\")\n\n    @property\n    def frontend_address(self) -&gt; str:\n        \"\"\"Get the frontend address based on protocol configuration.\"\"\"\n        return f\"ipc://{self.path}/{self.name}_frontend.ipc\"\n\n    @property\n    def backend_address(self) -&gt; str:\n        \"\"\"Get the backend address based on protocol configuration.\"\"\"\n        return f\"ipc://{self.path}/{self.name}_backend.ipc\"\n\n    @property\n    def control_address(self) -&gt; str | None:\n        \"\"\"Get the control address based on protocol configuration.\"\"\"\n        return (\n            f\"ipc://{self.path}/{self.name}_control.ipc\"\n            if self.enable_control\n            else None\n        )\n\n    @property\n    def capture_address(self) -&gt; str | None:\n        \"\"\"Get the capture address based on protocol configuration.\"\"\"\n        return (\n            f\"ipc://{self.path}/{self.name}_capture.ipc\"\n            if self.enable_capture\n            else None\n        )\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.backend_address","title":"<code>backend_address</code>  <code>property</code>","text":"<p>Get the backend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.capture_address","title":"<code>capture_address</code>  <code>property</code>","text":"<p>Get the capture address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.control_address","title":"<code>control_address</code>  <code>property</code>","text":"<p>Get the control address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.frontend_address","title":"<code>frontend_address</code>  <code>property</code>","text":"<p>Get the frontend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig","title":"<code>ZMQTCPConfig</code>","text":"<p>               Bases: <code>BaseZMQCommunicationConfig</code></p> <p>Configuration for TCP transport.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQTCPConfig(BaseZMQCommunicationConfig):\n    \"\"\"Configuration for TCP transport.\"\"\"\n\n    host: str = Field(\n        default=\"0.0.0.0\",\n        description=\"Host address for TCP connections\",\n    )\n    records_push_pull_port: int = Field(\n        default=5557, description=\"Port for inference push/pull messages\"\n    )\n    credit_drop_port: int = Field(\n        default=5562, description=\"Port for credit drop operations\"\n    )\n    credit_return_port: int = Field(\n        default=5563, description=\"Port for credit return operations\"\n    )\n    dataset_manager_proxy_config: ZMQTCPProxyConfig = Field(  # type: ignore\n        default=ZMQTCPProxyConfig(\n            frontend_port=5661,\n            backend_port=5662,\n        ),\n        description=\"Configuration for the ZMQ Proxy. If provided, the proxy will be created and started.\",\n    )\n    event_bus_proxy_config: ZMQTCPProxyConfig = Field(  # type: ignore\n        default=ZMQTCPProxyConfig(\n            frontend_port=5663,\n            backend_port=5664,\n        ),\n        description=\"Configuration for the ZMQ Proxy. If provided, the proxy will be created and started.\",\n    )\n    raw_inference_proxy_config: ZMQTCPProxyConfig = Field(  # type: ignore\n        default=ZMQTCPProxyConfig(\n            frontend_port=5665,\n            backend_port=5666,\n        ),\n        description=\"Configuration for the ZMQ Proxy. If provided, the proxy will be created and started.\",\n    )\n\n    @property\n    def records_push_pull_address(self) -&gt; str:\n        \"\"\"Get the records push/pull address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.records_push_pull_port}\"\n\n    @property\n    def credit_drop_address(self) -&gt; str:\n        \"\"\"Get the credit drop address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.credit_drop_port}\"\n\n    @property\n    def credit_return_address(self) -&gt; str:\n        \"\"\"Get the credit return address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.credit_return_port}\"\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.credit_drop_address","title":"<code>credit_drop_address</code>  <code>property</code>","text":"<p>Get the credit drop address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.credit_return_address","title":"<code>credit_return_address</code>  <code>property</code>","text":"<p>Get the credit return address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.records_push_pull_address","title":"<code>records_push_pull_address</code>  <code>property</code>","text":"<p>Get the records push/pull address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig","title":"<code>ZMQTCPProxyConfig</code>","text":"<p>               Bases: <code>BaseZMQProxyConfig</code></p> <p>Configuration for TCP proxy.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQTCPProxyConfig(BaseZMQProxyConfig):\n    \"\"\"Configuration for TCP proxy.\"\"\"\n\n    host: str = Field(\n        default=\"0.0.0.0\",\n        description=\"Host address for TCP connections\",\n    )\n    frontend_port: int = Field(\n        default=15555, description=\"Port for frontend address for proxy\"\n    )\n    backend_port: int = Field(\n        default=15556, description=\"Port for backend address for proxy\"\n    )\n    control_port: int | None = Field(\n        default=None, description=\"Port for control address for proxy\"\n    )\n    capture_port: int | None = Field(\n        default=None, description=\"Port for capture address for proxy\"\n    )\n\n    @property\n    def frontend_address(self) -&gt; str:\n        \"\"\"Get the frontend address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.frontend_port}\"\n\n    @property\n    def backend_address(self) -&gt; str:\n        \"\"\"Get the backend address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.backend_port}\"\n\n    @property\n    def control_address(self) -&gt; str | None:\n        \"\"\"Get the control address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.control_port}\" if self.control_port else None\n\n    @property\n    def capture_address(self) -&gt; str | None:\n        \"\"\"Get the capture address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.capture_port}\" if self.capture_port else None\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.backend_address","title":"<code>backend_address</code>  <code>property</code>","text":"<p>Get the backend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.capture_address","title":"<code>capture_address</code>  <code>property</code>","text":"<p>Get the capture address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.control_address","title":"<code>control_address</code>  <code>property</code>","text":"<p>Get the control address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.frontend_address","title":"<code>frontend_address</code>  <code>property</code>","text":"<p>Get the frontend address based on protocol configuration.</p>"},{"location":"api/#aiperfcommonconstants","title":"aiperf.common.constants","text":""},{"location":"api/#aiperf.common.constants.DEFAULT_COMMS_REQUEST_TIMEOUT","title":"<code>DEFAULT_COMMS_REQUEST_TIMEOUT = 10.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for requests from req_clients to rep_clients in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_PULL_CLIENT_MAX_CONCURRENCY","title":"<code>DEFAULT_PULL_CLIENT_MAX_CONCURRENCY = 100000</code>  <code>module-attribute</code>","text":"<p>Default maximum concurrency for pull clients.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_SERVICE_REGISTRATION_TIMEOUT","title":"<code>DEFAULT_SERVICE_REGISTRATION_TIMEOUT = 5.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for service registration in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_SERVICE_START_TIMEOUT","title":"<code>DEFAULT_SERVICE_START_TIMEOUT = 5.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for service start in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_STREAMING_MAX_QUEUE_SIZE","title":"<code>DEFAULT_STREAMING_MAX_QUEUE_SIZE = 100000</code>  <code>module-attribute</code>","text":"<p>Default maximum queue size for streaming post processors.</p>"},{"location":"api/#aiperf.common.constants.TASK_CANCEL_TIMEOUT_LONG","title":"<code>TASK_CANCEL_TIMEOUT_LONG = 5.0</code>  <code>module-attribute</code>","text":"<p>Maximum time to wait for complex tasks to complete when cancelling them (like parent tasks).</p>"},{"location":"api/#aiperf.common.constants.TASK_CANCEL_TIMEOUT_SHORT","title":"<code>TASK_CANCEL_TIMEOUT_SHORT = 2.0</code>  <code>module-attribute</code>","text":"<p>Maximum time to wait for simple tasks to complete when cancelling them.</p>"},{"location":"api/#aiperfcommondecorators","title":"aiperf.common.decorators","text":"<p>Decorators for AIPerf components. Note that these are not the same as hooks. Hooks are used to specify that a function should be called at a specific time, while decorators are used to specify that a class or function should be treated a specific way.</p> <p>see also: :mod:<code>aiperf.common.hooks</code> for hook decorators.</p>"},{"location":"api/#aiperf.common.decorators.DecoratorAttrs","title":"<code>DecoratorAttrs</code>","text":"<p>Constant attribute names for decorators.</p> <p>When you decorate a class with a decorator, the decorator type and parameters are set as attributes on the class.</p> Source code in <code>aiperf/common/decorators.py</code> <pre><code>class DecoratorAttrs:\n    \"\"\"Constant attribute names for decorators.\n\n    When you decorate a class with a decorator, the decorator type and parameters are\n    set as attributes on the class.\n    \"\"\"\n\n    IMPLEMENTS_PROTOCOL = \"__implements_protocol__\"\n</code></pre>"},{"location":"api/#aiperf.common.decorators.implements_protocol","title":"<code>implements_protocol(protocol)</code>","text":"<p>Decorator to specify that the class implements the given protocol.</p> <p>Example:</p> <pre><code>@implements_protocol(ServiceProtocol)\nclass BaseService:\n    pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>BaseService.__implements_protocol__ = ServiceProtocol\n</code></pre> Source code in <code>aiperf/common/decorators.py</code> <pre><code>def implements_protocol(protocol: type[ProtocolT]) -&gt; Callable:\n    \"\"\"Decorator to specify that the class implements the given protocol.\n\n    Example:\n    ```python\n    @implements_protocol(ServiceProtocol)\n    class BaseService:\n        pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    BaseService.__implements_protocol__ = ServiceProtocol\n    ```\n    \"\"\"\n\n    def decorator(cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n        if TYPE_CHECKING:\n            if not hasattr(protocol, \"_is_runtime_protocol\"):\n                warn(\n                    f\"Protocol {protocol.__name__} is not a runtime protocol. \"\n                    \"Please use the @runtime_checkable decorator to mark it as a runtime protocol.\",\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n                raise TypeError(\n                    f\"Protocol {protocol.__name__} is not a runtime protocol. \"\n                    \"Please use the @runtime_checkable decorator to mark it as a runtime protocol.\"\n                )\n            if not issubclass(cls, protocol):\n                warn(\n                    f\"Class {cls.__name__} does not implement the {protocol.__name__} protocol.\",\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n                raise TypeError(\n                    f\"Class {cls.__name__} does not implement the {protocol.__name__} protocol.\"\n                )\n        setattr(cls, DecoratorAttrs.IMPLEMENTS_PROTOCOL, protocol)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperfcommonenumsbase_enums","title":"aiperf.common.enums.base_enums","text":""},{"location":"api/#aiperf.common.enums.base_enums.CaseInsensitiveStrEnum","title":"<code>CaseInsensitiveStrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>CaseInsensitiveStrEnum is a custom enumeration class that extends <code>str</code> and <code>Enum</code> to provide case-insensitive lookup functionality for its members.</p> Source code in <code>aiperf/common/enums/base_enums.py</code> <pre><code>class CaseInsensitiveStrEnum(str, Enum):\n    \"\"\"\n    CaseInsensitiveStrEnum is a custom enumeration class that extends `str` and `Enum` to provide case-insensitive\n    lookup functionality for its members.\n    \"\"\"\n\n    def __str__(self) -&gt; str:\n        return self.value\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}.{self.name}\"\n\n    def __eq__(self, other: str | Enum) -&gt; bool:\n        if isinstance(other, str):\n            return self.value.lower() == other.lower()\n        if isinstance(other, Enum):\n            return self.value.lower() == other.value.lower()\n        return super().__eq__(other)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.value.lower())\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Handles cases where a value is not directly found in the enumeration.\n\n        This method is called when an attempt is made to access an enumeration\n        member using a value that does not directly match any of the defined\n        members. It provides custom logic to handle such cases.\n\n        Returns:\n            The matching enumeration member if a case-insensitive match is found\n            for string values; otherwise, returns None.\n        \"\"\"\n        if isinstance(value, str):\n            for member in cls:\n                if member.value.lower() == value.lower():\n                    return member\n        return None\n</code></pre>"},{"location":"api/#aiperfcommonenumsbenchmark_suite_enums","title":"aiperf.common.enums.benchmark_suite_enums","text":""},{"location":"api/#aiperf.common.enums.benchmark_suite_enums.BenchmarkSuiteCompletionTrigger","title":"<code>BenchmarkSuiteCompletionTrigger</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines how the suite completion is determined in order to know how to track the progress.</p> Source code in <code>aiperf/common/enums/benchmark_suite_enums.py</code> <pre><code>class BenchmarkSuiteCompletionTrigger(CaseInsensitiveStrEnum):\n    \"\"\"Determines how the suite completion is determined in order to know how to track the progress.\"\"\"\n\n    COMPLETED_PROFILES = \"completed_profiles\"\n    \"\"\"The suite will run until all profiles are completed.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.benchmark_suite_enums.BenchmarkSuiteCompletionTrigger.COMPLETED_PROFILES","title":"<code>COMPLETED_PROFILES = 'completed_profiles'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The suite will run until all profiles are completed.</p>"},{"location":"api/#aiperf.common.enums.benchmark_suite_enums.BenchmarkSuiteType","title":"<code>BenchmarkSuiteType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines the type of suite to know how to track the progress.</p> Source code in <code>aiperf/common/enums/benchmark_suite_enums.py</code> <pre><code>class BenchmarkSuiteType(CaseInsensitiveStrEnum):\n    \"\"\"Determines the type of suite to know how to track the progress.\"\"\"\n\n    SINGLE_PROFILE = \"single_profile\"\n    \"\"\"A suite with a single profile run.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.benchmark_suite_enums.BenchmarkSuiteType.SINGLE_PROFILE","title":"<code>SINGLE_PROFILE = 'single_profile'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A suite with a single profile run.</p>"},{"location":"api/#aiperfcommonenumscommand_enums","title":"aiperf.common.enums.command_enums","text":""},{"location":"api/#aiperfcommonenumscommunication_enums","title":"aiperf.common.enums.communication_enums","text":""},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress","title":"<code>CommAddress</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Enum for specifying the address type for communication clients. This is used to lookup the address in the communication config.</p> Source code in <code>aiperf/common/enums/communication_enums.py</code> <pre><code>class CommAddress(CaseInsensitiveStrEnum):\n    \"\"\"Enum for specifying the address type for communication clients.\n    This is used to lookup the address in the communication config.\"\"\"\n\n    EVENT_BUS_PROXY_FRONTEND = \"event_bus_proxy_frontend\"\n    \"\"\"Frontend address for services to publish messages to.\"\"\"\n\n    EVENT_BUS_PROXY_BACKEND = \"event_bus_proxy_backend\"\n    \"\"\"Backend address for services to subscribe to messages.\"\"\"\n\n    CREDIT_DROP = \"credit_drop\"\n    \"\"\"Address to send CreditDrop messages from the TimingManager to the Worker.\"\"\"\n\n    CREDIT_RETURN = \"credit_return\"\n    \"\"\"Address to send CreditReturn messages from the Worker to the TimingManager.\"\"\"\n\n    RECORDS = \"records\"\n    \"\"\"Address to send parsed records from InferenceParser to RecordManager.\"\"\"\n\n    DATASET_MANAGER_PROXY_FRONTEND = \"dataset_manager_proxy_frontend\"\n    \"\"\"Frontend address for sending requests to the DatasetManager.\"\"\"\n\n    DATASET_MANAGER_PROXY_BACKEND = \"dataset_manager_proxy_backend\"\n    \"\"\"Backend address for the DatasetManager to receive requests from clients.\"\"\"\n\n    RAW_INFERENCE_PROXY_FRONTEND = \"raw_inference_proxy_frontend\"\n    \"\"\"Frontend address for sending raw inference messages to the InferenceParser from Workers.\"\"\"\n\n    RAW_INFERENCE_PROXY_BACKEND = \"raw_inference_proxy_backend\"\n    \"\"\"Backend address for the InferenceParser to receive raw inference messages from Workers.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.CREDIT_DROP","title":"<code>CREDIT_DROP = 'credit_drop'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address to send CreditDrop messages from the TimingManager to the Worker.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.CREDIT_RETURN","title":"<code>CREDIT_RETURN = 'credit_return'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address to send CreditReturn messages from the Worker to the TimingManager.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.DATASET_MANAGER_PROXY_BACKEND","title":"<code>DATASET_MANAGER_PROXY_BACKEND = 'dataset_manager_proxy_backend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend address for the DatasetManager to receive requests from clients.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.DATASET_MANAGER_PROXY_FRONTEND","title":"<code>DATASET_MANAGER_PROXY_FRONTEND = 'dataset_manager_proxy_frontend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frontend address for sending requests to the DatasetManager.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.EVENT_BUS_PROXY_BACKEND","title":"<code>EVENT_BUS_PROXY_BACKEND = 'event_bus_proxy_backend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend address for services to subscribe to messages.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.EVENT_BUS_PROXY_FRONTEND","title":"<code>EVENT_BUS_PROXY_FRONTEND = 'event_bus_proxy_frontend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frontend address for services to publish messages to.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.RAW_INFERENCE_PROXY_BACKEND","title":"<code>RAW_INFERENCE_PROXY_BACKEND = 'raw_inference_proxy_backend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend address for the InferenceParser to receive raw inference messages from Workers.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.RAW_INFERENCE_PROXY_FRONTEND","title":"<code>RAW_INFERENCE_PROXY_FRONTEND = 'raw_inference_proxy_frontend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frontend address for sending raw inference messages to the InferenceParser from Workers.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.RECORDS","title":"<code>RECORDS = 'records'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address to send parsed records from InferenceParser to RecordManager.</p>"},{"location":"api/#aiperfcommonenumsdata_exporter_enums","title":"aiperf.common.enums.data_exporter_enums","text":""},{"location":"api/#aiperfcommonenumsdataset_enums","title":"aiperf.common.enums.dataset_enums","text":""},{"location":"api/#aiperfcommonenumsendpoints_enums","title":"aiperf.common.enums.endpoints_enums","text":""},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType","title":"<code>EndpointType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Endpoint types.</p> <p>These determine the format of request payload to send to the model.</p> <p>Similar to <code>endpoint_type_map</code> and <code>OutputFormat</code> from <code>genai-perf</code>.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>class EndpointType(CaseInsensitiveStrEnum):\n    \"\"\"Endpoint types.\n\n    These determine the format of request payload to send to the model.\n\n    Similar to `endpoint_type_map` and `OutputFormat` from `genai-perf`.\n    \"\"\"\n\n    OPENAI_CHAT_COMPLETIONS = \"chat\"\n    OPENAI_COMPLETIONS = \"completions\"\n    OPENAI_EMBEDDINGS = \"embeddings\"\n    # OPENAI_MULTIMODAL = \"multimodal\"\n    OPENAI_RESPONSES = \"responses\"\n\n    # TODO: implement other endpoints\n    # HUGGINGFACE_GENERATE = \"generate\"\n\n    # DYNAMIC_GRPC = \"dynamic_grpc\"\n    # NVCLIP = \"nvclip\"\n    # TEMPLATE = \"template\"\n\n    # RANKINGS = \"rankings\"\n    # IMAGE_RETRIEVAL = \"image_retrieval\"\n\n    # TENSORRTLLM = \"tensorrtllm\"\n    # TENSORRTLLM_ENGINE = \"tensorrtllm_engine\"\n\n    # TRITON_GENERATE = \"triton_generate\"\n\n    # DYNAMO_ENGINE = \"dynamo_engine\"\n\n    def endpoint_path(self) -&gt; str | None:\n        \"\"\"Get the endpoint path for the endpoint type.\"\"\"\n        endpoint_path_map = {\n            # OpenAI endpoints\n            EndpointType.OPENAI_CHAT_COMPLETIONS: \"/v1/chat/completions\",\n            # EndpointType.OPENAI_MULTIMODAL: \"/v1/chat/completions\",\n            EndpointType.OPENAI_COMPLETIONS: \"/v1/completions\",\n            EndpointType.OPENAI_EMBEDDINGS: \"/v1/embeddings\",\n            EndpointType.OPENAI_RESPONSES: \"/v1/responses\",\n            # TODO: implement other endpoints\n            # Other\n            # EndpointType.NVCLIP: \"/v1/embeddings\",\n            # EndpointType.HUGGINGFACE_GENERATE: \"/\",  # HuggingFace TGI only exposes root endpoint\n            # EndpointType.RANKINGS: \"/v1/ranking\",  # TODO: Not implemented yet\n            # EndpointType.IMAGE_RETRIEVAL: \"/v1/infer\",  # TODO: Not implemented yet\n            # EndpointType.TRITON_GENERATE: \"/v2/models/{MODEL_NAME}/generate\",  # TODO: Not implemented yet\n            # # These endpoints do not have a specific path\n            # EndpointType.DYNAMIC_GRPC: None,  # TODO: Not implemented yet\n            # EndpointType.TEMPLATE: None,  # TODO: Not implemented yet\n            # EndpointType.TENSORRTLLM: None,  # TODO: Not implemented yet\n            # EndpointType.TENSORRTLLM_ENGINE: None,  # TODO: Not implemented yet\n            # EndpointType.DYNAMO_ENGINE: None,  # TODO: Not implemented yet\n        }\n\n        if self not in endpoint_path_map:\n            raise NotImplementedError(f\"Endpoint not implemented for {self}\")\n\n        return endpoint_path_map[self]\n\n    def metrics_title(self) -&gt; str:\n        \"\"\"Get the title string for the endpoint type.\"\"\"\n        metrics_title_map = {\n            EndpointType.OPENAI_EMBEDDINGS: \"Embeddings Metrics\",\n            # EndpointType.RANKINGS: \"Rankings Metrics\",\n            # EndpointType.IMAGE_RETRIEVAL: \"Image Retrieval Metrics\",\n            # EndpointType.OPENAI_MULTIMODAL: \"Multi-Modal Metrics\",\n        }\n        return metrics_title_map.get(self, \"LLM Metrics\")\n\n    def response_payload_type(self) -&gt; \"ResponsePayloadType\":\n        \"\"\"Get the response payload type for the request payload type.\"\"\"\n        return ResponsePayloadType.from_endpoint_type(self)\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.endpoint_path","title":"<code>endpoint_path()</code>","text":"<p>Get the endpoint path for the endpoint type.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>def endpoint_path(self) -&gt; str | None:\n    \"\"\"Get the endpoint path for the endpoint type.\"\"\"\n    endpoint_path_map = {\n        # OpenAI endpoints\n        EndpointType.OPENAI_CHAT_COMPLETIONS: \"/v1/chat/completions\",\n        # EndpointType.OPENAI_MULTIMODAL: \"/v1/chat/completions\",\n        EndpointType.OPENAI_COMPLETIONS: \"/v1/completions\",\n        EndpointType.OPENAI_EMBEDDINGS: \"/v1/embeddings\",\n        EndpointType.OPENAI_RESPONSES: \"/v1/responses\",\n        # TODO: implement other endpoints\n        # Other\n        # EndpointType.NVCLIP: \"/v1/embeddings\",\n        # EndpointType.HUGGINGFACE_GENERATE: \"/\",  # HuggingFace TGI only exposes root endpoint\n        # EndpointType.RANKINGS: \"/v1/ranking\",  # TODO: Not implemented yet\n        # EndpointType.IMAGE_RETRIEVAL: \"/v1/infer\",  # TODO: Not implemented yet\n        # EndpointType.TRITON_GENERATE: \"/v2/models/{MODEL_NAME}/generate\",  # TODO: Not implemented yet\n        # # These endpoints do not have a specific path\n        # EndpointType.DYNAMIC_GRPC: None,  # TODO: Not implemented yet\n        # EndpointType.TEMPLATE: None,  # TODO: Not implemented yet\n        # EndpointType.TENSORRTLLM: None,  # TODO: Not implemented yet\n        # EndpointType.TENSORRTLLM_ENGINE: None,  # TODO: Not implemented yet\n        # EndpointType.DYNAMO_ENGINE: None,  # TODO: Not implemented yet\n    }\n\n    if self not in endpoint_path_map:\n        raise NotImplementedError(f\"Endpoint not implemented for {self}\")\n\n    return endpoint_path_map[self]\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.metrics_title","title":"<code>metrics_title()</code>","text":"<p>Get the title string for the endpoint type.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>def metrics_title(self) -&gt; str:\n    \"\"\"Get the title string for the endpoint type.\"\"\"\n    metrics_title_map = {\n        EndpointType.OPENAI_EMBEDDINGS: \"Embeddings Metrics\",\n        # EndpointType.RANKINGS: \"Rankings Metrics\",\n        # EndpointType.IMAGE_RETRIEVAL: \"Image Retrieval Metrics\",\n        # EndpointType.OPENAI_MULTIMODAL: \"Multi-Modal Metrics\",\n    }\n    return metrics_title_map.get(self, \"LLM Metrics\")\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.response_payload_type","title":"<code>response_payload_type()</code>","text":"<p>Get the response payload type for the request payload type.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>def response_payload_type(self) -&gt; \"ResponsePayloadType\":\n    \"\"\"Get the response payload type for the request payload type.\"\"\"\n    return ResponsePayloadType.from_endpoint_type(self)\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.ResponsePayloadType","title":"<code>ResponsePayloadType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Response payload types.</p> <p>These determine the format of the response payload that the model will return.</p> <p>Equivalent to <code>output_format</code> from <code>genai-perf</code>.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>class ResponsePayloadType(CaseInsensitiveStrEnum):\n    \"\"\"Response payload types.\n\n    These determine the format of the response payload that the model will return.\n\n    Equivalent to `output_format` from `genai-perf`.\n    \"\"\"\n\n    OPENAI_CHAT_COMPLETIONS = \"openai_chat_completions\"\n    OPENAI_COMPLETIONS = \"openai_completions\"\n    OPENAI_EMBEDDINGS = \"openai_embeddings\"\n    # OPENAI_MULTIMODAL = \"openai_multimodal\"\n    OPENAI_RESPONSES = \"openai_responses\"\n\n    # TODO: implement other endpoints\n    # HUGGINGFACE_GENERATE = \"huggingface_generate\"\n\n    # RANKINGS = \"rankings\"\n\n    # IMAGE_RETRIEVAL = \"image_retrieval\"\n\n    @classmethod\n    def from_endpoint_type(cls, endpoint_type: EndpointType) -&gt; \"ResponsePayloadType\":\n        \"\"\"Get the response payload type for the endpoint type.\"\"\"\n        endpoint_to_payload_map = {\n            EndpointType.OPENAI_CHAT_COMPLETIONS: ResponsePayloadType.OPENAI_CHAT_COMPLETIONS,\n            # EndpointType.OPENAI_MULTIMODAL: ResponsePayloadType.OPENAI_CHAT_COMPLETIONS,\n            EndpointType.OPENAI_COMPLETIONS: ResponsePayloadType.OPENAI_COMPLETIONS,\n            EndpointType.OPENAI_EMBEDDINGS: ResponsePayloadType.OPENAI_EMBEDDINGS,\n            EndpointType.OPENAI_RESPONSES: ResponsePayloadType.OPENAI_RESPONSES,\n            # TODO: implement other endpoints\n            # EndpointType.HUGGINGFACE_GENERATE: ResponsePayloadType.HUGGINGFACE_GENERATE,\n            # EndpointType.RANKINGS: ResponsePayloadType.RANKINGS,\n            # EndpointType.IMAGE_RETRIEVAL: ResponsePayloadType.IMAGE_RETRIEVAL,\n        }\n\n        if endpoint_type not in endpoint_to_payload_map:\n            raise NotImplementedError(\n                f\"Payload type not implemented for {endpoint_type}\"\n            )\n\n        return endpoint_to_payload_map[endpoint_type]\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.ResponsePayloadType.from_endpoint_type","title":"<code>from_endpoint_type(endpoint_type)</code>  <code>classmethod</code>","text":"<p>Get the response payload type for the endpoint type.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>@classmethod\ndef from_endpoint_type(cls, endpoint_type: EndpointType) -&gt; \"ResponsePayloadType\":\n    \"\"\"Get the response payload type for the endpoint type.\"\"\"\n    endpoint_to_payload_map = {\n        EndpointType.OPENAI_CHAT_COMPLETIONS: ResponsePayloadType.OPENAI_CHAT_COMPLETIONS,\n        # EndpointType.OPENAI_MULTIMODAL: ResponsePayloadType.OPENAI_CHAT_COMPLETIONS,\n        EndpointType.OPENAI_COMPLETIONS: ResponsePayloadType.OPENAI_COMPLETIONS,\n        EndpointType.OPENAI_EMBEDDINGS: ResponsePayloadType.OPENAI_EMBEDDINGS,\n        EndpointType.OPENAI_RESPONSES: ResponsePayloadType.OPENAI_RESPONSES,\n        # TODO: implement other endpoints\n        # EndpointType.HUGGINGFACE_GENERATE: ResponsePayloadType.HUGGINGFACE_GENERATE,\n        # EndpointType.RANKINGS: ResponsePayloadType.RANKINGS,\n        # EndpointType.IMAGE_RETRIEVAL: ResponsePayloadType.IMAGE_RETRIEVAL,\n    }\n\n    if endpoint_type not in endpoint_to_payload_map:\n        raise NotImplementedError(\n            f\"Payload type not implemented for {endpoint_type}\"\n        )\n\n    return endpoint_to_payload_map[endpoint_type]\n</code></pre>"},{"location":"api/#aiperfcommonenumslogging_enums","title":"aiperf.common.enums.logging_enums","text":""},{"location":"api/#aiperfcommonenumsmeasurement_enums","title":"aiperf.common.enums.measurement_enums","text":""},{"location":"api/#aiperfcommonenumsmessage_enums","title":"aiperf.common.enums.message_enums","text":""},{"location":"api/#aiperf.common.enums.message_enums.MessageType","title":"<code>MessageType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The various types of messages that can be sent between services.</p> <p>The message type is used to determine what Pydantic model the message maps to, based on the message_type field in the message model. For detailed explanations of each message type, go to its definition in :mod:<code>aiperf.common.messages</code>.</p> Source code in <code>aiperf/common/enums/message_enums.py</code> <pre><code>class MessageType(CaseInsensitiveStrEnum):\n    \"\"\"The various types of messages that can be sent between services.\n\n    The message type is used to determine what Pydantic model the message maps to,\n    based on the message_type field in the message model. For detailed explanations\n    of each message type, go to its definition in :mod:`aiperf.common.messages`.\n    \"\"\"\n\n    ALL_RECORDS_RECEIVED = \"all_records_received\"\n    COMMAND = \"command\"\n    COMMAND_RESPONSE = \"command_response\"\n    CONVERSATION_REQUEST = \"conversation_request\"\n    CONVERSATION_RESPONSE = \"conversation_response\"\n    CONVERSATION_TURN_REQUEST = \"conversation_turn_request\"\n    CONVERSATION_TURN_RESPONSE = \"conversation_turn_response\"\n    CREDITS_COMPLETE = \"credits_complete\"\n    CREDIT_DROP = \"credit_drop\"\n    CREDIT_PHASE_COMPLETE = \"credit_phase_complete\"\n    CREDIT_PHASE_PROGRESS = \"credit_phase_progress\"\n    CREDIT_PHASE_SENDING_COMPLETE = \"credit_phase_sending_complete\"\n    CREDIT_PHASE_START = \"credit_phase_start\"\n    CREDIT_RETURN = \"credit_return\"\n    DATASET_CONFIGURED_NOTIFICATION = \"dataset_configured_notification\"\n    DATASET_TIMING_REQUEST = \"dataset_timing_request\"\n    DATASET_TIMING_RESPONSE = \"dataset_timing_response\"\n    ERROR = \"error\"\n    HEARTBEAT = \"heartbeat\"\n    INFERENCE_RESULTS = \"inference_results\"\n    NOTIFICATION = \"notification\"\n    PARSED_INFERENCE_RESULTS = \"parsed_inference_results\"\n    PROCESSING_STATS = \"processing_stats\"\n    PROCESS_RECORDS_REQUEST = \"process_records_request\"\n    PROCESS_RECORDS_RESPONSE = \"process_records_response\"\n    PROCESS_RECORDS_RESULT = \"process_records_result\"\n    PROFILE_ERROR = \"profile_error\"\n    PROFILE_PROGRESS = \"profile_progress\"\n    PROFILE_RESULTS = \"profile_results\"\n    REGISTRATION = \"registration\"\n    SERVICE_ERROR = \"service_error\"\n    STATUS = \"status\"\n    SWEEP_BEGIN = \"sweep_begin\"\n    SWEEP_CONFIGURE = \"sweep_configure\"\n    SWEEP_END = \"sweep_end\"\n    SWEEP_ERROR = \"sweep_error\"\n    SWEEP_PROGRESS = \"sweep_progress\"\n    SWEEP_RESULTS = \"sweep_results\"\n    UNKNOWN = \"unknown\"\n    WORKER_HEALTH = \"worker_health\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.message_enums.NotificationType","title":"<code>NotificationType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Types of notifications that can be sent to other services.</p> Source code in <code>aiperf/common/enums/message_enums.py</code> <pre><code>class NotificationType(CaseInsensitiveStrEnum):\n    \"\"\"Types of notifications that can be sent to other services.\"\"\"\n\n    DATASET_CONFIGURED = \"dataset_configured\"\n    \"\"\"A notification sent to notify other services that the dataset has been configured.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.message_enums.NotificationType.DATASET_CONFIGURED","title":"<code>DATASET_CONFIGURED = 'dataset_configured'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A notification sent to notify other services that the dataset has been configured.</p>"},{"location":"api/#aiperfcommonenumsmetric_enums","title":"aiperf.common.enums.metric_enums","text":""},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeType","title":"<code>MetricTimeType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Defines the time types for metrics.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricTimeType(CaseInsensitiveStrEnum):\n    \"\"\"Defines the time types for metrics.\"\"\"\n\n    NANOSECONDS = \"nanoseconds\"\n    MILLISECONDS = \"milliseconds\"\n    SECONDS = \"seconds\"\n\n    def short_name(self) -&gt; str:\n        \"\"\"Get the short name for the time type.\"\"\"\n        _short_name_map = {\n            MetricTimeType.NANOSECONDS: \"ns\",\n            MetricTimeType.MILLISECONDS: \"ms\",\n            MetricTimeType.SECONDS: \"s\",\n        }\n        return _short_name_map[self]\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeType.short_name","title":"<code>short_name()</code>","text":"<p>Get the short name for the time type.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def short_name(self) -&gt; str:\n    \"\"\"Get the short name for the time type.\"\"\"\n    _short_name_map = {\n        MetricTimeType.NANOSECONDS: \"ns\",\n        MetricTimeType.MILLISECONDS: \"ms\",\n        MetricTimeType.SECONDS: \"s\",\n    }\n    return _short_name_map[self]\n</code></pre>"},{"location":"api/#aiperfcommonenumsmodel_enums","title":"aiperf.common.enums.model_enums","text":""},{"location":"api/#aiperf.common.enums.model_enums.Modality","title":"<code>Modality</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Modality of the model. Can be used to determine the type of data to send to the model in conjunction with the ModelSelectionStrategy.MODALITY_AWARE.</p> Source code in <code>aiperf/common/enums/model_enums.py</code> <pre><code>class Modality(CaseInsensitiveStrEnum):\n    \"\"\"Modality of the model. Can be used to determine the type of data to send to the model in\n    conjunction with the ModelSelectionStrategy.MODALITY_AWARE.\"\"\"\n\n    TEXT = \"text\"\n    IMAGE = \"image\"\n    AUDIO = \"audio\"\n    VIDEO = \"video\"\n    MULTIMODAL = \"multimodal\"\n    CUSTOM = \"custom\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.model_enums.ModelSelectionStrategy","title":"<code>ModelSelectionStrategy</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Strategy for selecting the model to use for the request.</p> Source code in <code>aiperf/common/enums/model_enums.py</code> <pre><code>class ModelSelectionStrategy(CaseInsensitiveStrEnum):\n    \"\"\"Strategy for selecting the model to use for the request.\"\"\"\n\n    ROUND_ROBIN = \"round_robin\"\n    RANDOM = \"random\"\n    MODALITY_AWARE = \"modality_aware\"\n</code></pre>"},{"location":"api/#aiperfcommonenumspost_processor_enums","title":"aiperf.common.enums.post_processor_enums","text":""},{"location":"api/#aiperf.common.enums.post_processor_enums.StreamingPostProcessorType","title":"<code>StreamingPostProcessorType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Type of response streamer.</p> Source code in <code>aiperf/common/enums/post_processor_enums.py</code> <pre><code>class StreamingPostProcessorType(CaseInsensitiveStrEnum):\n    \"\"\"Type of response streamer.\"\"\"\n\n    PROCESSING_STATS = \"processing_stats\"\n    \"\"\"Streamer that provides the processing stats of the records.\"\"\"\n\n    BASIC_METRICS = \"basic_metrics\"\n    \"\"\"Streamer that handles the basic metrics of the records.\"\"\"\n\n    JSONL = \"jsonl\"\n    \"\"\"Streams all parsed records to a JSONL file.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.post_processor_enums.StreamingPostProcessorType.BASIC_METRICS","title":"<code>BASIC_METRICS = 'basic_metrics'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Streamer that handles the basic metrics of the records.</p>"},{"location":"api/#aiperf.common.enums.post_processor_enums.StreamingPostProcessorType.JSONL","title":"<code>JSONL = 'jsonl'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Streams all parsed records to a JSONL file.</p>"},{"location":"api/#aiperf.common.enums.post_processor_enums.StreamingPostProcessorType.PROCESSING_STATS","title":"<code>PROCESSING_STATS = 'processing_stats'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Streamer that provides the processing stats of the records.</p>"},{"location":"api/#aiperfcommonenumsservice_enums","title":"aiperf.common.enums.service_enums","text":""},{"location":"api/#aiperf.common.enums.service_enums.LifecycleState","title":"<code>LifecycleState</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>This is the various states a lifecycle can be in.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class LifecycleState(CaseInsensitiveStrEnum):\n    \"\"\"This is the various states a lifecycle can be in.\"\"\"\n\n    CREATED = \"created\"\n    INITIALIZING = \"initializing\"\n    INITIALIZED = \"initialized\"\n    STARTING = \"starting\"\n    RUNNING = \"running\"\n    STOPPING = \"stopping\"\n    STOPPED = \"stopped\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus","title":"<code>ServiceRegistrationStatus</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Defines the various states a service can be in during registration with the SystemController.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class ServiceRegistrationStatus(CaseInsensitiveStrEnum):\n    \"\"\"Defines the various states a service can be in during registration with\n    the SystemController.\"\"\"\n\n    UNREGISTERED = \"unregistered\"\n    \"\"\"The service is not registered with the SystemController. This is the\n    initial state.\"\"\"\n\n    WAITING = \"waiting\"\n    \"\"\"The service is waiting for the SystemController to register it.\n    This is a temporary state that should be followed by REGISTERED, TIMEOUT, or ERROR.\"\"\"\n\n    REGISTERED = \"registered\"\n    \"\"\"The service is registered with the SystemController.\"\"\"\n\n    TIMEOUT = \"timeout\"\n    \"\"\"The service registration timed out.\"\"\"\n\n    ERROR = \"error\"\n    \"\"\"The service registration failed.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.ERROR","title":"<code>ERROR = 'error'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service registration failed.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.REGISTERED","title":"<code>REGISTERED = 'registered'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service is registered with the SystemController.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.TIMEOUT","title":"<code>TIMEOUT = 'timeout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service registration timed out.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.UNREGISTERED","title":"<code>UNREGISTERED = 'unregistered'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service is not registered with the SystemController. This is the initial state.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.WAITING","title":"<code>WAITING = 'waiting'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service is waiting for the SystemController to register it. This is a temporary state that should be followed by REGISTERED, TIMEOUT, or ERROR.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRunType","title":"<code>ServiceRunType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The different ways the SystemController should run the component services.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class ServiceRunType(CaseInsensitiveStrEnum):\n    \"\"\"The different ways the SystemController should run the component services.\"\"\"\n\n    MULTIPROCESSING = \"process\"\n    \"\"\"Run each service as a separate process.\n    This is the default way for single-node deployments.\"\"\"\n\n    KUBERNETES = \"k8s\"\n    \"\"\"Run each service as a separate Kubernetes pod.\n    This is the default way for multi-node deployments.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRunType.KUBERNETES","title":"<code>KUBERNETES = 'k8s'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Run each service as a separate Kubernetes pod. This is the default way for multi-node deployments.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRunType.MULTIPROCESSING","title":"<code>MULTIPROCESSING = 'process'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Run each service as a separate process. This is the default way for single-node deployments.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceType","title":"<code>ServiceType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Types of services in the AIPerf system.</p> <p>This is used to identify the service type when registering with the SystemController. It can also be used for tracking purposes if multiple instances of the same service type are running.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class ServiceType(CaseInsensitiveStrEnum):\n    \"\"\"Types of services in the AIPerf system.\n\n    This is used to identify the service type when registering with the\n    SystemController. It can also be used for tracking purposes if multiple\n    instances of the same service type are running.\n    \"\"\"\n\n    SYSTEM_CONTROLLER = \"system_controller\"\n    DATASET_MANAGER = \"dataset_manager\"\n    TIMING_MANAGER = \"timing_manager\"\n    RECORDS_MANAGER = \"records_manager\"\n    INFERENCE_RESULT_PARSER = \"inference_result_parser\"\n    WORKER_MANAGER = \"worker_manager\"\n    WORKER = \"worker\"\n\n    # For testing purposes only\n    TEST = \"test_service\"\n</code></pre>"},{"location":"api/#aiperfcommonenumssse_enums","title":"aiperf.common.enums.sse_enums","text":""},{"location":"api/#aiperf.common.enums.sse_enums.SSEEventType","title":"<code>SSEEventType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Event types in an SSE message. Many of these are custom and not defined by the SSE spec.</p> Source code in <code>aiperf/common/enums/sse_enums.py</code> <pre><code>class SSEEventType(CaseInsensitiveStrEnum):\n    \"\"\"Event types in an SSE message. Many of these are custom and not defined by the SSE spec.\"\"\"\n\n    ERROR = \"error\"\n    LLM_METRICS = \"llm_metrics\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.sse_enums.SSEFieldType","title":"<code>SSEFieldType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Field types in an SSE message.</p> Source code in <code>aiperf/common/enums/sse_enums.py</code> <pre><code>class SSEFieldType(CaseInsensitiveStrEnum):\n    \"\"\"Field types in an SSE message.\"\"\"\n\n    DATA = \"data\"\n    EVENT = \"event\"\n    ID = \"id\"\n    RETRY = \"retry\"\n    COMMENT = \"comment\"\n</code></pre>"},{"location":"api/#aiperfcommonenumssystem_enums","title":"aiperf.common.enums.system_enums","text":""},{"location":"api/#aiperf.common.enums.system_enums.SystemState","title":"<code>SystemState</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>State of the system as a whole.</p> <p>This is used to track the state of the system as a whole, and is used to determine what actions to take when a signal is received.</p> Source code in <code>aiperf/common/enums/system_enums.py</code> <pre><code>class SystemState(CaseInsensitiveStrEnum):\n    \"\"\"State of the system as a whole.\n\n    This is used to track the state of the system as a whole, and is used to\n    determine what actions to take when a signal is received.\n    \"\"\"\n\n    INITIALIZING = \"initializing\"\n    \"\"\"The system is initializing. This is the initial state.\"\"\"\n\n    CONFIGURING = \"configuring\"\n    \"\"\"The system is configuring services.\"\"\"\n\n    READY = \"ready\"\n    \"\"\"The system is ready to start profiling. This is a temporary state that should be\n    followed by PROFILING.\"\"\"\n\n    PROFILING = \"profiling\"\n    \"\"\"The system is running a profiling run.\"\"\"\n\n    PROCESSING = \"processing\"\n    \"\"\"The system is processing results.\"\"\"\n\n    STOPPING = \"stopping\"\n    \"\"\"The system is stopping.\"\"\"\n\n    SHUTDOWN = \"shutdown\"\n    \"\"\"The system is shutting down. This is the final state.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.CONFIGURING","title":"<code>CONFIGURING = 'configuring'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is configuring services.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.INITIALIZING","title":"<code>INITIALIZING = 'initializing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is initializing. This is the initial state.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.PROCESSING","title":"<code>PROCESSING = 'processing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is processing results.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.PROFILING","title":"<code>PROFILING = 'profiling'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is running a profiling run.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.READY","title":"<code>READY = 'ready'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is ready to start profiling. This is a temporary state that should be followed by PROFILING.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.SHUTDOWN","title":"<code>SHUTDOWN = 'shutdown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is shutting down. This is the final state.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.STOPPING","title":"<code>STOPPING = 'stopping'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is stopping.</p>"},{"location":"api/#aiperfcommonenumstiming_enums","title":"aiperf.common.enums.timing_enums","text":""},{"location":"api/#aiperf.common.enums.timing_enums.CreditPhase","title":"<code>CreditPhase</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The type of credit phase. This is used to identify which phase of the benchmark the credit is being used in, for tracking and reporting purposes.</p> Source code in <code>aiperf/common/enums/timing_enums.py</code> <pre><code>class CreditPhase(CaseInsensitiveStrEnum):\n    \"\"\"The type of credit phase. This is used to identify which phase of the\n    benchmark the credit is being used in, for tracking and reporting purposes.\"\"\"\n\n    WARMUP = \"warmup\"\n    \"\"\"The credit phase is the warmup phase. This is used to warm up the model\n    before the benchmark starts.\"\"\"\n\n    PROFILING = \"profiling\"\n    \"\"\"The credit phase is the steady state phase. This is the primary phase of the\n    benchmark, and what is used to calculate the final results.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.timing_enums.CreditPhase.PROFILING","title":"<code>PROFILING = 'profiling'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The credit phase is the steady state phase. This is the primary phase of the benchmark, and what is used to calculate the final results.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.CreditPhase.WARMUP","title":"<code>WARMUP = 'warmup'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The credit phase is the warmup phase. This is used to warm up the model before the benchmark starts.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode","title":"<code>RequestRateMode</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The different ways the RequestRateStrategy should generate requests.</p> Source code in <code>aiperf/common/enums/timing_enums.py</code> <pre><code>class RequestRateMode(CaseInsensitiveStrEnum):\n    \"\"\"The different ways the RequestRateStrategy should generate requests.\"\"\"\n\n    CONSTANT = \"constant\"\n    \"\"\"Generate requests at a constant rate.\"\"\"\n\n    POISSON = \"poisson\"\n    \"\"\"Generate requests using a poisson distribution.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode.CONSTANT","title":"<code>CONSTANT = 'constant'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate requests at a constant rate.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode.POISSON","title":"<code>POISSON = 'poisson'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate requests using a poisson distribution.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode","title":"<code>TimingMode</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The different ways the TimingManager should generate requests.</p> Source code in <code>aiperf/common/enums/timing_enums.py</code> <pre><code>class TimingMode(CaseInsensitiveStrEnum):\n    \"\"\"The different ways the TimingManager should generate requests.\"\"\"\n\n    FIXED_SCHEDULE = \"fixed_schedule\"\n    \"\"\"A mode where the TimingManager will send requests according to a fixed schedule.\"\"\"\n\n    CONCURRENCY = \"concurrency\"\n    \"\"\"A mode where the TimingManager will maintain a continuous stream of concurrent requests.\"\"\"\n\n    REQUEST_RATE = \"request_rate\"\n    \"\"\"A mode where the TimingManager will send requests at either a constant request rate or based on a poisson distribution.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode.CONCURRENCY","title":"<code>CONCURRENCY = 'concurrency'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A mode where the TimingManager will maintain a continuous stream of concurrent requests.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode.FIXED_SCHEDULE","title":"<code>FIXED_SCHEDULE = 'fixed_schedule'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A mode where the TimingManager will send requests according to a fixed schedule.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode.REQUEST_RATE","title":"<code>REQUEST_RATE = 'request_rate'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A mode where the TimingManager will send requests at either a constant request rate or based on a poisson distribution.</p>"},{"location":"api/#aiperfcommonexceptions","title":"aiperf.common.exceptions","text":""},{"location":"api/#aiperf.common.exceptions.AIPerfError","title":"<code>AIPerfError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions raised by AIPerf.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class AIPerfError(Exception):\n    \"\"\"Base class for all exceptions raised by AIPerf.\"\"\"\n\n    def raw_str(self) -&gt; str:\n        \"\"\"Return the raw string representation of the exception.\"\"\"\n        return super().__str__()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the exception with the class name.\"\"\"\n        return f\"{self.__class__.__name__}: {super().__str__()}\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.AIPerfError.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the exception with the class name.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the exception with the class name.\"\"\"\n    return f\"{self.__class__.__name__}: {super().__str__()}\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.AIPerfError.raw_str","title":"<code>raw_str()</code>","text":"<p>Return the raw string representation of the exception.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>def raw_str(self) -&gt; str:\n    \"\"\"Return the raw string representation of the exception.\"\"\"\n    return super().__str__()\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.AIPerfMultiError","title":"<code>AIPerfMultiError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when running multiple tasks and one or more fail.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class AIPerfMultiError(AIPerfError):\n    \"\"\"Exception raised when running multiple tasks and one or more fail.\"\"\"\n\n    def __init__(self, message: str, exceptions: list[Exception]) -&gt; None:\n        err_strings = [\n            e.raw_str() if isinstance(e, AIPerfError) else str(e) for e in exceptions\n        ]\n        super().__init__(f\"{message}: {','.join(err_strings)}\")\n        self.exceptions = exceptions\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.CommunicationError","title":"<code>CommunicationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic communication error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class CommunicationError(AIPerfError):\n    \"\"\"Generic communication error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something fails to configure, or there is a configuration error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ConfigurationError(AIPerfError):\n    \"\"\"Exception raised when something fails to configure, or there is a configuration error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.DatasetError","title":"<code>DatasetError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic dataset error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class DatasetError(AIPerfError):\n    \"\"\"Generic dataset error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.DatasetGeneratorError","title":"<code>DatasetGeneratorError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic dataset generator error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class DatasetGeneratorError(AIPerfError):\n    \"\"\"Generic dataset generator error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.FactoryCreationError","title":"<code>FactoryCreationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a factory encounters an error while creating a class.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class FactoryCreationError(AIPerfError):\n    \"\"\"Exception raised when a factory encounters an error while creating a class.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InferenceClientError","title":"<code>InferenceClientError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a inference client encounters an error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InferenceClientError(AIPerfError):\n    \"\"\"Exception raised when a inference client encounters an error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InitializationError","title":"<code>InitializationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something fails to initialize.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InitializationError(AIPerfError):\n    \"\"\"Exception raised when something fails to initialize.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InvalidOperationError","title":"<code>InvalidOperationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when an operation is invalid.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InvalidOperationError(AIPerfError):\n    \"\"\"Exception raised when an operation is invalid.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InvalidPayloadError","title":"<code>InvalidPayloadError</code>","text":"<p>               Bases: <code>InferenceClientError</code></p> <p>Exception raised when a inference client receives an invalid payload.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InvalidPayloadError(InferenceClientError):\n    \"\"\"Exception raised when a inference client receives an invalid payload.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InvalidStateError","title":"<code>InvalidStateError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something is in an invalid state.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InvalidStateError(AIPerfError):\n    \"\"\"Exception raised when something is in an invalid state.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.MetricTypeError","title":"<code>MetricTypeError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a metric type encounters an error while creating a class.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class MetricTypeError(AIPerfError):\n    \"\"\"Exception raised when a metric type encounters an error while creating a class.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something is not found or not available.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class NotFoundError(AIPerfError):\n    \"\"\"Exception raised when something is not found or not available.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.NotInitializedError","title":"<code>NotInitializedError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something that should be initialized is not.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class NotInitializedError(AIPerfError):\n    \"\"\"Exception raised when something that should be initialized is not.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ProxyError","title":"<code>ProxyError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a proxy encounters an error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ProxyError(AIPerfError):\n    \"\"\"Exception raised when a proxy encounters an error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ServiceError","title":"<code>ServiceError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic service error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ServiceError(AIPerfError):\n    \"\"\"Generic service error.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        service_type: ServiceTypeT,\n        service_id: str,\n    ) -&gt; None:\n        super().__init__(\n            f\"{message} for service of type {service_type} with id {service_id}\"\n        )\n        self.service_type = service_type\n        self.service_id = service_id\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ShutdownError","title":"<code>ShutdownError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a service encounters an error while shutting down.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ShutdownError(AIPerfError):\n    \"\"\"Exception raised when a service encounters an error while shutting down.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.UnsupportedHookError","title":"<code>UnsupportedHookError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a hook is defined on a class that does not have any base classes that provide that hook type.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class UnsupportedHookError(AIPerfError):\n    \"\"\"Exception raised when a hook is defined on a class that does not have any base classes that provide that hook type.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something fails validation.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ValidationError(AIPerfError):\n    \"\"\"Exception raised when something fails validation.\"\"\"\n</code></pre>"},{"location":"api/#aiperfcommonfactories","title":"aiperf.common.factories","text":""},{"location":"api/#aiperf.common.factories.AIPerfFactory","title":"<code>AIPerfFactory</code>","text":"<p>               Bases: <code>Generic[ClassEnumT, ClassProtocolT]</code></p> <p>Defines a custom factory for AIPerf components.</p> <p>This class is used to create a factory for a given class type and protocol.</p> <p>Example:</p> <pre><code>    # Define a new enum for the expected implementation types\n    # This is optional, but recommended for type safety.\n    class DatasetLoaderType(CaseInsensitiveStrEnum):\n        FILE = \"file\"\n        S3 = \"s3\"\n\n    # Define a new class protocol.\n    class DatasetLoaderProtocol(Protocol):\n        def load(self) -&gt; Dataset:\n            pass\n\n    # Create a new factory for a given class type and protocol.\n    class DatasetFactory(FactoryMixin[DatasetLoaderType, DatasetLoaderProtocol]):\n        pass\n\n    # Register a new class type mapping to its corresponding class. It should implement the class protocol.\n    @DatasetFactory.register(DatasetLoaderType.FILE)\n    class FileDatasetLoader:\n        def __init__(self, filename: str):\n            self.filename = filename\n\n        def load(self) -&gt; Dataset:\n            return Dataset.from_file(self.filename)\n\n    DatasetConfig = {\n        \"type\": DatasetLoaderType.FILE,\n        \"filename\": \"data.csv\"\n    }\n\n    # Create a new instance of the class.\n    if DatasetConfig[\"type\"] == DatasetLoaderType.FILE:\n        dataset_instance = DatasetFactory.create_instance(DatasetLoaderType.FILE, filename=DatasetConfig[\"filename\"])\n    else:\n        raise ValueError(f\"Unsupported dataset loader type: {DatasetConfig['type']}\")\n\n    dataset_instance.load()\n</code></pre> Source code in <code>aiperf/common/factories.py</code> <pre><code>class AIPerfFactory(Generic[ClassEnumT, ClassProtocolT]):\n    \"\"\"Defines a custom factory for AIPerf components.\n\n    This class is used to create a factory for a given class type and protocol.\n\n    Example:\n    ```python\n        # Define a new enum for the expected implementation types\n        # This is optional, but recommended for type safety.\n        class DatasetLoaderType(CaseInsensitiveStrEnum):\n            FILE = \"file\"\n            S3 = \"s3\"\n\n        # Define a new class protocol.\n        class DatasetLoaderProtocol(Protocol):\n            def load(self) -&gt; Dataset:\n                pass\n\n        # Create a new factory for a given class type and protocol.\n        class DatasetFactory(FactoryMixin[DatasetLoaderType, DatasetLoaderProtocol]):\n            pass\n\n        # Register a new class type mapping to its corresponding class. It should implement the class protocol.\n        @DatasetFactory.register(DatasetLoaderType.FILE)\n        class FileDatasetLoader:\n            def __init__(self, filename: str):\n                self.filename = filename\n\n            def load(self) -&gt; Dataset:\n                return Dataset.from_file(self.filename)\n\n        DatasetConfig = {\n            \"type\": DatasetLoaderType.FILE,\n            \"filename\": \"data.csv\"\n        }\n\n        # Create a new instance of the class.\n        if DatasetConfig[\"type\"] == DatasetLoaderType.FILE:\n            dataset_instance = DatasetFactory.create_instance(DatasetLoaderType.FILE, filename=DatasetConfig[\"filename\"])\n        else:\n            raise ValueError(f\"Unsupported dataset loader type: {DatasetConfig['type']}\")\n\n        dataset_instance.load()\n    ```\n    \"\"\"\n\n    _logger: AIPerfLogger\n    _registry: dict[ClassEnumT | str, type[ClassProtocolT]]\n    _override_priorities: dict[ClassEnumT | str, int]\n\n    def __init_subclass__(cls) -&gt; None:\n        cls._registry = {}\n        cls._override_priorities = {}\n        cls._logger = AIPerfLogger(cls.__name__)\n        super().__init_subclass__()\n\n    @classmethod\n    def register_all(\n        cls, *class_types: ClassEnumT | str, override_priority: int = 0\n    ) -&gt; Callable:\n        \"\"\"Register multiple class types mapping to a single corresponding class.\n        This is useful if a single class implements multiple types. Currently only supports\n        registering as a single override priority for all types.\"\"\"\n\n        def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n            for class_type in class_types:\n                cls.register(class_type, override_priority)(class_cls)\n            return class_cls\n\n        return decorator\n\n    @classmethod\n    def register(\n        cls, class_type: ClassEnumT | str, override_priority: int = 0\n    ) -&gt; Callable:\n        \"\"\"Register a new class type mapping to its corresponding class.\n\n        Args:\n            class_type: The type of class to register\n            override_priority: The priority of the override. The higher the priority,\n                the more precedence the override has when multiple classes are registered\n                for the same class type. Built-in classes have a priority of 0.\n\n        Returns:\n            Decorator for the class that implements the class protocol\n        \"\"\"\n\n        def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n            existing_priority = cls._override_priorities.get(class_type, -1)\n            if class_type in cls._registry and existing_priority &gt;= override_priority:\n                cls._logger.warning(\n                    f\"{class_type!r} class {cls._registry[class_type].__name__} already registered with same or higher priority \"\n                    f\"({existing_priority}). The new registration of class {class_cls.__name__} with priority \"\n                    f\"{override_priority} will be ignored.\",\n                )\n                return class_cls\n\n            if class_type not in cls._registry:\n                cls._logger.debug(\n                    lambda: f\"{class_type!r} class {class_cls.__name__} registered with priority {override_priority}.\",\n                )\n            else:\n                cls._logger.warning(\n                    f\"{class_type!r} class {class_cls.__name__} with priority {override_priority} overrides \"\n                    f\"already registered class {cls._registry[class_type].__name__} with lower priority ({existing_priority}).\",\n                )\n            cls._registry[class_type] = class_cls\n            cls._override_priorities[class_type] = override_priority\n            return class_cls\n\n        return decorator\n\n    @classmethod\n    def create_instance(\n        cls,\n        class_type: ClassEnumT | str,\n        **kwargs: Any,\n    ) -&gt; ClassProtocolT:\n        \"\"\"Create a new class instance.\n\n        Args:\n            class_type: The type of class to create\n            **kwargs: Additional arguments for the class\n\n        Returns:\n            The created class instance\n\n        Raises:\n            FactoryCreationError: If the class type is not registered or there is an error creating the instance\n        \"\"\"\n        if class_type not in cls._registry:\n            raise FactoryCreationError(\n                f\"No implementation registered for {class_type!r} in {cls.__name__}.\"\n            )\n        try:\n            return cls._registry[class_type](**kwargs)\n        except Exception as e:\n            raise FactoryCreationError(\n                f\"Error creating {class_type!r} instance for {cls.__name__}: {e}\"\n            ) from e\n\n    @classmethod\n    def get_class_from_type(cls, class_type: ClassEnumT | str) -&gt; type[ClassProtocolT]:\n        \"\"\"Get the class from a class type.\n\n        Args:\n            class_type: The class type to get the class from\n\n        Returns:\n            The class for the given class type\n\n        Raises:\n            TypeError: If the class type is not registered\n        \"\"\"\n        if class_type not in cls._registry:\n            raise TypeError(\n                f\"No class found for {class_type!r}. Please register the class first.\"\n            )\n        return cls._registry[class_type]\n\n    @classmethod\n    def get_all_classes(cls) -&gt; list[type[ClassProtocolT]]:\n        \"\"\"Get all registered classes.\n\n        Returns:\n            A list of all registered class types implementing the expected protocol\n        \"\"\"\n        return list(cls._registry.values())\n\n    @classmethod\n    def get_all_class_types(cls) -&gt; list[ClassEnumT | str]:\n        \"\"\"Get all registered class types.\"\"\"\n        return list(cls._registry.keys())\n\n    @classmethod\n    def get_all_classes_and_types(\n        cls,\n    ) -&gt; list[tuple[type[ClassProtocolT], ClassEnumT | str]]:\n        \"\"\"Get all registered classes and their corresponding class types.\"\"\"\n        return [(cls, class_type) for class_type, cls in cls._registry.items()]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.create_instance","title":"<code>create_instance(class_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new class instance.</p> <p>Parameters:</p> Name Type Description Default <code>class_type</code> <code>ClassEnumT | str</code> <p>The type of class to create</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the class</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClassProtocolT</code> <p>The created class instance</p> <p>Raises:</p> Type Description <code>FactoryCreationError</code> <p>If the class type is not registered or there is an error creating the instance</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef create_instance(\n    cls,\n    class_type: ClassEnumT | str,\n    **kwargs: Any,\n) -&gt; ClassProtocolT:\n    \"\"\"Create a new class instance.\n\n    Args:\n        class_type: The type of class to create\n        **kwargs: Additional arguments for the class\n\n    Returns:\n        The created class instance\n\n    Raises:\n        FactoryCreationError: If the class type is not registered or there is an error creating the instance\n    \"\"\"\n    if class_type not in cls._registry:\n        raise FactoryCreationError(\n            f\"No implementation registered for {class_type!r} in {cls.__name__}.\"\n        )\n    try:\n        return cls._registry[class_type](**kwargs)\n    except Exception as e:\n        raise FactoryCreationError(\n            f\"Error creating {class_type!r} instance for {cls.__name__}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_all_class_types","title":"<code>get_all_class_types()</code>  <code>classmethod</code>","text":"<p>Get all registered class types.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_all_class_types(cls) -&gt; list[ClassEnumT | str]:\n    \"\"\"Get all registered class types.\"\"\"\n    return list(cls._registry.keys())\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_all_classes","title":"<code>get_all_classes()</code>  <code>classmethod</code>","text":"<p>Get all registered classes.</p> <p>Returns:</p> Type Description <code>list[type[ClassProtocolT]]</code> <p>A list of all registered class types implementing the expected protocol</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_all_classes(cls) -&gt; list[type[ClassProtocolT]]:\n    \"\"\"Get all registered classes.\n\n    Returns:\n        A list of all registered class types implementing the expected protocol\n    \"\"\"\n    return list(cls._registry.values())\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_all_classes_and_types","title":"<code>get_all_classes_and_types()</code>  <code>classmethod</code>","text":"<p>Get all registered classes and their corresponding class types.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_all_classes_and_types(\n    cls,\n) -&gt; list[tuple[type[ClassProtocolT], ClassEnumT | str]]:\n    \"\"\"Get all registered classes and their corresponding class types.\"\"\"\n    return [(cls, class_type) for class_type, cls in cls._registry.items()]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_class_from_type","title":"<code>get_class_from_type(class_type)</code>  <code>classmethod</code>","text":"<p>Get the class from a class type.</p> <p>Parameters:</p> Name Type Description Default <code>class_type</code> <code>ClassEnumT | str</code> <p>The class type to get the class from</p> required <p>Returns:</p> Type Description <code>type[ClassProtocolT]</code> <p>The class for the given class type</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the class type is not registered</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_class_from_type(cls, class_type: ClassEnumT | str) -&gt; type[ClassProtocolT]:\n    \"\"\"Get the class from a class type.\n\n    Args:\n        class_type: The class type to get the class from\n\n    Returns:\n        The class for the given class type\n\n    Raises:\n        TypeError: If the class type is not registered\n    \"\"\"\n    if class_type not in cls._registry:\n        raise TypeError(\n            f\"No class found for {class_type!r}. Please register the class first.\"\n        )\n    return cls._registry[class_type]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.register","title":"<code>register(class_type, override_priority=0)</code>  <code>classmethod</code>","text":"<p>Register a new class type mapping to its corresponding class.</p> <p>Parameters:</p> Name Type Description Default <code>class_type</code> <code>ClassEnumT | str</code> <p>The type of class to register</p> required <code>override_priority</code> <code>int</code> <p>The priority of the override. The higher the priority, the more precedence the override has when multiple classes are registered for the same class type. Built-in classes have a priority of 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Decorator for the class that implements the class protocol</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef register(\n    cls, class_type: ClassEnumT | str, override_priority: int = 0\n) -&gt; Callable:\n    \"\"\"Register a new class type mapping to its corresponding class.\n\n    Args:\n        class_type: The type of class to register\n        override_priority: The priority of the override. The higher the priority,\n            the more precedence the override has when multiple classes are registered\n            for the same class type. Built-in classes have a priority of 0.\n\n    Returns:\n        Decorator for the class that implements the class protocol\n    \"\"\"\n\n    def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n        existing_priority = cls._override_priorities.get(class_type, -1)\n        if class_type in cls._registry and existing_priority &gt;= override_priority:\n            cls._logger.warning(\n                f\"{class_type!r} class {cls._registry[class_type].__name__} already registered with same or higher priority \"\n                f\"({existing_priority}). The new registration of class {class_cls.__name__} with priority \"\n                f\"{override_priority} will be ignored.\",\n            )\n            return class_cls\n\n        if class_type not in cls._registry:\n            cls._logger.debug(\n                lambda: f\"{class_type!r} class {class_cls.__name__} registered with priority {override_priority}.\",\n            )\n        else:\n            cls._logger.warning(\n                f\"{class_type!r} class {class_cls.__name__} with priority {override_priority} overrides \"\n                f\"already registered class {cls._registry[class_type].__name__} with lower priority ({existing_priority}).\",\n            )\n        cls._registry[class_type] = class_cls\n        cls._override_priorities[class_type] = override_priority\n        return class_cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.register_all","title":"<code>register_all(*class_types, override_priority=0)</code>  <code>classmethod</code>","text":"<p>Register multiple class types mapping to a single corresponding class. This is useful if a single class implements multiple types. Currently only supports registering as a single override priority for all types.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef register_all(\n    cls, *class_types: ClassEnumT | str, override_priority: int = 0\n) -&gt; Callable:\n    \"\"\"Register multiple class types mapping to a single corresponding class.\n    This is useful if a single class implements multiple types. Currently only supports\n    registering as a single override priority for all types.\"\"\"\n\n    def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n        for class_type in class_types:\n            cls.register(class_type, override_priority)(class_cls)\n        return class_cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfSingletonFactory","title":"<code>AIPerfSingletonFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ClassEnumT, ClassProtocolT]</code></p> <p>Factory for registering and creating singleton instances of a given class type and protocol. This factory is useful for creating instances that are shared across the application, such as communication clients. Calling create_instance will create a new instance if it doesn't exist, otherwise it will return the existing instance. Calling get_instance will return the existing instance if it exists, otherwise it will raise an error. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class AIPerfSingletonFactory(AIPerfFactory[ClassEnumT, ClassProtocolT]):\n    \"\"\"Factory for registering and creating singleton instances of a given class type and protocol.\n    This factory is useful for creating instances that are shared across the application, such as communication clients.\n    Calling create_instance will create a new instance if it doesn't exist, otherwise it will return the existing instance.\n    Calling get_instance will return the existing instance if it exists, otherwise it will raise an error.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    _instances: dict[ClassEnumT | str, ClassProtocolT]\n    _instances_lock: Lock\n    _instances_pid: dict[ClassEnumT | str, int]\n\n    def __init_subclass__(cls) -&gt; None:\n        cls._instances = {}\n        cls._instances_lock = Lock()\n        cls._instances_pid = {}\n        super().__init_subclass__()\n\n    @classmethod\n    def set_instance(\n        cls, class_type: ClassEnumT | str, instance: ClassProtocolT\n    ) -&gt; None:\n        cls._instances[class_type] = instance\n\n    @classmethod\n    def get_or_create_instance(\n        cls, class_type: ClassEnumT | str, **kwargs: Any\n    ) -&gt; ClassProtocolT:\n        \"\"\"Syntactic sugar for create_instance, but with a more descriptive name for singleton factories.\"\"\"\n        return cls.create_instance(class_type, **kwargs)\n\n    @classmethod\n    def create_instance(\n        cls, class_type: ClassEnumT | str, **kwargs: Any\n    ) -&gt; ClassProtocolT:\n        \"\"\"Create a new instance of the given class type.\n        If the instance does not exist, or the process ID has changed, a new instance will be created.\n        \"\"\"\n        # TODO: Technically, this this should handle the case where kwargs are different,\n        #       but that would require a more complex implementation.\n        if (\n            class_type not in cls._instances\n            or os.getpid() != cls._instances_pid[class_type]\n        ):\n            cls._logger.debug(\n                lambda: f\"Creating new instance for {class_type!r} in {cls.__name__}.\"\n            )\n            with cls._instances_lock:\n                if (\n                    class_type not in cls._instances\n                    or os.getpid() != cls._instances_pid[class_type]\n                ):\n                    cls._instances[class_type] = super().create_instance(\n                        class_type, **kwargs\n                    )\n                    cls._instances_pid[class_type] = os.getpid()\n                    cls._logger.debug(\n                        lambda: f\"New instance for {class_type!r} in {cls.__name__} created.\"\n                    )\n        else:\n            cls._logger.debug(\n                lambda: f\"Instance for {class_type!r} in {cls.__name__} already exists. Returning existing instance.\"\n            )\n        return cls._instances[class_type]\n\n    @classmethod\n    def get_instance(cls, class_type: ClassEnumT | str) -&gt; ClassProtocolT:\n        if class_type not in cls._instances:\n            raise InvalidStateError(\n                f\"No instance found for {class_type!r} in {cls.__name__}. \"\n                f\"Ensure you call AIPerfSingletonFactory.create_instance({class_type!r}) first.\"\n            )\n        if os.getpid() != cls._instances_pid[class_type]:\n            raise InvalidStateError(\n                f\"Instance for {class_type!r} in {cls.__name__} is not valid for the current process. \"\n                f\"Ensure you call AIPerfSingletonFactory.create_instance({class_type!r}) first after forking.\"\n            )\n        return cls._instances[class_type]\n\n    @classmethod\n    def get_all_instances(cls) -&gt; dict[ClassEnumT | str, ClassProtocolT]:\n        return cls._instances\n\n    @classmethod\n    def has_instance(cls, class_type: ClassEnumT | str) -&gt; bool:\n        return class_type in cls._instances\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfSingletonFactory.create_instance","title":"<code>create_instance(class_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the given class type. If the instance does not exist, or the process ID has changed, a new instance will be created.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef create_instance(\n    cls, class_type: ClassEnumT | str, **kwargs: Any\n) -&gt; ClassProtocolT:\n    \"\"\"Create a new instance of the given class type.\n    If the instance does not exist, or the process ID has changed, a new instance will be created.\n    \"\"\"\n    # TODO: Technically, this this should handle the case where kwargs are different,\n    #       but that would require a more complex implementation.\n    if (\n        class_type not in cls._instances\n        or os.getpid() != cls._instances_pid[class_type]\n    ):\n        cls._logger.debug(\n            lambda: f\"Creating new instance for {class_type!r} in {cls.__name__}.\"\n        )\n        with cls._instances_lock:\n            if (\n                class_type not in cls._instances\n                or os.getpid() != cls._instances_pid[class_type]\n            ):\n                cls._instances[class_type] = super().create_instance(\n                    class_type, **kwargs\n                )\n                cls._instances_pid[class_type] = os.getpid()\n                cls._logger.debug(\n                    lambda: f\"New instance for {class_type!r} in {cls.__name__} created.\"\n                )\n    else:\n        cls._logger.debug(\n            lambda: f\"Instance for {class_type!r} in {cls.__name__} already exists. Returning existing instance.\"\n        )\n    return cls._instances[class_type]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfSingletonFactory.get_or_create_instance","title":"<code>get_or_create_instance(class_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Syntactic sugar for create_instance, but with a more descriptive name for singleton factories.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_or_create_instance(\n    cls, class_type: ClassEnumT | str, **kwargs: Any\n) -&gt; ClassProtocolT:\n    \"\"\"Syntactic sugar for create_instance, but with a more descriptive name for singleton factories.\"\"\"\n    return cls.create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.CommunicationClientFactory","title":"<code>CommunicationClientFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[CommClientType, 'CommunicationClientProtocol']</code></p> <p>Factory for registering and creating CommunicationClientProtocol instances based on the specified communication client type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class CommunicationClientFactory(\n    AIPerfFactory[CommClientType, \"CommunicationClientProtocol\"]\n):\n    \"\"\"Factory for registering and creating CommunicationClientProtocol instances based on the specified communication client type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: CommClientType | str,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; \"CommunicationClientProtocol\":\n        return super().create_instance(\n            class_type, address=address, bind=bind, socket_ops=socket_ops, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.CommunicationFactory","title":"<code>CommunicationFactory</code>","text":"<p>               Bases: <code>AIPerfSingletonFactory[CommunicationBackend, 'CommunicationProtocol']</code></p> <p>Factory for registering and creating CommunicationProtocol instances based on the specified communication backend. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class CommunicationFactory(\n    AIPerfSingletonFactory[CommunicationBackend, \"CommunicationProtocol\"]\n):\n    \"\"\"Factory for registering and creating CommunicationProtocol instances based on the specified communication backend.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: CommunicationBackend | str,\n        config: \"BaseZMQCommunicationConfig\",\n        **kwargs,\n    ) -&gt; \"CommunicationProtocol\":\n        return super().create_instance(class_type, config=config, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.ComposerFactory","title":"<code>ComposerFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ComposerType, 'BaseDatasetComposer']</code></p> <p>Factory for registering and creating BaseDatasetComposer instances based on the specified composer type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ComposerFactory(AIPerfFactory[ComposerType, \"BaseDatasetComposer\"]):\n    \"\"\"Factory for registering and creating BaseDatasetComposer instances based on the specified composer type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ComposerType | str,\n        **kwargs,\n    ) -&gt; \"BaseDatasetComposer\":\n        return super().create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.CustomDatasetFactory","title":"<code>CustomDatasetFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[CustomDatasetType, 'CustomDatasetLoaderProtocol']</code></p> <p>Factory for registering and creating CustomDatasetLoaderProtocol instances based on the specified custom dataset type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class CustomDatasetFactory(\n    AIPerfFactory[CustomDatasetType, \"CustomDatasetLoaderProtocol\"]\n):\n    \"\"\"Factory for registering and creating CustomDatasetLoaderProtocol instances based on the specified custom dataset type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: CustomDatasetType | str,\n        **kwargs,\n    ) -&gt; \"CustomDatasetLoaderProtocol\":\n        return super().create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.DataExporterFactory","title":"<code>DataExporterFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[DataExporterType, 'DataExporterProtocol']</code></p> <p>Factory for registering and creating DataExporterProtocol instances based on the specified data exporter type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class DataExporterFactory(AIPerfFactory[DataExporterType, \"DataExporterProtocol\"]):\n    \"\"\"Factory for registering and creating DataExporterProtocol instances based on the specified data exporter type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: DataExporterType | str,\n        exporter_config: \"ExporterConfig\",\n        **kwargs,\n    ) -&gt; \"DataExporterProtocol\":\n        return super().create_instance(\n            class_type, exporter_config=exporter_config, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.InferenceClientFactory","title":"<code>InferenceClientFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[EndpointType, 'InferenceClientProtocol']</code></p> <p>Factory for registering and creating InferenceClientProtocol instances based on the specified endpoint type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class InferenceClientFactory(AIPerfFactory[EndpointType, \"InferenceClientProtocol\"]):\n    \"\"\"Factory for registering and creating InferenceClientProtocol instances based on the specified endpoint type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: EndpointType | str,\n        model_endpoint: \"ModelEndpointInfo\",\n        **kwargs,\n    ) -&gt; \"InferenceClientProtocol\":\n        return super().create_instance(\n            class_type, model_endpoint=model_endpoint, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.PostProcessorFactory","title":"<code>PostProcessorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[PostProcessorType, 'PostProcessorProtocol']</code></p> <p>Factory for registering and creating PostProcessorProtocol instances based on the specified post processor type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class PostProcessorFactory(AIPerfFactory[PostProcessorType, \"PostProcessorProtocol\"]):\n    \"\"\"Factory for registering and creating PostProcessorProtocol instances based on the specified post processor type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: PostProcessorType | str,\n        **kwargs,\n    ) -&gt; \"PostProcessorProtocol\":\n        return super().create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.RequestConverterFactory","title":"<code>RequestConverterFactory</code>","text":"<p>               Bases: <code>AIPerfSingletonFactory[EndpointType, 'RequestConverterProtocol']</code></p> <p>Factory for registering and creating RequestConverterProtocol instances based on the specified request payload type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class RequestConverterFactory(\n    AIPerfSingletonFactory[EndpointType, \"RequestConverterProtocol\"]\n):\n    \"\"\"Factory for registering and creating RequestConverterProtocol instances based on the specified request payload type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.factories.ResponseExtractorFactory","title":"<code>ResponseExtractorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[EndpointType, 'ResponseExtractorProtocol']</code></p> <p>Factory for registering and creating ResponseExtractorProtocol instances based on the specified response extractor type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ResponseExtractorFactory(\n    AIPerfFactory[EndpointType, \"ResponseExtractorProtocol\"]\n):\n    \"\"\"Factory for registering and creating ResponseExtractorProtocol instances based on the specified response extractor type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: EndpointType | str,\n        model_endpoint: \"ModelEndpointInfo\",\n        **kwargs,\n    ) -&gt; \"ResponseExtractorProtocol\":\n        return super().create_instance(\n            class_type, model_endpoint=model_endpoint, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.ServiceFactory","title":"<code>ServiceFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ServiceType, 'ServiceProtocol']</code></p> <p>Factory for registering and creating ServiceProtocol instances based on the specified service type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ServiceFactory(AIPerfFactory[ServiceType, \"ServiceProtocol\"]):\n    \"\"\"Factory for registering and creating ServiceProtocol instances based on the specified service type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def register_all(\n        cls, *class_types: ServiceTypeT, override_priority: int = 0\n    ) -&gt; Callable[..., Any]:\n        raise InvalidOperationError(\n            \"ServiceFactory.register_all is not supported. A single service can only be registered with a single type.\"\n        )\n\n    @classmethod\n    def register(\n        cls, class_type: ServiceTypeT, override_priority: int = 0\n    ) -&gt; Callable[..., Any]:\n        # Override the register method to set the service_type on the class\n        original_decorator = super().register(class_type, override_priority)\n\n        def decorator(class_cls: type[ServiceProtocolT]) -&gt; type[ServiceProtocolT]:\n            class_cls.service_type = class_type\n            original_decorator(class_cls)\n            return class_cls\n\n        return decorator\n</code></pre>"},{"location":"api/#aiperf.common.factories.ServiceManagerFactory","title":"<code>ServiceManagerFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ServiceRunType, 'ServiceManagerProtocol']</code></p> <p>Factory for registering and creating ServiceManagerProtocol instances based on the specified service run type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ServiceManagerFactory(AIPerfFactory[ServiceRunType, \"ServiceManagerProtocol\"]):\n    \"\"\"Factory for registering and creating ServiceManagerProtocol instances based on the specified service run type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ServiceRunType | str,\n        required_services: dict[ServiceTypeT, int],\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        **kwargs,\n    ) -&gt; \"ServiceManagerProtocol\":\n        return super().create_instance(\n            class_type,\n            required_services=required_services,\n            service_config=service_config,\n            user_config=user_config,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.StreamingPostProcessorFactory","title":"<code>StreamingPostProcessorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[StreamingPostProcessorType, 'StreamingPostProcessorProtocol']</code></p> <p>Factory for registering and creating StreamingPostProcessorProtocol instances based on the specified streaming post processor type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class StreamingPostProcessorFactory(\n    AIPerfFactory[StreamingPostProcessorType, \"StreamingPostProcessorProtocol\"]\n):\n    \"\"\"Factory for registering and creating StreamingPostProcessorProtocol instances based on the specified streaming post processor type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: StreamingPostProcessorType | str,\n        service_id: str,\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        max_queue_size: int = DEFAULT_STREAMING_MAX_QUEUE_SIZE,\n        **kwargs,\n    ) -&gt; \"StreamingPostProcessorProtocol\":\n        return super().create_instance(\n            class_type,\n            service_id=service_id,\n            service_config=service_config,\n            user_config=user_config,\n            max_queue_size=max_queue_size,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.ZMQProxyFactory","title":"<code>ZMQProxyFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ZMQProxyType, 'BaseZMQProxy']</code></p> <p>Factory for registering and creating BaseZMQProxy instances based on the specified ZMQ proxy type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ZMQProxyFactory(AIPerfFactory[ZMQProxyType, \"BaseZMQProxy\"]):\n    \"\"\"Factory for registering and creating BaseZMQProxy instances based on the specified ZMQ proxy type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ZMQProxyType | str,\n        zmq_proxy_config: \"BaseZMQProxyConfig\",\n        **kwargs,\n    ) -&gt; \"BaseZMQProxy\":\n        return super().create_instance(\n            class_type, zmq_proxy_config=zmq_proxy_config, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperfcommonhooks","title":"aiperf.common.hooks","text":"<p>This module provides an extensive set of hook definitions for AIPerf. It is designed to be used in conjunction with the :class:<code>HooksMixin</code> for classes to provide support for hooks. It provides a simple interface for registering hooks.</p> <p>Classes should inherit from the :class:<code>HooksMixin</code>, and specify the provided hook types by decorating the class with the :func:<code>provides_hooks</code> decorator.</p> <p>The hook functions are registered by decorating functions with the various hook decorators such as :func:<code>on_init</code>, :func:<code>on_start</code>, :func:<code>on_stop</code>, etc.</p> <p>More than one hook can be registered for a given hook type, and classes that inherit from classes with existing hooks will inherit the hooks from the base classes as well.</p> <p>The hooks are run by calling the :meth:<code>HooksMixin.run_hooks</code> method or retrieved via the :meth:<code>HooksMixin.get_hooks</code> method on the class.</p>"},{"location":"api/#aiperf.common.hooks.HookType","title":"<code>HookType = AIPerfHook | str</code>  <code>module-attribute</code>","text":"<p>Type alias for valid hook types. This is a union of the AIPerfHook enum and any user-defined custom strings.</p>"},{"location":"api/#aiperf.common.hooks.Hook","title":"<code>Hook</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[HookParamsT]</code></p> <p>A hook is a function that is decorated with a hook type and optional parameters. The HookParamsT is the type of the parameters. You can either have a static value, or a callable that returns the parameters.</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>class Hook(BaseModel, Generic[HookParamsT]):\n    \"\"\"A hook is a function that is decorated with a hook type and optional parameters.\n    The HookParamsT is the type of the parameters. You can either have a static value,\n    or a callable that returns the parameters.\n    \"\"\"\n\n    func: Callable\n    params: HookParamsT | Callable[[SelfT], HookParamsT] | None = None  # type: ignore\n\n    @property\n    def hook_type(self) -&gt; HookType:\n        return getattr(self.func, HookAttrs.HOOK_TYPE)\n\n    @property\n    def func_name(self) -&gt; str:\n        return self.func.__name__\n\n    @property\n    def qualified_name(self) -&gt; str:\n        return f\"{self.func.__qualname__}\"\n\n    def resolve_params(self, self_obj: SelfT) -&gt; HookParamsT | None:\n        \"\"\"Resolve the parameters for the hook. If the parameters are a callable, it will be called\n        with the self_obj as the argument, otherwise the parameters are returned as is.\"\"\"\n        if self.params is None:\n            return None\n        # With variable length parameters, you get a tuple with 1 item in it, so we need to check for that.\n        if (\n            isinstance(self.params, Iterable)\n            and len(self.params) == 1\n            and callable(self.params[0])\n        ):  # type: ignore\n            return self.params[0](self_obj)  # type: ignore\n        if callable(self.params):\n            return self.params(self_obj)\n        return self.params  # type: ignore\n\n    async def __call__(self, **kwargs) -&gt; None:\n        if asyncio.iscoroutinefunction(self.func):\n            await self.func(**kwargs)\n        else:\n            await asyncio.to_thread(self.func, **kwargs)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.hook_type} \ud83e\udc52 {self.qualified_name}\"\n</code></pre>"},{"location":"api/#aiperf.common.hooks.Hook.resolve_params","title":"<code>resolve_params(self_obj)</code>","text":"<p>Resolve the parameters for the hook. If the parameters are a callable, it will be called with the self_obj as the argument, otherwise the parameters are returned as is.</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def resolve_params(self, self_obj: SelfT) -&gt; HookParamsT | None:\n    \"\"\"Resolve the parameters for the hook. If the parameters are a callable, it will be called\n    with the self_obj as the argument, otherwise the parameters are returned as is.\"\"\"\n    if self.params is None:\n        return None\n    # With variable length parameters, you get a tuple with 1 item in it, so we need to check for that.\n    if (\n        isinstance(self.params, Iterable)\n        and len(self.params) == 1\n        and callable(self.params[0])\n    ):  # type: ignore\n        return self.params[0](self_obj)  # type: ignore\n    if callable(self.params):\n        return self.params(self_obj)\n    return self.params  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.common.hooks.HookAttrs","title":"<code>HookAttrs</code>","text":"<p>Constant attribute names for hooks.</p> <p>When you decorate a function with a hook decorator, the hook type and parameters are set as attributes on the function or class.</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>class HookAttrs:\n    \"\"\"Constant attribute names for hooks.\n\n    When you decorate a function with a hook decorator, the hook type and parameters are\n    set as attributes on the function or class.\n    \"\"\"\n\n    HOOK_TYPE = \"__aiperf_hook_type__\"\n    HOOK_PARAMS = \"__aiperf_hook_params__\"\n    PROVIDES_HOOKS = \"__provides_hooks__\"\n</code></pre>"},{"location":"api/#aiperf.common.hooks.background_task","title":"<code>background_task(interval=None, immediate=True, stop_on_error=False)</code>","text":"<p>Decorator to mark a method as a background task with automatic management.</p> <p>Tasks are automatically started when the service starts and stopped when the service stops. The decorated method will be run periodically in the background when the service is running.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float | Callable[[SelfT], float] | None</code> <p>Time between task executions in seconds. If None, the task will run once. Can be a callable that returns the interval, and will be called with 'self' as the argument.</p> <code>None</code> <code>immediate</code> <code>bool</code> <p>If True, run the task immediately on start, otherwise wait for the interval first.</p> <code>True</code> <code>stop_on_error</code> <code>bool</code> <p>If True, stop the task on any exception, otherwise log and continue.</p> <code>False</code> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @background_task(interval=1.0)\n    def _background_task(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._background_task.__aiperf_hook_type__ = AIPerfHook.BACKGROUND_TASK\nMyPlugin._background_task.__aiperf_hook_params__ = BackgroundTaskParams(\n    interval=1.0, immediate=True, stop_on_error=False\n)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def background_task(\n    interval: float | Callable[[SelfT], float] | None = None,\n    immediate: bool = True,\n    stop_on_error: bool = False,\n) -&gt; Callable:\n    \"\"\"\n    Decorator to mark a method as a background task with automatic management.\n\n    Tasks are automatically started when the service starts and stopped when the service stops.\n    The decorated method will be run periodically in the background when the service is running.\n\n    Args:\n        interval: Time between task executions in seconds. If None, the task will run once.\n            Can be a callable that returns the interval, and will be called with 'self' as the argument.\n        immediate: If True, run the task immediately on start, otherwise wait for the interval first.\n        stop_on_error: If True, stop the task on any exception, otherwise log and continue.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @background_task(interval=1.0)\n        def _background_task(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._background_task.__aiperf_hook_type__ = AIPerfHook.BACKGROUND_TASK\n    MyPlugin._background_task.__aiperf_hook_params__ = BackgroundTaskParams(\n        interval=1.0, immediate=True, stop_on_error=False\n    )\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(\n        AIPerfHook.BACKGROUND_TASK,\n        BackgroundTaskParams(\n            interval=interval, immediate=immediate, stop_on_error=stop_on_error\n        ),\n    )\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_command","title":"<code>on_command(*command_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a CommandMessage with the given command type(s) is received from the message bus. See :func:<code>aiperf.common.hooks._hook_decorator_for_message_types</code>.</p> <p>Example:</p> <pre><code>class MyService(BaseComponentService):\n    @on_command(CommandType.PROFILE_START)\n    def _on_profile_start(self, message: ProfileStartCommand) -&gt; CommandResponse:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyService._on_profile_start.__aiperf_hook_type__ = AIPerfHook.ON_COMMAND\nMyService._on_profile_start.__aiperf_hook_params__ = (CommandType.PROFILE_START,)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_command(\n    *command_types: CommandTypeT | Callable[[SelfT], Iterable[CommandTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a CommandMessage with the given\n    command type(s) is received from the message bus.\n    See :func:`aiperf.common.hooks._hook_decorator_for_message_types`.\n\n    Example:\n    ```python\n    class MyService(BaseComponentService):\n        @on_command(CommandType.PROFILE_START)\n        def _on_profile_start(self, message: ProfileStartCommand) -&gt; CommandResponse:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._on_profile_start.__aiperf_hook_type__ = AIPerfHook.ON_COMMAND\n    MyService._on_profile_start.__aiperf_hook_params__ = (CommandType.PROFILE_START,)\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_COMMAND, command_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_init","title":"<code>on_init(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during initialization. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_init\n    def _init_plugin(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._init_plugin.__aiperf_hook_type__ = AIPerfHook.ON_INIT\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_init(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called during initialization.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_init\n        def _init_plugin(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._init_plugin.__aiperf_hook_type__ = AIPerfHook.ON_INIT\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_INIT, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_message","title":"<code>on_message(*message_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when messages of the given type(s) are received from the message bus. See :func:<code>aiperf.common.hooks._hook_decorator_with_params</code>.</p> <p>Example:</p> <pre><code>class MyService(MessageBusClientMixin):\n    @on_message(MessageType.STATUS)\n    def _on_status_message(self, message: StatusMessage) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyService._on_status_message.__aiperf_hook_type__ = AIPerfHook.ON_MESSAGE\nMyService._on_status_message.__aiperf_hook_params__ = (MessageType.STATUS,)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_message(\n    *message_types: MessageTypeT | Callable[[SelfT], Iterable[MessageTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when messages of the\n    given type(s) are received from the message bus.\n    See :func:`aiperf.common.hooks._hook_decorator_with_params`.\n\n    Example:\n    ```python\n    class MyService(MessageBusClientMixin):\n        @on_message(MessageType.STATUS)\n        def _on_status_message(self, message: StatusMessage) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._on_status_message.__aiperf_hook_type__ = AIPerfHook.ON_MESSAGE\n    MyService._on_status_message.__aiperf_hook_params__ = (MessageType.STATUS,)\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_MESSAGE, message_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_pull_message","title":"<code>on_pull_message(*message_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called a pull client receives a message of the given type(s). See :func:<code>aiperf.common.hooks._hook_decorator_for_message_types</code>.</p> <p>Example:</p> <pre><code>class MyService(PullClientMixin, BaseComponentService):\n    @on_pull_message(MessageType.CREDIT_DROP)\n    def _on_credit_drop_pull(self, message: CreditDropMessage) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting: ```python MyService._on_pull_message.aiperf_hook_type = AIPerfHook.ON_PULL_MESSAGE MyService._on_pull_message.aiperf_hook_params = (MessageType.CREDIT_DROP,)</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_pull_message(\n    *message_types: MessageTypeT | Callable[[SelfT], Iterable[MessageTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called a pull client\n    receives a message of the given type(s).\n    See :func:`aiperf.common.hooks._hook_decorator_for_message_types`.\n\n    Example:\n    ```python\n    class MyService(PullClientMixin, BaseComponentService):\n        @on_pull_message(MessageType.CREDIT_DROP)\n        def _on_credit_drop_pull(self, message: CreditDropMessage) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._on_pull_message.__aiperf_hook_type__ = AIPerfHook.ON_PULL_MESSAGE\n    MyService._on_pull_message.__aiperf_hook_params__ = (MessageType.CREDIT_DROP,)\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_PULL_MESSAGE, message_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_request","title":"<code>on_request(*message_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when requests of the given type(s) are received from a ReplyClient. See :func:<code>aiperf.common.hooks._hook_decorator_for_message_types</code>.</p> <p>Example:</p> <pre><code>class MyService(RequestClientMixin, BaseComponentService):\n    @on_request(MessageType.CONVERSATION_REQUEST)\n    async def _handle_conversation_request(\n        self, message: ConversationRequestMessage\n    ) -&gt; ConversationResponseMessage:\n        return ConversationResponseMessage(\n            ...\n        )\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyService._handle_conversation_request.__aiperf_hook_type__ = AIPerfHook.ON_REQUEST\nMyService._handle_conversation_request.__aiperf_hook_params__ = (MessageType.CONVERSATION_REQUEST,)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_request(\n    *message_types: MessageTypeT | Callable[[SelfT], Iterable[MessageTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when requests of the\n    given type(s) are received from a ReplyClient.\n    See :func:`aiperf.common.hooks._hook_decorator_for_message_types`.\n\n    Example:\n    ```python\n    class MyService(RequestClientMixin, BaseComponentService):\n        @on_request(MessageType.CONVERSATION_REQUEST)\n        async def _handle_conversation_request(\n            self, message: ConversationRequestMessage\n        ) -&gt; ConversationResponseMessage:\n            return ConversationResponseMessage(\n                ...\n            )\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._handle_conversation_request.__aiperf_hook_type__ = AIPerfHook.ON_REQUEST\n    MyService._handle_conversation_request.__aiperf_hook_params__ = (MessageType.CONVERSATION_REQUEST,)\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_REQUEST, message_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_start","title":"<code>on_start(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during start. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_start\n    def _start_plugin(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._start_plugin.__aiperf_hook_type__ = AIPerfHook.ON_START\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_start(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called during start.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_start\n        def _start_plugin(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._start_plugin.__aiperf_hook_type__ = AIPerfHook.ON_START\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_START, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_state_change","title":"<code>on_state_change(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during the service state change. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_state_change\n    def _on_state_change(self, old_state: LifecycleState, new_state: LifecycleState) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_state_change.__aiperf_hook_type__ = AIPerfHook.ON_STATE_CHANGE\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_state_change(\n    func: Callable[[\"HooksMixinT\", LifecycleState, LifecycleState], Awaitable],\n) -&gt; Callable[[\"HooksMixinT\", LifecycleState, LifecycleState], Awaitable]:\n    \"\"\"Decorator to specify that the function is a hook that should be called during the service state change.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_state_change\n        def _on_state_change(self, old_state: LifecycleState, new_state: LifecycleState) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_state_change.__aiperf_hook_type__ = AIPerfHook.ON_STATE_CHANGE\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_STATE_CHANGE, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_stop","title":"<code>on_stop(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during stop. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_stop\n    def _stop_plugin(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._stop_plugin.__aiperf_hook_type__ = AIPerfHook.ON_STOP\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_stop(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called during stop.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_stop\n        def _stop_plugin(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._stop_plugin.__aiperf_hook_type__ = AIPerfHook.ON_STOP\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_STOP, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.provides_hooks","title":"<code>provides_hooks(*hook_types)</code>","text":"<p>Decorator to specify that the class provides a hook of the given type to all of its subclasses.</p> <p>Example:</p> <pre><code>@provides_hooks(AIPerfHook.ON_MESSAGE)\nclass MessageBusClientMixin(CommunicationMixin):\n    pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MessageBusClientMixin.__provides_hooks__ = {AIPerfHook.ON_MESSAGE}\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def provides_hooks(\n    *hook_types: HookType,\n) -&gt; Callable[[type[HooksMixinT]], type[HooksMixinT]]:\n    \"\"\"Decorator to specify that the class provides a hook of the given type to all of its subclasses.\n\n    Example:\n    ```python\n    @provides_hooks(AIPerfHook.ON_MESSAGE)\n    class MessageBusClientMixin(CommunicationMixin):\n        pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MessageBusClientMixin.__provides_hooks__ = {AIPerfHook.ON_MESSAGE}\n    ```\n    \"\"\"\n\n    def decorator(cls: type[HooksMixinT]) -&gt; type[HooksMixinT]:\n        setattr(cls, HookAttrs.PROVIDES_HOOKS, set(hook_types))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperfcommonlogging","title":"aiperf.common.logging","text":""},{"location":"api/#aiperf.common.logging.MultiProcessLogHandler","title":"<code>MultiProcessLogHandler</code>","text":"<p>               Bases: <code>RichHandler</code></p> <p>Custom logging handler that forwards log records to a multiprocessing queue.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>class MultiProcessLogHandler(RichHandler):\n    \"\"\"Custom logging handler that forwards log records to a multiprocessing queue.\"\"\"\n\n    def __init__(\n        self, log_queue: multiprocessing.Queue, service_id: str | None = None\n    ) -&gt; None:\n        super().__init__()\n        self.log_queue = log_queue\n        self.service_id = service_id\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"Emit a log record to the queue.\"\"\"\n        try:\n            # Create a serializable log data structure\n            log_data = {\n                \"name\": record.name,\n                \"levelname\": record.levelname,\n                \"levelno\": record.levelno,\n                \"msg\": record.getMessage(),\n                \"created\": record.created,\n                \"process_name\": multiprocessing.current_process().name,\n                \"process_id\": multiprocessing.current_process().pid,\n                \"service_id\": self.service_id,\n            }\n            self.log_queue.put_nowait(log_data)\n        except queue.Full:\n            # Drop logs if queue is full to prevent blocking. Do not log to prevent recursion.\n            pass\n        except Exception:\n            # Do not log to prevent recursion\n            pass\n</code></pre>"},{"location":"api/#aiperf.common.logging.MultiProcessLogHandler.emit","title":"<code>emit(record)</code>","text":"<p>Emit a log record to the queue.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>def emit(self, record: logging.LogRecord) -&gt; None:\n    \"\"\"Emit a log record to the queue.\"\"\"\n    try:\n        # Create a serializable log data structure\n        log_data = {\n            \"name\": record.name,\n            \"levelname\": record.levelname,\n            \"levelno\": record.levelno,\n            \"msg\": record.getMessage(),\n            \"created\": record.created,\n            \"process_name\": multiprocessing.current_process().name,\n            \"process_id\": multiprocessing.current_process().pid,\n            \"service_id\": self.service_id,\n        }\n        self.log_queue.put_nowait(log_data)\n    except queue.Full:\n        # Drop logs if queue is full to prevent blocking. Do not log to prevent recursion.\n        pass\n    except Exception:\n        # Do not log to prevent recursion\n        pass\n</code></pre>"},{"location":"api/#aiperf.common.logging.create_file_handler","title":"<code>create_file_handler(log_folder, level)</code>","text":"<p>Configure a file handler for logging.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>def create_file_handler(\n    log_folder: Path,\n    level: str | int,\n) -&gt; logging.FileHandler:\n    \"\"\"Configure a file handler for logging.\"\"\"\n\n    log_folder.mkdir(parents=True, exist_ok=True)\n    log_file_path = log_folder / \"aiperf.log\"\n\n    file_handler = logging.FileHandler(log_file_path, encoding=\"utf-8\")\n    file_handler.setLevel(level)\n    file_handler.setFormatter(\n        logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n    )\n    return file_handler\n</code></pre>"},{"location":"api/#aiperf.common.logging.get_global_log_queue","title":"<code>get_global_log_queue()</code>  <code>cached</code>","text":"<p>Get the global log queue. Will create a new queue if it doesn't exist.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_global_log_queue() -&gt; multiprocessing.Queue:\n    \"\"\"Get the global log queue. Will create a new queue if it doesn't exist.\"\"\"\n    return multiprocessing.Queue(maxsize=LOG_QUEUE_MAXSIZE)\n</code></pre>"},{"location":"api/#aiperf.common.logging.setup_child_process_logging","title":"<code>setup_child_process_logging(log_queue=None, service_id=None, service_config=None, user_config=None)</code>","text":"<p>Set up logging for a child process to send logs to the main process.</p> <p>This should be called early in child process initialization.</p> <p>Parameters:</p> Name Type Description Default <code>log_queue</code> <code>Queue | None</code> <p>The multiprocessing queue to send logs to. If None, tries to get the global queue.</p> <code>None</code> <code>service_id</code> <code>str | None</code> <p>The ID of the service to log under. If None, logs will be under the process name.</p> <code>None</code> <code>service_config</code> <code>ServiceConfig | None</code> <p>The service configuration used to determine the log level.</p> <code>None</code> <code>user_config</code> <code>UserConfig | None</code> <p>The user configuration used to determine the log folder.</p> <code>None</code> Source code in <code>aiperf/common/logging.py</code> <pre><code>def setup_child_process_logging(\n    log_queue: \"multiprocessing.Queue | None\" = None,\n    service_id: str | None = None,\n    service_config: ServiceConfig | None = None,\n    user_config: UserConfig | None = None,\n) -&gt; None:\n    \"\"\"Set up logging for a child process to send logs to the main process.\n\n    This should be called early in child process initialization.\n\n    Args:\n        log_queue: The multiprocessing queue to send logs to. If None, tries to get the global queue.\n        service_id: The ID of the service to log under. If None, logs will be under the process name.\n        service_config: The service configuration used to determine the log level.\n        user_config: The user configuration used to determine the log folder.\n    \"\"\"\n    root_logger = logging.getLogger()\n    level = ServiceDefaults.LOG_LEVEL.upper()\n    if service_config:\n        level = service_config.log_level.upper()\n\n        if service_id:\n            # If the service is in the trace or debug services, set the level to trace or debug\n            if service_config.trace_services and _is_service_in_types(\n                service_id, service_config.trace_services\n            ):\n                level = _TRACE\n            elif service_config.debug_services and _is_service_in_types(\n                service_id, service_config.debug_services\n            ):\n                level = _DEBUG\n\n    # Set the root logger level to ensure logs are passed to handlers\n    root_logger.setLevel(level)\n\n    # Remove all existing handlers to avoid duplicate logs\n    for existing_handler in root_logger.handlers[:]:\n        root_logger.removeHandler(existing_handler)\n\n    if log_queue is not None:\n        # Set up handler for child process\n        queue_handler = MultiProcessLogHandler(log_queue, service_id)\n        queue_handler.setLevel(level)\n        root_logger.addHandler(queue_handler)\n\n    if service_config:\n        # Set up rich logging to the console\n        rich_handler = RichHandler(\n            rich_tracebacks=True,\n            show_path=True,\n            console=Console(),\n            show_time=True,\n            show_level=True,\n            tracebacks_show_locals=False,\n            log_time_format=\"%H:%M:%S.%f\",\n            omit_repeated_times=False,\n        )\n        rich_handler.setLevel(level)\n        root_logger.addHandler(rich_handler)\n\n    if user_config and user_config.output.artifact_directory:\n        file_handler = create_file_handler(\n            user_config.output.artifact_directory / \"logs\", level\n        )\n        root_logger.addHandler(file_handler)\n</code></pre>"},{"location":"api/#aiperf.common.logging.setup_rich_logging","title":"<code>setup_rich_logging(user_config, service_config)</code>","text":"<p>Set up rich logging with appropriate configuration.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>def setup_rich_logging(user_config: UserConfig, service_config: ServiceConfig) -&gt; None:\n    \"\"\"Set up rich logging with appropriate configuration.\"\"\"\n    # Set logging level for the root logger (affects all loggers)\n    level = service_config.log_level.upper()\n    logging.root.setLevel(level)\n\n    rich_handler = RichHandler(\n        rich_tracebacks=True,\n        show_path=True,\n        console=Console(),\n        show_time=True,\n        show_level=True,\n        tracebacks_show_locals=False,\n        log_time_format=\"%H:%M:%S.%f\",\n        omit_repeated_times=False,\n    )\n    logging.root.addHandler(rich_handler)\n\n    # Enable file logging for services\n    # TODO: Use config to determine if file logging is enabled and the folder path.\n    log_folder = user_config.output.artifact_directory / \"logs\"\n    log_folder.mkdir(parents=True, exist_ok=True)\n    file_handler = logging.FileHandler(log_folder / \"aiperf.log\")\n    file_handler.setLevel(level)\n    file_handler.formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    logging.root.addHandler(file_handler)\n\n    logger.debug(lambda: f\"Logging initialized with level: {level}\")\n</code></pre>"},{"location":"api/#aiperfcommonmessagesbase_messages","title":"aiperf.common.messages.base_messages","text":""},{"location":"api/#aiperf.common.messages.base_messages.ErrorMessage","title":"<code>ErrorMessage</code>","text":"<p>               Bases: <code>Message</code></p> <p>Message containing error data.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>class ErrorMessage(Message):\n    \"\"\"Message containing error data.\"\"\"\n\n    message_type: MessageTypeT = MessageType.ERROR\n\n    error: ErrorDetails = Field(..., description=\"Error information\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base message class for optimized message handling. Based on the AIPerfBaseModel class, so it supports @exclude_if_none decorator. see :class:<code>AIPerfBaseModel</code> for more details.</p> <p>This class provides a base for all messages, including common fields like message_type, request_ns, and request_id. It also supports optional field exclusion based on the @exclude_if_none decorator.</p> <p>Each message model should inherit from this class, set the message_type field, and define its own additional fields.</p> <p>Example:</p> <pre><code>@exclude_if_none(\"some_field\")\nclass ExampleMessage(Message):\n    some_field: int | None = Field(default=None)\n    other_field: int = Field(default=1)\n</code></pre> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>@exclude_if_none(\"request_ns\", \"request_id\")\nclass Message(AIPerfBaseModel):\n    \"\"\"Base message class for optimized message handling. Based on the AIPerfBaseModel class,\n    so it supports @exclude_if_none decorator. see :class:`AIPerfBaseModel` for more details.\n\n    This class provides a base for all messages, including common fields like message_type,\n    request_ns, and request_id. It also supports optional field exclusion based on the\n    @exclude_if_none decorator.\n\n    Each message model should inherit from this class, set the message_type field,\n    and define its own additional fields.\n\n    Example:\n    ```python\n    @exclude_if_none(\"some_field\")\n    class ExampleMessage(Message):\n        some_field: int | None = Field(default=None)\n        other_field: int = Field(default=1)\n    ```\n    \"\"\"\n\n    _message_type_lookup: ClassVar[dict[MessageTypeT, type[\"Message\"]]] = {}\n    \"\"\"Lookup table for message types to their corresponding message classes. This is used to automatically\n    deserialize messages from JSON strings to their corresponding class type.\"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if hasattr(cls, \"message_type\") and cls.message_type is not None:\n            # Store concrete message classes in the lookup table\n            cls._message_type_lookup[cls.message_type] = cls\n            _logger.trace(f\"Added {cls.message_type} to message type lookup\")\n\n    message_type: MessageTypeT = Field(\n        ...,\n        description=\"The type of the message. Must be set in the subclass.\",\n    )\n\n    request_ns: int | None = Field(\n        default=None,\n        description=\"Timestamp of the request\",\n    )\n\n    request_id: str | None = Field(\n        default=None,\n        description=\"ID of the request\",\n    )\n\n    # TODO: Does this allow you to use model_validate_json and have it forward it to from_json? Need to test.\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.from_json\n\n    @classmethod\n    def from_json(cls, json_str: str | bytes | bytearray) -&gt; \"Message\":\n        \"\"\"Deserialize a message from a JSON string, attempting to auto-detect the message type.\n        NOTE: If you already know the message type, use the more performant :meth:`from_json_with_type` instead.\"\"\"\n        data = json.loads(json_str)\n        message_type = data.get(\"message_type\")\n        if not message_type:\n            raise ValueError(f\"Missing message_type: {json_str}\")\n\n        # Use cached message type lookup\n        message_class = cls._message_type_lookup[message_type]\n        if not message_class:\n            raise ValueError(f\"Unknown message type: {message_type}\")\n\n        return message_class.model_validate(data)\n\n    @classmethod\n    def from_json_with_type(\n        cls, message_type: MessageTypeT, json_str: str | bytes | bytearray\n    ) -&gt; \"Message\":\n        \"\"\"Deserialize a message from a JSON string with a specific message type.\n        NOTE: This is more performant than :meth:`from_json` because it does not need to\n        convert the JSON string to a dictionary first.\"\"\"\n        # Use cached message type lookup\n        message_class = cls._message_type_lookup[message_type]\n        if not message_class:\n            raise ValueError(f\"Unknown message type: {message_type}\")\n        return message_class.model_validate_json(json_str)\n\n    def to_json(self) -&gt; str:\n        \"\"\"Fast serialization without full validation\"\"\"\n        return orjson.dumps(self.__dict__).decode(\"utf-8\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a message from a JSON string, attempting to auto-detect the message type. NOTE: If you already know the message type, use the more performant :meth:<code>from_json_with_type</code> instead.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str | bytes | bytearray) -&gt; \"Message\":\n    \"\"\"Deserialize a message from a JSON string, attempting to auto-detect the message type.\n    NOTE: If you already know the message type, use the more performant :meth:`from_json_with_type` instead.\"\"\"\n    data = json.loads(json_str)\n    message_type = data.get(\"message_type\")\n    if not message_type:\n        raise ValueError(f\"Missing message_type: {json_str}\")\n\n    # Use cached message type lookup\n    message_class = cls._message_type_lookup[message_type]\n    if not message_class:\n        raise ValueError(f\"Unknown message type: {message_type}\")\n\n    return message_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message.from_json_with_type","title":"<code>from_json_with_type(message_type, json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a message from a JSON string with a specific message type. NOTE: This is more performant than :meth:<code>from_json</code> because it does not need to convert the JSON string to a dictionary first.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>@classmethod\ndef from_json_with_type(\n    cls, message_type: MessageTypeT, json_str: str | bytes | bytearray\n) -&gt; \"Message\":\n    \"\"\"Deserialize a message from a JSON string with a specific message type.\n    NOTE: This is more performant than :meth:`from_json` because it does not need to\n    convert the JSON string to a dictionary first.\"\"\"\n    # Use cached message type lookup\n    message_class = cls._message_type_lookup[message_type]\n    if not message_class:\n        raise ValueError(f\"Unknown message type: {message_type}\")\n    return message_class.model_validate_json(json_str)\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message.to_json","title":"<code>to_json()</code>","text":"<p>Fast serialization without full validation</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Fast serialization without full validation\"\"\"\n    return orjson.dumps(self.__dict__).decode(\"utf-8\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.RequiresRequestNSMixin","title":"<code>RequiresRequestNSMixin</code>","text":"<p>               Bases: <code>Message</code></p> <p>Mixin for messages that require a request_ns field.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>class RequiresRequestNSMixin(Message):\n    \"\"\"Mixin for messages that require a request_ns field.\"\"\"\n\n    request_ns: int = Field(  # type: ignore[assignment]\n        default_factory=time.time_ns,\n        description=\"Timestamp of the request in nanoseconds\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagescommand_messages","title":"aiperf.common.messages.command_messages","text":""},{"location":"api/#aiperf.common.messages.command_messages.CommandMessage","title":"<code>CommandMessage</code>","text":"<p>               Bases: <code>TargetedServiceMessage</code></p> <p>Message containing command data. This message is sent by the system controller to a service to command it to do something.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class CommandMessage(TargetedServiceMessage):\n    \"\"\"Message containing command data.\n    This message is sent by the system controller to a service to command it to do something.\n    \"\"\"\n\n    _command_type_lookup: ClassVar[dict[CommandTypeT, type[\"CommandMessage\"]]] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if hasattr(cls, \"command\"):\n            cls._command_type_lookup[cls.command] = cls\n\n    message_type: MessageTypeT = MessageType.COMMAND\n\n    command: CommandTypeT = Field(\n        ...,\n        description=\"Command to execute\",\n    )\n    command_id: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for this command. If not provided, a random UUID will be generated.\",\n    )\n    # TODO: Not really using this for anything right now.\n    require_response: bool = Field(\n        default=False,\n        description=\"Whether a response is required for this command\",\n    )\n\n    @classmethod\n    def from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandMessage\":\n        \"\"\"Deserialize a command message from a JSON string, attempting to auto-detect the command type.\"\"\"\n        data = json.loads(json_str)\n        command_type = data.get(\"command\")\n        if not command_type:\n            raise ValueError(f\"Missing command: {json_str}\")\n\n        # Use cached command type lookup\n        command_class = cls._command_type_lookup[command_type]\n        if not command_class:\n            _logger.debug(\n                lambda: f\"No command class found for command type: {command_type}\"\n            )\n            # fallback to regular command class\n            command_class = cls\n\n        return command_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandMessage.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a command message from a JSON string, attempting to auto-detect the command type.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandMessage\":\n    \"\"\"Deserialize a command message from a JSON string, attempting to auto-detect the command type.\"\"\"\n    data = json.loads(json_str)\n    command_type = data.get(\"command\")\n    if not command_type:\n        raise ValueError(f\"Missing command: {json_str}\")\n\n    # Use cached command type lookup\n    command_class = cls._command_type_lookup[command_type]\n    if not command_class:\n        _logger.debug(\n            lambda: f\"No command class found for command type: {command_type}\"\n        )\n        # fallback to regular command class\n        command_class = cls\n\n    return command_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandResponse","title":"<code>CommandResponse</code>","text":"<p>               Bases: <code>TargetedServiceMessage</code></p> <p>Message containing a command response.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class CommandResponse(TargetedServiceMessage):\n    \"\"\"Message containing a command response.\"\"\"\n\n    # Specialized lookup for command response messages by status\n    _command_status_lookup: ClassVar[\n        dict[CommandResponseStatus, type[\"CommandResponse\"]]\n    ] = {}\n    # Specialized lookup for command response messages by command type, for success messages\n    _command_success_type_lookup: ClassVar[\n        dict[CommandTypeT, type[\"CommandResponse\"]]\n    ] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if (\n            hasattr(cls, \"status\")\n            and cls.status is not None\n            and cls.status not in cls._command_status_lookup\n        ):\n            cls._command_status_lookup[cls.status] = cls\n        elif (\n            cls.__pydantic_fields__.get(\"status\") is not None\n            and cls.__pydantic_fields__.get(\"status\").default\n            == CommandResponseStatus.SUCCESS\n        ):\n            # Cache the specialized lookup by command type for success messages\n            cls._command_success_type_lookup[cls.command] = cls\n\n    message_type: MessageTypeT = MessageType.COMMAND_RESPONSE\n\n    command: CommandTypeT = Field(\n        ...,\n        description=\"Command type that is being responded to\",\n    )\n    command_id: str = Field(\n        ..., description=\"The ID of the command that is being responded to\"\n    )\n    status: CommandResponseStatus = Field(..., description=\"The status of the command\")\n\n    @classmethod\n    def from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandResponse\":\n        \"\"\"Deserialize a command response message from a JSON string, attempting to auto-detect the command response type.\"\"\"\n        data = json.loads(json_str)\n        status = data.get(\"status\")\n        if not status:\n            raise ValueError(f\"Missing command response status: {json_str}\")\n        command = data.get(\"command\")\n        if not command:\n            raise ValueError(f\"Missing command in command response: {json_str}\")\n\n        if status not in cls._command_status_lookup:\n            raise ValueError(\n                f\"Unknown command response status: {status}. Valid statuses are: {list(cls._command_status_lookup.keys())}\"\n            )\n\n        # Use cached command response type lookup by status\n        command_response_class = cls._command_status_lookup[status]\n\n        if (\n            status == CommandResponseStatus.SUCCESS\n            and command in cls._command_success_type_lookup\n        ):\n            # For success messages, use the specialized lookup by command type if it exists\n            command_response_class = cls._command_success_type_lookup[command]\n\n        return command_response_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandResponse.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a command response message from a JSON string, attempting to auto-detect the command response type.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandResponse\":\n    \"\"\"Deserialize a command response message from a JSON string, attempting to auto-detect the command response type.\"\"\"\n    data = json.loads(json_str)\n    status = data.get(\"status\")\n    if not status:\n        raise ValueError(f\"Missing command response status: {json_str}\")\n    command = data.get(\"command\")\n    if not command:\n        raise ValueError(f\"Missing command in command response: {json_str}\")\n\n    if status not in cls._command_status_lookup:\n        raise ValueError(\n            f\"Unknown command response status: {status}. Valid statuses are: {list(cls._command_status_lookup.keys())}\"\n        )\n\n    # Use cached command response type lookup by status\n    command_response_class = cls._command_status_lookup[status]\n\n    if (\n        status == CommandResponseStatus.SUCCESS\n        and command in cls._command_success_type_lookup\n    ):\n        # For success messages, use the specialized lookup by command type if it exists\n        command_response_class = cls._command_success_type_lookup[command]\n\n    return command_response_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandSuccessResponse","title":"<code>CommandSuccessResponse</code>","text":"<p>               Bases: <code>CommandResponse</code></p> <p>Generic command response message when a command succeeds. It should be subclassed for specific command types.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class CommandSuccessResponse(CommandResponse):\n    \"\"\"Generic command response message when a command succeeds. It should be\n    subclassed for specific command types.\"\"\"\n\n    status: CommandResponseStatus = CommandResponseStatus.SUCCESS\n    data: Any | None = Field(\n        default=None,\n        description=\"The data of the command response\",\n    )\n\n    @classmethod\n    def from_command_message(\n        cls, command_message: CommandMessage, service_id: str, data: Any | None = None\n    ) -&gt; Self:\n        return cls(\n            service_id=service_id,\n            target_service_id=command_message.service_id,\n            command=command_message.command,\n            command_id=command_message.command_id,\n            data=data,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.DiscoverServicesCommand","title":"<code>DiscoverServicesCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request services to discover services.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class DiscoverServicesCommand(CommandMessage):\n    \"\"\"Command message sent to request services to discover services.\"\"\"\n\n    command: CommandTypeT = CommandType.DISCOVER_SERVICES\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProcessRecordsCommand","title":"<code>ProcessRecordsCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Data to send with the process records command.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProcessRecordsCommand(CommandMessage):\n    \"\"\"Data to send with the process records command.\"\"\"\n\n    command: CommandTypeT = CommandType.PROCESS_RECORDS\n\n    cancelled: bool = Field(\n        default=False,\n        description=\"Whether the profile run was cancelled\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProcessRecordsResponse","title":"<code>ProcessRecordsResponse</code>","text":"<p>               Bases: <code>CommandSuccessResponse</code></p> <p>Response to the process records command.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProcessRecordsResponse(CommandSuccessResponse):\n    \"\"\"Response to the process records command.\"\"\"\n\n    command: CommandTypeT = CommandType.PROCESS_RECORDS\n\n    data: ProcessRecordsResult | None = Field(  # type: ignore[assignment]\n        default=None,\n        description=\"The result of the process records command\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProfileCancelCommand","title":"<code>ProfileCancelCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request services to cancel profiling.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProfileCancelCommand(CommandMessage):\n    \"\"\"Command message sent to request services to cancel profiling.\"\"\"\n\n    command: CommandTypeT = CommandType.PROFILE_CANCEL\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProfileConfigureCommand","title":"<code>ProfileConfigureCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Data to send with the profile configure command.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProfileConfigureCommand(CommandMessage):\n    \"\"\"Data to send with the profile configure command.\"\"\"\n\n    command: CommandTypeT = CommandType.PROFILE_CONFIGURE\n\n    # TODO: Define this type\n    config: Any = Field(..., description=\"Configuration for the profile\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProfileStartCommand","title":"<code>ProfileStartCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request services to start profiling.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProfileStartCommand(CommandMessage):\n    \"\"\"Command message sent to request services to start profiling.\"\"\"\n\n    command: CommandTypeT = CommandType.PROFILE_START\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ShutdownCommand","title":"<code>ShutdownCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request a service to shutdown.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ShutdownCommand(CommandMessage):\n    \"\"\"Command message sent to request a service to shutdown.\"\"\"\n\n    command: CommandTypeT = CommandType.SHUTDOWN\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.TargetedServiceMessage","title":"<code>TargetedServiceMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message that can be targeted to a specific service by id or type. If both <code>target_service_type</code> and <code>target_service_id</code> are None, the message is sent to all services that are subscribed to the message type.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>@exclude_if_none(\"target_service_id\", \"target_service_type\")\nclass TargetedServiceMessage(BaseServiceMessage):\n    \"\"\"Message that can be targeted to a specific service by id or type.\n    If both `target_service_type` and `target_service_id` are None, the message is\n    sent to all services that are subscribed to the message type.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_target_service(self) -&gt; Self:\n        if self.target_service_id is not None and self.target_service_type is not None:\n            raise ValueError(\n                \"Either target_service_id or target_service_type can be provided, but not both\"\n            )\n        return self\n\n    target_service_id: str | None = Field(\n        default=None,\n        description=\"ID of the target service to send the message to. \"\n        \"If both `target_service_type` and `target_service_id` are None, the message is \"\n        \"sent to all services that are subscribed to the message type.\",\n    )\n    target_service_type: ServiceTypeT | None = Field(\n        default=None,\n        description=\"Type of the service to send the message to. \"\n        \"If both `target_service_type` and `target_service_id` are None, the message is \"\n        \"sent to all services that are subscribed to the message type.\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagescredit_messages","title":"aiperf.common.messages.credit_messages","text":""},{"location":"api/#aiperf.common.messages.credit_messages.CreditDropMessage","title":"<code>CreditDropMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message indicating that a credit has been dropped. This message is sent by the timing manager to workers to indicate that credit(s) have been dropped.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditDropMessage(BaseServiceMessage):\n    \"\"\"Message indicating that a credit has been dropped.\n    This message is sent by the timing manager to workers to indicate that credit(s)\n    have been dropped.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_DROP\n\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    conversation_id: str | None = Field(\n        default=None, description=\"The ID of the conversation, if applicable.\"\n    )\n    credit_drop_ns: int | None = Field(\n        default=None,\n        description=\"Timestamp of the credit drop, if applicable. None means send ASAP.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseCompleteMessage","title":"<code>CreditPhaseCompleteMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for credit phase complete. Sent by the TimingManager to report that a credit phase has completed.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseCompleteMessage(BaseServiceMessage):\n    \"\"\"Message for credit phase complete. Sent by the TimingManager to report that a credit phase has completed.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_COMPLETE\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    completed: int = Field(\n        ...,\n        description=\"The number of completed credits (returned from the workers). This is the final count of completed credits.\",\n    )\n    end_ns: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The time in which the last credit was returned from the workers in nanoseconds. If None, the phase has not completed.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseProgressMessage","title":"<code>CreditPhaseProgressMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Sent by the TimingManager to report the progress of a credit phase.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseProgressMessage(BaseServiceMessage):\n    \"\"\"Sent by the TimingManager to report the progress of a credit phase.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_PROGRESS\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    sent: int = Field(default=0, description=\"The number of sent credits\")\n    completed: int = Field(\n        default=0,\n        description=\"The number of completed credits (returned from the workers)\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseSendingCompleteMessage","title":"<code>CreditPhaseSendingCompleteMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for credit phase sending complete. Sent by the TimingManager to report that a credit phase has completed sending.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseSendingCompleteMessage(BaseServiceMessage):\n    \"\"\"Message for credit phase sending complete. Sent by the TimingManager to report that a credit phase has completed sending.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_SENDING_COMPLETE\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    sent_end_ns: int | None = Field(\n        default=None,\n        description=\"The time of the last sent credit in nanoseconds. If None, the phase has not sent all credits.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseStartMessage","title":"<code>CreditPhaseStartMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for credit phase start. Sent by the TimingManager to report that a credit phase has started.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseStartMessage(BaseServiceMessage):\n    \"\"\"Message for credit phase start. Sent by the TimingManager to report that a credit phase has started.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_START\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    start_ns: int = Field(\n        ge=1,\n        description=\"The start time of the credit phase in nanoseconds.\",\n    )\n    total_expected_requests: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The total number of expected requests. If None, the phase is not request count based.\",\n    )\n    expected_duration_sec: float | None = Field(\n        default=None,\n        ge=1,\n        description=\"The expected duration of the credit phase in seconds. If None, the phase is not time based.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditReturnMessage","title":"<code>CreditReturnMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message indicating that a credit has been returned. This message is sent by a worker to the timing manager to indicate that work has been completed.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditReturnMessage(BaseServiceMessage):\n    \"\"\"Message indicating that a credit has been returned.\n    This message is sent by a worker to the timing manager to indicate that work has\n    been completed.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_RETURN\n\n    phase: CreditPhase = Field(\n        ...,\n        description=\"The Credit Phase of the credit drop. This is so the TimingManager can track the progress of the credit phase.\",\n    )\n    delayed_ns: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The number of nanoseconds the credit drop was delayed by, or None if the credit was sent on time. \"\n        \"NOTE: This is only applicable if the original credit_drop_ns was not None.\",\n    )\n    # TODO: Does it make more sense for this to be part of the RequestRecord?\n    pre_inference_ns: int | None = Field(\n        default=None,\n        description=\"The latency of the credit in nanoseconds from when it was first received to when the inference request was sent. \"\n        \"This can be used to trace the latency in order to identify bottlenecks or other issues.\",\n        ge=0,\n    )\n\n    @property\n    def delayed(self) -&gt; bool:\n        return self.delayed_ns is not None\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditsCompleteMessage","title":"<code>CreditsCompleteMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Credits complete message sent by the TimingManager to the System controller to signify all Credit Phases have been completed.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditsCompleteMessage(BaseServiceMessage):\n    \"\"\"Credits complete message sent by the TimingManager to the System controller to signify all Credit Phases\n    have been completed.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDITS_COMPLETE\n</code></pre>"},{"location":"api/#aiperfcommonmessagesdataset_messages","title":"aiperf.common.messages.dataset_messages","text":""},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationRequestMessage","title":"<code>ConversationRequestMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message to request a full conversation by ID.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationRequestMessage(BaseServiceMessage):\n    \"\"\"Message to request a full conversation by ID.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_REQUEST\n\n    conversation_id: str | None = Field(\n        default=None, description=\"The session ID of the conversation\"\n    )\n    credit_phase: CreditPhase | None = Field(\n        default=None,\n        description=\"The type of credit phase (either warmup or profiling). If not provided, the timing manager will use the default credit phase.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationResponseMessage","title":"<code>ConversationResponseMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message containing a full conversation.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationResponseMessage(BaseServiceMessage):\n    \"\"\"Message containing a full conversation.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_RESPONSE\n    conversation: Conversation = Field(..., description=\"The conversation data\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationTurnRequestMessage","title":"<code>ConversationTurnRequestMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message to request a single turn from a conversation.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationTurnRequestMessage(BaseServiceMessage):\n    \"\"\"Message to request a single turn from a conversation.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_TURN_REQUEST\n\n    conversation_id: str = Field(\n        ...,\n        description=\"The ID of the conversation.\",\n    )\n    turn_index: int = Field(\n        ...,\n        ge=0,\n        description=\"The index of the turn in the conversation.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationTurnResponseMessage","title":"<code>ConversationTurnResponseMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message containing a single turn from a conversation.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationTurnResponseMessage(BaseServiceMessage):\n    \"\"\"Message containing a single turn from a conversation.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_TURN_RESPONSE\n\n    turn: Turn = Field(..., description=\"The turn data\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.DatasetConfiguredNotification","title":"<code>DatasetConfiguredNotification</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Notification sent to notify other services that the dataset has been configured.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class DatasetConfiguredNotification(BaseServiceMessage):\n    \"\"\"Notification sent to notify other services that the dataset has been configured.\"\"\"\n\n    message_type: MessageTypeT = MessageType.DATASET_CONFIGURED_NOTIFICATION\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.DatasetTimingRequest","title":"<code>DatasetTimingRequest</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a dataset timing request.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class DatasetTimingRequest(BaseServiceMessage):\n    \"\"\"Message for a dataset timing request.\"\"\"\n\n    message_type: MessageTypeT = MessageType.DATASET_TIMING_REQUEST\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.DatasetTimingResponse","title":"<code>DatasetTimingResponse</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a dataset timing response.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class DatasetTimingResponse(BaseServiceMessage):\n    \"\"\"Message for a dataset timing response.\"\"\"\n\n    message_type: MessageTypeT = MessageType.DATASET_TIMING_RESPONSE\n\n    timing_data: list[tuple[int, str]] = Field(\n        ...,\n        description=\"The timing data of the dataset. Tuple of (timestamp, conversation_id)\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessageshealth_messages","title":"aiperf.common.messages.health_messages","text":""},{"location":"api/#aiperf.common.messages.health_messages.WorkerHealthMessage","title":"<code>WorkerHealthMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a worker health check.</p> Source code in <code>aiperf/common/messages/health_messages.py</code> <pre><code>class WorkerHealthMessage(BaseServiceMessage):\n    \"\"\"Message for a worker health check.\"\"\"\n\n    message_type: MessageTypeT = MessageType.WORKER_HEALTH\n\n    process: ProcessHealth = Field(..., description=\"The health of the worker process\")\n\n    # Worker specific fields\n    task_stats: dict[CreditPhase, WorkerPhaseTaskStats] = Field(\n        ...,\n        description=\"Stats for the tasks that have been sent to the worker, keyed by the credit phase\",\n    )\n\n    @property\n    def total_tasks(self) -&gt; int:\n        \"\"\"The total number of tasks that have been sent to the worker.\"\"\"\n        return sum(task_stats.total for task_stats in self.task_stats.values())\n\n    @property\n    def completed_tasks(self) -&gt; int:\n        \"\"\"The number of tasks that have been completed by the worker.\"\"\"\n        return sum(task_stats.completed for task_stats in self.task_stats.values())\n\n    @property\n    def failed_tasks(self) -&gt; int:\n        \"\"\"The number of tasks that have failed by the worker.\"\"\"\n        return sum(task_stats.failed for task_stats in self.task_stats.values())\n\n    @property\n    def in_progress_tasks(self) -&gt; int:\n        \"\"\"The number of tasks that are currently in progress by the worker.\"\"\"\n        return sum(task_stats.in_progress for task_stats in self.task_stats.values())\n\n    @property\n    def error_rate(self) -&gt; float:\n        \"\"\"The error rate of the worker.\"\"\"\n        if self.total_tasks == 0:\n            return 0\n        return self.failed_tasks / self.total_tasks\n</code></pre>"},{"location":"api/#aiperf.common.messages.health_messages.WorkerHealthMessage.completed_tasks","title":"<code>completed_tasks</code>  <code>property</code>","text":"<p>The number of tasks that have been completed by the worker.</p>"},{"location":"api/#aiperf.common.messages.health_messages.WorkerHealthMessage.error_rate","title":"<code>error_rate</code>  <code>property</code>","text":"<p>The error rate of the worker.</p>"},{"location":"api/#aiperf.common.messages.health_messages.WorkerHealthMessage.failed_tasks","title":"<code>failed_tasks</code>  <code>property</code>","text":"<p>The number of tasks that have failed by the worker.</p>"},{"location":"api/#aiperf.common.messages.health_messages.WorkerHealthMessage.in_progress_tasks","title":"<code>in_progress_tasks</code>  <code>property</code>","text":"<p>The number of tasks that are currently in progress by the worker.</p>"},{"location":"api/#aiperf.common.messages.health_messages.WorkerHealthMessage.total_tasks","title":"<code>total_tasks</code>  <code>property</code>","text":"<p>The total number of tasks that have been sent to the worker.</p>"},{"location":"api/#aiperfcommonmessagesinference_messages","title":"aiperf.common.messages.inference_messages","text":""},{"location":"api/#aiperf.common.messages.inference_messages.InferenceResultsMessage","title":"<code>InferenceResultsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a inference results.</p> Source code in <code>aiperf/common/messages/inference_messages.py</code> <pre><code>class InferenceResultsMessage(BaseServiceMessage):\n    \"\"\"Message for a inference results.\"\"\"\n\n    message_type: MessageTypeT = MessageType.INFERENCE_RESULTS\n\n    record: SerializeAsAny[RequestRecord] = Field(\n        ..., description=\"The inference results record\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.inference_messages.ParsedInferenceResultsMessage","title":"<code>ParsedInferenceResultsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a parsed inference results.</p> Source code in <code>aiperf/common/messages/inference_messages.py</code> <pre><code>class ParsedInferenceResultsMessage(BaseServiceMessage):\n    \"\"\"Message for a parsed inference results.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PARSED_INFERENCE_RESULTS\n\n    record: SerializeAsAny[ParsedResponseRecord] = Field(\n        ..., description=\"The post process results record\"\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagesprogress_messages","title":"aiperf.common.messages.progress_messages","text":""},{"location":"api/#aiperf.common.messages.progress_messages.AllRecordsReceivedMessage","title":"<code>AllRecordsReceivedMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code>, <code>RequiresRequestNSMixin</code></p> <p>This is sent by the RecordsManager to signal that all parsed records have been received, and the final processing stats are available.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class AllRecordsReceivedMessage(BaseServiceMessage, RequiresRequestNSMixin):\n    \"\"\"This is sent by the RecordsManager to signal that all parsed records have been received, and the final processing stats are available.\"\"\"\n\n    message_type: MessageTypeT = MessageType.ALL_RECORDS_RECEIVED\n    final_processing_stats: PhaseProcessingStats = Field(\n        ..., description=\"The final processing stats for the profile run\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProcessRecordsResultMessage","title":"<code>ProcessRecordsResultMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for process records result.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProcessRecordsResultMessage(BaseServiceMessage):\n    \"\"\"Message for process records result.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROCESS_RECORDS_RESULT\n\n    process_records_result: ProcessRecordsResult = Field(\n        ..., description=\"The process records result\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProcessingStatsMessage","title":"<code>ProcessingStatsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for processing stats. Sent by the records manager to the system controller to report the stats of the profile run.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProcessingStatsMessage(BaseServiceMessage):\n    \"\"\"Message for processing stats. Sent by the records manager to the system controller to report the stats of the profile run.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROCESSING_STATS\n\n    error_count: int = Field(default=0, description=\"The number of errors encountered\")\n    completed: int = Field(\n        default=0, description=\"The number of requests processed by the records manager\"\n    )\n    worker_completed: dict[str, int] = Field(\n        default_factory=dict,\n        description=\"Per-worker request completion counts, keyed by worker service_id\",\n    )\n    worker_errors: dict[str, int] = Field(\n        default_factory=dict,\n        description=\"Per-worker error counts, keyed by worker service_id\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProfileProgressMessage","title":"<code>ProfileProgressMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for profile progress. Sent by the timing manager to the system controller to report the progress of the profile run.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProfileProgressMessage(BaseServiceMessage):\n    \"\"\"Message for profile progress. Sent by the timing manager to the system controller to report the progress of the profile run.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROFILE_PROGRESS\n\n    profile_id: str | None = Field(\n        default=None, description=\"The ID of the current profile\"\n    )\n    start_ns: int = Field(\n        ..., description=\"The start time of the profile run in nanoseconds\"\n    )\n    end_ns: int | None = Field(\n        default=None, description=\"The end time of the profile run in nanoseconds\"\n    )\n    total: int = Field(\n        ..., description=\"The total number of inference requests to be made (if known)\"\n    )\n    completed: int = Field(\n        ..., description=\"The number of inference requests completed\"\n    )\n    warmup: bool = Field(\n        default=False,\n        description=\"Whether this is the warmup phase of the profile run\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProfileResultsMessage","title":"<code>ProfileResultsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for profile results.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProfileResultsMessage(BaseServiceMessage):\n    \"\"\"Message for profile results.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROFILE_RESULTS\n\n    profile_results: ProfileResults = Field(..., description=\"The profile results\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.RecordsProcessingStatsMessage","title":"<code>RecordsProcessingStatsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for processing stats. Sent by the RecordsManager to report the stats of the profile run. This contains the stats for a single credit phase only.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class RecordsProcessingStatsMessage(BaseServiceMessage):\n    \"\"\"Message for processing stats. Sent by the RecordsManager to report the stats of the profile run.\n    This contains the stats for a single credit phase only.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROCESSING_STATS\n\n    processing_stats: PhaseProcessingStats = Field(\n        ..., description=\"The stats for the credit phase\"\n    )\n    worker_stats: dict[str, PhaseProcessingStats] = Field(\n        default_factory=dict,\n        description=\"The stats for each worker how many requests were processed and how many errors were \"\n        \"encountered, keyed by worker service_id\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.SweepProgressMessage","title":"<code>SweepProgressMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for sweep progress.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class SweepProgressMessage(BaseServiceMessage):\n    \"\"\"Message for sweep progress.\"\"\"\n\n    # TODO: add profile information\n\n    message_type: MessageTypeT = MessageType.SWEEP_PROGRESS\n\n    sweep_id: str = Field(..., description=\"The ID of the current sweep\")\n    sweep_start_ns: int = Field(\n        ..., description=\"The start time of the sweep in nanoseconds\"\n    )\n    end_ns: int | None = Field(\n        default=None, description=\"The end time of the profile run in nanoseconds\"\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagesservice_messages","title":"aiperf.common.messages.service_messages","text":""},{"location":"api/#aiperf.common.messages.service_messages.BaseServiceErrorMessage","title":"<code>BaseServiceErrorMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Base message containing error data.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class BaseServiceErrorMessage(BaseServiceMessage):\n    \"\"\"Base message containing error data.\"\"\"\n\n    message_type: MessageTypeT = MessageType.SERVICE_ERROR\n\n    error: ErrorDetails = Field(..., description=\"Error information\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.BaseServiceMessage","title":"<code>BaseServiceMessage</code>","text":"<p>               Bases: <code>Message</code></p> <p>Base message that is sent from a service. Requires a service_id field to specify the service that sent the message.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class BaseServiceMessage(Message):\n    \"\"\"Base message that is sent from a service. Requires a service_id field to specify\n    the service that sent the message.\"\"\"\n\n    service_id: str = Field(\n        ...,\n        description=\"ID of the service sending the message\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.BaseStatusMessage","title":"<code>BaseStatusMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Base message containing status data. This message is sent by a service to the system controller to report its status.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class BaseStatusMessage(BaseServiceMessage):\n    \"\"\"Base message containing status data.\n    This message is sent by a service to the system controller to report its status.\n    \"\"\"\n\n    # override request_ns to be auto-filled if not provided\n    request_ns: int | None = Field(\n        default=time.time_ns(),\n        description=\"Timestamp of the request\",\n    )\n    state: LifecycleState = Field(\n        ...,\n        description=\"Current state of the service\",\n    )\n    service_type: ServiceTypeT = Field(\n        ...,\n        description=\"Type of service\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.HeartbeatMessage","title":"<code>HeartbeatMessage</code>","text":"<p>               Bases: <code>BaseStatusMessage</code></p> <p>Message containing heartbeat data. This message is sent by a service to the system controller to indicate that it is still running.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class HeartbeatMessage(BaseStatusMessage):\n    \"\"\"Message containing heartbeat data.\n    This message is sent by a service to the system controller to indicate that it is\n    still running.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.HEARTBEAT\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.NotificationMessage","title":"<code>NotificationMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message containing a notification from a service. This is used to notify other services of events.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class NotificationMessage(BaseServiceMessage):\n    \"\"\"Message containing a notification from a service. This is used to notify other services of events.\"\"\"\n\n    message_type: MessageTypeT = MessageType.NOTIFICATION\n\n    notification_type: NotificationType = Field(\n        ...,\n        description=\"The type of notification\",\n    )\n\n    data: SerializeAsAny[BaseModel | None] = Field(\n        default=None,\n        description=\"Data to send with the notification\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.RegistrationMessage","title":"<code>RegistrationMessage</code>","text":"<p>               Bases: <code>BaseStatusMessage</code></p> <p>Message containing registration data. This message is sent by a service to the system controller to register itself.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class RegistrationMessage(BaseStatusMessage):\n    \"\"\"Message containing registration data.\n    This message is sent by a service to the system controller to register itself.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.REGISTRATION\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.StatusMessage","title":"<code>StatusMessage</code>","text":"<p>               Bases: <code>BaseStatusMessage</code></p> <p>Message containing status data. This message is sent by a service to the system controller to report its status.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class StatusMessage(BaseStatusMessage):\n    \"\"\"Message containing status data.\n    This message is sent by a service to the system controller to report its status.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.STATUS\n</code></pre>"},{"location":"api/#aiperfcommonmixinsaiperf_lifecycle_mixin","title":"aiperf.common.mixins.aiperf_lifecycle_mixin","text":""},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin","title":"<code>AIPerfLifecycleMixin</code>","text":"<p>               Bases: <code>TaskManagerMixin</code>, <code>HooksMixin</code></p> <p>This mixin provides a lifecycle state machine, and is the basis for most components in the AIPerf framework. It provides a set of hooks that are run at each state transition, and the ability to define background tasks that are automatically ran on @on_start, and canceled via @on_stop.</p> <p>It exposes to the outside world <code>initialize</code>, <code>start</code>, and <code>stop</code> methods, as well as getting the current state of the lifecycle. These simple methods promote a simple interface for users to interact with.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>@provides_hooks(\n    AIPerfHook.ON_INIT,\n    AIPerfHook.ON_START,\n    AIPerfHook.ON_STOP,\n    AIPerfHook.ON_STATE_CHANGE,\n    AIPerfHook.BACKGROUND_TASK,\n)\n@implements_protocol(AIPerfLifecycleProtocol)\nclass AIPerfLifecycleMixin(TaskManagerMixin, HooksMixin):\n    \"\"\"This mixin provides a lifecycle state machine, and is the basis for most components in the AIPerf framework.\n    It provides a set of hooks that are run at each state transition, and the ability to define background tasks\n    that are automatically ran on @on_start, and canceled via @on_stop.\n\n    It exposes to the outside world `initialize`, `start`, and `stop` methods, as well as getting the\n    current state of the lifecycle. These simple methods promote a simple interface for users to interact with.\n    \"\"\"\n\n    def __init__(self, id: str | None = None, **kwargs) -&gt; None:\n        \"\"\"\n        Args:\n            id: The id of the lifecycle. If not provided, a random uuid will be generated.\n        \"\"\"\n        self.id = id or f\"{self.__class__.__name__}_{uuid.uuid4().hex[:8]}\"\n        self._state = LifecycleState.CREATED\n        self.initialized_event = asyncio.Event()\n        self.started_event = asyncio.Event()\n        self._stop_requested_event = asyncio.Event()\n        self.stopped_event = asyncio.Event()  # set on stop or failure\n        self._children: list[AIPerfLifecycleProtocol] = []\n        if \"logger_name\" not in kwargs:\n            kwargs[\"logger_name\"] = self.id\n        super().__init__(**kwargs)\n\n    @property\n    def state(self) -&gt; LifecycleState:\n        return self._state\n\n    # NOTE: This was moved to not be a property setter, as we want it to be async so we can\n    # run the hooks and await them. Otherwise there is issues with creating a task when the\n    # lifecycle is trying to stop.\n    async def _set_state(self, state: LifecycleState) -&gt; None:\n        if state == self._state:\n            return\n        old_state = self._state\n        self._state = state\n        if self.is_debug_enabled:\n            self.debug(f\"State changed from {old_state!r} to {state!r} for {self}\")\n        await self.run_hooks(\n            AIPerfHook.ON_STATE_CHANGE, old_state=old_state, new_state=state\n        )\n\n    @property\n    def was_initialized(self) -&gt; bool:\n        return self.initialized_event.is_set()\n\n    @property\n    def was_started(self) -&gt; bool:\n        return self.started_event.is_set()\n\n    @property\n    def was_stopped(self) -&gt; bool:\n        return self.stopped_event.is_set()\n\n    @property\n    def is_running(self) -&gt; bool:\n        \"\"\"Whether the lifecycle's current state is LifecycleState.RUNNING.\"\"\"\n        return self.state == LifecycleState.RUNNING\n\n    @property\n    def stop_requested(self) -&gt; bool:\n        \"\"\"Whether the lifecycle has been requested to stop.\"\"\"\n        return self._stop_requested_event.is_set()\n\n    @stop_requested.setter\n    def stop_requested(self, value: bool) -&gt; None:\n        if value:\n            self._stop_requested_event.set()\n        else:\n            self._stop_requested_event.clear()\n\n    async def _execute_state_transition(\n        self,\n        transient_state: LifecycleState,\n        final_state: LifecycleState,\n        hook_type: AIPerfHook,\n        event: asyncio.Event,\n        reverse: bool = False,\n    ) -&gt; None:\n        \"\"\"This method wraps the functionality of changing the state of the lifecycle, and running the hooks.\n        It is used to ensure that the state change and hook running are atomic, and that the state change is\n        only made after the hooks have completed. It also takes in an event that is set when the state change is complete.\n        This is useful for external code waiting for the state change to complete before continuing.\n\n        If reverse is True, the hooks are run in reverse order. This is useful for stopping the lifecycle in the reverse order of starting it.\n        \"\"\"\n        await self._set_state(transient_state)\n        self.debug(lambda: f\"{transient_state.title()} {self}\")\n        try:\n            await self.run_hooks(hook_type, reverse=reverse)\n            await self._set_state(final_state)\n            self.debug(lambda: f\"{self} is now {final_state.title()}\")\n            event.set()\n        except Exception as e:\n            await self._fail(e)\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the lifecycle and run the @on_init hooks.\n\n        NOTE: It is generally discouraged from overriding this method.\n        Instead, use the @on_init hook to handle your own initialization logic.\n        \"\"\"\n        if self.state in (\n            LifecycleState.INITIALIZING,\n            LifecycleState.INITIALIZED,\n            LifecycleState.STARTING,\n            LifecycleState.RUNNING,\n        ):\n            self.debug(\n                lambda: f\"Ignoring initialize request for {self} in state {self.state}\"\n            )\n            return\n\n        if self.state != LifecycleState.CREATED:\n            raise InvalidStateError(\n                f\"Cannot initialize from state {self.state} for {self}\"\n            )\n\n        await self._execute_state_transition(\n            LifecycleState.INITIALIZING,\n            LifecycleState.INITIALIZED,\n            AIPerfHook.ON_INIT,\n            self.initialized_event,\n        )\n\n    async def start(self) -&gt; None:\n        \"\"\"Start the lifecycle and run the @on_start hooks.\n\n        NOTE: It is generally discouraged from overriding this method.\n        Instead, use the @on_start hook to handle your own starting logic.\n        \"\"\"\n        if self.state in (\n            LifecycleState.STARTING,\n            LifecycleState.RUNNING,\n        ):\n            self.debug(\n                lambda: f\"Ignoring start request for {self} in state {self.state}\"\n            )\n            return\n\n        if self.state != LifecycleState.INITIALIZED:\n            raise InvalidStateError(f\"Cannot start from state {self.state} for {self}\")\n\n        await self._execute_state_transition(\n            LifecycleState.STARTING,\n            LifecycleState.RUNNING,\n            AIPerfHook.ON_START,\n            self.started_event,\n        )\n\n    async def initialize_and_start(self) -&gt; None:\n        \"\"\"Initialize and start the lifecycle. This is a convenience method that calls `initialize` and `start` in sequence.\"\"\"\n        await self.initialize()\n        await self.start()\n\n    async def stop(self) -&gt; None:\n        \"\"\"Stop the lifecycle and run the @on_stop hooks.\n\n        NOTE: It is generally discouraged from overriding this method.\n        Instead, use the @on_stop hook to handle your own stopping logic.\n        \"\"\"\n        if self.stop_requested:\n            self.debug(\n                lambda: f\"Ignoring stop request for {self} in state {self.state}\"\n            )\n            return\n\n        self.stop_requested = True\n        await self._execute_state_transition(\n            LifecycleState.STOPPING,\n            LifecycleState.STOPPED,\n            AIPerfHook.ON_STOP,\n            self.stopped_event,\n            reverse=True,  # run the stop hooks in reverse order\n        )\n\n    @on_start\n    async def _start_background_tasks(self) -&gt; None:\n        \"\"\"Start all tasks that are decorated with the @background_task decorator.\"\"\"\n        for hook in self.get_hooks(AIPerfHook.BACKGROUND_TASK):\n            if not isinstance(hook.params, BackgroundTaskParams):\n                raise AttributeError(\n                    f\"Invalid hook parameters for {hook}: {hook.params}. Expected BackgroundTaskParams.\"\n                )\n            self.start_background_task(\n                hook.func,\n                interval=hook.params.interval,\n                immediate=hook.params.immediate,\n                stop_on_error=hook.params.stop_on_error,\n                stop_event=self._stop_requested_event,\n            )\n\n    @on_stop\n    async def _stop_all_tasks(self) -&gt; None:\n        \"\"\"Stop all tasks that are decorated with the @background_task decorator,\n        and any custom ones that were ran using `self.execute_async()`.\n        \"\"\"\n        await self.cancel_all_tasks()\n\n    async def _fail(self, e: Exception) -&gt; None:\n        \"\"\"Set the state to FAILED and raise an asyncio.CancelledError.\n        This is used when the transition from one state to another fails.\n        \"\"\"\n        await self._set_state(LifecycleState.FAILED)\n        self.exception(f\"Failed for {self}: {e}\")\n        self.stop_requested = True\n        self.stopped_event.set()\n        raise asyncio.CancelledError(f\"Failed for {self}: {e}\") from e\n\n    def attach_child_lifecycle(self, child: AIPerfLifecycleProtocol) -&gt; None:\n        \"\"\"Attach a child lifecycle to manage. This child will now have its lifecycle managed and\n        controlled by this lifecycle. Common use cases are having a Service be a parent lifecycle,\n        and having supporting components such as streaming post processors, progress reporters, etc. be children.\n\n        Children will be called in the order they were attached for initialize and start,\n        and in reverse order for stop.\n        \"\"\"\n        if self.state != LifecycleState.CREATED:\n            raise InvalidStateError(\n                f\"Cannot attach child {child} to {self} in state {self.state}. \"\n                \"Please attach children before initializing or starting the lifecycle.\"\n            )\n        self._children.append(child)\n\n    @on_init\n    async def _initialize_children(self) -&gt; None:\n        \"\"\"Initialize all children. This is done via the @on_init hook to ensure that the children\n        initialize along with the parent hooks, and not after the parent hooks, which would cause\n        a race condition.\n        \"\"\"\n        for child in self._children:\n            await child.initialize()\n\n    @on_start\n    async def _start_children(self) -&gt; None:\n        \"\"\"Start all children. This is done via the @on_start hook to ensure that the children\n        start along with the parent hooks, and not after the parent hooks, which would cause\n        a race condition.\n        \"\"\"\n        for child in self._children:\n            await child.start()\n\n    @on_stop\n    async def _stop_children(self) -&gt; None:\n        \"\"\"Stop all children. This is done via the @on_stop hook to ensure that the children\n        are stopped along with the parent hooks, and not after the parent hooks, which would cause\n        a race condition.\n        \"\"\"\n        for child in reversed(self._children):\n            await child.stop()\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__} (id={self.id})\"\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__qualname__} {self.id} (state={self.state})&gt;\"\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.is_running","title":"<code>is_running</code>  <code>property</code>","text":"<p>Whether the lifecycle's current state is LifecycleState.RUNNING.</p>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.stop_requested","title":"<code>stop_requested</code>  <code>property</code> <code>writable</code>","text":"<p>Whether the lifecycle has been requested to stop.</p>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.__init__","title":"<code>__init__(id=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | None</code> <p>The id of the lifecycle. If not provided, a random uuid will be generated.</p> <code>None</code> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>def __init__(self, id: str | None = None, **kwargs) -&gt; None:\n    \"\"\"\n    Args:\n        id: The id of the lifecycle. If not provided, a random uuid will be generated.\n    \"\"\"\n    self.id = id or f\"{self.__class__.__name__}_{uuid.uuid4().hex[:8]}\"\n    self._state = LifecycleState.CREATED\n    self.initialized_event = asyncio.Event()\n    self.started_event = asyncio.Event()\n    self._stop_requested_event = asyncio.Event()\n    self.stopped_event = asyncio.Event()  # set on stop or failure\n    self._children: list[AIPerfLifecycleProtocol] = []\n    if \"logger_name\" not in kwargs:\n        kwargs[\"logger_name\"] = self.id\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.attach_child_lifecycle","title":"<code>attach_child_lifecycle(child)</code>","text":"<p>Attach a child lifecycle to manage. This child will now have its lifecycle managed and controlled by this lifecycle. Common use cases are having a Service be a parent lifecycle, and having supporting components such as streaming post processors, progress reporters, etc. be children.</p> <p>Children will be called in the order they were attached for initialize and start, and in reverse order for stop.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>def attach_child_lifecycle(self, child: AIPerfLifecycleProtocol) -&gt; None:\n    \"\"\"Attach a child lifecycle to manage. This child will now have its lifecycle managed and\n    controlled by this lifecycle. Common use cases are having a Service be a parent lifecycle,\n    and having supporting components such as streaming post processors, progress reporters, etc. be children.\n\n    Children will be called in the order they were attached for initialize and start,\n    and in reverse order for stop.\n    \"\"\"\n    if self.state != LifecycleState.CREATED:\n        raise InvalidStateError(\n            f\"Cannot attach child {child} to {self} in state {self.state}. \"\n            \"Please attach children before initializing or starting the lifecycle.\"\n        )\n    self._children.append(child)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the lifecycle and run the @on_init hooks.</p> <p>NOTE: It is generally discouraged from overriding this method. Instead, use the @on_init hook to handle your own initialization logic.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the lifecycle and run the @on_init hooks.\n\n    NOTE: It is generally discouraged from overriding this method.\n    Instead, use the @on_init hook to handle your own initialization logic.\n    \"\"\"\n    if self.state in (\n        LifecycleState.INITIALIZING,\n        LifecycleState.INITIALIZED,\n        LifecycleState.STARTING,\n        LifecycleState.RUNNING,\n    ):\n        self.debug(\n            lambda: f\"Ignoring initialize request for {self} in state {self.state}\"\n        )\n        return\n\n    if self.state != LifecycleState.CREATED:\n        raise InvalidStateError(\n            f\"Cannot initialize from state {self.state} for {self}\"\n        )\n\n    await self._execute_state_transition(\n        LifecycleState.INITIALIZING,\n        LifecycleState.INITIALIZED,\n        AIPerfHook.ON_INIT,\n        self.initialized_event,\n    )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.initialize_and_start","title":"<code>initialize_and_start()</code>  <code>async</code>","text":"<p>Initialize and start the lifecycle. This is a convenience method that calls <code>initialize</code> and <code>start</code> in sequence.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def initialize_and_start(self) -&gt; None:\n    \"\"\"Initialize and start the lifecycle. This is a convenience method that calls `initialize` and `start` in sequence.\"\"\"\n    await self.initialize()\n    await self.start()\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the lifecycle and run the @on_start hooks.</p> <p>NOTE: It is generally discouraged from overriding this method. Instead, use the @on_start hook to handle your own starting logic.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the lifecycle and run the @on_start hooks.\n\n    NOTE: It is generally discouraged from overriding this method.\n    Instead, use the @on_start hook to handle your own starting logic.\n    \"\"\"\n    if self.state in (\n        LifecycleState.STARTING,\n        LifecycleState.RUNNING,\n    ):\n        self.debug(\n            lambda: f\"Ignoring start request for {self} in state {self.state}\"\n        )\n        return\n\n    if self.state != LifecycleState.INITIALIZED:\n        raise InvalidStateError(f\"Cannot start from state {self.state} for {self}\")\n\n    await self._execute_state_transition(\n        LifecycleState.STARTING,\n        LifecycleState.RUNNING,\n        AIPerfHook.ON_START,\n        self.started_event,\n    )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the lifecycle and run the @on_stop hooks.</p> <p>NOTE: It is generally discouraged from overriding this method. Instead, use the @on_stop hook to handle your own stopping logic.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the lifecycle and run the @on_stop hooks.\n\n    NOTE: It is generally discouraged from overriding this method.\n    Instead, use the @on_stop hook to handle your own stopping logic.\n    \"\"\"\n    if self.stop_requested:\n        self.debug(\n            lambda: f\"Ignoring stop request for {self} in state {self.state}\"\n        )\n        return\n\n    self.stop_requested = True\n    await self._execute_state_transition(\n        LifecycleState.STOPPING,\n        LifecycleState.STOPPED,\n        AIPerfHook.ON_STOP,\n        self.stopped_event,\n        reverse=True,  # run the stop hooks in reverse order\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsaiperf_logger_mixin","title":"aiperf.common.mixins.aiperf_logger_mixin","text":""},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin","title":"<code>AIPerfLoggerMixin</code>","text":"<p>               Bases: <code>BaseMixin</code></p> <p>Mixin to provide lazy evaluated logging for f-strings.</p> <p>This mixin provides a logger with lazy evaluation support for f-strings, and direct log functions for all standard and custom logging levels.</p> <p>see :class:<code>AIPerfLogger</code> for more details.</p> Usage <p>class MyClass(AIPerfLoggerMixin):     def init(self):         super().init()         self.trace(lambda: f\"Processing {item} of {count} ({item / count * 100}% complete)\")         self.info(\"Simple string message\")         self.debug(lambda i=i: f\"Binding loop variable: {i}\")         self.warning(\"Warning message: %s\", \"legacy support\")         self.success(\"Benchmark completed successfully\")         self.notice(\"Warmup has completed\")         self.exception(f\"Direct f-string usage: {e}\")</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>@implements_protocol(AIPerfLoggerProtocol)\nclass AIPerfLoggerMixin(BaseMixin):\n    \"\"\"Mixin to provide lazy evaluated logging for f-strings.\n\n    This mixin provides a logger with lazy evaluation support for f-strings,\n    and direct log functions for all standard and custom logging levels.\n\n    see :class:`AIPerfLogger` for more details.\n\n    Usage:\n        class MyClass(AIPerfLoggerMixin):\n            def __init__(self):\n                super().__init__()\n                self.trace(lambda: f\"Processing {item} of {count} ({item / count * 100}% complete)\")\n                self.info(\"Simple string message\")\n                self.debug(lambda i=i: f\"Binding loop variable: {i}\")\n                self.warning(\"Warning message: %s\", \"legacy support\")\n                self.success(\"Benchmark completed successfully\")\n                self.notice(\"Warmup has completed\")\n                self.exception(f\"Direct f-string usage: {e}\")\n    \"\"\"\n\n    def __init__(self, logger_name: str | None = None, **kwargs) -&gt; None:\n        self.logger = AIPerfLogger(logger_name or self.__class__.__name__)\n        self._log = self.logger._log\n        self.is_enabled_for = self.logger._logger.isEnabledFor\n        super().__init__(**kwargs)\n\n    @property\n    def is_debug_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_DEBUG)\n\n    @property\n    def is_trace_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_TRACE)\n\n    def log(\n        self, level: int, message: str | Callable[..., str], *args, **kwargs\n    ) -&gt; None:\n        \"\"\"Log a message at a specified level with lazy evaluation.\"\"\"\n        if self.is_enabled_for(level):\n            self._log(level, message, *args, **kwargs)\n\n    def trace(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a trace message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_TRACE):\n            self._log(_TRACE, message, *args, **kwargs)\n\n    def debug(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a debug message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_DEBUG):\n            self._log(_DEBUG, message, *args, **kwargs)\n\n    def info(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an info message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_INFO):\n            self._log(_INFO, message, *args, **kwargs)\n\n    def notice(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a notice message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_NOTICE):\n            self._log(_NOTICE, message, *args, **kwargs)\n\n    def warning(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a warning message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_WARNING):\n            self._log(_WARNING, message, *args, **kwargs)\n\n    def success(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a success message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_SUCCESS):\n            self._log(_SUCCESS, message, *args, **kwargs)\n\n    def error(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an error message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, message, *args, **kwargs)\n\n    def exception(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an exception message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, message, *args, exc_info=True, **kwargs)\n\n    def critical(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a critical message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_CRITICAL):\n            self._log(_CRITICAL, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.critical","title":"<code>critical(message, *args, **kwargs)</code>","text":"<p>Log a critical message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def critical(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a critical message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_CRITICAL):\n        self._log(_CRITICAL, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.debug","title":"<code>debug(message, *args, **kwargs)</code>","text":"<p>Log a debug message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def debug(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a debug message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_DEBUG):\n        self._log(_DEBUG, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.error","title":"<code>error(message, *args, **kwargs)</code>","text":"<p>Log an error message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def error(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an error message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.exception","title":"<code>exception(message, *args, **kwargs)</code>","text":"<p>Log an exception message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def exception(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an exception message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, message, *args, exc_info=True, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.info","title":"<code>info(message, *args, **kwargs)</code>","text":"<p>Log an info message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def info(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_INFO):\n        self._log(_INFO, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.log","title":"<code>log(level, message, *args, **kwargs)</code>","text":"<p>Log a message at a specified level with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def log(\n    self, level: int, message: str | Callable[..., str], *args, **kwargs\n) -&gt; None:\n    \"\"\"Log a message at a specified level with lazy evaluation.\"\"\"\n    if self.is_enabled_for(level):\n        self._log(level, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.notice","title":"<code>notice(message, *args, **kwargs)</code>","text":"<p>Log a notice message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def notice(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a notice message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_NOTICE):\n        self._log(_NOTICE, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.success","title":"<code>success(message, *args, **kwargs)</code>","text":"<p>Log a success message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def success(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a success message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_SUCCESS):\n        self._log(_SUCCESS, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.trace","title":"<code>trace(message, *args, **kwargs)</code>","text":"<p>Log a trace message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def trace(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a trace message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_TRACE):\n        self._log(_TRACE, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.warning","title":"<code>warning(message, *args, **kwargs)</code>","text":"<p>Log a warning message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def warning(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a warning message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_WARNING):\n        self._log(_WARNING, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperfcommonmixinsbase_mixin","title":"aiperf.common.mixins.base_mixin","text":""},{"location":"api/#aiperf.common.mixins.base_mixin.BaseMixin","title":"<code>BaseMixin</code>","text":"<p>Base mixin class.</p> <p>This Mixin creates a contract that Mixins should always pass **kwargs to super().init, regardless of whether they extend another mixin or not.</p> <p>This will ensure that the BaseMixin is the last mixin to have its init method called, which means that all other mixins will have a proper chain of init methods with the correct arguments and no accidental broken inheritance.</p> Source code in <code>aiperf/common/mixins/base_mixin.py</code> <pre><code>class BaseMixin:\n    \"\"\"Base mixin class.\n\n    This Mixin creates a contract that Mixins should always pass **kwargs to\n    super().__init__, regardless of whether they extend another mixin or not.\n\n    This will ensure that the BaseMixin is the last mixin to have its __init__\n    method called, which means that all other mixins will have a proper\n    chain of __init__ methods with the correct arguments and no accidental\n    broken inheritance.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        # object.__init__ does not take any arguments\n        super().__init__()\n</code></pre>"},{"location":"api/#aiperfcommonmixinscommunication_mixin","title":"aiperf.common.mixins.communication_mixin","text":""},{"location":"api/#aiperf.common.mixins.communication_mixin.CommunicationMixin","title":"<code>CommunicationMixin</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>Mixin to provide access to a CommunicationProtocol instance. This mixin should be inherited by any mixin that needs access to the communication layer to create Communication clients.</p> Source code in <code>aiperf/common/mixins/communication_mixin.py</code> <pre><code>class CommunicationMixin(AIPerfLifecycleMixin, ABC):\n    \"\"\"Mixin to provide access to a CommunicationProtocol instance. This mixin should be inherited\n    by any mixin that needs access to the communication layer to create Communication clients.\n    \"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        self.service_config = service_config\n        self.comms: CommunicationProtocol = CommunicationFactory.get_or_create_instance(\n            self.service_config.comm_backend,\n            config=self.service_config.comm_config,\n        )\n        self.attach_child_lifecycle(self.comms)\n</code></pre>"},{"location":"api/#aiperfcommonmixinshooks_mixin","title":"aiperf.common.mixins.hooks_mixin","text":""},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin","title":"<code>HooksMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Mixin for a class to be able to provide hooks to its subclasses, and to be able to run them. A \"hook\" is a function that is decorated with a hook type (AIPerfHook), and optional parameters.</p> <p>In order to provide hooks, a class MUST use the <code>@provides_hooks</code> decorator to declare the hook types it provides. Only list hook types that you call <code>get_hooks</code> or <code>run_hooks</code> on, to get or run the functions that are decorated with those hook types.</p> <p>Provided hooks are recursively inherited by subclasses, so if a base class provides a hook, all subclasses will also provide that hook (without having to explicitly declare it, or call <code>get_hooks</code> or <code>run_hooks</code>). In fact, you typically should not get or run hooks from the base class, as this may lead to calling hooks twice.</p> <p>Hooks are registered in the order they are defined within the same class from top to bottom, and each class's hooks are inspected starting with hooks defined in the lowest level of base classes, moving up to the highest subclass.</p> <p>IMPORTANT: - Hook decorated methods from one class can be named the same as methods in their base classes, and BOTH will be registered. Meaning if class A and class B both have a method named <code>_initialize</code>, which is decorated with <code>@on_init</code>, and class B inherits from class A, then both <code>_initialize</code> methods will be registered as hooks, and will be run in the order A._initialize, then B._initialize. This is done without requiring the user to call <code>super()._initialize</code> in the subclass, as the base class hook will be run automatically. However, the caveat is that it is not possible to disable the hook from the base class without extra work, and if the user does accidentally call <code>super()._initialize</code> in the subclass, the base class hook may be called twice.</p> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>@implements_protocol(HooksProtocol)\nclass HooksMixin(AIPerfLoggerMixin):\n    \"\"\"Mixin for a class to be able to provide hooks to its subclasses, and to be able to run them. A \"hook\" is a function\n    that is decorated with a hook type (AIPerfHook), and optional parameters.\n\n    In order to provide hooks, a class MUST use the `@provides_hooks` decorator to declare the hook types it provides.\n    Only list hook types that you call `get_hooks` or `run_hooks` on, to get or run the functions that are decorated\n    with those hook types.\n\n    Provided hooks are recursively inherited by subclasses, so if a base class provides a hook,\n    all subclasses will also provide that hook (without having to explicitly declare it, or call `get_hooks` or `run_hooks`).\n    In fact, you typically should not get or run hooks from the base class, as this may lead to calling hooks twice.\n\n    Hooks are registered in the order they are defined within the same class from top to bottom, and each class's hooks\n    are inspected starting with hooks defined in the lowest level of base classes, moving up to the highest subclass.\n\n    IMPORTANT:\n    - Hook decorated methods from one class can be named the same as methods in their base classes, and BOTH will be registered.\n    Meaning if class A and class B both have a method named `_initialize`, which is decorated with `@on_init`, and class B inherits from class A,\n    then both `_initialize` methods will be registered as hooks, and will be run in the order A._initialize, then B._initialize.\n    This is done without requiring the user to call `super()._initialize` in the subclass, as the base class hook will be run automatically.\n    However, the caveat is that it is not possible to disable the hook from the base class without extra work, and if the user does accidentally\n    call `super()._initialize` in the subclass, the base class hook may be called twice.\n    \"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self._provided_hook_types: set[HookType] = set()\n\n        self._hooks: dict[HookType, list[Hook]] = {}\n        # Go through the MRO in reverse order to ensure that the hooks are\n        # registered in the correct order (base classes first, then subclasses).\n        for cls in reversed(self.__class__.__mro__):\n            if hasattr(cls, HookAttrs.PROVIDES_HOOKS):\n                # As we find base classes that provide hooks, we add them to the\n                # set of provided hook types, which is used for validation.\n                self._provided_hook_types.update(getattr(cls, HookAttrs.PROVIDES_HOOKS))\n\n            # Go through the class's methods to find the hooks.\n            for method in cls.__dict__.values():\n                if not callable(method):\n                    continue\n\n                # If the method has the AIPERF_HOOK_TYPE attribute, it is a hook.\n                if hasattr(method, HookAttrs.HOOK_TYPE):\n                    method_hook_type = getattr(method, HookAttrs.HOOK_TYPE)\n                    # If the hook type is not provided by any base class, it is an error.\n                    # This is to ensure that the hook is only registered if it is provided by a base class.\n                    # This is to avoid the case where a developer accidentally uses a hook that is not provided by a base class.\n                    if method_hook_type not in self._provided_hook_types:\n                        raise UnsupportedHookError(\n                            f\"Hook {method_hook_type} is not provided by any base class of {self.__class__.__name__}. \"\n                            f\"(Provided Hooks: {[f'{hook_type}' for hook_type in self._provided_hook_types]})\"\n                        )\n\n                    # Bind the method to the instance (\"self\"), extract the hook parameters,\n                    # and add it to the hooks dictionary.\n                    bound_method = method.__get__(self)\n                    self._hooks.setdefault(method_hook_type, []).append(\n                        Hook(\n                            func=bound_method,\n                            params=getattr(method, HookAttrs.HOOK_PARAMS, None),\n                        ),\n                    )\n\n        self.debug(\n            lambda: f\"Provided hook types: {self._provided_hook_types} for {self.__class__.__name__}\"\n        )\n\n    def get_hooks(self, *hook_types: HookType, reverse: bool = False) -&gt; list[Hook]:\n        \"\"\"Get the hooks that are defined by the class for the given hook type(s), optionally reversed.\n        This will return a list of Hook objects that can be inspected for their type and parameters,\n        and optionally called.\"\"\"\n        hooks = [\n            hook\n            for hook_type, hooks in self._hooks.items()\n            if not hook_types or hook_type in hook_types\n            for hook in hooks\n        ]\n        if reverse:\n            hooks.reverse()\n        return hooks\n\n    def for_each_hook_param(\n        self,\n        *hook_types: HookType,\n        self_obj: Any,\n        param_type: AnyT,\n        lambda_func: Callable[[Hook, AnyT], None],\n        reverse: bool = False,\n    ) -&gt; None:\n        \"\"\"Iterate over the hooks for the given hook type(s), optionally reversed.\n        If a lambda_func is provided, it will be called for each parameter of the hook,\n        and the hook and parameter will be passed as arguments.\n\n        Args:\n            hook_types: The hook types to iterate over.\n            self_obj: The object to pass to the lambda_func.\n            param_type: The type of the parameter to pass to the lambda_func (for validation).\n            lambda_func: The function to call for each hook.\n            reverse: Whether to iterate over the hooks in reverse order.\n        \"\"\"\n        for hook in self.get_hooks(*hook_types, reverse=reverse):\n            # in case the hook params are a callable, we need to resolve them to get the actual params\n            params = hook.resolve_params(self_obj)\n            if not isinstance(params, Iterable):\n                raise ValueError(\n                    f\"Invalid hook params: {params}. Expected Iterable but got {type(params)}\"\n                )\n            for param in params:\n                self.trace(\n                    lambda param=param,\n                    type=param_type: f\"param: {param}, param_type: {type}\"\n                )\n                if not isinstance(param, param_type):\n                    raise ValueError(\n                        f\"Invalid hook param: {param}. Expected {param_type} but got {type(param)}\"\n                    )\n                # Call the lambda_func for each parameter of each hook.\n                lambda_func(hook, param)\n\n    async def run_hooks(\n        self, *hook_types: HookType, reverse: bool = False, **kwargs\n    ) -&gt; None:\n        \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n        Hooks are run in the order they are defined by the class, starting with hooks defined in the lowest level\n        of base classes, moving up to the top level class. If more than one hook type is provided, the hooks\n        from each level of classes will be run in the order of the hook types provided.\n\n        If reverse is True, the hooks will be run in reverse order. This is useful for stop/cleanup hooks, where you\n        want to start with the children and ending with the parent.\n\n        The kwargs are passed through as keyword arguments to each hook.\n        \"\"\"\n        exceptions: list[Exception] = []\n        for hook in self.get_hooks(*hook_types, reverse=reverse):\n            self.debug(lambda hook=hook: f\"Running hook: {hook!r}\")\n            try:\n                await hook(**kwargs)\n            except Exception as e:\n                exceptions.append(e)\n                self.exception(\n                    f\"Error running {hook!r} hook for {self.__class__.__name__}: {e}\"\n                )\n        if exceptions:\n            raise AIPerfMultiError(\n                f\"Errors running {hook_types} hooks for {self.__class__.__name__}\",\n                exceptions,\n            )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.for_each_hook_param","title":"<code>for_each_hook_param(*hook_types, self_obj, param_type, lambda_func, reverse=False)</code>","text":"<p>Iterate over the hooks for the given hook type(s), optionally reversed. If a lambda_func is provided, it will be called for each parameter of the hook, and the hook and parameter will be passed as arguments.</p> <p>Parameters:</p> Name Type Description Default <code>hook_types</code> <code>HookType</code> <p>The hook types to iterate over.</p> <code>()</code> <code>self_obj</code> <code>Any</code> <p>The object to pass to the lambda_func.</p> required <code>param_type</code> <code>AnyT</code> <p>The type of the parameter to pass to the lambda_func (for validation).</p> required <code>lambda_func</code> <code>Callable[[Hook, AnyT], None]</code> <p>The function to call for each hook.</p> required <code>reverse</code> <code>bool</code> <p>Whether to iterate over the hooks in reverse order.</p> <code>False</code> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>def for_each_hook_param(\n    self,\n    *hook_types: HookType,\n    self_obj: Any,\n    param_type: AnyT,\n    lambda_func: Callable[[Hook, AnyT], None],\n    reverse: bool = False,\n) -&gt; None:\n    \"\"\"Iterate over the hooks for the given hook type(s), optionally reversed.\n    If a lambda_func is provided, it will be called for each parameter of the hook,\n    and the hook and parameter will be passed as arguments.\n\n    Args:\n        hook_types: The hook types to iterate over.\n        self_obj: The object to pass to the lambda_func.\n        param_type: The type of the parameter to pass to the lambda_func (for validation).\n        lambda_func: The function to call for each hook.\n        reverse: Whether to iterate over the hooks in reverse order.\n    \"\"\"\n    for hook in self.get_hooks(*hook_types, reverse=reverse):\n        # in case the hook params are a callable, we need to resolve them to get the actual params\n        params = hook.resolve_params(self_obj)\n        if not isinstance(params, Iterable):\n            raise ValueError(\n                f\"Invalid hook params: {params}. Expected Iterable but got {type(params)}\"\n            )\n        for param in params:\n            self.trace(\n                lambda param=param,\n                type=param_type: f\"param: {param}, param_type: {type}\"\n            )\n            if not isinstance(param, param_type):\n                raise ValueError(\n                    f\"Invalid hook param: {param}. Expected {param_type} but got {type(param)}\"\n                )\n            # Call the lambda_func for each parameter of each hook.\n            lambda_func(hook, param)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.get_hooks","title":"<code>get_hooks(*hook_types, reverse=False)</code>","text":"<p>Get the hooks that are defined by the class for the given hook type(s), optionally reversed. This will return a list of Hook objects that can be inspected for their type and parameters, and optionally called.</p> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>def get_hooks(self, *hook_types: HookType, reverse: bool = False) -&gt; list[Hook]:\n    \"\"\"Get the hooks that are defined by the class for the given hook type(s), optionally reversed.\n    This will return a list of Hook objects that can be inspected for their type and parameters,\n    and optionally called.\"\"\"\n    hooks = [\n        hook\n        for hook_type, hooks in self._hooks.items()\n        if not hook_types or hook_type in hook_types\n        for hook in hooks\n    ]\n    if reverse:\n        hooks.reverse()\n    return hooks\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.run_hooks","title":"<code>run_hooks(*hook_types, reverse=False, **kwargs)</code>  <code>async</code>","text":"<p>Run the hooks for the given hook type, waiting for each hook to complete before running the next one. Hooks are run in the order they are defined by the class, starting with hooks defined in the lowest level of base classes, moving up to the top level class. If more than one hook type is provided, the hooks from each level of classes will be run in the order of the hook types provided.</p> <p>If reverse is True, the hooks will be run in reverse order. This is useful for stop/cleanup hooks, where you want to start with the children and ending with the parent.</p> <p>The kwargs are passed through as keyword arguments to each hook.</p> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>async def run_hooks(\n    self, *hook_types: HookType, reverse: bool = False, **kwargs\n) -&gt; None:\n    \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n    Hooks are run in the order they are defined by the class, starting with hooks defined in the lowest level\n    of base classes, moving up to the top level class. If more than one hook type is provided, the hooks\n    from each level of classes will be run in the order of the hook types provided.\n\n    If reverse is True, the hooks will be run in reverse order. This is useful for stop/cleanup hooks, where you\n    want to start with the children and ending with the parent.\n\n    The kwargs are passed through as keyword arguments to each hook.\n    \"\"\"\n    exceptions: list[Exception] = []\n    for hook in self.get_hooks(*hook_types, reverse=reverse):\n        self.debug(lambda hook=hook: f\"Running hook: {hook!r}\")\n        try:\n            await hook(**kwargs)\n        except Exception as e:\n            exceptions.append(e)\n            self.exception(\n                f\"Error running {hook!r} hook for {self.__class__.__name__}: {e}\"\n            )\n    if exceptions:\n        raise AIPerfMultiError(\n            f\"Errors running {hook_types} hooks for {self.__class__.__name__}\",\n            exceptions,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsmessage_bus_mixin","title":"aiperf.common.mixins.message_bus_mixin","text":""},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin","title":"<code>MessageBusClientMixin</code>","text":"<p>               Bases: <code>CommunicationMixin</code>, <code>ABC</code></p> <p>Mixin to provide message bus clients (pub and sub)for AIPerf components, as well as a hook to handle messages: @on_message.</p> Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_MESSAGE)\n@implements_protocol(MessageBusClientProtocol)\nclass MessageBusClientMixin(CommunicationMixin, ABC):\n    \"\"\"Mixin to provide message bus clients (pub and sub)for AIPerf components, as well as\n    a hook to handle messages: @on_message.\"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        # NOTE: The communication base class will automatically manage the pub/sub clients' lifecycle.\n        self.sub_client = self.comms.create_sub_client(\n            CommAddress.EVENT_BUS_PROXY_BACKEND\n        )\n        self.pub_client = self.comms.create_pub_client(\n            CommAddress.EVENT_BUS_PROXY_FRONTEND\n        )\n\n    @on_init\n    async def _setup_on_message_hooks(self) -&gt; None:\n        \"\"\"Send subscription requests for all @on_message hook decorators.\"\"\"\n        subscription_map: MessageCallbackMapT = {}\n\n        def _add_to_subscription_map(hook: Hook, message_type: MessageTypeT) -&gt; None:\n            \"\"\"\n            This function is called for every message_type parameter of every @on_message hook.\n            We use this to build a map of message types to callbacks, which is then used to call\n            subscribe_all for efficiency\n            \"\"\"\n            self.debug(\n                lambda: f\"Subscribing to message type: '{message_type}' for hook: {hook}\"\n            )\n            subscription_map.setdefault(message_type, []).append(hook.func)\n\n        # For each @on_message hook, add each message type to the subscription map.\n        self.for_each_hook_param(\n            AIPerfHook.ON_MESSAGE,\n            self_obj=self,\n            param_type=MessageTypeT,\n            lambda_func=_add_to_subscription_map,\n        )\n        await self.sub_client.subscribe_all(subscription_map)\n\n    async def subscribe(\n        self,\n        message_type: MessageTypeT,\n        callback: Callable[[Message], Coroutine[Any, Any, None]],\n    ) -&gt; None:\n        \"\"\"Subscribe to a specific message type. The callback will be called when\n        a message is received for the given message type.\"\"\"\n        await self.sub_client.subscribe(message_type, callback)\n\n    async def subscribe_all(\n        self,\n        message_callback_map: MessageCallbackMapT,\n    ) -&gt; None:\n        \"\"\"Subscribe to all message types in the map. The callback(s) will be called when\n        a message is received for the given message type.\n\n        Args:\n            message_callback_map: A map of message types to callbacks. The callbacks can be a single callback or a list of callbacks.\n        \"\"\"\n        await self.sub_client.subscribe_all(message_callback_map)\n\n    async def publish(self, message: Message) -&gt; None:\n        \"\"\"Publish a message. The message will be routed automatically based on the message type.\"\"\"\n        await self.pub_client.publish(message)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin.publish","title":"<code>publish(message)</code>  <code>async</code>","text":"<p>Publish a message. The message will be routed automatically based on the message type.</p> Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>async def publish(self, message: Message) -&gt; None:\n    \"\"\"Publish a message. The message will be routed automatically based on the message type.\"\"\"\n    await self.pub_client.publish(message)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin.subscribe","title":"<code>subscribe(message_type, callback)</code>  <code>async</code>","text":"<p>Subscribe to a specific message type. The callback will be called when a message is received for the given message type.</p> Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>async def subscribe(\n    self,\n    message_type: MessageTypeT,\n    callback: Callable[[Message], Coroutine[Any, Any, None]],\n) -&gt; None:\n    \"\"\"Subscribe to a specific message type. The callback will be called when\n    a message is received for the given message type.\"\"\"\n    await self.sub_client.subscribe(message_type, callback)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin.subscribe_all","title":"<code>subscribe_all(message_callback_map)</code>  <code>async</code>","text":"<p>Subscribe to all message types in the map. The callback(s) will be called when a message is received for the given message type.</p> <p>Parameters:</p> Name Type Description Default <code>message_callback_map</code> <code>MessageCallbackMapT</code> <p>A map of message types to callbacks. The callbacks can be a single callback or a list of callbacks.</p> required Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>async def subscribe_all(\n    self,\n    message_callback_map: MessageCallbackMapT,\n) -&gt; None:\n    \"\"\"Subscribe to all message types in the map. The callback(s) will be called when\n    a message is received for the given message type.\n\n    Args:\n        message_callback_map: A map of message types to callbacks. The callbacks can be a single callback or a list of callbacks.\n    \"\"\"\n    await self.sub_client.subscribe_all(message_callback_map)\n</code></pre>"},{"location":"api/#aiperfcommonmixinsprocess_health_mixin","title":"aiperf.common.mixins.process_health_mixin","text":""},{"location":"api/#aiperf.common.mixins.process_health_mixin.ProcessHealthMixin","title":"<code>ProcessHealthMixin</code>","text":"<p>               Bases: <code>BaseMixin</code></p> <p>Mixin to provide process health information.</p> Source code in <code>aiperf/common/mixins/process_health_mixin.py</code> <pre><code>class ProcessHealthMixin(BaseMixin):\n    \"\"\"Mixin to provide process health information.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Initialize process-specific CPU monitoring\n        self.process: psutil.Process = psutil.Process()\n        self.process.cpu_percent()  # throw away the first result (will be 0)\n        self.create_time: float = self.process.create_time()\n\n        self.process_health: ProcessHealth | None = None\n        self.previous: ProcessHealth | None = None\n\n    def get_process_health(self) -&gt; ProcessHealth:\n        \"\"\"Get the process health information for the current process.\"\"\"\n\n        # Get process-specific CPU and memory usage\n        raw_cpu_times = self.process.cpu_times()\n        cpu_times = CPUTimes(\n            user=raw_cpu_times[0],\n            system=raw_cpu_times[1],\n            iowait=raw_cpu_times[4] if len(raw_cpu_times) &gt; 4 else 0.0,  # type: ignore\n        )\n\n        self.previous = self.process_health\n\n        self.process_health = ProcessHealth(\n            pid=self.process.pid,\n            create_time=self.create_time,\n            uptime=time.time() - self.create_time,\n            cpu_usage=self.process.cpu_percent(),\n            memory_usage=self.process.memory_info().rss / BYTES_PER_MIB,\n            io_counters=self.process.io_counters(),\n            cpu_times=cpu_times,\n            num_ctx_switches=CtxSwitches(*self.process.num_ctx_switches()),\n            num_threads=self.process.num_threads(),\n        )\n        return self.process_health\n</code></pre>"},{"location":"api/#aiperf.common.mixins.process_health_mixin.ProcessHealthMixin.get_process_health","title":"<code>get_process_health()</code>","text":"<p>Get the process health information for the current process.</p> Source code in <code>aiperf/common/mixins/process_health_mixin.py</code> <pre><code>def get_process_health(self) -&gt; ProcessHealth:\n    \"\"\"Get the process health information for the current process.\"\"\"\n\n    # Get process-specific CPU and memory usage\n    raw_cpu_times = self.process.cpu_times()\n    cpu_times = CPUTimes(\n        user=raw_cpu_times[0],\n        system=raw_cpu_times[1],\n        iowait=raw_cpu_times[4] if len(raw_cpu_times) &gt; 4 else 0.0,  # type: ignore\n    )\n\n    self.previous = self.process_health\n\n    self.process_health = ProcessHealth(\n        pid=self.process.pid,\n        create_time=self.create_time,\n        uptime=time.time() - self.create_time,\n        cpu_usage=self.process.cpu_percent(),\n        memory_usage=self.process.memory_info().rss / BYTES_PER_MIB,\n        io_counters=self.process.io_counters(),\n        cpu_times=cpu_times,\n        num_ctx_switches=CtxSwitches(*self.process.num_ctx_switches()),\n        num_threads=self.process.num_threads(),\n    )\n    return self.process_health\n</code></pre>"},{"location":"api/#aiperfcommonmixinspull_client_mixin","title":"aiperf.common.mixins.pull_client_mixin","text":""},{"location":"api/#aiperf.common.mixins.pull_client_mixin.PullClientMixin","title":"<code>PullClientMixin</code>","text":"<p>               Bases: <code>CommunicationMixin</code>, <code>ABC</code></p> <p>Mixin to provide a pull client for AIPerf components using a PullClient for the specified CommAddress. Add the @on_pull_message decorator to specify a function that will be called when a pull is received.</p> <p>NOTE: This currently only supports a single pull client per service, as that is our current use case.</p> Source code in <code>aiperf/common/mixins/pull_client_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_PULL_MESSAGE)\nclass PullClientMixin(CommunicationMixin, ABC):\n    \"\"\"Mixin to provide a pull client for AIPerf components using a PullClient for the specified CommAddress.\n    Add the @on_pull_message decorator to specify a function that will be called when a pull is received.\n\n    NOTE: This currently only supports a single pull client per service, as that is our current use case.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        pull_client_address: CommAddress,\n        pull_client_bind: bool = False,\n        max_pull_concurrency: int | None = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        # NOTE: The communication base class will automatically manage the pull client's lifecycle.\n        self.pull_client = self.comms.create_pull_client(\n            pull_client_address,\n            bind=pull_client_bind,\n            max_pull_concurrency=max_pull_concurrency,\n        )\n\n    @on_init\n    async def _setup_pull_handler_hooks(self) -&gt; None:\n        \"\"\"Configure the pull client to register callbacks for all @on_pull_message hook decorators.\"\"\"\n\n        def _register_pull_callback(hook: Hook, message_type: MessageTypeT) -&gt; None:\n            self.debug(\n                lambda: f\"Registering pull callback for message type: {message_type} for hook: {hook}\"\n            )\n            self.pull_client.register_pull_callback(\n                message_type=message_type,\n                callback=hook.func,\n            )\n\n        # For each @on_pull_message hook, register a pull callback for each specified message type.\n        self.for_each_hook_param(\n            AIPerfHook.ON_PULL_MESSAGE,\n            self_obj=self,\n            param_type=MessageTypeT,\n            lambda_func=_register_pull_callback,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsreply_client_mixin","title":"aiperf.common.mixins.reply_client_mixin","text":""},{"location":"api/#aiperf.common.mixins.reply_client_mixin.ReplyClientMixin","title":"<code>ReplyClientMixin</code>","text":"<p>               Bases: <code>CommunicationMixin</code>, <code>ABC</code></p> <p>Mixin to provide a reply client for AIPerf components using a ReplyClient for the specified CommAddress. Add the @on_request decorator to specify a function that will be called when a request is received.</p> <p>NOTE: This currently only supports a single reply client per service, as that is our current use case.</p> Source code in <code>aiperf/common/mixins/reply_client_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_REQUEST)\nclass ReplyClientMixin(CommunicationMixin, ABC):\n    \"\"\"Mixin to provide a reply client for AIPerf components using a ReplyClient for the specified CommAddress.\n    Add the @on_request decorator to specify a function that will be called when a request is received.\n\n    NOTE: This currently only supports a single reply client per service, as that is our current use case.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        reply_client_address: CommAddress,\n        reply_client_bind: bool = False,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        # NOTE: The communication base class will automatically manage the reply client's lifecycle.\n        self.reply_client = self.comms.create_reply_client(\n            reply_client_address, bind=reply_client_bind\n        )\n\n    @on_init\n    async def _setup_request_handler_hooks(self) -&gt; None:\n        \"\"\"Configure the reply client to handle requests for all @request_handler hook decorators.\"\"\"\n\n        def _register_request_handler(hook: Hook, message_type: MessageTypeT) -&gt; None:\n            self.debug(\n                lambda: f\"Registering request handler for message type: {message_type} for hook: {hook}\"\n            )\n            self.reply_client.register_request_handler(\n                service_id=self.id,\n                message_type=message_type,\n                handler=hook.func,\n            )\n\n        # For each @on_request hook, register a request handler for each message type.\n        self.for_each_hook_param(\n            AIPerfHook.ON_REQUEST,\n            self_obj=self,\n            param_type=MessageTypeT,\n            lambda_func=_register_request_handler,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinstask_manager_mixin","title":"aiperf.common.mixins.task_manager_mixin","text":""},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin","title":"<code>TaskManagerMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Mixin to manage a set of async tasks, and provide background task loop capabilities. Can be used standalone, but it is most useful as part of the :class:<code>AIPerfLifecycleMixin</code> mixin, where the lifecycle methods are automatically integrated with the task manager.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>@implements_protocol(TaskManagerProtocol)\nclass TaskManagerMixin(AIPerfLoggerMixin):\n    \"\"\"Mixin to manage a set of async tasks, and provide background task loop capabilities.\n    Can be used standalone, but it is most useful as part of the :class:`AIPerfLifecycleMixin`\n    mixin, where the lifecycle methods are automatically integrated with the task manager.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.tasks: set[asyncio.Task] = set()\n        super().__init__(**kwargs)\n\n    def execute_async(self, coro: Coroutine) -&gt; asyncio.Task:\n        \"\"\"Create a task from a coroutine and add it to the set of tasks, and return immediately.\n        The task will be automatically cleaned up when it completes.\n        \"\"\"\n        task = asyncio.create_task(coro)\n        self.tasks.add(task)\n        task.add_done_callback(self.tasks.discard)\n        return task\n\n    async def wait_for_tasks(self) -&gt; list[BaseException | None]:\n        \"\"\"Wait for all current tasks to complete.\"\"\"\n        return await asyncio.gather(*list(self.tasks), return_exceptions=True)\n\n    async def cancel_all_tasks(\n        self, timeout: float = TASK_CANCEL_TIMEOUT_SHORT\n    ) -&gt; None:\n        \"\"\"Cancel all tasks in the set and wait for up to timeout seconds for them to complete.\n\n        Args:\n            timeout: The timeout to wait for the tasks to complete.\n        \"\"\"\n        if not self.tasks:\n            return\n\n        task_list = list(self.tasks)\n        for task in task_list:\n            task.cancel()\n\n    def start_background_task(\n        self,\n        method: Callable,\n        interval: float | Callable[[TaskManagerProtocol], float] | None = None,\n        immediate: bool = False,\n        stop_on_error: bool = False,\n        stop_event: asyncio.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Run a task in the background, in a loop until cancelled.\"\"\"\n        self.execute_async(\n            self._background_task_loop(\n                method, interval, immediate, stop_on_error, stop_event\n            )\n        )\n\n    async def _background_task_loop(\n        self,\n        method: Callable,\n        interval: float | Callable[[TaskManagerProtocol], float] | None = None,\n        immediate: bool = False,\n        stop_on_error: bool = False,\n        stop_event: asyncio.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Run a background task in a loop until cancelled.\n\n        Args:\n            method: The method to run as a background task.\n            interval: The interval to run the task in seconds. Can be a callable that returns the interval, and will be called with 'self' as the argument.\n            immediate: If True, run the task immediately on start, otherwise wait for the interval first.\n            stop_on_error: If True, stop the task on any exception, otherwise log and continue.\n        \"\"\"\n        while stop_event is None or not stop_event.is_set():\n            try:\n                if interval is None or immediate:\n                    await yield_to_event_loop()\n                    # Reset immediate flag for next iteration otherwise we will not sleep\n                    immediate = False\n                else:\n                    sleep_time = interval(self) if callable(interval) else interval\n                    await asyncio.sleep(sleep_time)\n\n                if inspect.iscoroutinefunction(method):\n                    await method()\n                else:\n                    await asyncio.to_thread(method)\n\n                if interval is None:\n                    break\n            except asyncio.CancelledError:\n                self.debug(f\"Background task {method.__name__} cancelled\")\n                break\n            except Exception as e:\n                self.exception(f\"Error in background task {method.__name__}: {e}\")\n                if stop_on_error:\n                    self.exception(\n                        f\"Background task {method.__name__} stopped due to error\"\n                    )\n                    break\n                # Give some time to recover, just in case\n                await asyncio.sleep(0.001)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.cancel_all_tasks","title":"<code>cancel_all_tasks(timeout=TASK_CANCEL_TIMEOUT_SHORT)</code>  <code>async</code>","text":"<p>Cancel all tasks in the set and wait for up to timeout seconds for them to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>The timeout to wait for the tasks to complete.</p> <code>TASK_CANCEL_TIMEOUT_SHORT</code> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>async def cancel_all_tasks(\n    self, timeout: float = TASK_CANCEL_TIMEOUT_SHORT\n) -&gt; None:\n    \"\"\"Cancel all tasks in the set and wait for up to timeout seconds for them to complete.\n\n    Args:\n        timeout: The timeout to wait for the tasks to complete.\n    \"\"\"\n    if not self.tasks:\n        return\n\n    task_list = list(self.tasks)\n    for task in task_list:\n        task.cancel()\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.execute_async","title":"<code>execute_async(coro)</code>","text":"<p>Create a task from a coroutine and add it to the set of tasks, and return immediately. The task will be automatically cleaned up when it completes.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>def execute_async(self, coro: Coroutine) -&gt; asyncio.Task:\n    \"\"\"Create a task from a coroutine and add it to the set of tasks, and return immediately.\n    The task will be automatically cleaned up when it completes.\n    \"\"\"\n    task = asyncio.create_task(coro)\n    self.tasks.add(task)\n    task.add_done_callback(self.tasks.discard)\n    return task\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.start_background_task","title":"<code>start_background_task(method, interval=None, immediate=False, stop_on_error=False, stop_event=None)</code>","text":"<p>Run a task in the background, in a loop until cancelled.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>def start_background_task(\n    self,\n    method: Callable,\n    interval: float | Callable[[TaskManagerProtocol], float] | None = None,\n    immediate: bool = False,\n    stop_on_error: bool = False,\n    stop_event: asyncio.Event | None = None,\n) -&gt; None:\n    \"\"\"Run a task in the background, in a loop until cancelled.\"\"\"\n    self.execute_async(\n        self._background_task_loop(\n            method, interval, immediate, stop_on_error, stop_event\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.wait_for_tasks","title":"<code>wait_for_tasks()</code>  <code>async</code>","text":"<p>Wait for all current tasks to complete.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>async def wait_for_tasks(self) -&gt; list[BaseException | None]:\n    \"\"\"Wait for all current tasks to complete.\"\"\"\n    return await asyncio.gather(*list(self.tasks), return_exceptions=True)\n</code></pre>"},{"location":"api/#aiperfcommonmodelsbase_models","title":"aiperf.common.models.base_models","text":""},{"location":"api/#aiperf.common.models.base_models.AIPerfBaseModel","title":"<code>AIPerfBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for all AIPerf Pydantic models. This class is configured to allow arbitrary types to be used as fields as to allow for more flexible model definitions by end users without breaking the existing code.</p> <p>The @exclude_if_none decorator can also be used to specify which fields should be excluded from the serialized model if they are None. This is a workaround for the fact that pydantic does not support specifying exclude_none on a per-field basis.</p> Source code in <code>aiperf/common/models/base_models.py</code> <pre><code>class AIPerfBaseModel(BaseModel):\n    \"\"\"Base model for all AIPerf Pydantic models. This class is configured to allow\n    arbitrary types to be used as fields as to allow for more flexible model definitions\n    by end users without breaking the existing code.\n\n    The @exclude_if_none decorator can also be used to specify which fields\n    should be excluded from the serialized model if they are None. This is a workaround\n    for the fact that pydantic does not support specifying exclude_none on a per-field basis.\n    \"\"\"\n\n    _exclude_if_none_fields: ClassVar[set[str]] = set()\n    \"\"\"Set of field names that should be excluded from the serialized model if they\n    are None. This is set by the @exclude_if_none decorator.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_serializer\n    def _serialize_model(self) -&gt; dict[str, Any]:\n        \"\"\"Serialize the model to a dictionary.\n\n        This method overrides the default serializer to exclude fields that with a\n        value of None and were marked with the @exclude_if_none decorator.\n        \"\"\"\n        return {\n            k: v\n            for k, v in self\n            if not (k in self._exclude_if_none_fields and v is None)\n        }\n</code></pre>"},{"location":"api/#aiperf.common.models.base_models.exclude_if_none","title":"<code>exclude_if_none(*field_names)</code>","text":"<p>Decorator to set the _exclude_if_none_fields class attribute to the set of field names that should be excluded if they are None.</p> Source code in <code>aiperf/common/models/base_models.py</code> <pre><code>def exclude_if_none(*field_names: str):\n    \"\"\"Decorator to set the _exclude_if_none_fields class attribute to the set of\n    field names that should be excluded if they are None.\n    \"\"\"\n\n    def decorator(model: type[AIPerfBaseModelT]) -&gt; type[AIPerfBaseModelT]:\n        # This attribute is defined by the AIPerfBaseModel class.\n        if not hasattr(model, \"_exclude_if_none_fields\"):\n            model._exclude_if_none_fields = set()\n        model._exclude_if_none_fields.update(set(field_names))\n        return model\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperfcommonmodelscredit_models","title":"aiperf.common.models.credit_models","text":""},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseConfig","title":"<code>CreditPhaseConfig</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Model for phase credit config. This is used by the TimingManager to configure the credit phases.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>class CreditPhaseConfig(AIPerfBaseModel):\n    \"\"\"Model for phase credit config. This is used by the TimingManager to configure the credit phases.\"\"\"\n\n    type: CreditPhase = Field(..., description=\"The type of credit phase\")\n    total_expected_requests: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The total number of expected credits. If None, the phase is not request count based.\",\n    )\n    expected_duration_sec: float | None = Field(\n        default=None,\n        ge=1,\n        description=\"The expected duration of the credit phase in seconds. If None, the phase is not time based.\",\n    )\n\n    @property\n    def is_time_based(self) -&gt; bool:\n        return self.expected_duration_sec is not None\n\n    @property\n    def is_request_count_based(self) -&gt; bool:\n        return self.total_expected_requests is not None\n\n    @property\n    def is_valid(self) -&gt; bool:\n        \"\"\"A phase config is valid if it is exactly one of the following:\n        - is_time_based (expected_duration_sec is set and &gt; 0)\n        - is_request_count_based (total_expected_requests is set and &gt; 0)\n        \"\"\"\n        is_time_based = self.is_time_based\n        is_request_count_based = self.is_request_count_based\n        return (is_time_based and not is_request_count_based) or (\n            not is_time_based and is_request_count_based\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseConfig.is_valid","title":"<code>is_valid</code>  <code>property</code>","text":"<p>A phase config is valid if it is exactly one of the following: - is_time_based (expected_duration_sec is set and &gt; 0) - is_request_count_based (total_expected_requests is set and &gt; 0)</p>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats","title":"<code>CreditPhaseStats</code>","text":"<p>               Bases: <code>CreditPhaseConfig</code></p> <p>Model for phase credit stats. Extends the CreditPhaseConfig fields to track the progress of the credit phases. How many credits were dropped and how many were returned, as well as the progress percentage of the phase.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>class CreditPhaseStats(CreditPhaseConfig):\n    \"\"\"Model for phase credit stats. Extends the CreditPhaseConfig fields to track the progress of the credit phases.\n    How many credits were dropped and how many were returned, as well as the progress percentage of the phase.\"\"\"\n\n    start_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the credit phase in nanoseconds.\",\n    )\n    sent_end_ns: int | None = Field(\n        default=None,\n        description=\"The time of the last sent credit in nanoseconds. If None, the phase has not sent all credits.\",\n    )\n    end_ns: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The time in which the last credit was returned from the workers in nanoseconds. If None, the phase has not completed.\",\n    )\n    sent: int = Field(default=0, description=\"The number of sent credits\")\n    completed: int = Field(\n        default=0,\n        description=\"The number of completed credits (returned from the workers)\",\n    )\n\n    @property\n    def is_sending_complete(self) -&gt; bool:\n        return self.sent_end_ns is not None\n\n    @property\n    def is_complete(self) -&gt; bool:\n        return self.is_sending_complete and self.end_ns is not None\n\n    @property\n    def is_started(self) -&gt; bool:\n        return self.start_ns is not None\n\n    @property\n    def in_flight(self) -&gt; int:\n        \"\"\"Calculate the number of in-flight credits (sent but not completed).\"\"\"\n        return self.sent - self.completed\n\n    @property\n    def should_send(self) -&gt; bool:\n        \"\"\"Whether the phase should send more credits.\"\"\"\n        if self.is_time_based:\n            return (\n                time.time_ns() - (self.start_ns or 0)\n                &lt;= (self.expected_duration_sec * NANOS_PER_SECOND)  # type: ignore\n            )\n        elif self.is_request_count_based:\n            return self.sent &lt; self.total_expected_requests  # type: ignore\n        raise InvalidStateError(\"Phase is not time or request count based\")\n\n    @property\n    def progress_percent(self) -&gt; float | None:\n        if self.start_ns is None:\n            return None\n\n        if self.is_complete:\n            return 100\n\n        if self.is_time_based:\n            # Time based, so progress is the percentage of time elapsed compared to the duration\n\n            return (\n                (time.time_ns() - self.start_ns)\n                / (self.expected_duration_sec * NANOS_PER_SECOND)  # type: ignore\n            ) * 100\n\n        elif self.total_expected_requests is not None:\n            # Credit count based, so progress is the percentage of credits returned\n            return (self.completed / self.total_expected_requests) * 100\n\n        # We don't know the progress\n        return None\n\n    @classmethod\n    def from_phase_config(cls, phase_config: CreditPhaseConfig) -&gt; \"CreditPhaseStats\":\n        \"\"\"Create a CreditPhaseStats from a CreditPhaseConfig. This is used to initialize the stats for a phase.\"\"\"\n        return cls(\n            type=phase_config.type,\n            total_expected_requests=phase_config.total_expected_requests,\n            expected_duration_sec=phase_config.expected_duration_sec,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats.in_flight","title":"<code>in_flight</code>  <code>property</code>","text":"<p>Calculate the number of in-flight credits (sent but not completed).</p>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats.should_send","title":"<code>should_send</code>  <code>property</code>","text":"<p>Whether the phase should send more credits.</p>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats.from_phase_config","title":"<code>from_phase_config(phase_config)</code>  <code>classmethod</code>","text":"<p>Create a CreditPhaseStats from a CreditPhaseConfig. This is used to initialize the stats for a phase.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>@classmethod\ndef from_phase_config(cls, phase_config: CreditPhaseConfig) -&gt; \"CreditPhaseStats\":\n    \"\"\"Create a CreditPhaseStats from a CreditPhaseConfig. This is used to initialize the stats for a phase.\"\"\"\n    return cls(\n        type=phase_config.type,\n        total_expected_requests=phase_config.total_expected_requests,\n        expected_duration_sec=phase_config.expected_duration_sec,\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.PhaseProcessingStats","title":"<code>PhaseProcessingStats</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Model for phase processing stats. How many requests were processed and how many errors were encountered.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>class PhaseProcessingStats(AIPerfBaseModel):\n    \"\"\"Model for phase processing stats. How many requests were processed and\n    how many errors were encountered.\"\"\"\n\n    processed: int = Field(\n        default=0, description=\"The number of records processed successfully\"\n    )\n    errors: int = Field(\n        default=0, description=\"The number of record errors encountered\"\n    )\n    total_expected_requests: int | None = Field(\n        default=None,\n        description=\"The total number of expected requests to process. If None, the phase is not request count based.\",\n    )\n\n    @property\n    def total_records(self) -&gt; int:\n        \"\"\"The total number of records processed successfully or in error.\"\"\"\n        return self.processed + self.errors\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.PhaseProcessingStats.total_records","title":"<code>total_records</code>  <code>property</code>","text":"<p>The total number of records processed successfully or in error.</p>"},{"location":"api/#aiperfcommonmodelsdataset_models","title":"aiperf.common.models.dataset_models","text":""},{"location":"api/#aiperf.common.models.dataset_models.Conversation","title":"<code>Conversation</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>A dataset representation of a full conversation.</p> <p>A conversation is a sequence of turns between a user and an endpoint, and it contains the session ID and all the turns that consists the conversation.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>class Conversation(AIPerfBaseModel):\n    \"\"\"A dataset representation of a full conversation.\n\n    A conversation is a sequence of turns between a user and an endpoint,\n    and it contains the session ID and all the turns that consists the conversation.\n    \"\"\"\n\n    turns: list[Turn] = Field(\n        default=[], description=\"List of turns in the conversation.\"\n    )\n    session_id: str = Field(default=\"\", description=\"Session ID of the conversation.\")\n</code></pre>"},{"location":"api/#aiperf.common.models.dataset_models.Turn","title":"<code>Turn</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>A dataset representation of a single turn within a conversation.</p> <p>A turn is a single interaction between a user and an AI assistant, and it contains timestamp, delay, and raw data that user sends in each turn.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>@exclude_if_none(\"role\")\nclass Turn(AIPerfBaseModel):\n    \"\"\"A dataset representation of a single turn within a conversation.\n\n    A turn is a single interaction between a user and an AI assistant,\n    and it contains timestamp, delay, and raw data that user sends in each turn.\n    \"\"\"\n\n    timestamp: int | None = Field(\n        default=None, description=\"Timestamp of the turn in milliseconds.\"\n    )\n    delay: int | None = Field(\n        default=None,\n        description=\"Amount of milliseconds to wait before sending the turn.\",\n    )\n    role: str | None = Field(default=None, description=\"Role of the turn.\")\n    texts: list[Text] = Field(\n        default=[], description=\"Collection of text data in each turn.\"\n    )\n    images: list[Image] = Field(\n        default=[], description=\"Collection of image data in each turn.\"\n    )\n    audios: list[Audio] = Field(\n        default=[], description=\"Collection of audio data in each turn.\"\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelserror_models","title":"aiperf.common.models.error_models","text":""},{"location":"api/#aiperf.common.models.error_models.ErrorDetails","title":"<code>ErrorDetails</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Encapsulates details about an error.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>class ErrorDetails(AIPerfBaseModel):\n    \"\"\"Encapsulates details about an error.\"\"\"\n\n    code: int | None = Field(\n        default=None,\n        description=\"The error code.\",\n    )\n    type: str | None = Field(\n        default=None,\n        description=\"The error type.\",\n    )\n    message: str = Field(\n        ...,\n        description=\"The error message.\",\n    )\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Check if the error details are equal by comparing the code, type, and message.\"\"\"\n        if not isinstance(other, ErrorDetails):\n            return False\n        return (\n            self.code == other.code\n            and self.type == other.type\n            and self.message == other.message\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash the error details by hashing the code, type, and message.\"\"\"\n        return hash((self.code, self.type, self.message))\n\n    @classmethod\n    def from_exception(cls, e: Exception) -&gt; \"ErrorDetails\":\n        \"\"\"Create an error details object from an exception.\"\"\"\n        return cls(\n            type=e.__class__.__name__,\n            message=str(e),\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetails.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check if the error details are equal by comparing the code, type, and message.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check if the error details are equal by comparing the code, type, and message.\"\"\"\n    if not isinstance(other, ErrorDetails):\n        return False\n    return (\n        self.code == other.code\n        and self.type == other.type\n        and self.message == other.message\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetails.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash the error details by hashing the code, type, and message.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash the error details by hashing the code, type, and message.\"\"\"\n    return hash((self.code, self.type, self.message))\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetails.from_exception","title":"<code>from_exception(e)</code>  <code>classmethod</code>","text":"<p>Create an error details object from an exception.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>@classmethod\ndef from_exception(cls, e: Exception) -&gt; \"ErrorDetails\":\n    \"\"\"Create an error details object from an exception.\"\"\"\n    return cls(\n        type=e.__class__.__name__,\n        message=str(e),\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetailsCount","title":"<code>ErrorDetailsCount</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Count of error details.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>class ErrorDetailsCount(AIPerfBaseModel):\n    \"\"\"Count of error details.\"\"\"\n\n    error_details: ErrorDetails\n    count: int = Field(\n        ...,\n        description=\"The count of the error details.\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelshealth_models","title":"aiperf.common.models.health_models","text":""},{"location":"api/#aiperf.common.models.health_models.ProcessHealth","title":"<code>ProcessHealth</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Model for process health data.</p> Source code in <code>aiperf/common/models/health_models.py</code> <pre><code>class ProcessHealth(AIPerfBaseModel):\n    \"\"\"Model for process health data.\"\"\"\n\n    pid: int | None = Field(\n        default=None,\n        description=\"The PID of the process\",\n    )\n    create_time: float = Field(\n        ..., description=\"The creation time of the process in seconds\"\n    )\n    uptime: float = Field(..., description=\"The uptime of the process in seconds\")\n    cpu_usage: float = Field(\n        ..., description=\"The current CPU usage of the process in %\"\n    )\n    memory_usage: float = Field(\n        ..., description=\"The current memory usage of the process in MiB (rss)\"\n    )\n    io_counters: IOCounters | tuple | None = Field(\n        default=None,\n        description=\"The current I/O counters of the process (read_count, write_count, read_bytes, write_bytes, read_chars, write_chars)\",\n    )\n    cpu_times: CPUTimes | tuple | None = Field(\n        default=None,\n        description=\"The current CPU times of the process (user, system, iowait)\",\n    )\n    num_ctx_switches: CtxSwitches | tuple | None = Field(\n        default=None,\n        description=\"The current number of context switches (voluntary, involuntary)\",\n    )\n    num_threads: int | None = Field(\n        default=None,\n        description=\"The current number of threads\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsrecord_models","title":"aiperf.common.models.record_models","text":""},{"location":"api/#aiperf.common.models.record_models.InferenceServerResponse","title":"<code>InferenceServerResponse</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Response from a inference client.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class InferenceServerResponse(AIPerfBaseModel):\n    \"\"\"Response from a inference client.\"\"\"\n\n    perf_ns: int = Field(\n        ...,\n        description=\"The timestamp of the response in nanoseconds (perf_counter_ns).\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.MetricResult","title":"<code>MetricResult</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>The result values of a single metric.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class MetricResult(AIPerfBaseModel):\n    \"\"\"The result values of a single metric.\"\"\"\n\n    tag: str = Field(description=\"The unique identifier of the metric\")\n    unit: str = Field(description=\"The unit of the metric, e.g. 'ms'\")\n    header: str = Field(\n        description=\"The user friendly name of the metric (e.g. 'Inter Token Latency')\"\n    )\n    avg: float | None = None\n    min: float | None = None\n    max: float | None = None\n    p1: float | None = None\n    p5: float | None = None\n    p25: float | None = None\n    p50: float | None = None\n    p75: float | None = None\n    p90: float | None = None\n    p95: float | None = None\n    p99: float | None = None\n    std: float | None = None\n    count: int | None = Field(\n        default=None,\n        description=\"The total number of records used to calculate the metric\",\n    )\n    streaming_only: bool = Field(\n        default=False,\n        description=\"Whether the metric only applies when streaming is enabled\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord","title":"<code>ParsedResponseRecord</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Record of a request and its associated responses, already parsed and ready for metrics.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ParsedResponseRecord(AIPerfBaseModel):\n    \"\"\"Record of a request and its associated responses, already parsed and ready for metrics.\"\"\"\n\n    worker_id: str = Field(\n        description=\"The ID of the worker that processed the request.\"\n    )\n    request: RequestRecord = Field(description=\"The original request record\")\n    responses: list[ResponseData] = Field(description=\"The parsed response data.\")\n    input_token_count: int | None = Field(\n        default=None,\n        description=\"The number of tokens in the input. If None, the number of tokens could not be calculated.\",\n    )\n    output_token_count: int | None = Field(\n        default=None,\n        description=\"The number of tokens across all responses. If None, the number of tokens could not be calculated.\",\n    )\n\n    @cached_property\n    def start_perf_ns(self) -&gt; int:\n        \"\"\"Get the start time of the request in nanoseconds (perf_counter_ns).\"\"\"\n        return self.request.start_perf_ns\n\n    @cached_property\n    def timestamp_ns(self) -&gt; int:\n        \"\"\"Get the wall clock timestamp of the request in nanoseconds. DO NOT USE FOR LATENCY CALCULATIONS. (time.time_ns).\"\"\"\n        return self.request.timestamp_ns\n\n    # TODO: How do we differentiate the end of the request vs the time of the last response?\n    #       Which one should we use for the latency metrics?\n    @cached_property\n    def end_perf_ns(self) -&gt; int:\n        \"\"\"Get the end time of the request in nanoseconds (perf_counter_ns).\n        If request.end_perf_ns is not set, use the time of the last response.\n        If there are no responses, use sys.maxsize.\n        \"\"\"\n        return (\n            self.request.end_perf_ns\n            if self.request.end_perf_ns\n            else self.responses[-1].perf_ns\n            if self.responses\n            else sys.maxsize\n        )\n\n    @cached_property\n    def request_duration_ns(self) -&gt; int:\n        \"\"\"Get the duration of the request in nanoseconds.\"\"\"\n        return self.end_perf_ns - self.start_perf_ns\n\n    @cached_property\n    def tokens_per_second(self) -&gt; float | None:\n        \"\"\"Get the number of tokens per second of the request.\"\"\"\n        if self.output_token_count is None or self.request_duration_ns == 0:\n            return None\n        return self.output_token_count / (self.request_duration_ns / NANOS_PER_SECOND)\n\n    @cached_property\n    def has_error(self) -&gt; bool:\n        \"\"\"Check if the response record has an error.\"\"\"\n        return self.request.has_error\n\n    @cached_property\n    def valid(self) -&gt; bool:\n        \"\"\"Check if the response record is valid.\n\n        Checks:\n        - Request has no errors\n        - Has at least one response\n        - Start time is before the end time\n        - Response timestamps are within valid ranges\n\n        Returns:\n            bool: True if the record is valid, False otherwise.\n        \"\"\"\n        return (\n            not self.has_error\n            and len(self.responses) &gt; 0\n            and 0 &lt;= self.start_perf_ns &lt; self.end_perf_ns &lt; sys.maxsize\n            and all(0 &lt; response.perf_ns &lt; sys.maxsize for response in self.responses)\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.end_perf_ns","title":"<code>end_perf_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the end time of the request in nanoseconds (perf_counter_ns). If request.end_perf_ns is not set, use the time of the last response. If there are no responses, use sys.maxsize.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.has_error","title":"<code>has_error</code>  <code>cached</code> <code>property</code>","text":"<p>Check if the response record has an error.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.request_duration_ns","title":"<code>request_duration_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the duration of the request in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.start_perf_ns","title":"<code>start_perf_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the start time of the request in nanoseconds (perf_counter_ns).</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.timestamp_ns","title":"<code>timestamp_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the wall clock timestamp of the request in nanoseconds. DO NOT USE FOR LATENCY CALCULATIONS. (time.time_ns).</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.tokens_per_second","title":"<code>tokens_per_second</code>  <code>cached</code> <code>property</code>","text":"<p>Get the number of tokens per second of the request.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.valid","title":"<code>valid</code>  <code>cached</code> <code>property</code>","text":"<p>Check if the response record is valid.</p> <p>Checks: - Request has no errors - Has at least one response - Start time is before the end time - Response timestamps are within valid ranges</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the record is valid, False otherwise.</p>"},{"location":"api/#aiperf.common.models.record_models.ProcessRecordsResult","title":"<code>ProcessRecordsResult</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Result of the process records command.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ProcessRecordsResult(AIPerfBaseModel):\n    \"\"\"Result of the process records command.\"\"\"\n\n    records: list[ProfileResults] | None = Field(\n        default=None,\n        description=\"The records of the profile results\",\n    )\n    errors: list[ErrorDetails] | None = Field(\n        default=None,\n        description=\"The errors of the profile results\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord","title":"<code>RequestRecord</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Record of a request with its associated responses.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class RequestRecord(AIPerfBaseModel):\n    \"\"\"Record of a request with its associated responses.\"\"\"\n\n    request: Any | None = Field(\n        default=None,\n        description=\"The request payload formatted for the inference API.\",\n    )\n    conversation_id: str | None = Field(\n        default=None,\n        description=\"The ID of the conversation (if applicable).\",\n    )\n    turn_index: int | None = Field(\n        default=None,\n        ge=0,\n        description=\"The index of the turn in the conversation (if applicable).\",\n    )\n    model_name: str | None = Field(\n        default=None,\n        description=\"The name of the model targeted by the request.\",\n    )\n    timestamp_ns: int = Field(\n        default_factory=time.time_ns,\n        description=\"The wall clock timestamp of the request in nanoseconds. DO NOT USE FOR LATENCY CALCULATIONS. (time.time_ns).\",\n    )\n    start_perf_ns: int = Field(\n        default_factory=time.perf_counter_ns,\n        description=\"The start reference time of the request in nanoseconds used for latency calculations (perf_counter_ns).\",\n    )\n    end_perf_ns: int | None = Field(\n        default=None,\n        description=\"The end time of the request in nanoseconds (perf_counter_ns).\",\n    )\n    recv_start_perf_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the streaming response in nanoseconds (perf_counter_ns).\",\n    )\n    status: int | None = Field(\n        default=None,\n        description=\"The HTTP status code of the response.\",\n    )\n    # NOTE: We need to use SerializeAsAny to allow for generic subclass support\n    # NOTE: The order of the types is important, as that is the order they are type checked.\n    #       Start with the most specific types and work towards the most general types.\n    responses: SerializeAsAny[\n        list[SSEMessage | TextResponse | InferenceServerResponse | Any]\n    ] = Field(\n        default_factory=list,\n        description=\"The raw responses received from the request.\",\n    )\n    error: ErrorDetails | None = Field(\n        default=None,\n        description=\"The error details if the request failed.\",\n    )\n    delayed_ns: int | None = Field(\n        default=None,\n        ge=0,\n        description=\"The number of nanoseconds the request was delayed from when it was expected to be sent, \"\n        \"or None if the request was sent on time, or did not have a credit_drop_ns timestamp.\",\n    )\n    credit_phase: CreditPhase = Field(\n        default=CreditPhase.PROFILING,\n        description=\"The type of credit phase (either warmup or profiling)\",\n    )\n\n    @property\n    def delayed(self) -&gt; bool:\n        \"\"\"Check if the request was delayed.\"\"\"\n        return self.delayed_ns is not None and self.delayed_ns &gt; 0\n\n    # TODO: Most of these properties will be removed once we have proper record handling and metrics.\n\n    @property\n    def has_error(self) -&gt; bool:\n        \"\"\"Check if the request record has an error.\"\"\"\n        return self.error is not None\n\n    @property\n    def valid(self) -&gt; bool:\n        \"\"\"Check if the request record is valid by ensuring that the start time\n        and response timestamps are within valid ranges.\n\n        Returns:\n            bool: True if the record is valid, False otherwise.\n        \"\"\"\n        return not self.has_error and (\n            0 &lt;= self.start_perf_ns &lt; sys.maxsize\n            and len(self.responses) &gt; 0\n            and all(0 &lt; response.perf_ns &lt; sys.maxsize for response in self.responses)\n        )\n\n    @property\n    def time_to_first_response_ns(self) -&gt; int | None:\n        \"\"\"Get the time to the first response in nanoseconds.\"\"\"\n        if not self.valid:\n            return None\n        return (\n            self.responses[0].perf_ns - self.start_perf_ns\n            if self.start_perf_ns\n            else None\n        )\n\n    @property\n    def time_to_second_response_ns(self) -&gt; int | None:\n        \"\"\"Get the time to the second response in nanoseconds.\"\"\"\n        if not self.valid or len(self.responses) &lt; 2:\n            return None\n        return (\n            self.responses[1].perf_ns - self.responses[0].perf_ns\n            if self.responses[1].perf_ns and self.responses[0].perf_ns\n            else None\n        )\n\n    @property\n    def time_to_last_response_ns(self) -&gt; int | None:\n        \"\"\"Get the time to the last response in nanoseconds.\"\"\"\n        if not self.valid:\n            return None\n        if self.end_perf_ns is None or self.start_perf_ns is None:\n            return None\n        return self.end_perf_ns - self.start_perf_ns if self.start_perf_ns else None\n\n    @property\n    def inter_token_latency_ns(self) -&gt; float | None:\n        \"\"\"Get the interval between responses in nanoseconds.\"\"\"\n        if not self.valid or len(self.responses) &lt; 2:\n            return None\n\n        if (\n            isinstance(self.responses[-1], SSEMessage)\n            and self.responses[-1].packets[-1].value == \"[DONE]\"\n        ):\n            return (\n                (self.responses[-2].perf_ns - self.responses[0].perf_ns)\n                / (len(self.responses) - 2)\n                if self.responses[-2].perf_ns and self.responses[0].perf_ns\n                else None\n            )\n\n        return (\n            (self.responses[-1].perf_ns - self.responses[0].perf_ns)\n            / (len(self.responses) - 1)\n            if self.responses[-1].perf_ns and self.responses[0].perf_ns\n            else None\n        )\n\n    def token_latency_ns(self, index: int) -&gt; float | None:\n        \"\"\"Get the latency of a token in nanoseconds.\"\"\"\n        if not self.valid or len(self.responses) &lt; 1:\n            return None\n        if index == 0:\n            return (\n                self.responses[0].perf_ns - self.recv_start_perf_ns\n                if self.recv_start_perf_ns\n                else None\n            )\n        return (\n            self.responses[index].perf_ns - self.responses[index - 1].perf_ns\n            if self.responses[index].perf_ns and self.responses[index - 1].perf_ns\n            else None\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.delayed","title":"<code>delayed</code>  <code>property</code>","text":"<p>Check if the request was delayed.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.has_error","title":"<code>has_error</code>  <code>property</code>","text":"<p>Check if the request record has an error.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.inter_token_latency_ns","title":"<code>inter_token_latency_ns</code>  <code>property</code>","text":"<p>Get the interval between responses in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.time_to_first_response_ns","title":"<code>time_to_first_response_ns</code>  <code>property</code>","text":"<p>Get the time to the first response in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.time_to_last_response_ns","title":"<code>time_to_last_response_ns</code>  <code>property</code>","text":"<p>Get the time to the last response in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.time_to_second_response_ns","title":"<code>time_to_second_response_ns</code>  <code>property</code>","text":"<p>Get the time to the second response in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.valid","title":"<code>valid</code>  <code>property</code>","text":"<p>Check if the request record is valid by ensuring that the start time and response timestamps are within valid ranges.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the record is valid, False otherwise.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.token_latency_ns","title":"<code>token_latency_ns(index)</code>","text":"<p>Get the latency of a token in nanoseconds.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def token_latency_ns(self, index: int) -&gt; float | None:\n    \"\"\"Get the latency of a token in nanoseconds.\"\"\"\n    if not self.valid or len(self.responses) &lt; 1:\n        return None\n    if index == 0:\n        return (\n            self.responses[0].perf_ns - self.recv_start_perf_ns\n            if self.recv_start_perf_ns\n            else None\n        )\n    return (\n        self.responses[index].perf_ns - self.responses[index - 1].perf_ns\n        if self.responses[index].perf_ns and self.responses[index - 1].perf_ns\n        else None\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ResponseData","title":"<code>ResponseData</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base class for all response data.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ResponseData(AIPerfBaseModel):\n    \"\"\"Base class for all response data.\"\"\"\n\n    perf_ns: int = Field(description=\"The performance timestamp of the response.\")\n    raw_text: list[str] = Field(description=\"The raw text of the response.\")\n    parsed_text: list[str | None] = Field(\n        description=\"The parsed text of the response.\"\n    )\n    token_count: int | None = Field(\n        default=None,\n        description=\"The total number of tokens in the response from the parsed text.\",\n    )\n    metadata: dict[str, Any] = Field(\n        default_factory=dict, description=\"The metadata of the response.\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.SSEField","title":"<code>SSEField</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base model for a single field in an SSE message.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class SSEField(AIPerfBaseModel):\n    \"\"\"Base model for a single field in an SSE message.\"\"\"\n\n    name: SSEFieldType | str = Field(\n        ...,\n        description=\"The name of the field. e.g. 'data', 'event', 'id', 'retry', 'comment'.\",\n    )\n    value: str | None = Field(\n        default=None,\n        description=\"The value of the field.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.SSEMessage","title":"<code>SSEMessage</code>","text":"<p>               Bases: <code>InferenceServerResponse</code></p> <p>Individual SSE message from an SSE stream. Delimited by </p> <p>.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class SSEMessage(InferenceServerResponse):\n    \"\"\"Individual SSE message from an SSE stream. Delimited by \\n\\n.\"\"\"\n\n    # Note: \"fields\" is a restricted keyword in pydantic\n    packets: list[SSEField] = Field(\n        default_factory=list,\n        description=\"The fields contained in the message.\",\n    )\n\n    def extract_data_content(self) -&gt; list[str]:\n        \"\"\"Extract the data contents from the SSE message as a list of strings. Note that the SSE spec specifies\n        that each data content should be combined and delimited by a single \\n. We have left\n        it as a list to allow the caller to decide how to handle the data.\n\n        Returns:\n            list[str]: A list of strings containing the data contents of the SSE message.\n        \"\"\"\n        return [\n            packet.value\n            for packet in self.packets\n            if packet.name == SSEFieldType.DATA and packet.value is not None\n        ]\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.SSEMessage.extract_data_content","title":"<code>extract_data_content()</code>","text":"<p>Extract the data contents from the SSE message as a list of strings. Note that the SSE spec specifies         that each data content should be combined and delimited by a single  . We have left         it as a list to allow the caller to decide how to handle the data.</p> <pre><code>    Returns:\n        list[str]: A list of strings containing the data contents of the SSE message.\n</code></pre> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def extract_data_content(self) -&gt; list[str]:\n    \"\"\"Extract the data contents from the SSE message as a list of strings. Note that the SSE spec specifies\n    that each data content should be combined and delimited by a single \\n. We have left\n    it as a list to allow the caller to decide how to handle the data.\n\n    Returns:\n        list[str]: A list of strings containing the data contents of the SSE message.\n    \"\"\"\n    return [\n        packet.value\n        for packet in self.packets\n        if packet.name == SSEFieldType.DATA and packet.value is not None\n    ]\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.TextResponse","title":"<code>TextResponse</code>","text":"<p>               Bases: <code>InferenceServerResponse</code></p> <p>Raw text response from a inference client including an optional content type.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class TextResponse(InferenceServerResponse):\n    \"\"\"Raw text response from a inference client including an optional content type.\"\"\"\n\n    content_type: str | None = Field(\n        default=None,\n        description=\"The content type of the response. e.g. 'text/plain', 'application/json'.\",\n    )\n    text: str = Field(\n        ...,\n        description=\"The text of the response.\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsservice_models","title":"aiperf.common.models.service_models","text":""},{"location":"api/#aiperf.common.models.service_models.ServiceRunInfo","title":"<code>ServiceRunInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for tracking service run information.</p> Source code in <code>aiperf/common/models/service_models.py</code> <pre><code>class ServiceRunInfo(BaseModel):\n    \"\"\"Base model for tracking service run information.\"\"\"\n\n    service_type: ServiceTypeT = Field(\n        ...,\n        description=\"The type of service\",\n    )\n    registration_status: ServiceRegistrationStatus = Field(\n        ...,\n        description=\"The registration status of the service\",\n    )\n    service_id: str = Field(\n        ...,\n        description=\"The ID of the service\",\n    )\n    first_seen: int | None = Field(\n        default_factory=time.time_ns,\n        description=\"The first time the service was seen\",\n    )\n    last_seen: int | None = Field(\n        default_factory=time.time_ns,\n        description=\"The last time the service was seen\",\n    )\n    state: LifecycleState = Field(\n        default=LifecycleState.CREATED,\n        description=\"The current state of the service\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsworker_models","title":"aiperf.common.models.worker_models","text":""},{"location":"api/#aiperf.common.models.worker_models.WorkerPhaseTaskStats","title":"<code>WorkerPhaseTaskStats</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Stats for the tasks that have been sent to the worker for a given credit phase.</p> Source code in <code>aiperf/common/models/worker_models.py</code> <pre><code>class WorkerPhaseTaskStats(AIPerfBaseModel):\n    \"\"\"Stats for the tasks that have been sent to the worker for a given credit phase.\"\"\"\n\n    total: int = Field(\n        default=0,\n        description=\"The total number of tasks that have been sent to the worker. \"\n        \"Not all tasks will be completed.\",\n    )\n    failed: int = Field(\n        default=0,\n        description=\"The number of tasks that returned an error\",\n    )\n    completed: int = Field(\n        default=0,\n        description=\"The number of tasks that were completed successfully\",\n    )\n\n    @property\n    def in_progress(self) -&gt; int:\n        \"\"\"The number of tasks that are currently in progress.\n\n        This is the total number of tasks sent to the worker minus the number of failed and successfully completed tasks.\n        \"\"\"\n        return self.total - self.completed - self.failed\n</code></pre>"},{"location":"api/#aiperf.common.models.worker_models.WorkerPhaseTaskStats.in_progress","title":"<code>in_progress</code>  <code>property</code>","text":"<p>The number of tasks that are currently in progress.</p> <p>This is the total number of tasks sent to the worker minus the number of failed and successfully completed tasks.</p>"},{"location":"api/#aiperfcommonprotocols","title":"aiperf.common.protocols","text":""},{"location":"api/#aiperf.common.protocols.AIPerfLifecycleProtocol","title":"<code>AIPerfLifecycleProtocol</code>","text":"<p>               Bases: <code>TaskManagerProtocol</code>, <code>Protocol</code></p> <p>Protocol for AIPerf lifecycle methods. see :class:<code>aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin</code> for more details.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass AIPerfLifecycleProtocol(TaskManagerProtocol, Protocol):\n    \"\"\"Protocol for AIPerf lifecycle methods.\n    see :class:`aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin` for more details.\n    \"\"\"\n\n    @property\n    def was_initialized(self) -&gt; bool: ...\n    @property\n    def was_started(self) -&gt; bool: ...\n    @property\n    def was_stopped(self) -&gt; bool: ...\n    @property\n    def is_running(self) -&gt; bool: ...\n\n    initialized_event: asyncio.Event\n    started_event: asyncio.Event\n    stopped_event: asyncio.Event\n\n    @property\n    def state(self) -&gt; LifecycleState: ...\n\n    async def initialize(self) -&gt; None: ...\n    async def start(self) -&gt; None: ...\n    async def initialize_and_start(self) -&gt; None: ...\n    async def stop(self) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol","title":"<code>CommunicationProtocol</code>","text":"<p>               Bases: <code>AIPerfLifecycleProtocol</code>, <code>Protocol</code></p> <p>Protocol for the base communication layer. see :class:<code>aiperf.common.comms.base_comms.BaseCommunication</code> for more details.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass CommunicationProtocol(AIPerfLifecycleProtocol, Protocol):\n    \"\"\"Protocol for the base communication layer.\n    see :class:`aiperf.common.comms.base_comms.BaseCommunication` for more details.\n    \"\"\"\n\n    def get_address(self, address_type: CommAddressType) -&gt; str: ...\n\n    \"\"\"Get the address for the given address type can be an enum value for lookup, or a string for direct use.\"\"\"\n\n    def create_client(\n        self,\n        client_type: CommClientType,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; CommunicationClientProtocol:\n        \"\"\"Create a client for the given client type and address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_pub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PubClientProtocol:\n        \"\"\"Create a PUB client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_sub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; SubClientProtocol:\n        \"\"\"Create a SUB client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_push_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PushClientProtocol:\n        \"\"\"Create a PUSH client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_pull_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; PullClientProtocol:\n        \"\"\"Create a PULL client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_request_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; RequestClientProtocol:\n        \"\"\"Create a REQUEST client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_reply_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; ReplyClientProtocol:\n        \"\"\"Create a REPLY client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_client","title":"<code>create_client(client_type, address, bind=False, socket_ops=None, max_pull_concurrency=None)</code>","text":"<p>Create a client for the given client type and address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_client(\n    self,\n    client_type: CommClientType,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n) -&gt; CommunicationClientProtocol:\n    \"\"\"Create a client for the given client type and address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_pub_client","title":"<code>create_pub_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a PUB client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_pub_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; PubClientProtocol:\n    \"\"\"Create a PUB client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_pull_client","title":"<code>create_pull_client(address, bind=False, socket_ops=None, max_pull_concurrency=None)</code>","text":"<p>Create a PULL client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_pull_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n) -&gt; PullClientProtocol:\n    \"\"\"Create a PULL client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_push_client","title":"<code>create_push_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a PUSH client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_push_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; PushClientProtocol:\n    \"\"\"Create a PUSH client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_reply_client","title":"<code>create_reply_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a REPLY client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_reply_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; ReplyClientProtocol:\n    \"\"\"Create a REPLY client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_request_client","title":"<code>create_request_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a REQUEST client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_request_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; RequestClientProtocol:\n    \"\"\"Create a REQUEST client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_sub_client","title":"<code>create_sub_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a SUB client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_sub_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; SubClientProtocol:\n    \"\"\"Create a SUB client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.DataExporterProtocol","title":"<code>DataExporterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for data exporters. Any class implementing this protocol must provide an <code>export</code> method that takes a list of Record objects and handles exporting them appropriately.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass DataExporterProtocol(Protocol):\n    \"\"\"\n    Protocol for data exporters.\n    Any class implementing this protocol must provide an `export` method\n    that takes a list of Record objects and handles exporting them appropriately.\n    \"\"\"\n\n    async def export(self) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.HooksProtocol","title":"<code>HooksProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for hooks methods provided by the HooksMixin.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass HooksProtocol(Protocol):\n    \"\"\"Protocol for hooks methods provided by the HooksMixin.\"\"\"\n\n    def get_hooks(self, *hook_types: HookType, reversed: bool = False) -&gt; list[Hook]:\n        \"\"\"Get the hooks for the given hook type(s), optionally reversed.\"\"\"\n        ...\n\n    async def run_hooks(\n        self, *hook_types: HookType, reversed: bool = False, **kwargs\n    ) -&gt; None:\n        \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n        If reversed is True, the hooks will be run in reverse order. This is useful for stop/cleanup starting with\n        the children and ending with the parent.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.HooksProtocol.get_hooks","title":"<code>get_hooks(*hook_types, reversed=False)</code>","text":"<p>Get the hooks for the given hook type(s), optionally reversed.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def get_hooks(self, *hook_types: HookType, reversed: bool = False) -&gt; list[Hook]:\n    \"\"\"Get the hooks for the given hook type(s), optionally reversed.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.HooksProtocol.run_hooks","title":"<code>run_hooks(*hook_types, reversed=False, **kwargs)</code>  <code>async</code>","text":"<p>Run the hooks for the given hook type, waiting for each hook to complete before running the next one. If reversed is True, the hooks will be run in reverse order. This is useful for stop/cleanup starting with the children and ending with the parent.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def run_hooks(\n    self, *hook_types: HookType, reversed: bool = False, **kwargs\n) -&gt; None:\n    \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n    If reversed is True, the hooks will be run in reverse order. This is useful for stop/cleanup starting with\n    the children and ending with the parent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol","title":"<code>InferenceClientProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for an inference server client.</p> <p>This protocol defines the methods that must be implemented by any inference server client implementation that is compatible with the AIPerf framework.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass InferenceClientProtocol(Protocol):\n    \"\"\"Protocol for an inference server client.\n\n    This protocol defines the methods that must be implemented by any inference server client\n    implementation that is compatible with the AIPerf framework.\n    \"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfoT) -&gt; None:\n        \"\"\"Create a new inference server client based on the provided configuration.\"\"\"\n        ...\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the inference server client in an asynchronous context.\"\"\"\n        ...\n\n    async def send_request(\n        self,\n        model_endpoint: ModelEndpointInfoT,\n        payload: RequestInputT,\n    ) -&gt; RequestRecord:\n        \"\"\"Send a request to the inference server.\n\n        This method is used to send a request to the inference server.\n\n        Args:\n            model_endpoint: The endpoint to send the request to.\n            payload: The payload to send to the inference server.\n        Returns:\n            The raw response from the inference server.\n        \"\"\"\n        ...\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the client.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.__init__","title":"<code>__init__(model_endpoint)</code>","text":"<p>Create a new inference server client based on the provided configuration.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def __init__(self, model_endpoint: ModelEndpointInfoT) -&gt; None:\n    \"\"\"Create a new inference server client based on the provided configuration.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the client.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the client.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the inference server client in an asynchronous context.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the inference server client in an asynchronous context.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.send_request","title":"<code>send_request(model_endpoint, payload)</code>  <code>async</code>","text":"<p>Send a request to the inference server.</p> <p>This method is used to send a request to the inference server.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>ModelEndpointInfoT</code> <p>The endpoint to send the request to.</p> required <code>payload</code> <code>RequestInputT</code> <p>The payload to send to the inference server.</p> required <p>Returns:     The raw response from the inference server.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def send_request(\n    self,\n    model_endpoint: ModelEndpointInfoT,\n    payload: RequestInputT,\n) -&gt; RequestRecord:\n    \"\"\"Send a request to the inference server.\n\n    This method is used to send a request to the inference server.\n\n    Args:\n        model_endpoint: The endpoint to send the request to.\n        payload: The payload to send to the inference server.\n    Returns:\n        The raw response from the inference server.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.MessageBusClientProtocol","title":"<code>MessageBusClientProtocol</code>","text":"<p>               Bases: <code>PubClientProtocol</code>, <code>SubClientProtocol</code>, <code>Protocol</code></p> <p>A message bus client is a client that can publish and subscribe to messages on the event bus/message bus.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass MessageBusClientProtocol(PubClientProtocol, SubClientProtocol, Protocol):\n    \"\"\"A message bus client is a client that can publish and subscribe to messages\n    on the event bus/message bus.\"\"\"\n\n    comms: CommunicationProtocol\n    sub_client: SubClientProtocol\n    pub_client: PubClientProtocol\n</code></pre>"},{"location":"api/#aiperf.common.protocols.PostProcessorProtocol","title":"<code>PostProcessorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>PostProcessorProtocol is a protocol that defines the API for post-processors.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass PostProcessorProtocol(Protocol):\n    \"\"\"\n    PostProcessorProtocol is a protocol that defines the API for post-processors.\n    \"\"\"\n\n    def process_record(self, record: \"ParsedResponseRecord\") -&gt; None:\n        \"\"\"Process a single record.\"\"\"\n        ...\n\n    def post_process(self) -&gt; Any:\n        \"\"\"\n        Execute the post-processing logic on the records.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.PostProcessorProtocol.post_process","title":"<code>post_process()</code>","text":"<p>Execute the post-processing logic on the records.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def post_process(self) -&gt; Any:\n    \"\"\"\n    Execute the post-processing logic on the records.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.PostProcessorProtocol.process_record","title":"<code>process_record(record)</code>","text":"<p>Process a single record.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def process_record(self, record: \"ParsedResponseRecord\") -&gt; None:\n    \"\"\"Process a single record.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.RequestConverterProtocol","title":"<code>RequestConverterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a request converter that converts a raw request to a formatted request for the inference server.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass RequestConverterProtocol(Protocol):\n    \"\"\"Protocol for a request converter that converts a raw request to a formatted request for the inference server.\"\"\"\n\n    async def format_payload(\n        self, model_endpoint: ModelEndpointInfoT, turn: Turn\n    ) -&gt; RequestOutputT:\n        \"\"\"Format the turn for the inference server.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.RequestConverterProtocol.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format the turn for the inference server.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def format_payload(\n    self, model_endpoint: ModelEndpointInfoT, turn: Turn\n) -&gt; RequestOutputT:\n    \"\"\"Format the turn for the inference server.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ResponseExtractorProtocol","title":"<code>ResponseExtractorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a response extractor that extracts the response data from a raw inference server response and converts it to a list of ResponseData objects.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ResponseExtractorProtocol(Protocol):\n    \"\"\"Protocol for a response extractor that extracts the response data from a raw inference server\n    response and converts it to a list of ResponseData objects.\"\"\"\n\n    async def extract_response_data(\n        self, record: RequestRecord, tokenizer: Tokenizer | None\n    ) -&gt; list[ResponseData]:\n        \"\"\"Extract the response data from a raw inference server response and convert it to a list of ResponseData objects.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ResponseExtractorProtocol.extract_response_data","title":"<code>extract_response_data(record, tokenizer)</code>  <code>async</code>","text":"<p>Extract the response data from a raw inference server response and convert it to a list of ResponseData objects.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def extract_response_data(\n    self, record: RequestRecord, tokenizer: Tokenizer | None\n) -&gt; list[ResponseData]:\n    \"\"\"Extract the response data from a raw inference server response and convert it to a list of ResponseData objects.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ServiceManagerProtocol","title":"<code>ServiceManagerProtocol</code>","text":"<p>               Bases: <code>AIPerfLifecycleProtocol</code>, <code>Protocol</code></p> <p>Protocol for a service manager that manages the running of services using the specific ServiceRunType. Abstracts away the details of service deployment and management. see :class:<code>aiperf.services.system_controller.base.BaseServiceManager</code> for more details.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ServiceManagerProtocol(AIPerfLifecycleProtocol, Protocol):\n    \"\"\"Protocol for a service manager that manages the running of services using the specific ServiceRunType.\n    Abstracts away the details of service deployment and management.\n    see :class:`aiperf.services.system_controller.base.BaseServiceManager` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        log_queue: \"multiprocessing.Queue | None\" = None,\n    ): ...\n\n    required_services: dict[ServiceTypeT, int]\n    service_map: dict[ServiceTypeT, list[ServiceRunInfo]]\n    service_id_map: dict[str, ServiceRunInfo]\n\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None: ...\n\n    async def run_services(self, service_types: dict[ServiceTypeT, int]) -&gt; None: ...\n    async def run_required_services(self) -&gt; None: ...\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]: ...\n    async def kill_all_services(self) -&gt; list[BaseException | None]: ...\n    async def stop_service(\n        self, service_type: ServiceTypeT, service_id: str | None = None\n    ) -&gt; list[BaseException | None]: ...\n    async def stop_services_by_type(\n        self, service_types: list[ServiceTypeT]\n    ) -&gt; list[BaseException | None]: ...\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None: ...\n\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ServiceProtocol","title":"<code>ServiceProtocol</code>","text":"<p>               Bases: <code>MessageBusClientProtocol</code>, <code>Protocol</code></p> <p>Protocol for a service. Essentially a MessageBusClientProtocol with a service_type and service_id attributes.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ServiceProtocol(MessageBusClientProtocol, Protocol):\n    \"\"\"Protocol for a service. Essentially a MessageBusClientProtocol with a service_type and service_id attributes.\"\"\"\n\n    def __init__(\n        self,\n        user_config: \"UserConfig\",\n        service_config: \"ServiceConfig\",\n        service_id: str | None = None,\n        **kwargs,\n    ) -&gt; None: ...\n\n    service_type: ServiceTypeT\n    service_id: str\n</code></pre>"},{"location":"api/#aiperf.common.protocols.StreamingPostProcessorProtocol","title":"<code>StreamingPostProcessorProtocol</code>","text":"<p>               Bases: <code>AIPerfLifecycleProtocol</code>, <code>Protocol</code></p> <p>Protocol for a streaming post processor that streams the incoming records to the post processor.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass StreamingPostProcessorProtocol(AIPerfLifecycleProtocol, Protocol):\n    \"\"\"Protocol for a streaming post processor that streams the incoming records to the post processor.\"\"\"\n\n    def __init__(\n        self,\n        class_type: StreamingPostProcessorType | str,\n        service_id: str,\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        max_queue_size: int = DEFAULT_STREAMING_MAX_QUEUE_SIZE,\n        **kwargs,\n    ) -&gt; None: ...\n\n    records_queue: asyncio.Queue[ParsedResponseRecord]\n    cancellation_event: asyncio.Event\n\n    async def stream_record(self, record: ParsedResponseRecord) -&gt; None: ...\n\n    async def process_records(self, cancelled: bool) -&gt; Any: ...\n</code></pre>"},{"location":"api/#aiperfcommontokenizer","title":"aiperf.common.tokenizer","text":""},{"location":"api/#aiperf.common.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>This class provides a simplified interface for using Huggingface tokenizers, with default arguments for common operations.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>class Tokenizer:\n    \"\"\"\n    This class provides a simplified interface for using Huggingface\n    tokenizers, with default arguments for common operations.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the tokenizer with default values for call, encode, and decode.\n        \"\"\"\n        self._tokenizer = None\n        self._call_args = {\"add_special_tokens\": False}\n        self._encode_args = {\"add_special_tokens\": False}\n        self._decode_args = {\"skip_special_tokens\": True}\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        name: str,\n        trust_remote_code: bool = False,\n        revision: str = \"main\",\n    ) -&gt; \"Tokenizer\":\n        \"\"\"\n        Factory to load a tokenizer for the given pretrained model name.\n\n        Args:\n            name: The name or path of the pretrained tokenizer model.\n            trust_remote_code: Whether to trust remote code when loading the tokenizer.\n            revision: The specific model version to use.\n        \"\"\"\n        try:\n            tokenizer_cls = cls()\n            tokenizer_cls._tokenizer = AutoTokenizer.from_pretrained(\n                name, trust_remote_code=trust_remote_code, revision=revision\n            )\n        except Exception as e:\n            raise InitializationError(e) from e\n        return tokenizer_cls\n\n    def __call__(self, text, **kwargs) -&gt; \"BatchEncoding\":\n        \"\"\"\n        Call the underlying Huggingface tokenizer with default arguments,\n        which can be overridden by kwargs.\n\n        Args:\n            text: The input text to tokenize.\n\n        Returns:\n            A BatchEncoding object containing the tokenized output.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer(text, **{**self._call_args, **kwargs})\n\n    def encode(self, text, **kwargs) -&gt; list[int]:\n        \"\"\"\n        Encode the input text into a list of token IDs.\n\n        This method calls the underlying Huggingface tokenizer's encode\n        method with default arguments, which can be overridden by kwargs.\n\n        Args:\n            text: The input text to encode.\n\n        Returns:\n            A list of token IDs.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.encode(text, **{**self._encode_args, **kwargs})\n\n    def decode(self, token_ids, **kwargs) -&gt; str:\n        \"\"\"\n        Decode a list of token IDs back into a string.\n\n        This method calls the underlying Huggingface tokenizer's decode\n        method with default arguments, which can be overridden by kwargs.\n\n        Args:\n            token_ids: A list of token IDs to decode.\n\n        Returns:\n            The decoded string.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.decode(token_ids, **{**self._decode_args, **kwargs})\n\n    @property\n    def bos_token_id(self) -&gt; int:\n        \"\"\"\n        Return the beginning-of-sequence (BOS) token ID.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.bos_token_id\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the underlying tokenizer.\n\n        Returns:\n            The string representation of the tokenizer.\n        \"\"\"\n        return self._tokenizer.__repr__()\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Return a user-friendly string representation of the underlying tokenizer.\n\n        Returns:\n            The string representation of the tokenizer.\n        \"\"\"\n        return self._tokenizer.__str__()\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>Return the beginning-of-sequence (BOS) token ID.</p>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__call__","title":"<code>__call__(text, **kwargs)</code>","text":"<p>Call the underlying Huggingface tokenizer with default arguments, which can be overridden by kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The input text to tokenize.</p> required <p>Returns:</p> Type Description <code>BatchEncoding</code> <p>A BatchEncoding object containing the tokenized output.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __call__(self, text, **kwargs) -&gt; \"BatchEncoding\":\n    \"\"\"\n    Call the underlying Huggingface tokenizer with default arguments,\n    which can be overridden by kwargs.\n\n    Args:\n        text: The input text to tokenize.\n\n    Returns:\n        A BatchEncoding object containing the tokenized output.\n    \"\"\"\n    if self._tokenizer is None:\n        raise NotInitializedError(\"Tokenizer is not initialized.\")\n    return self._tokenizer(text, **{**self._call_args, **kwargs})\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the tokenizer with default values for call, encode, and decode.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the tokenizer with default values for call, encode, and decode.\n    \"\"\"\n    self._tokenizer = None\n    self._call_args = {\"add_special_tokens\": False}\n    self._encode_args = {\"add_special_tokens\": False}\n    self._decode_args = {\"skip_special_tokens\": True}\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the underlying tokenizer.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the tokenizer.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the underlying tokenizer.\n\n    Returns:\n        The string representation of the tokenizer.\n    \"\"\"\n    return self._tokenizer.__repr__()\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__str__","title":"<code>__str__()</code>","text":"<p>Return a user-friendly string representation of the underlying tokenizer.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the tokenizer.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Return a user-friendly string representation of the underlying tokenizer.\n\n    Returns:\n        The string representation of the tokenizer.\n    \"\"\"\n    return self._tokenizer.__str__()\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.decode","title":"<code>decode(token_ids, **kwargs)</code>","text":"<p>Decode a list of token IDs back into a string.</p> <p>This method calls the underlying Huggingface tokenizer's decode method with default arguments, which can be overridden by kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <p>A list of token IDs to decode.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def decode(self, token_ids, **kwargs) -&gt; str:\n    \"\"\"\n    Decode a list of token IDs back into a string.\n\n    This method calls the underlying Huggingface tokenizer's decode\n    method with default arguments, which can be overridden by kwargs.\n\n    Args:\n        token_ids: A list of token IDs to decode.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    if self._tokenizer is None:\n        raise NotInitializedError(\"Tokenizer is not initialized.\")\n    return self._tokenizer.decode(token_ids, **{**self._decode_args, **kwargs})\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.encode","title":"<code>encode(text, **kwargs)</code>","text":"<p>Encode the input text into a list of token IDs.</p> <p>This method calls the underlying Huggingface tokenizer's encode method with default arguments, which can be overridden by kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The input text to encode.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of token IDs.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def encode(self, text, **kwargs) -&gt; list[int]:\n    \"\"\"\n    Encode the input text into a list of token IDs.\n\n    This method calls the underlying Huggingface tokenizer's encode\n    method with default arguments, which can be overridden by kwargs.\n\n    Args:\n        text: The input text to encode.\n\n    Returns:\n        A list of token IDs.\n    \"\"\"\n    if self._tokenizer is None:\n        raise NotInitializedError(\"Tokenizer is not initialized.\")\n    return self._tokenizer.encode(text, **{**self._encode_args, **kwargs})\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.from_pretrained","title":"<code>from_pretrained(name, trust_remote_code=False, revision='main')</code>  <code>classmethod</code>","text":"<p>Factory to load a tokenizer for the given pretrained model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name or path of the pretrained tokenizer model.</p> required <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading the tokenizer.</p> <code>False</code> <code>revision</code> <code>str</code> <p>The specific model version to use.</p> <code>'main'</code> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    name: str,\n    trust_remote_code: bool = False,\n    revision: str = \"main\",\n) -&gt; \"Tokenizer\":\n    \"\"\"\n    Factory to load a tokenizer for the given pretrained model name.\n\n    Args:\n        name: The name or path of the pretrained tokenizer model.\n        trust_remote_code: Whether to trust remote code when loading the tokenizer.\n        revision: The specific model version to use.\n    \"\"\"\n    try:\n        tokenizer_cls = cls()\n        tokenizer_cls._tokenizer = AutoTokenizer.from_pretrained(\n            name, trust_remote_code=trust_remote_code, revision=revision\n        )\n    except Exception as e:\n        raise InitializationError(e) from e\n    return tokenizer_cls\n</code></pre>"},{"location":"api/#aiperfcommontypes","title":"aiperf.common.types","text":"<p>This module defines common used alias types for AIPerf. This both helps prevent circular imports and helps with type hinting.</p>"},{"location":"api/#aiperfcommonutils","title":"aiperf.common.utils","text":""},{"location":"api/#aiperf.common.utils.call_all_functions","title":"<code>call_all_functions(funcs, *args, **kwargs)</code>  <code>async</code>","text":"<p>Call all functions in the list with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>The object to call the functions on.</p> required <code>func_names</code> <p>The names of the functions to call.</p> required <code>*args</code> <p>The arguments to pass to the functions.</p> <code>()</code> <code>**kwargs</code> <p>The keyword arguments to pass to the functions.</p> <code>{}</code> <p>Raises:</p> Type Description <code>AIPerfMultiError</code> <p>If any of the functions raise an exception.</p> Source code in <code>aiperf/common/utils.py</code> <pre><code>async def call_all_functions(funcs: list[Callable], *args, **kwargs) -&gt; None:\n    \"\"\"Call all functions in the list with the given name.\n\n    Args:\n        obj: The object to call the functions on.\n        func_names: The names of the functions to call.\n        *args: The arguments to pass to the functions.\n        **kwargs: The keyword arguments to pass to the functions.\n\n    Raises:\n        AIPerfMultiError: If any of the functions raise an exception.\n    \"\"\"\n\n    exceptions = []\n    for func in funcs:\n        try:\n            if inspect.iscoroutinefunction(func):\n                await func(*args, **kwargs)\n            else:\n                func(*args, **kwargs)\n        except Exception as e:\n            # TODO: error handling, logging\n            traceback.print_exc()\n            exceptions.append(e)\n\n    if len(exceptions) &gt; 0:\n        raise AIPerfMultiError(\"Errors calling functions\", exceptions)\n</code></pre>"},{"location":"api/#aiperf.common.utils.call_all_functions_self","title":"<code>call_all_functions_self(self_, funcs, *args, **kwargs)</code>  <code>async</code>","text":"<p>Call all functions in the list with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>The object to call the functions on.</p> required <code>func_names</code> <p>The names of the functions to call.</p> required <code>*args</code> <p>The arguments to pass to the functions.</p> <code>()</code> <code>**kwargs</code> <p>The keyword arguments to pass to the functions.</p> <code>{}</code> <p>Raises:</p> Type Description <code>AIPerfMultiError</code> <p>If any of the functions raise an exception.</p> Source code in <code>aiperf/common/utils.py</code> <pre><code>async def call_all_functions_self(\n    self_: object, funcs: list[Callable], *args, **kwargs\n) -&gt; None:\n    \"\"\"Call all functions in the list with the given name.\n\n    Args:\n        obj: The object to call the functions on.\n        func_names: The names of the functions to call.\n        *args: The arguments to pass to the functions.\n        **kwargs: The keyword arguments to pass to the functions.\n\n    Raises:\n        AIPerfMultiError: If any of the functions raise an exception.\n    \"\"\"\n\n    exceptions = []\n    for func in funcs:\n        try:\n            if inspect.iscoroutinefunction(func):\n                await func(self_, *args, **kwargs)\n            else:\n                func(self_, *args, **kwargs)\n        except Exception as e:\n            # TODO: error handling, logging\n            traceback.print_exc()\n            exceptions.append(e)\n\n    if len(exceptions) &gt; 0:\n        raise AIPerfMultiError(\"Errors calling functions\", exceptions)\n</code></pre>"},{"location":"api/#aiperf.common.utils.load_json_str","title":"<code>load_json_str(json_str, func=lambda x: x)</code>","text":"<p>Deserializes JSON encoded string into Python object.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>json_str</code> <p>string   JSON encoded string</p> required <code>-</code> <code>func</code> <p>callable   A function that takes deserialized JSON object. This can be used to   run validation checks on the object. Defaults to identity function.</p> required Source code in <code>aiperf/common/utils.py</code> <pre><code>def load_json_str(json_str: str, func: Callable = lambda x: x) -&gt; dict[str, Any]:\n    \"\"\"\n    Deserializes JSON encoded string into Python object.\n\n    Args:\n      - json_str: string\n          JSON encoded string\n      - func: callable\n          A function that takes deserialized JSON object. This can be used to\n          run validation checks on the object. Defaults to identity function.\n    \"\"\"\n    try:\n        # Note: orjson may not parse JSON the same way as Python's standard json library,\n        # notably being stricter on UTF-8 conformance.\n        # Refer to https://github.com/ijl/orjson?tab=readme-ov-file#str for details.\n        return func(orjson.loads(json_str))\n    except orjson.JSONDecodeError:\n        snippet = json_str[:200] + (\"...\" if len(json_str) &gt; 200 else \"\")\n        logger.error(\"Failed to parse JSON string: '%s'\", snippet)\n        raise\n</code></pre>"},{"location":"api/#aiperf.common.utils.yield_to_event_loop","title":"<code>yield_to_event_loop()</code>  <code>async</code>","text":"<p>Yield to the event loop. This forces the current coroutine to yield and allow other coroutines to run, preventing starvation. Use this when you do not want to delay your coroutine via sleep, but still want to allow other coroutines to run if there is a potential for an infinite loop.</p> Source code in <code>aiperf/common/utils.py</code> <pre><code>async def yield_to_event_loop() -&gt; None:\n    \"\"\"Yield to the event loop. This forces the current coroutine to yield and allow\n    other coroutines to run, preventing starvation. Use this when you do not want to\n    delay your coroutine via sleep, but still want to allow other coroutines to run if\n    there is a potential for an infinite loop.\n    \"\"\"\n    await asyncio.sleep(0)\n</code></pre>"},{"location":"api/#aiperfdata_exporterconsole_error_exporter","title":"aiperf.data_exporter.console_error_exporter","text":""},{"location":"api/#aiperf.data_exporter.console_error_exporter.ConsoleErrorExporter","title":"<code>ConsoleErrorExporter</code>","text":"<p>A class that exports error data to the console</p> Source code in <code>aiperf/data_exporter/console_error_exporter.py</code> <pre><code>@DataExporterFactory.register(DataExporterType.CONSOLE_ERROR)\nclass ConsoleErrorExporter:\n    \"\"\"A class that exports error data to the console\"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs):\n        self._results = exporter_config.results\n\n    async def export(self, width: int | None = None) -&gt; None:\n        console = Console()\n\n        if len(self._results.error_summary) &gt; 0:\n            table = Table(title=self._get_title(), width=width)\n            table.add_column(\"Code\", justify=\"right\", style=\"yellow\")\n            table.add_column(\"Type\", justify=\"right\", style=\"yellow\")\n            table.add_column(\"Message\", justify=\"left\", style=\"yellow\")\n            table.add_column(\"Count\", justify=\"right\", style=\"yellow\")\n            self._construct_table(table, self._results.error_summary)\n\n            console.print(\"\\n\")\n            console.print(table)\n\n            if self._results.was_cancelled:\n                console.print(\"[red][bold]Profile run was cancelled early[/bold][/red]\")\n\n        console.file.flush()\n\n    def _construct_table(\n        self, table: Table, errors_by_type: list[ErrorDetailsCount]\n    ) -&gt; None:\n        for error_details_count in errors_by_type:\n            table.add_row(*self._format_row(error_details_count))\n\n    def _format_row(self, error_details_count: ErrorDetailsCount) -&gt; list[str]:\n        details = error_details_count.error_details\n        count = error_details_count.count\n\n        return [\n            str(details.code) if details.code else \"[dim]N/A[/dim]\",\n            str(details.type) if details.type else \"[dim]N/A[/dim]\",\n            str(details.message),\n            f\"{count:,}\",\n        ]\n\n    def _get_title(self) -&gt; str:\n        return \"[bold][red]NVIDIA AIPerf | Error Summary[/red][/bold]\"\n</code></pre>"},{"location":"api/#aiperfdata_exporterconsole_exporter","title":"aiperf.data_exporter.console_exporter","text":""},{"location":"api/#aiperf.data_exporter.console_exporter.ConsoleExporter","title":"<code>ConsoleExporter</code>","text":"<p>A class that exports data to the console</p> Source code in <code>aiperf/data_exporter/console_exporter.py</code> <pre><code>@implements_protocol(DataExporterProtocol)\n@DataExporterFactory.register(DataExporterType.CONSOLE)\nclass ConsoleExporter:\n    \"\"\"A class that exports data to the console\"\"\"\n\n    STAT_COLUMN_KEYS = [\"avg\", \"min\", \"max\", \"p99\", \"p90\", \"p75\", \"std\", \"count\"]\n\n    def __init__(self, exporter_config: ExporterConfig) -&gt; None:\n        self._results = exporter_config.results\n        self._endpoint_type = exporter_config.input_config.endpoint.type\n        self._streaming = exporter_config.input_config.endpoint.streaming\n\n    async def export(self, width: int | None = None) -&gt; None:\n        table = Table(title=self._get_title(), width=width)\n        table.add_column(\"Metric\", justify=\"right\", style=\"cyan\")\n        for key in self.STAT_COLUMN_KEYS:\n            table.add_column(key, justify=\"right\", style=\"green\")\n        self._construct_table(table, self._results.records)\n\n        console = Console()\n        console.print(\"\\n\")\n        console.print(table)\n        if self._results.was_cancelled:\n            console.print(\"[red][bold]Profile run was cancelled early[/bold][/red]\")\n        console.file.flush()\n\n    def _construct_table(self, table: Table, records: list[MetricResult]) -&gt; None:\n        for record in records:\n            if self._should_skip(record):\n                continue\n            table.add_row(*self._format_row(record))\n\n    def _should_skip(self, record: MetricResult) -&gt; bool:\n        if self._endpoint_type == \"embeddings\":\n            return False\n\n        return record.streaming_only and not self._streaming\n\n    def _format_row(self, record: MetricResult) -&gt; list[str]:\n        row = [f\"{record.header} ({record.unit})\"]\n        for stat in self.STAT_COLUMN_KEYS:\n            value = getattr(record, stat, None)\n            row.append(\n                f\"{value:,.2f}\"\n                if isinstance(value, float)\n                else f\"{value:,}\"\n                if isinstance(value, int)\n                else \"[dim]N/A[/dim]\"\n            )\n        return row\n\n    def _get_title(self) -&gt; str:\n        return f\"NVIDIA AIPerf | {self._endpoint_type.metrics_title()}\"\n</code></pre>"},{"location":"api/#aiperfdata_exporterexporter_config","title":"aiperf.data_exporter.exporter_config","text":""},{"location":"api/#aiperfdata_exporterexporter_manager","title":"aiperf.data_exporter.exporter_manager","text":""},{"location":"api/#aiperf.data_exporter.exporter_manager.ExporterManager","title":"<code>ExporterManager</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>ExporterManager is responsible for exporting records using all registered data exporters.</p> Source code in <code>aiperf/data_exporter/exporter_manager.py</code> <pre><code>class ExporterManager(AIPerfLoggerMixin):\n    \"\"\"\n    ExporterManager is responsible for exporting records using all\n    registered data exporters.\n    \"\"\"\n\n    def __init__(self, results: ProfileResults, input_config: UserConfig, **kwargs):\n        super().__init__(**kwargs)\n        self._results = results\n        self._input_config = input_config\n\n    async def export_all(self) -&gt; None:\n        self.info(\"Exporting all records\")\n        tasks: set[asyncio.Task] = set()\n        exporter_config = ExporterConfig(\n            results=self._results,\n            input_config=self._input_config,\n        )\n\n        def task_done_callback(task: asyncio.Task) -&gt; None:\n            self.debug(lambda: f\"Task done: {task}\")\n            if task.exception():\n                self.error(f\"Error exporting records: {task.exception()}\")\n            else:\n                self.debug(f\"Exported records: {task.result()}\")\n            tasks.discard(task)\n\n        for exporter_type in DataExporterFactory.get_all_class_types():\n            exporter = DataExporterFactory.create_instance(\n                exporter_type, exporter_config=exporter_config\n            )\n            self.debug(f\"Creating task for exporter: {exporter_type}\")\n            task = asyncio.create_task(exporter.export())\n            tasks.add(task)\n            task.add_done_callback(task_done_callback)\n\n        await asyncio.gather(*tasks, return_exceptions=True)\n        self.debug(\"Exporting all records completed\")\n</code></pre>"},{"location":"api/#aiperfdata_exporterjson_exporter","title":"aiperf.data_exporter.json_exporter","text":""},{"location":"api/#aiperf.data_exporter.json_exporter.JsonExportData","title":"<code>JsonExportData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data to be exported to a JSON file.</p> Source code in <code>aiperf/data_exporter/json_exporter.py</code> <pre><code>class JsonExportData(BaseModel):\n    \"\"\"Data to be exported to a JSON file.\"\"\"\n\n    input_config: UserConfig | None = None\n    records: dict[MetricTagT, MetricResult] | None = None\n    was_cancelled: bool | None = None\n    error_summary: list[ErrorDetailsCount] | None = None\n    start_time: datetime | None = None\n    end_time: datetime | None = None\n</code></pre>"},{"location":"api/#aiperf.data_exporter.json_exporter.JsonExporter","title":"<code>JsonExporter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>A class to export records to a JSON file.</p> Source code in <code>aiperf/data_exporter/json_exporter.py</code> <pre><code>@DataExporterFactory.register(DataExporterType.JSON)\n@implements_protocol(DataExporterProtocol)\nclass JsonExporter(AIPerfLoggerMixin):\n    \"\"\"\n    A class to export records to a JSON file.\n    \"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.debug(lambda: f\"Initializing JsonExporter with config: {exporter_config}\")\n        self._results = exporter_config.results\n        self._output_directory = exporter_config.input_config.output.artifact_directory\n        self._input_config = exporter_config.input_config\n\n    async def export(self) -&gt; None:\n        filename = self._output_directory / \"profile_export_aiperf.json\"\n        self._output_directory.mkdir(parents=True, exist_ok=True)\n\n        start_time = (\n            datetime.fromtimestamp(self._results.start_ns / NANOS_PER_SECOND)\n            if self._results.start_ns\n            else None\n        )\n        end_time = (\n            datetime.fromtimestamp(self._results.end_ns / NANOS_PER_SECOND)\n            if self._results.end_ns\n            else None\n        )\n\n        export_data = JsonExportData(\n            input_config=self._input_config,\n            records={record.tag: record for record in self._results.records},\n            was_cancelled=self._results.was_cancelled,\n            error_summary=self._results.error_summary,\n            start_time=start_time,\n            end_time=end_time,\n        )\n\n        self.debug(lambda: f\"Exporting data to JSON file: {export_data}\")\n        export_data_json = export_data.model_dump_json(indent=2, exclude_unset=True)\n        async with aiofiles.open(filename, \"w\") as f:\n            await f.write(export_data_json)\n</code></pre>"},{"location":"api/#aiperfprogressprogress_models","title":"aiperf.progress.progress_models","text":""},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteCompletionTrigger","title":"<code>BenchmarkSuiteCompletionTrigger</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines how the suite completion is determined in order to know how to track the progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class BenchmarkSuiteCompletionTrigger(CaseInsensitiveStrEnum):\n    \"\"\"Determines how the suite completion is determined in order to know how to track the progress.\"\"\"\n\n    UNKNOWN = \"unknown\"\n    COMPLETED_SWEEPS = \"completed_sweeps\"\n    COMPLETED_PROFILES = \"completed_profiles\"\n    STABILIZATION_BASED = \"stabilization_based\"\n    CUSTOM = \"custom\"  # TBD\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteProgress","title":"<code>BenchmarkSuiteProgress</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>State of the suite progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class BenchmarkSuiteProgress(BaseModel, ABC):\n    \"\"\"State of the suite progress.\"\"\"\n\n    suite_type: BenchmarkSuiteType = Field(\n        default=BenchmarkSuiteType.SINGLE_PROFILE,\n        description=\"The type of suite. Default is SINGLE_PROFILE.\",\n    )\n    suite_completion_trigger: BenchmarkSuiteCompletionTrigger = Field(\n        default=BenchmarkSuiteCompletionTrigger.COMPLETED_PROFILES,\n        description=\"The trigger of suite completion\",\n    )\n    start_time_ns: int | None = Field(\n        default=None,\n        description=\"The overall start time of the suite in nanoseconds. If it has not been started, this will be None.\",\n    )\n    end_time_ns: int | None = Field(\n        default=None,\n        description=\"The overall end time of the suite in nanoseconds. If it has not been completed, this will be None.\",\n    )\n    was_cancelled: bool = Field(\n        default=False,\n        description=\"Whether the suite was cancelled early\",\n    )\n\n    @property\n    def current_sweep(self) -&gt; SweepProgress | None:\n        if not isinstance(self, SweepSuiteProgress) or self.current_sweep_idx is None:\n            return None\n        return self.sweeps[self.current_sweep_idx]\n\n    @property\n    def current_profile(self) -&gt; ProfileProgress | None:\n        if isinstance(self, ProfileSuiteProgress):\n            if self.current_profile_idx is None or self.current_profile_idx &gt;= len(\n                self.profiles\n            ):\n                return None\n            return self.profiles[self.current_profile_idx]\n\n        elif isinstance(self, SweepSuiteProgress):\n            if self.current_sweep is None:\n                return None\n            return self.current_sweep.current_profile\n\n        return None\n\n    @abstractmethod\n    def next_profile(self) -&gt; ProfileProgress | None: ...\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteType","title":"<code>BenchmarkSuiteType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines the type of suite to know how to track the progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class BenchmarkSuiteType(CaseInsensitiveStrEnum):\n    \"\"\"Determines the type of suite to know how to track the progress.\"\"\"\n\n    SINGLE_PROFILE = \"single_profile\"\n    \"\"\"An suite with a single profile run.\"\"\"\n\n    MULTI_PROFILE = \"multi_profile\"\n    \"\"\"An suite with multiple profile runs. As opposed to a sweep, more than one parameter can be varied. TBD\"\"\"\n\n    SINGLE_SWEEP = \"single_sweep\"\n    \"\"\"An suite with a single sweep over one or more varying parameters. TBD\"\"\"\n\n    MULTI_SWEEP = \"multi_sweep\"\n    \"\"\"An suite with multiple sweep runs over multiple varying parameters. TBD\"\"\"\n\n    CUSTOM = \"custom\"\n    \"\"\"User defined suite type. TBD\"\"\"\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteType.CUSTOM","title":"<code>CUSTOM = 'custom'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User defined suite type. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteType.MULTI_PROFILE","title":"<code>MULTI_PROFILE = 'multi_profile'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An suite with multiple profile runs. As opposed to a sweep, more than one parameter can be varied. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteType.MULTI_SWEEP","title":"<code>MULTI_SWEEP = 'multi_sweep'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An suite with multiple sweep runs over multiple varying parameters. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteType.SINGLE_PROFILE","title":"<code>SINGLE_PROFILE = 'single_profile'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An suite with a single profile run.</p>"},{"location":"api/#aiperf.progress.progress_models.BenchmarkSuiteType.SINGLE_SWEEP","title":"<code>SINGLE_SWEEP = 'single_sweep'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An suite with a single sweep over one or more varying parameters. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.ProfileCompletionTrigger","title":"<code>ProfileCompletionTrigger</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines how the profile completion is determined in order to know how to track the progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class ProfileCompletionTrigger(CaseInsensitiveStrEnum):\n    \"\"\"Determines how the profile completion is determined in order to know how to track the progress.\"\"\"\n\n    REQUEST_COUNT = \"request_count\"\n    \"\"\"The profile will run for a fixed number of requests.\"\"\"\n\n    TIME_BASED = \"time_based\"\n    \"\"\"The profile will run for a fixed amount of time.\"\"\"\n\n    STABILIZATION_BASED = \"stabilization_based\"\n    \"\"\"The profile will run until the metrics stabilize. TDB\"\"\"\n\n    GOODPUT_THRESHOLD = \"goodput_threshold\"\n    \"\"\"The profile will run until the goodput threshold is met. TDB\"\"\"\n\n    CUSTOM = \"custom\"\n    \"\"\"User defined trigger. TBD\"\"\"\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.ProfileCompletionTrigger.CUSTOM","title":"<code>CUSTOM = 'custom'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User defined trigger. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.ProfileCompletionTrigger.GOODPUT_THRESHOLD","title":"<code>GOODPUT_THRESHOLD = 'goodput_threshold'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The profile will run until the goodput threshold is met. TDB</p>"},{"location":"api/#aiperf.progress.progress_models.ProfileCompletionTrigger.REQUEST_COUNT","title":"<code>REQUEST_COUNT = 'request_count'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The profile will run for a fixed number of requests.</p>"},{"location":"api/#aiperf.progress.progress_models.ProfileCompletionTrigger.STABILIZATION_BASED","title":"<code>STABILIZATION_BASED = 'stabilization_based'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The profile will run until the metrics stabilize. TDB</p>"},{"location":"api/#aiperf.progress.progress_models.ProfileCompletionTrigger.TIME_BASED","title":"<code>TIME_BASED = 'time_based'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The profile will run for a fixed amount of time.</p>"},{"location":"api/#aiperf.progress.progress_models.ProfileProgress","title":"<code>ProfileProgress</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>State of the profile progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class ProfileProgress(BaseModel):\n    \"\"\"State of the profile progress.\"\"\"\n\n    profile_id: str = Field(..., description=\"The ID of the profile\")\n\n    profile_completion_trigger: ProfileCompletionTrigger = Field(\n        default=ProfileCompletionTrigger.REQUEST_COUNT,\n        description=\"The trigger of profile completion\",\n    )\n\n    start_time_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the profile run in nanoseconds. If it has not been started, this will be None.\",\n    )\n    end_time_ns: int | None = Field(\n        default=None,\n        description=\"The end time of the profile run in nanoseconds. If it has not been completed, this will be None.\",\n    )\n\n    total_expected_requests: int | None = Field(\n        default=None,\n        description=\"The total number of inference requests to be made. This will be None if the profile completion trigger is not request-based.\",\n    )\n    requests_completed: int = Field(\n        default=0,\n        description=\"The number of inference requests completed during the profile run\",\n    )\n    request_errors: int = Field(\n        default=0,\n        description=\"The total number of request errors encountered during the profile run\",\n    )\n    successful_requests: int = Field(\n        default=0,\n        description=\"The total number of successful requests completed during the profile run\",\n    )\n    requests_processed: int = Field(\n        default=0,\n        description=\"The total number of requests processed by the records manager \"\n        \"during the profile run. This can be less than the requests_completed if \"\n        \"the records manager processing requests is slower than the inference requests \"\n        \"are being made.\",\n    )\n    requests_per_second: float | None = Field(\n        default=None,\n        description=\"The number of requests completed per second during the profile run\",\n    )\n    processed_per_second: float | None = Field(\n        default=None,\n        description=\"The number of requests processed by the records manager per second during the profile run\",\n    )\n    worker_completed: dict[str, int] = Field(\n        default_factory=dict,\n        description=\"Per-worker request completion counts, keyed by worker service_id during the profile run\",\n    )\n    worker_errors: dict[str, int] = Field(\n        default_factory=dict,\n        description=\"Per-worker error counts, keyed by worker service_id during the profile run\",\n    )\n    was_cancelled: bool = Field(\n        default=False,\n        description=\"Whether the profile run was cancelled early\",\n    )\n    elapsed_time: float = Field(\n        default=0,\n        description=\"The elapsed time of the profile run in seconds\",\n    )\n    eta: float | None = Field(\n        default=None,\n        description=\"The estimated time remaining for the profile run in seconds\",\n    )\n    processing_eta: float | None = Field(\n        default=None,\n        description=\"The estimated time remaining for processing the records in seconds\",\n    )\n    records: SerializeAsAny[list[MetricResult]] = Field(\n        default_factory=list, description=\"The records of the profile results\"\n    )\n    errors_by_type: list[ErrorDetailsCount] = Field(\n        default_factory=list,\n        description=\"A list of the unique error details and their counts\",\n    )\n    is_complete: bool = Field(\n        default=False,\n        description=\"Whether the profile run is complete\",\n    )\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.ProfileSuiteProgress","title":"<code>ProfileSuiteProgress</code>","text":"<p>               Bases: <code>BenchmarkSuiteProgress</code></p> <p>State of a profile based suite with 1 or more profile runs.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class ProfileSuiteProgress(BenchmarkSuiteProgress):\n    \"\"\"State of a profile based suite with 1 or more profile runs.\"\"\"\n\n    profiles: list[ProfileProgress] = Field(\n        default_factory=list, description=\"The state of the profiles in the suite\"\n    )\n    total_profiles: int = Field(default=0, description=\"The total number of profiles\")\n    completed_profiles: int = Field(\n        default=0, description=\"The number of completed profiles\"\n    )\n    current_profile_idx: int | None = Field(\n        default=None,\n        description=\"The index of the current profile run. If it has not been started, this will be None.\",\n    )\n\n    def next_profile(self) -&gt; ProfileProgress | None:\n        if self.current_profile_idx is None:\n            self.current_profile_idx = 0\n        else:\n            self.current_profile_idx += 1\n\n        if self.current_profile_idx &gt;= len(self.profiles):\n            return None\n\n        return self.profiles[self.current_profile_idx]\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepCompletionTrigger","title":"<code>SweepCompletionTrigger</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines how the sweep completion is determined in order to know how to track the progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class SweepCompletionTrigger(CaseInsensitiveStrEnum):\n    \"\"\"Determines how the sweep completion is determined in order to know how to track the progress.\"\"\"\n\n    COMPLETED_PROFILES = \"completed_profiles\"\n    \"\"\"The sweep will run until all profiles are completed.\"\"\"\n\n    STABILIZATION_BASED = \"stabilization_based\"\n    \"\"\"The sweep will run until the metrics stabilize. TDB\"\"\"\n\n    GOODPUT_THRESHOLD = \"goodput_threshold\"\n    \"\"\"The sweep will run until the goodput threshold is met. TDB\"\"\"\n\n    CUSTOM = \"custom\"\n    \"\"\"User defined trigger. TBD\"\"\"\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepCompletionTrigger.COMPLETED_PROFILES","title":"<code>COMPLETED_PROFILES = 'completed_profiles'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will run until all profiles are completed.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepCompletionTrigger.CUSTOM","title":"<code>CUSTOM = 'custom'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User defined trigger. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.SweepCompletionTrigger.GOODPUT_THRESHOLD","title":"<code>GOODPUT_THRESHOLD = 'goodput_threshold'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will run until the goodput threshold is met. TDB</p>"},{"location":"api/#aiperf.progress.progress_models.SweepCompletionTrigger.STABILIZATION_BASED","title":"<code>STABILIZATION_BASED = 'stabilization_based'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will run until the metrics stabilize. TDB</p>"},{"location":"api/#aiperf.progress.progress_models.SweepMultiParamOrder","title":"<code>SweepMultiParamOrder</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines the order in which the sweep parameters are tested for a multi-parameter sweep. This is only applicable for multi-parameter sweeps.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class SweepMultiParamOrder(CaseInsensitiveStrEnum):\n    \"\"\"Determines the order in which the sweep parameters are tested for a multi-parameter sweep.\n    This is only applicable for multi-parameter sweeps.\"\"\"\n\n    DEPTH_FIRST = \"depth_first\"\n    \"\"\"The parameters are tested in depth-first order.\"\"\"\n\n    BREADTH_FIRST = \"breadth_first\"\n    \"\"\"The parameters are tested in breadth-first order.\"\"\"\n\n    RANDOM = \"random\"\n    \"\"\"The parameters are tested in random order. TBD\"\"\"\n\n    CUSTOM = \"custom\"\n    \"\"\"User defined order. TBD\"\"\"\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepMultiParamOrder.BREADTH_FIRST","title":"<code>BREADTH_FIRST = 'breadth_first'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameters are tested in breadth-first order.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepMultiParamOrder.CUSTOM","title":"<code>CUSTOM = 'custom'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User defined order. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.SweepMultiParamOrder.DEPTH_FIRST","title":"<code>DEPTH_FIRST = 'depth_first'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameters are tested in depth-first order.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepMultiParamOrder.RANDOM","title":"<code>RANDOM = 'random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameters are tested in random order. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamOrder","title":"<code>SweepParamOrder</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines the order in which the sweep parameters are tested.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class SweepParamOrder(CaseInsensitiveStrEnum):\n    \"\"\"Determines the order in which the sweep parameters are tested.\"\"\"\n\n    ASCENDING = \"ascending\"\n    \"\"\"The parameters are tested in ascending order.\"\"\"\n\n    DESCENDING = \"descending\"\n    \"\"\"The parameters are tested in descending order.\"\"\"\n\n    RANDOM = \"random\"\n    \"\"\"The parameters are tested in random order. TBD\"\"\"\n\n    CUSTOM = \"custom\"\n    \"\"\"User defined order. TBD\"\"\"\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepParamOrder.ASCENDING","title":"<code>ASCENDING = 'ascending'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameters are tested in ascending order.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamOrder.CUSTOM","title":"<code>CUSTOM = 'custom'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User defined order. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamOrder.DESCENDING","title":"<code>DESCENDING = 'descending'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameters are tested in descending order.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamOrder.RANDOM","title":"<code>RANDOM = 'random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameters are tested in random order. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamType","title":"<code>SweepParamType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Determines the type of sweep parameter.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class SweepParamType(CaseInsensitiveStrEnum):\n    \"\"\"Determines the type of sweep parameter.\"\"\"\n\n    INT = \"int\"\n    \"\"\"The parameter is an integer.\"\"\"\n\n    FLOAT = \"float\"\n    \"\"\"The parameter is a float.\"\"\"\n\n    STRING = \"string\"\n    \"\"\"The parameter is a string.\"\"\"\n\n    BOOLEAN = \"boolean\"\n    \"\"\"The parameter is a boolean.\"\"\"\n\n    CUSTOM = \"custom\"\n    \"\"\"User defined parameter type. TBD\"\"\"\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepParamType.BOOLEAN","title":"<code>BOOLEAN = 'boolean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameter is a boolean.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamType.CUSTOM","title":"<code>CUSTOM = 'custom'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User defined parameter type. TBD</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamType.FLOAT","title":"<code>FLOAT = 'float'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameter is a float.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamType.INT","title":"<code>INT = 'int'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameter is an integer.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepParamType.STRING","title":"<code>STRING = 'string'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The parameter is a string.</p>"},{"location":"api/#aiperf.progress.progress_models.SweepProgress","title":"<code>SweepProgress</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>State of the sweep progress.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class SweepProgress(BaseModel):\n    \"\"\"State of the sweep progress.\"\"\"\n\n    sweep_id: str = Field(..., description=\"The ID of the current sweep\")\n    sweep_completion_trigger: SweepCompletionTrigger = Field(\n        default=SweepCompletionTrigger.COMPLETED_PROFILES,\n        description=\"The trigger of sweep completion\",\n    )\n    profiles: list[ProfileProgress] = Field(\n        default_factory=list, description=\"The state of the profiles in the sweep\"\n    )\n    current_profile_idx: int | None = Field(\n        default=None,\n        description=\"The index of the current profile. If it has not been started, this will be None.\",\n    )\n    completed_profiles: int = Field(\n        default=0, description=\"The number of completed profiles in the sweep\"\n    )\n    start_time_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the sweep in nanoseconds. If it has not been started, this will be None.\",\n    )\n    end_time_ns: int | None = Field(\n        default=None,\n        description=\"The end time of the sweep in nanoseconds. If it has not been completed, this will be None.\",\n    )\n    was_cancelled: bool = Field(\n        default=False,\n        description=\"Whether the sweep was cancelled early\",\n    )\n\n    @property\n    def current_profile(self) -&gt; ProfileProgress | None:\n        if self.current_profile_idx is None:\n            return None\n        return self.profiles[self.current_profile_idx]\n\n    def next_profile(self) -&gt; ProfileProgress | None:\n        if self.current_profile_idx is None:\n            self.current_profile_idx = 0\n        else:\n            self.current_profile_idx += 1\n\n        if self.current_profile_idx &gt;= len(self.profiles):\n            return None\n\n        return self.profiles[self.current_profile_idx]\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepSuiteProgress","title":"<code>SweepSuiteProgress</code>","text":"<p>               Bases: <code>BenchmarkSuiteProgress</code></p> <p>State of a sweep based suite with 1 or more sweep runs.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>class SweepSuiteProgress(BenchmarkSuiteProgress):\n    \"\"\"State of a sweep based suite with 1 or more sweep runs.\"\"\"\n\n    sweeps: list[SweepProgress] = Field(\n        default_factory=list, description=\"The state of the sweeps in the suite\"\n    )\n    total_sweeps: int = Field(default=0, description=\"The total number of sweeps\")\n    completed_sweeps: int = Field(\n        default=0, description=\"The number of completed sweeps\"\n    )\n    current_sweep_idx: int | None = Field(\n        default=None,\n        description=\"The index of the current sweep. If it has not been started, this will be None.\",\n    )\n\n    def next_profile(self) -&gt; ProfileProgress | None:\n        \"\"\"Get the next profile to run.\n\n        Returns:\n            The next profile to run, or None if there are no more profiles to run.\n        \"\"\"\n        if self.current_sweep is None or self.current_sweep.current_profile_idx is None:\n            next_sweep = self.next_sweep()\n            if next_sweep is None:\n                return None\n            return next_sweep.next_profile()\n\n        # Try to get the next profile in the current sweep\n        next_profile = self.current_sweep.next_profile()\n        if next_profile is not None:\n            return next_profile\n\n        # If no more profiles in current sweep, move to next sweep\n        next_sweep = self.next_sweep()\n        if next_sweep is None:\n            return None\n        return next_sweep.next_profile()\n\n    def next_sweep(self) -&gt; SweepProgress | None:\n        \"\"\"Get the next sweep to run.\n\n        Returns:\n            The next sweep to run, or None if there are no more sweeps to run.\n        \"\"\"\n        if self.current_sweep_idx is None:\n            self.current_sweep_idx = 0\n            return self.sweeps[0]\n        if self.current_sweep_idx &gt;= len(self.sweeps) - 1:\n            return None\n        self.current_sweep_idx += 1\n        return self.sweeps[self.current_sweep_idx]\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepSuiteProgress.next_profile","title":"<code>next_profile()</code>","text":"<p>Get the next profile to run.</p> <p>Returns:</p> Type Description <code>ProfileProgress | None</code> <p>The next profile to run, or None if there are no more profiles to run.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>def next_profile(self) -&gt; ProfileProgress | None:\n    \"\"\"Get the next profile to run.\n\n    Returns:\n        The next profile to run, or None if there are no more profiles to run.\n    \"\"\"\n    if self.current_sweep is None or self.current_sweep.current_profile_idx is None:\n        next_sweep = self.next_sweep()\n        if next_sweep is None:\n            return None\n        return next_sweep.next_profile()\n\n    # Try to get the next profile in the current sweep\n    next_profile = self.current_sweep.next_profile()\n    if next_profile is not None:\n        return next_profile\n\n    # If no more profiles in current sweep, move to next sweep\n    next_sweep = self.next_sweep()\n    if next_sweep is None:\n        return None\n    return next_sweep.next_profile()\n</code></pre>"},{"location":"api/#aiperf.progress.progress_models.SweepSuiteProgress.next_sweep","title":"<code>next_sweep()</code>","text":"<p>Get the next sweep to run.</p> <p>Returns:</p> Type Description <code>SweepProgress | None</code> <p>The next sweep to run, or None if there are no more sweeps to run.</p> Source code in <code>aiperf/progress/progress_models.py</code> <pre><code>def next_sweep(self) -&gt; SweepProgress | None:\n    \"\"\"Get the next sweep to run.\n\n    Returns:\n        The next sweep to run, or None if there are no more sweeps to run.\n    \"\"\"\n    if self.current_sweep_idx is None:\n        self.current_sweep_idx = 0\n        return self.sweeps[0]\n    if self.current_sweep_idx &gt;= len(self.sweeps) - 1:\n        return None\n    self.current_sweep_idx += 1\n    return self.sweeps[self.current_sweep_idx]\n</code></pre>"},{"location":"api/#aiperfservicesbase_component_service","title":"aiperf.services.base_component_service","text":""},{"location":"api/#aiperf.services.base_component_service.BaseComponentService","title":"<code>BaseComponentService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Base class for all Component services.</p> <p>This class provides a common interface for all Component services in the AIPerf framework such as the Timing Manager, Dataset Manager, etc.</p> <p>It extends the BaseService by adding heartbeat and registration functionality, as well as publishing the current state of the service to the system controller.</p> Source code in <code>aiperf/services/base_component_service.py</code> <pre><code>@implements_protocol(ServiceProtocol)\nclass BaseComponentService(BaseService):\n    \"\"\"Base class for all Component services.\n\n    This class provides a common interface for all Component services in the AIPerf\n    framework such as the Timing Manager, Dataset Manager, etc.\n\n    It extends the BaseService by adding heartbeat and registration functionality, as well as\n    publishing the current state of the service to the system controller.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            **kwargs,\n        )\n\n    @background_task(\n        interval=lambda self: self.service_config.heartbeat_interval_seconds,\n        immediate=False,\n    )\n    async def _heartbeat_task(self) -&gt; None:\n        \"\"\"Send a heartbeat notification to the system controller.\"\"\"\n        await self.publish(\n            HeartbeatMessage(\n                service_id=self.service_id,\n                service_type=self.service_type,\n                state=self.state,\n            )\n        )\n\n    @on_start\n    async def _register_service(self) -&gt; None:\n        \"\"\"Publish a registration request to the system controller.\n\n        This method should be called after the service has been initialized and is\n        ready to start processing messages.\n        \"\"\"\n        self.debug(\n            lambda: f\"Attempting to register service {self} ({self.service_id}) with system controller\"\n        )\n        await self.publish(\n            RegistrationMessage(\n                service_id=self.service_id,\n                service_type=self.service_type,\n                state=self.state,\n            )\n        )\n\n    @on_state_change\n    async def _on_state_change(\n        self, old_state: LifecycleState, new_state: LifecycleState\n    ) -&gt; None:\n        \"\"\"Action to take when the service state is set.\n\n        This method will also publish the status message to the status message_type if the\n        communications are initialized.\n        \"\"\"\n        if self.stop_requested:\n            return\n        if not self.comms.was_initialized:\n            return\n        await self.publish(\n            StatusMessage(\n                service_id=self.service_id,\n                service_type=self.service_type,\n                state=new_state,\n            )\n        )\n\n    @on_command(CommandType.SHUTDOWN)\n    async def _on_shutdown_command(self, message: CommandMessage) -&gt; None:\n        self.debug(f\"Received shutdown command: {message}, {self.service_id}\")\n        try:\n            await self.stop()\n        except Exception as e:\n            self.warning(\n                f\"Failed to stop service {self} ({self.service_id}) after receiving shutdown command: {e}. Killing.\"\n            )\n            await self._kill()\n        raise asyncio.CancelledError()\n</code></pre>"},{"location":"api/#aiperfservicesbase_service","title":"aiperf.services.base_service","text":""},{"location":"api/#aiperf.services.base_service.BaseService","title":"<code>BaseService</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code>, <code>ABC</code></p> <p>Base class for all AIPerf services, providing common functionality for communication, state management, and lifecycle operations. This class inherits from the MessageBusClientMixin, which provides the message bus client functionality.</p> <p>This class provides the foundation for implementing the various services of the AIPerf system. Some of the abstract methods are implemented here, while others are still required to be implemented by derived classes.</p> Source code in <code>aiperf/services/base_service.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_COMMAND)\nclass BaseService(MessageBusClientMixin, ABC):\n    \"\"\"Base class for all AIPerf services, providing common functionality for\n    communication, state management, and lifecycle operations.\n    This class inherits from the MessageBusClientMixin, which provides the\n    message bus client functionality.\n\n    This class provides the foundation for implementing the various services of the\n    AIPerf system. Some of the abstract methods are implemented here, while others\n    are still required to be implemented by derived classes.\n    \"\"\"\n\n    service_type: ClassVar[ServiceTypeT]\n    \"\"\"The type of service this class implements. This is set by the ServiceFactory.register decorator.\"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        self.service_config = service_config\n        self.user_config = user_config\n        self.service_id = service_id or f\"{self.service_type}_{uuid.uuid4().hex[:8]}\"\n        super().__init__(\n            service_id=self.service_id,\n            id=self.service_id,\n            service_config=self.service_config,\n            user_config=self.user_config,\n            **kwargs,\n        )\n        self.debug(\n            lambda: f\"__init__ {self.service_type} service (id: {self.service_id})\"\n        )\n        self._set_process_title()\n\n    def _set_process_title(self) -&gt; None:\n        try:\n            import setproctitle\n\n            setproctitle.setproctitle(f\"aiperf {self.service_id}\")\n        except Exception:\n            # setproctitle is not available on all platforms, so we ignore the error\n            self.debug(\"Failed to set process title, ignoring\")\n\n    def _service_error(self, message: str) -&gt; ServiceError:\n        return ServiceError(\n            message=message,\n            service_type=self.service_type,\n            service_id=self.service_id,\n        )\n\n    @on_message(\n        lambda self: {\n            MessageType.COMMAND,\n            f\"{MessageType.COMMAND}.{self.service_type}\",\n            f\"{MessageType.COMMAND}.{self.service_id}\",\n        }\n    )\n    async def _process_command_message(self, message: CommandMessage) -&gt; None:\n        \"\"\"Process a command message received from the controller, and forward it to the appropriate handler.\n        Wait for the handler to complete and publish the response, or handle the error and publish the failure response.\n        \"\"\"\n        if message.service_id == self.service_id:\n            self.debug(\n                lambda: f\"Received command message from self: {message}. Ignoring.\"\n            )\n            return\n\n        self.debug(lambda: f\"Received command message: {message}\")\n\n        # Go through the hooks and find the first one that matches the command type.\n        # Currently, we only support a single handler per command type, so we break out of the loop after the first one.\n        # TODO: Do we want/need to add support for multiple handlers per command type?\n        for hook in self.get_hooks(AIPerfHook.ON_COMMAND):\n            if isinstance(hook.params, Iterable) and message.command in hook.params:\n                try:\n                    response = await hook.func(message)\n                    if response is None:\n                        # If there is no data to send back, just send an acknowledged response.\n                        await self.publish(\n                            CommandAcknowledgedResponse.from_command_message(\n                                message, self.service_id\n                            )\n                        )\n                        return\n\n                    await self.publish(\n                        CommandSuccessResponse.from_command_message(\n                            message, self.service_id, response\n                        )\n                    )\n                except Exception as e:\n                    self.exception(\n                        f\"Failed to handle command {message.command} with hook {hook}: {e}\"\n                    )\n                    await self.publish(\n                        CommandErrorResponse.from_command_message(\n                            message,\n                            self.service_id,\n                            ErrorDetails.from_exception(e),\n                        )\n                    )\n\n                # Only one handler per command type, so return after the first handler.\n                return\n\n        # If we reach here, no handler was found for the command, so we publish an unhandled response.\n        await self.publish(\n            CommandUnhandledResponse.from_command_message(message, self.service_id)\n        )\n\n    @on_command(CommandType.SHUTDOWN)\n    async def _on_shutdown_command(self, message: CommandMessage) -&gt; None:\n        self.debug(f\"Received shutdown command from {message.service_id}\")\n        # Send an acknowledged response back to the sender, because we won't be able to send it after we stop.\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n\n        try:\n            await self.stop()\n        except Exception as e:\n            self.exception(\n                f\"Failed to stop service {self} ({self.service_id}) after receiving shutdown command: {e}. Killing.\"\n            )\n            await self._kill()\n\n    async def stop(self) -&gt; None:\n        \"\"\"This overrides the base class stop method to handle the case where the service is already stopping.\n        In this case, we need to kill the process to be safe.\"\"\"\n        if self.stop_requested:\n            self.error(f\"Attempted to stop {self} in state {self.state}. Killing.\")\n            await self._kill()\n            return\n        await super().stop()\n\n    async def _kill(self) -&gt; None:\n        \"\"\"Kill the lifecycle. This is used when the lifecycle is requested to stop, but is already in a stopping state.\n        This is a last resort to ensure that the lifecycle is stopped.\n        \"\"\"\n        await self._set_state(LifecycleState.FAILED)\n        self.error(lambda: f\"Killing {self}\")\n        self.stop_requested = True\n        self.stopped_event.set()\n        # TODO: This is a hack to ensure that the process is killed.\n        #       We should find a better way to do this.\n        os.kill(os.getpid(), signal.SIGKILL)\n        raise asyncio.CancelledError(f\"Killed {self}\")\n</code></pre>"},{"location":"api/#aiperf.services.base_service.BaseService.service_type","title":"<code>service_type</code>  <code>class-attribute</code>","text":"<p>The type of service this class implements. This is set by the ServiceFactory.register decorator.</p>"},{"location":"api/#aiperf.services.base_service.BaseService.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>This overrides the base class stop method to handle the case where the service is already stopping. In this case, we need to kill the process to be safe.</p> Source code in <code>aiperf/services/base_service.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"This overrides the base class stop method to handle the case where the service is already stopping.\n    In this case, we need to kill the process to be safe.\"\"\"\n    if self.stop_requested:\n        self.error(f\"Attempted to stop {self} in state {self.state}. Killing.\")\n        await self._kill()\n        return\n    await super().stop()\n</code></pre>"},{"location":"api/#aiperfservicesdatasetcomposerbase","title":"aiperf.services.dataset.composer.base","text":""},{"location":"api/#aiperf.services.dataset.composer.base.BaseDatasetComposer","title":"<code>BaseDatasetComposer</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>aiperf/services/dataset/composer/base.py</code> <pre><code>class BaseDatasetComposer(ABC):\n    def __init__(self, config: InputConfig, tokenizer: Tokenizer):\n        self.config = config\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n        self.prompt_generator = PromptGenerator(config.prompt, tokenizer)\n        self.image_generator = ImageGenerator(config.image)\n        self.audio_generator = AudioGenerator(config.audio)\n\n    @abstractmethod\n    def create_dataset(self) -&gt; list[Conversation]:\n        \"\"\"\n        Create a set of conversation objects from the given configuration.\n\n        Returns:\n            list[Conversation]: A list of conversation objects.\n        \"\"\"\n        ...\n\n    @property\n    def prefix_prompt_enabled(self) -&gt; bool:\n        return self.config.prompt.prefix_prompt.length &gt; 0\n</code></pre>"},{"location":"api/#aiperf.services.dataset.composer.base.BaseDatasetComposer.create_dataset","title":"<code>create_dataset()</code>  <code>abstractmethod</code>","text":"<p>Create a set of conversation objects from the given configuration.</p> <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of conversation objects.</p> Source code in <code>aiperf/services/dataset/composer/base.py</code> <pre><code>@abstractmethod\ndef create_dataset(self) -&gt; list[Conversation]:\n    \"\"\"\n    Create a set of conversation objects from the given configuration.\n\n    Returns:\n        list[Conversation]: A list of conversation objects.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperfservicesdatasetcomposercustom","title":"aiperf.services.dataset.composer.custom","text":""},{"location":"api/#aiperf.services.dataset.composer.custom.CustomDatasetComposer","title":"<code>CustomDatasetComposer</code>","text":"<p>               Bases: <code>BaseDatasetComposer</code></p> Source code in <code>aiperf/services/dataset/composer/custom.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ComposerFactory.register(ComposerType.CUSTOM)\nclass CustomDatasetComposer(BaseDatasetComposer):\n    def __init__(self, config: InputConfig, tokenizer: Tokenizer):\n        super().__init__(config, tokenizer)\n\n    def create_dataset(self) -&gt; list[Conversation]:\n        \"\"\"Create conversations from a file or directory.\n\n        Returns:\n            list[Conversation]: A list of conversation objects.\n        \"\"\"\n        # TODO: (future) for K8s, we need to transfer file data from SC (across node)\n        utils.check_file_exists(self.config.file)\n\n        self._create_loader_instance(self.config.custom_dataset_type)\n        dataset = self.loader.load_dataset()\n        conversations = self.loader.convert_to_conversations(dataset)\n        return conversations\n\n    def _create_loader_instance(self, dataset_type: CustomDatasetType) -&gt; None:\n        \"\"\"Initializes the dataset loader based on the custom dataset type.\n\n        Args:\n            dataset_type: The type of custom dataset to create.\n        \"\"\"\n        kwargs = {\"filename\": self.config.file}\n        if dataset_type == CustomDatasetType.MOONCAKE_TRACE:\n            kwargs[\"prompt_generator\"] = self.prompt_generator\n\n        self.loader = CustomDatasetFactory.create_instance(dataset_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.services.dataset.composer.custom.CustomDatasetComposer.create_dataset","title":"<code>create_dataset()</code>","text":"<p>Create conversations from a file or directory.</p> <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of conversation objects.</p> Source code in <code>aiperf/services/dataset/composer/custom.py</code> <pre><code>def create_dataset(self) -&gt; list[Conversation]:\n    \"\"\"Create conversations from a file or directory.\n\n    Returns:\n        list[Conversation]: A list of conversation objects.\n    \"\"\"\n    # TODO: (future) for K8s, we need to transfer file data from SC (across node)\n    utils.check_file_exists(self.config.file)\n\n    self._create_loader_instance(self.config.custom_dataset_type)\n    dataset = self.loader.load_dataset()\n    conversations = self.loader.convert_to_conversations(dataset)\n    return conversations\n</code></pre>"},{"location":"api/#aiperfservicesdatasetcomposersynthetic","title":"aiperf.services.dataset.composer.synthetic","text":""},{"location":"api/#aiperf.services.dataset.composer.synthetic.SyntheticDatasetComposer","title":"<code>SyntheticDatasetComposer</code>","text":"<p>               Bases: <code>BaseDatasetComposer</code></p> Source code in <code>aiperf/services/dataset/composer/synthetic.py</code> <pre><code>@ComposerFactory.register(ComposerType.SYNTHETIC)\nclass SyntheticDatasetComposer(BaseDatasetComposer):\n    def __init__(self, config: InputConfig, tokenizer: Tokenizer):\n        super().__init__(config, tokenizer)\n\n        if (\n            not self.include_prompt\n            and not self.include_image\n            and not self.include_audio\n        ):\n            raise ValueError(\n                \"All synthetic data are disabled. \"\n                \"Please enable at least one of prompt, image, or audio by \"\n                \"setting the mean to a positive value.\"\n            )\n\n    def create_dataset(self) -&gt; list[Conversation]:\n        \"\"\"Create a synthetic conversation dataset from the given configuration.\n\n        It generates a set of conversations with a varying number of turns,\n        where each turn contains synthetic text, image, and audio payloads.\n\n        Returns:\n            list[Conversation]: A list of conversation objects.\n        \"\"\"\n        conversations = []\n        for _ in range(self.config.conversation.num):\n            conversation = Conversation(session_id=str(uuid.uuid4()))\n\n            num_turns = utils.sample_positive_normal_integer(\n                self.config.conversation.turn.mean,\n                self.config.conversation.turn.stddev,\n            )\n            self.logger.debug(\"Creating conversation with %d turns\", num_turns)\n\n            for turn_idx in range(num_turns):\n                turn = self._create_turn(is_first=(turn_idx == 0))\n                conversation.turns.append(turn)\n            conversations.append(conversation)\n        return conversations\n\n    def _create_turn(self, is_first: bool) -&gt; Turn:\n        \"\"\"Create a turn object that contains synthetic payloads to send.\n\n        It generates multi-modal data (e.g. text, image, audio) using synthetic\n        generators and also the delay between turns.\n\n        Args:\n            is_first: Whether the turn is the first turn in the conversation.\n\n        Returns:\n            Turn: A dataset representation of a single turn.\n        \"\"\"\n        turn = Turn()\n\n        if self.include_prompt:\n            turn.texts.append(self._generate_text_payloads(is_first))\n        if self.include_image:\n            turn.images.append(self._generate_image_payloads())\n        if self.include_audio:\n            turn.audios.append(self._generate_audio_payloads())\n\n        # Add randomized delays between each turn. Skip if first turn.\n        if not is_first:\n            turn.delay = utils.sample_positive_normal_integer(\n                self.config.conversation.turn.delay.mean,\n                self.config.conversation.turn.delay.stddev,\n            )\n\n        if not turn.texts and not turn.images and not turn.audios:\n            self.logger.warning(\n                \"There were no synthetic payloads generated. \"\n                \"Please enable at least one of prompt, image, or audio by \"\n                \"setting the mean to a positive value.\"\n            )\n\n        return turn\n\n    def _generate_text_payloads(self, is_first: bool) -&gt; Text:\n        \"\"\"Generate synthetic text payloads.\n\n        If the turn is the first turn in the conversation, it could add a prefix prompt\n        to the prompt.\n\n        Args:\n            is_first: Whether the turn is the first turn in the conversation.\n\n        Returns:\n            Text: A text payload object.\n        \"\"\"\n        text = Text(name=\"text\")\n        for _ in range(self.config.prompt.batch_size):\n            prompt = self.prompt_generator.generate(\n                mean=self.config.prompt.input_tokens.mean,\n                stddev=self.config.prompt.input_tokens.stddev,\n            )\n\n            if self.prefix_prompt_enabled and is_first:\n                # TODO: Rename\n                prefix_prompt = self.prompt_generator.get_random_prefix_prompt()\n                prompt = f\"{prefix_prompt} {prompt}\"\n\n            text.contents.append(prompt)\n        return text\n\n    def _generate_image_payloads(self) -&gt; Image:\n        \"\"\"\n        Generate synthetic images if the image width and height are specified.\n\n        Returns:\n            Image: An image payload object.\n        \"\"\"\n        image = Image(name=\"image_url\")\n        for _ in range(self.config.image.batch_size):\n            data = self.image_generator.generate()\n            image.contents.append(data)\n        return image\n\n    def _generate_audio_payloads(self) -&gt; Audio:\n        \"\"\"\n        Generate synthetic audios if the audio length is specified.\n\n        Returns:\n            Audio: An audio payload object.\n        \"\"\"\n        audio = Audio(name=\"input_audio\")\n        for _ in range(self.config.audio.batch_size):\n            data = self.audio_generator.generate()\n            audio.contents.append(data)\n        return audio\n\n    @property\n    def include_prompt(self) -&gt; bool:\n        return self.config.prompt.input_tokens.mean &gt; 0\n\n    @property\n    def include_image(self) -&gt; bool:\n        return self.config.image.width.mean &gt; 0 and self.config.image.height.mean &gt; 0\n\n    @property\n    def include_audio(self) -&gt; bool:\n        return self.config.audio.length.mean &gt; 0\n</code></pre>"},{"location":"api/#aiperf.services.dataset.composer.synthetic.SyntheticDatasetComposer.create_dataset","title":"<code>create_dataset()</code>","text":"<p>Create a synthetic conversation dataset from the given configuration.</p> <p>It generates a set of conversations with a varying number of turns, where each turn contains synthetic text, image, and audio payloads.</p> <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of conversation objects.</p> Source code in <code>aiperf/services/dataset/composer/synthetic.py</code> <pre><code>def create_dataset(self) -&gt; list[Conversation]:\n    \"\"\"Create a synthetic conversation dataset from the given configuration.\n\n    It generates a set of conversations with a varying number of turns,\n    where each turn contains synthetic text, image, and audio payloads.\n\n    Returns:\n        list[Conversation]: A list of conversation objects.\n    \"\"\"\n    conversations = []\n    for _ in range(self.config.conversation.num):\n        conversation = Conversation(session_id=str(uuid.uuid4()))\n\n        num_turns = utils.sample_positive_normal_integer(\n            self.config.conversation.turn.mean,\n            self.config.conversation.turn.stddev,\n        )\n        self.logger.debug(\"Creating conversation with %d turns\", num_turns)\n\n        for turn_idx in range(num_turns):\n            turn = self._create_turn(is_first=(turn_idx == 0))\n            conversation.turns.append(turn)\n        conversations.append(conversation)\n    return conversations\n</code></pre>"},{"location":"api/#aiperfservicesdatasetdataset_manager","title":"aiperf.services.dataset.dataset_manager","text":""},{"location":"api/#aiperf.services.dataset.dataset_manager.DatasetManager","title":"<code>DatasetManager</code>","text":"<p>               Bases: <code>ReplyClientMixin</code>, <code>BaseComponentService</code></p> <p>The DatasetManager primary responsibility is to manage the data generation or acquisition. For synthetic generation, it contains the code to generate the prompts or tokens. It will have an API for dataset acquisition of a dataset if available in a remote repository or database.</p> Source code in <code>aiperf/services/dataset/dataset_manager.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ServiceFactory.register(ServiceType.DATASET_MANAGER)\nclass DatasetManager(ReplyClientMixin, BaseComponentService):\n    \"\"\"\n    The DatasetManager primary responsibility is to manage the data generation or acquisition.\n    For synthetic generation, it contains the code to generate the prompts or tokens.\n    It will have an API for dataset acquisition of a dataset if available in a remote repository or database.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            reply_client_address=CommAddress.DATASET_MANAGER_PROXY_BACKEND,\n            reply_client_bind=False,\n        )\n        self.debug(\"Dataset manager __init__\")\n        self.user_config = user_config\n        self.tokenizer: Tokenizer | None = None\n        self.dataset: dict[str, Conversation] = {}  # session ID -&gt; Conversation mapping\n        self.dataset_configured = asyncio.Event()\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize dataset manager-specific components.\"\"\"\n        self.debug(lambda: f\"Initializing dataset manager {self.service_id}\")\n        self.dataset_configured.clear()\n        await self._configure_dataset()\n        self.debug(lambda: f\"Dataset manager {self.service_id} initialized\")\n\n    async def _configure_dataset(self) -&gt; None:\n        if self.user_config is None:\n            raise self._service_error(\"User config is required for dataset manager\")\n\n        if self.user_config.input.file:\n            composer_type = ComposerType.CUSTOM\n            self.debug(\n                lambda: f\"Detected input file '{self.user_config.input.file}'. Setting the composer type to {ComposerType.CUSTOM}.\"\n            )\n        else:\n            composer_type = ComposerType.SYNTHETIC\n            self.debug(\n                lambda: f\"No input file detected. Setting the composer type to {ComposerType.SYNTHETIC}.\"\n            )\n\n        tokenizer_name = self.user_config.tokenizer.name\n        if tokenizer_name is None:\n            # TODO: What do we do if there are multiple models?\n            # How will we know which tokenizer to use?\n            tokenizer_name = self.user_config.model_names[0]\n\n        tokenizer = Tokenizer.from_pretrained(\n            tokenizer_name,\n            trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n            revision=self.user_config.tokenizer.revision,\n        )\n        composer = ComposerFactory.create_instance(\n            composer_type,\n            config=self.user_config.input,\n            tokenizer=tokenizer,\n        )\n        conversations = composer.create_dataset()\n        self.dataset = {conv.session_id: conv for conv in conversations}\n\n        self.dataset_configured.set()\n        await self.publish(\n            DatasetConfiguredNotification(\n                service_id=self.service_id,\n            ),\n        )\n\n    @on_request(MessageType.CONVERSATION_REQUEST)\n    async def _handle_conversation_request(\n        self, message: ConversationRequestMessage\n    ) -&gt; ConversationResponseMessage:\n        \"\"\"Handle a conversation request.\"\"\"\n        self.debug(lambda: f\"Handling conversation request: {message}\")\n\n        # Wait for the dataset to be configured if it is not already\n        if not self.dataset_configured.is_set():\n            self.debug(\n                \"Dataset not configured. Waiting for dataset to be configured...\"\n            )\n            await asyncio.wait_for(\n                self.dataset_configured.wait(), timeout=DATASET_CONFIGURATION_TIMEOUT\n            )\n\n        if not self.dataset:\n            raise self._service_error(\n                \"Dataset is empty and must be configured before handling requests.\",\n            )\n\n        if message.conversation_id is None:\n            return self._return_any_conversation(\n                request_id=message.request_id,\n            )\n        else:\n            return self._return_conversation_by_id(\n                request_id=message.request_id,\n                conversation_id=message.conversation_id,\n            )\n\n    def _return_any_conversation(\n        self, request_id: str | None\n    ) -&gt; ConversationResponseMessage:\n        \"\"\"Return any conversation from the dataset based on the user specified method.\"\"\"\n\n        # TODO: Implement the user specified method (random, round robin, etc.)\n        conversation = random.choice(list(self.dataset.values()))\n        self.debug(lambda: f\"Sending random conversation response: {conversation}\")\n        return ConversationResponseMessage(\n            service_id=self.service_id,\n            request_id=request_id,\n            conversation=conversation,\n        )\n\n    def _return_conversation_by_id(\n        self, request_id: str | None, conversation_id: str\n    ) -&gt; ConversationResponseMessage:\n        \"\"\"Return a conversation if it exists, otherwise raise an error.\"\"\"\n\n        if conversation_id not in self.dataset:\n            raise self._service_error(\n                f\"Conversation {conversation_id} not found in dataset.\",\n            )\n\n        conversation = self.dataset[conversation_id]\n        self.debug(lambda: f\"Sending conversation response: {conversation}\")\n        return ConversationResponseMessage(\n            service_id=self.service_id,\n            request_id=request_id,\n            conversation=conversation,\n        )\n\n    @on_request(MessageType.CONVERSATION_TURN_REQUEST)\n    async def _handle_conversation_turn_request(\n        self, message: ConversationTurnRequestMessage\n    ) -&gt; ConversationTurnResponseMessage:\n        \"\"\"Handle a turn request.\"\"\"\n        self.debug(lambda: f\"Handling turn request: {message}\")\n\n        if message.conversation_id not in self.dataset:\n            raise self._service_error(\n                f\"Conversation {message.conversation_id} not found in dataset.\",\n            )\n\n        conversation = self.dataset[message.conversation_id]\n        if message.turn_index &gt;= len(conversation.turns):\n            raise self._service_error(\n                f\"Turn index {message.turn_index} is out of range for conversation {message.conversation_id}.\",\n            )\n\n        turn = conversation.turns[message.turn_index]\n\n        self.debug(lambda: f\"Sending turn response: {turn}\")\n        return ConversationTurnResponseMessage(\n            service_id=self.service_id,\n            request_id=message.request_id,\n            turn=turn,\n        )\n\n    @on_request(MessageType.DATASET_TIMING_REQUEST)\n    async def _handle_dataset_timing_request(\n        self, message: DatasetTimingRequest\n    ) -&gt; DatasetTimingResponse:\n        \"\"\"Handle a dataset timing request.\"\"\"\n        self.debug(lambda: f\"Handling dataset timing request: {message}\")\n        if not self.dataset:\n            raise self._service_error(\n                \"Dataset is empty and must be configured before handling timing requests.\",\n            )\n\n        timing_dataset = []\n        for conversation_id, conversation in self.dataset.items():\n            for turn in conversation.turns:\n                timing_dataset.append((turn.timestamp, conversation_id))\n\n        return DatasetTimingResponse(\n            service_id=self.service_id,\n            request_id=message.request_id,\n            timing_data=timing_dataset,\n        )\n</code></pre>"},{"location":"api/#aiperf.services.dataset.dataset_manager.main","title":"<code>main()</code>","text":"<p>Main entry point for the dataset manager.</p> Source code in <code>aiperf/services/dataset/dataset_manager.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the dataset manager.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(DatasetManager)\n</code></pre>"},{"location":"api/#aiperfservicesdatasetgeneratoraudio","title":"aiperf.services.dataset.generator.audio","text":""},{"location":"api/#aiperf.services.dataset.generator.audio.AudioGenerator","title":"<code>AudioGenerator</code>","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>A class for generating synthetic audio data.</p> <p>This class provides methods to create audio samples with specified characteristics such as format (WAV, MP3), length, sampling rate, bit depth, and number of channels. It supports validation of audio parameters to ensure compatibility with chosen formats.</p> Source code in <code>aiperf/services/dataset/generator/audio.py</code> <pre><code>class AudioGenerator(BaseGenerator):\n    \"\"\"\n    A class for generating synthetic audio data.\n\n    This class provides methods to create audio samples with specified\n    characteristics such as format (WAV, MP3), length, sampling rate,\n    bit depth, and number of channels. It supports validation of audio\n    parameters to ensure compatibility with chosen formats.\n    \"\"\"\n\n    def __init__(self, config: AudioConfig):\n        super().__init__()\n        self.config = config\n\n    def _validate_sampling_rate(\n        self, sampling_rate_hz: int, audio_format: AudioFormat\n    ) -&gt; None:\n        \"\"\"\n        Validate sampling rate for the given output format.\n\n        Args:\n            sampling_rate_hz: Sampling rate in Hz\n            audio_format: Audio format\n\n        Raises:\n            ConfigurationError: If sampling rate is not supported for the given format\n        \"\"\"\n        if (\n            audio_format == AudioFormat.MP3\n            and sampling_rate_hz not in MP3_SUPPORTED_SAMPLE_RATES\n        ):\n            supported_rates = sorted(MP3_SUPPORTED_SAMPLE_RATES)\n            raise ConfigurationError(\n                f\"MP3 format only supports the following sample rates (in Hz): {supported_rates}. \"\n                f\"Got {sampling_rate_hz} Hz. Please choose a supported rate from the list.\"\n            )\n\n    def _validate_bit_depth(self, bit_depth: int) -&gt; None:\n        \"\"\"\n        Validate bit depth is supported.\n\n        Args:\n            bit_depth: Bit depth in bits\n\n        Raises:\n            ConfigurationError: If bit depth is not supported\n        \"\"\"\n        if bit_depth not in SUPPORTED_BIT_DEPTHS:\n            supported_depths = sorted(SUPPORTED_BIT_DEPTHS.keys())\n            raise ConfigurationError(\n                f\"Unsupported bit depth: {bit_depth}. \"\n                f\"Supported bit depths are: {supported_depths}\"\n            )\n\n    def generate(self, *args, **kwargs) -&gt; str:\n        \"\"\"Generate audio data with specified parameters.\n\n        Returns:\n            Data URI containing base64-encoded audio data with format specification\n\n        Raises:\n            ConfigurationError: If any of the following conditions are met:\n                - audio length is less than 0.01 seconds\n                - channels is not 1 (mono) or 2 (stereo)\n                - sampling rate is not supported for MP3 format\n                - bit depth is not supported (must be 8, 16, 24, or 32)\n                - audio format is not supported (must be 'wav' or 'mp3')\n        \"\"\"\n        if self.config.num_channels not in (1, 2):\n            raise ConfigurationError(\n                \"Only mono (1) and stereo (2) channels are supported\"\n            )\n\n        if self.config.length.mean &lt; 0.01:\n            raise ConfigurationError(\"Audio length must be greater than 0.01 seconds\")\n\n        # Sample audio length (in seconds) using rejection sampling\n        audio_length = utils.sample_normal(\n            self.config.length.mean, self.config.length.stddev, lower=0.01\n        )\n\n        # Randomly select sampling rate and bit depth\n        sampling_rate_hz = int(\n            np.random.choice(self.config.sample_rates) * 1000\n        )  # Convert kHz to Hz\n        bit_depth = np.random.choice(self.config.depths)\n\n        # Validate sampling rate and bit depth\n        self._validate_sampling_rate(sampling_rate_hz, self.config.format)\n        self._validate_bit_depth(bit_depth)\n\n        # Generate synthetic audio data (gaussian noise)\n        num_samples = int(audio_length * sampling_rate_hz)\n        audio_data = np.random.normal(\n            0,\n            0.3,\n            (\n                (num_samples, self.config.num_channels)\n                if self.config.num_channels &gt; 1\n                else num_samples\n            ),\n        )\n\n        # Ensure the signal is within [-1, 1] range\n        audio_data = np.clip(audio_data, -1, 1)\n\n        # Scale to the appropriate bit depth range\n        max_val = 2 ** (bit_depth - 1) - 1\n        numpy_type, _ = SUPPORTED_BIT_DEPTHS[bit_depth]\n        audio_data = (audio_data * max_val).astype(numpy_type)\n\n        # Write audio using soundfile\n        output_buffer = io.BytesIO()\n\n        # Select appropriate subtype based on format\n        if self.config.format == AudioFormat.MP3:\n            subtype = \"MPEG_LAYER_III\"\n        elif self.config.format == AudioFormat.WAV:\n            _, subtype = SUPPORTED_BIT_DEPTHS[bit_depth]\n        else:\n            raise ConfigurationError(\n                f\"Unsupported audio format: {self.config.format}. \"\n                f\"Supported formats are: {AudioFormat.WAV.name}, {AudioFormat.MP3.name}\"\n            )\n\n        sf.write(\n            output_buffer,\n            audio_data,\n            sampling_rate_hz,\n            format=self.config.format,\n            subtype=subtype,\n        )\n        audio_bytes = output_buffer.getvalue()\n\n        # Encode to base64 with data URI scheme: \"{format},{data}\"\n        base64_data = base64.b64encode(audio_bytes).decode(\"utf-8\")\n        return f\"{self.config.format.lower()},{base64_data}\"\n</code></pre>"},{"location":"api/#aiperf.services.dataset.generator.audio.AudioGenerator.generate","title":"<code>generate(*args, **kwargs)</code>","text":"<p>Generate audio data with specified parameters.</p> <p>Returns:</p> Type Description <code>str</code> <p>Data URI containing base64-encoded audio data with format specification</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If any of the following conditions are met: - audio length is less than 0.01 seconds - channels is not 1 (mono) or 2 (stereo) - sampling rate is not supported for MP3 format - bit depth is not supported (must be 8, 16, 24, or 32) - audio format is not supported (must be 'wav' or 'mp3')</p> Source code in <code>aiperf/services/dataset/generator/audio.py</code> <pre><code>def generate(self, *args, **kwargs) -&gt; str:\n    \"\"\"Generate audio data with specified parameters.\n\n    Returns:\n        Data URI containing base64-encoded audio data with format specification\n\n    Raises:\n        ConfigurationError: If any of the following conditions are met:\n            - audio length is less than 0.01 seconds\n            - channels is not 1 (mono) or 2 (stereo)\n            - sampling rate is not supported for MP3 format\n            - bit depth is not supported (must be 8, 16, 24, or 32)\n            - audio format is not supported (must be 'wav' or 'mp3')\n    \"\"\"\n    if self.config.num_channels not in (1, 2):\n        raise ConfigurationError(\n            \"Only mono (1) and stereo (2) channels are supported\"\n        )\n\n    if self.config.length.mean &lt; 0.01:\n        raise ConfigurationError(\"Audio length must be greater than 0.01 seconds\")\n\n    # Sample audio length (in seconds) using rejection sampling\n    audio_length = utils.sample_normal(\n        self.config.length.mean, self.config.length.stddev, lower=0.01\n    )\n\n    # Randomly select sampling rate and bit depth\n    sampling_rate_hz = int(\n        np.random.choice(self.config.sample_rates) * 1000\n    )  # Convert kHz to Hz\n    bit_depth = np.random.choice(self.config.depths)\n\n    # Validate sampling rate and bit depth\n    self._validate_sampling_rate(sampling_rate_hz, self.config.format)\n    self._validate_bit_depth(bit_depth)\n\n    # Generate synthetic audio data (gaussian noise)\n    num_samples = int(audio_length * sampling_rate_hz)\n    audio_data = np.random.normal(\n        0,\n        0.3,\n        (\n            (num_samples, self.config.num_channels)\n            if self.config.num_channels &gt; 1\n            else num_samples\n        ),\n    )\n\n    # Ensure the signal is within [-1, 1] range\n    audio_data = np.clip(audio_data, -1, 1)\n\n    # Scale to the appropriate bit depth range\n    max_val = 2 ** (bit_depth - 1) - 1\n    numpy_type, _ = SUPPORTED_BIT_DEPTHS[bit_depth]\n    audio_data = (audio_data * max_val).astype(numpy_type)\n\n    # Write audio using soundfile\n    output_buffer = io.BytesIO()\n\n    # Select appropriate subtype based on format\n    if self.config.format == AudioFormat.MP3:\n        subtype = \"MPEG_LAYER_III\"\n    elif self.config.format == AudioFormat.WAV:\n        _, subtype = SUPPORTED_BIT_DEPTHS[bit_depth]\n    else:\n        raise ConfigurationError(\n            f\"Unsupported audio format: {self.config.format}. \"\n            f\"Supported formats are: {AudioFormat.WAV.name}, {AudioFormat.MP3.name}\"\n        )\n\n    sf.write(\n        output_buffer,\n        audio_data,\n        sampling_rate_hz,\n        format=self.config.format,\n        subtype=subtype,\n    )\n    audio_bytes = output_buffer.getvalue()\n\n    # Encode to base64 with data URI scheme: \"{format},{data}\"\n    base64_data = base64.b64encode(audio_bytes).decode(\"utf-8\")\n    return f\"{self.config.format.lower()},{base64_data}\"\n</code></pre>"},{"location":"api/#aiperfservicesdatasetgeneratorbase","title":"aiperf.services.dataset.generator.base","text":""},{"location":"api/#aiperf.services.dataset.generator.base.BaseGenerator","title":"<code>BaseGenerator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all data generators.</p> <p>Provides a consistent interface for generating synthetic data while allowing each generator type to use its own specific configuration and runtime parameters.</p> Source code in <code>aiperf/services/dataset/generator/base.py</code> <pre><code>class BaseGenerator(ABC):\n    \"\"\"Abstract base class for all data generators.\n\n    Provides a consistent interface for generating synthetic data while allowing\n    each generator type to use its own specific configuration and runtime parameters.\n    \"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def generate(self, *args, **kwargs) -&gt; str:\n        \"\"\"Generate synthetic data.\n\n        Args:\n            *args: Variable length argument list (subclass-specific)\n            **kwargs: Arbitrary keyword arguments (subclass-specific)\n\n        Returns:\n            Generated data as a string (could be text, base64 encoded media, etc.)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.dataset.generator.base.BaseGenerator.generate","title":"<code>generate(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list (subclass-specific)</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments (subclass-specific)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated data as a string (could be text, base64 encoded media, etc.)</p> Source code in <code>aiperf/services/dataset/generator/base.py</code> <pre><code>@abstractmethod\ndef generate(self, *args, **kwargs) -&gt; str:\n    \"\"\"Generate synthetic data.\n\n    Args:\n        *args: Variable length argument list (subclass-specific)\n        **kwargs: Arbitrary keyword arguments (subclass-specific)\n\n    Returns:\n        Generated data as a string (could be text, base64 encoded media, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#aiperfservicesdatasetgeneratorimage","title":"aiperf.services.dataset.generator.image","text":""},{"location":"api/#aiperf.services.dataset.generator.image.ImageGenerator","title":"<code>ImageGenerator</code>","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>A class that generates images from source images.</p> <p>This class provides methods to create synthetic images by resizing source images (located in the 'assets/source_images' directory) to specified dimensions and converting them to a chosen image format (e.g., PNG, JPEG). The dimensions can be randomized based on mean and standard deviation values.</p> Source code in <code>aiperf/services/dataset/generator/image.py</code> <pre><code>class ImageGenerator(BaseGenerator):\n    \"\"\"A class that generates images from source images.\n\n    This class provides methods to create synthetic images by resizing\n    source images (located in the 'assets/source_images' directory)\n    to specified dimensions and converting them to a chosen image format (e.g., PNG, JPEG).\n    The dimensions can be randomized based on mean and standard deviation values.\n    \"\"\"\n\n    def __init__(self, config: ImageConfig):\n        super().__init__()\n        self.config = config\n\n    def generate(self, *args, **kwargs) -&gt; str:\n        \"\"\"Generate an image with the configured parameters.\n\n        Returns:\n            A base64 encoded string of the generated image.\n        \"\"\"\n        image_format = self.config.format\n        if image_format == ImageFormat.RANDOM:\n            image_format = random.choice(\n                [f for f in ImageFormat if f != ImageFormat.RANDOM]\n            )\n\n        width = utils.sample_positive_normal_integer(\n            self.config.width.mean, self.config.width.stddev\n        )\n        height = utils.sample_positive_normal_integer(\n            self.config.height.mean, self.config.height.stddev\n        )\n\n        self.logger.debug(\n            \"Generating image with width=%d, height=%d\",\n            width,\n            height,\n        )\n\n        image = self._sample_source_image()\n        image = image.resize(size=(width, height))\n        base64_image = utils.encode_image(image, image_format)\n        return f\"data:image/{image_format.name.lower()};base64,{base64_image}\"\n\n    def _sample_source_image(self):\n        \"\"\"Sample one image among the source images.\n\n        Returns:\n            A PIL Image object randomly selected from the source images.\n        \"\"\"\n        filepath = Path(__file__).parent.resolve() / \"assets\" / \"source_images\" / \"*\"\n        filenames = glob.glob(str(filepath))\n        if not filenames:\n            raise ValueError(f\"No source images found in '{filepath}'\")\n        return Image.open(random.choice(filenames))\n</code></pre>"},{"location":"api/#aiperf.services.dataset.generator.image.ImageGenerator.generate","title":"<code>generate(*args, **kwargs)</code>","text":"<p>Generate an image with the configured parameters.</p> <p>Returns:</p> Type Description <code>str</code> <p>A base64 encoded string of the generated image.</p> Source code in <code>aiperf/services/dataset/generator/image.py</code> <pre><code>def generate(self, *args, **kwargs) -&gt; str:\n    \"\"\"Generate an image with the configured parameters.\n\n    Returns:\n        A base64 encoded string of the generated image.\n    \"\"\"\n    image_format = self.config.format\n    if image_format == ImageFormat.RANDOM:\n        image_format = random.choice(\n            [f for f in ImageFormat if f != ImageFormat.RANDOM]\n        )\n\n    width = utils.sample_positive_normal_integer(\n        self.config.width.mean, self.config.width.stddev\n    )\n    height = utils.sample_positive_normal_integer(\n        self.config.height.mean, self.config.height.stddev\n    )\n\n    self.logger.debug(\n        \"Generating image with width=%d, height=%d\",\n        width,\n        height,\n    )\n\n    image = self._sample_source_image()\n    image = image.resize(size=(width, height))\n    base64_image = utils.encode_image(image, image_format)\n    return f\"data:image/{image_format.name.lower()};base64,{base64_image}\"\n</code></pre>"},{"location":"api/#aiperfservicesdatasetgeneratorprompt","title":"aiperf.services.dataset.generator.prompt","text":""},{"location":"api/#aiperf.services.dataset.generator.prompt.PromptGenerator","title":"<code>PromptGenerator</code>","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>A class for generating synthetic prompts from a text corpus.</p> <p>This class loads a text corpus (e.g., Shakespearean text), tokenizes it, and uses the tokenized corpus to generate synthetic prompts of specified lengths. It supports generating prompts with a target number of tokens (with optional randomization around a mean and standard deviation) and can reuse previously generated token blocks to optimize generation for certain use cases. It also allows for the creation of a pool of prefix prompts that can be randomly selected.</p> Source code in <code>aiperf/services/dataset/generator/prompt.py</code> <pre><code>class PromptGenerator(BaseGenerator):\n    \"\"\"A class for generating synthetic prompts from a text corpus.\n\n    This class loads a text corpus (e.g., Shakespearean text), tokenizes it,\n    and uses the tokenized corpus to generate synthetic prompts of specified\n    lengths. It supports generating prompts with a target number of tokens\n    (with optional randomization around a mean and standard deviation) and\n    can reuse previously generated token blocks to optimize generation for\n    certain use cases. It also allows for the creation of a pool of prefix\n    prompts that can be randomly selected.\n    \"\"\"\n\n    def __init__(self, config: PromptConfig, tokenizer: Tokenizer):\n        super().__init__()\n        self.config = config\n        self.tokenizer = tokenizer\n        self._tokenized_corpus = None\n        self._corpus_size = 0\n        self._prefix_prompts: list[str] = []\n\n        # Cached prompts: block ID -&gt; list of tokens\n        self._cache: dict[int, list[int]] = {}\n\n        # TODO: move this under initialize() method\n        # Initialize corpus if not already done\n        if self._tokenized_corpus is None:\n            self._initialize_corpus()\n\n        # Initialize prefix prompts pool if the pool size &gt; 0\n        if self.config.prefix_prompt.pool_size &gt; 0:\n            self._create_prefix_prompt_pool()\n\n    def _initialize_corpus(self) -&gt; None:\n        \"\"\"Load and tokenize the corpus once, storing it for reuse.\"\"\"\n        corpus_path = Path(__file__).parent / DEFAULT_CORPUS_FILE\n\n        with open(corpus_path) as f:\n            lines = f.readlines()\n\n        def tokenize_chunk(chunk):\n            cleaned_text = \" \".join(line.strip() for line in chunk if line.strip())\n            tokens = self.tokenizer.encode(cleaned_text)\n            return tokens\n\n        num_threads = os.cpu_count()\n        if num_threads is None:\n            num_threads = 4\n\n        # Ensure chunk_size is at least 1 to avoid division by zero in range()\n        chunk_size = max(1, len(lines) // num_threads)\n        chunks = [lines[i : i + chunk_size] for i in range(0, len(lines), chunk_size)]\n\n        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n            tokenized_chunks = list(executor.map(tokenize_chunk, chunks))\n\n        self._tokenized_corpus = [\n            token for chunk in tokenized_chunks for token in chunk\n        ]\n        self._corpus_size = len(self._tokenized_corpus)\n        self.logger.debug(\"Initialized corpus with %d tokens\", self._corpus_size)\n\n    def _create_prefix_prompt_pool(self) -&gt; None:\n        \"\"\"Generate a pool of prefix prompts to sample from.\"\"\"\n        if self._tokenized_corpus is None:\n            raise NotInitializedError(\"Tokenized corpus is not initialized.\")\n\n        self._prefix_prompts = [\n            self._generate_prompt(self.config.prefix_prompt.length)\n            for _ in range(self.config.prefix_prompt.pool_size)\n        ]\n        self.logger.debug(\n            \"Initialized prefix prompts pool with %d prompts\",\n            len(self._prefix_prompts),\n        )\n\n    def generate(\n        self,\n        mean: int | None = None,\n        stddev: int | None = None,\n        hash_ids: list[int] | None = None,\n    ) -&gt; str:\n        \"\"\"Generate a synthetic prompt with the configuration parameters.\n\n        Args:\n            mean: The mean of the normal distribution.\n            stddev: The standard deviation of the normal distribution.\n            hash_ids: A list of hash indices used for token reuse.\n\n        Returns:\n            A synthetic prompt as a string.\n        \"\"\"\n        if hash_ids:\n            return self._generate_cached_prompt(\n                mean, hash_ids, self.config.input_tokens.block_size\n            )\n\n        num_tokens = utils.sample_positive_normal_integer(mean, stddev)\n        return self._generate_prompt(num_tokens)\n\n    def _generate_prompt(self, num_tokens: int) -&gt; str:\n        \"\"\"Generate a prompt containing exactly `num_tokens` number of tokens.\n\n        Args:\n            num_tokens: Number of tokens required in the prompt.\n\n        Returns:\n            A synthetic prompt as a string.\n        \"\"\"\n        return self.tokenizer.decode(self._sample_tokens(num_tokens))\n\n    def _generate_cached_prompt(\n        self,\n        num_tokens: int,\n        hash_ids: list[int],\n        block_size: int,\n    ) -&gt; str:\n        \"\"\"\n        Generate a prompt containing exactly `num_tokens` by reusing previously generated prompts\n        stored in `_cache`. Each hash index in `hash_ids` corresponds to a block of\n        `block_size` tokens. If a hash index is found in `_cache`, its stored prompt is reused.\n        Otherwise, a new prompt is generated using `_generate_prompt()` and stored in `_cache`.\n\n        Args:\n            num_tokens: The number of tokens required in the prompt.\n            hash_ids: A list of hash IDs to use for token reuse.\n            block_size: The number of tokens allocated per hash block.\n\n        Returns:\n            str: A synthetic prompt as a string.\n\n        Raises:\n            ConfigurationError: If the input parameters are not compatible.\n        \"\"\"\n        final_prompt: list[int] = []\n        current_block_size = block_size\n\n        # Sanity check the final block size\n        final_block_size = num_tokens - ((len(hash_ids) - 1) * block_size)\n        if final_block_size &lt;= 0 or block_size &lt; final_block_size:\n            raise ConfigurationError(\n                f\"Input length: {num_tokens}, Hash IDs: {hash_ids}, Block size: {block_size} \"\n                f\"are not compatible. The final hash block size: {final_block_size} must be \"\n                f\"greater than 0 and less than or equal to {block_size}.\"\n            )\n\n        for index, hash_id in enumerate(hash_ids):\n            # For the last hash ID, use the remaining tokens as the block size\n            if index == len(hash_ids) - 1:\n                current_block_size = final_block_size\n\n            if hash_id not in self._cache:\n                # To ensure that the prompt doesn't merge chunks, we pop the last token\n                # and insert the bos token at the beginning. Length is maintained and\n                # the prompt generates the expected number of tokens.\n                prompt_tokens: list[int] = self._sample_tokens(current_block_size)\n                prompt_tokens.pop(0)\n                prompt_tokens.insert(0, self.tokenizer.bos_token_id)\n                self._cache[hash_id] = prompt_tokens  # store to cache\n\n            final_prompt.extend(self._cache[hash_id])\n\n        return self.tokenizer.decode(final_prompt, skip_special_tokens=False)\n\n    def _sample_tokens(self, num_tokens: int) -&gt; list[int]:\n        \"\"\"Generate a list of token IDs containing exactly `num_tokens` number of tokens\n        using the preloaded tokenized corpus.\n\n        Args:\n            num_tokens: Number of tokens required in the prompt.\n\n        Returns:\n            A list of token IDs.\n\n        Raises:\n            NotInitializedError: If the tokenized corpus is not initialized\n        \"\"\"\n        if not self._tokenized_corpus:\n            raise NotInitializedError(\"Tokenized corpus is not initialized.\")\n        if num_tokens &gt; self._corpus_size:\n            logger.warning(\n                f\"Requested prompt length {num_tokens} is longer than the corpus. \"\n                f\"Returning a prompt of length {self._corpus_size}.\"\n            )\n\n        start_idx = random.randrange(self._corpus_size)\n\n        end_idx = start_idx + num_tokens\n        prompt_tokens = self._tokenized_corpus[start_idx:end_idx]\n        if end_idx &gt; self._corpus_size:\n            prompt_tokens += self._tokenized_corpus[: end_idx - self._corpus_size]\n\n        self.logger.debug(\"Sampled %d tokens from corpus\", len(prompt_tokens))\n        return prompt_tokens\n\n    def get_random_prefix_prompt(self) -&gt; str:\n        \"\"\"\n        Fetch a random prefix prompt from the pool.\n\n        Returns:\n            A random prefix prompt.\n\n        Raises:\n            InvalidStateError: If the prefix prompts pool is empty.\n        \"\"\"\n        if not self._prefix_prompts:\n            raise InvalidStateError(\n                \"Attempted to sample a prefix prompt but the prefix prompts pool is empty. \"\n                \"Please ensure that the prefix prompts pool is initialized.\"\n            )\n        return random.choice(self._prefix_prompts)\n</code></pre>"},{"location":"api/#aiperf.services.dataset.generator.prompt.PromptGenerator.generate","title":"<code>generate(mean=None, stddev=None, hash_ids=None)</code>","text":"<p>Generate a synthetic prompt with the configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>int | None</code> <p>The mean of the normal distribution.</p> <code>None</code> <code>stddev</code> <code>int | None</code> <p>The standard deviation of the normal distribution.</p> <code>None</code> <code>hash_ids</code> <code>list[int] | None</code> <p>A list of hash indices used for token reuse.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A synthetic prompt as a string.</p> Source code in <code>aiperf/services/dataset/generator/prompt.py</code> <pre><code>def generate(\n    self,\n    mean: int | None = None,\n    stddev: int | None = None,\n    hash_ids: list[int] | None = None,\n) -&gt; str:\n    \"\"\"Generate a synthetic prompt with the configuration parameters.\n\n    Args:\n        mean: The mean of the normal distribution.\n        stddev: The standard deviation of the normal distribution.\n        hash_ids: A list of hash indices used for token reuse.\n\n    Returns:\n        A synthetic prompt as a string.\n    \"\"\"\n    if hash_ids:\n        return self._generate_cached_prompt(\n            mean, hash_ids, self.config.input_tokens.block_size\n        )\n\n    num_tokens = utils.sample_positive_normal_integer(mean, stddev)\n    return self._generate_prompt(num_tokens)\n</code></pre>"},{"location":"api/#aiperf.services.dataset.generator.prompt.PromptGenerator.get_random_prefix_prompt","title":"<code>get_random_prefix_prompt()</code>","text":"<p>Fetch a random prefix prompt from the pool.</p> <p>Returns:</p> Type Description <code>str</code> <p>A random prefix prompt.</p> <p>Raises:</p> Type Description <code>InvalidStateError</code> <p>If the prefix prompts pool is empty.</p> Source code in <code>aiperf/services/dataset/generator/prompt.py</code> <pre><code>def get_random_prefix_prompt(self) -&gt; str:\n    \"\"\"\n    Fetch a random prefix prompt from the pool.\n\n    Returns:\n        A random prefix prompt.\n\n    Raises:\n        InvalidStateError: If the prefix prompts pool is empty.\n    \"\"\"\n    if not self._prefix_prompts:\n        raise InvalidStateError(\n            \"Attempted to sample a prefix prompt but the prefix prompts pool is empty. \"\n            \"Please ensure that the prefix prompts pool is initialized.\"\n        )\n    return random.choice(self._prefix_prompts)\n</code></pre>"},{"location":"api/#aiperfservicesdatasetloadermodels","title":"aiperf.services.dataset.loader.models","text":""},{"location":"api/#aiperf.services.dataset.loader.models.CustomData","title":"<code>CustomData = Annotated[SingleTurn | MooncakeTrace | MultiTurn, Field(discriminator='type')]</code>  <code>module-attribute</code>","text":"<p>A union type of all custom data types.</p>"},{"location":"api/#aiperf.services.dataset.loader.models.MooncakeTrace","title":"<code>MooncakeTrace</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for Mooncake trace data.</p> <p>See https://github.com/kvcache-ai/Mooncake for more details.</p> <p>Example:</p> <pre><code>{\"timestamp\": 1000, \"input_length\": 10, \"output_length\": 4, \"hash_ids\": [123, 456]}\n</code></pre> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>class MooncakeTrace(AIPerfBaseModel):\n    \"\"\"Defines the schema for Mooncake trace data.\n\n    See https://github.com/kvcache-ai/Mooncake for more details.\n\n    Example:\n    ```json\n    {\"timestamp\": 1000, \"input_length\": 10, \"output_length\": 4, \"hash_ids\": [123, 456]}\n    ```\n    \"\"\"\n\n    type: Literal[CustomDatasetType.MOONCAKE_TRACE] = CustomDatasetType.MOONCAKE_TRACE\n\n    input_length: int = Field(..., description=\"The input sequence length of a request\")\n    output_length: int = Field(\n        ..., description=\"The output sequence length of a request\"\n    )\n    hash_ids: list[int] = Field(..., description=\"The hash ids of a request\")\n    timestamp: int = Field(..., description=\"The timestamp of a request\")\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.MultiTurn","title":"<code>MultiTurn</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for multi-turn conversations.</p> <p>The multi-turn custom dataset   - supports multi-modal data (e.g. text, image, audio)   - supports multi-turn features (e.g. delay, sessions, etc.)   - supports client-side batching for each data (e.g. batch size &gt; 1)</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>class MultiTurn(AIPerfBaseModel):\n    \"\"\"Defines the schema for multi-turn conversations.\n\n    The multi-turn custom dataset\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports multi-turn features (e.g. delay, sessions, etc.)\n      - supports client-side batching for each data (e.g. batch size &gt; 1)\n    \"\"\"\n\n    type: Literal[CustomDatasetType.MULTI_TURN] = CustomDatasetType.MULTI_TURN\n\n    session_id: str | None = Field(\n        None, description=\"Unique identifier for the conversation session\"\n    )\n    turns: list[SingleTurn] = Field(\n        ..., description=\"List of turns in the conversation\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_turns_not_empty(self) -&gt; \"MultiTurn\":\n        \"\"\"Ensure at least one turn is provided\"\"\"\n        if not self.turns:\n            raise ValueError(\"At least one turn must be provided\")\n        return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.MultiTurn.validate_turns_not_empty","title":"<code>validate_turns_not_empty()</code>","text":"<p>Ensure at least one turn is provided</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_turns_not_empty(self) -&gt; \"MultiTurn\":\n    \"\"\"Ensure at least one turn is provided\"\"\"\n    if not self.turns:\n        raise ValueError(\"At least one turn must be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.RandomPool","title":"<code>RandomPool</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for random pool data entry.</p> <p>The random pool custom dataset   - supports multi-modal data (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch size &gt; 1)   - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)   - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>class RandomPool(AIPerfBaseModel):\n    \"\"\"Defines the schema for random pool data entry.\n\n    The random pool custom dataset\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch size &gt; 1)\n      - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)\n      - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)\n    \"\"\"\n\n    type: Literal[CustomDatasetType.RANDOM_POOL] = CustomDatasetType.RANDOM_POOL\n\n    text: str | None = Field(None, description=\"Simple text string content\")\n    texts: list[str] | list[Text] | None = Field(\n        None,\n        description=\"List of text strings or Text objects format\",\n    )\n    image: str | None = Field(None, description=\"Simple image string content\")\n    images: list[str] | list[Image] | None = Field(\n        None,\n        description=\"List of image strings or Image objects format\",\n    )\n    audio: str | None = Field(None, description=\"Simple audio string content\")\n    audios: list[str] | list[Audio] | None = Field(\n        None,\n        description=\"List of audio strings or Audio objects format\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_mutually_exclusive_fields(self) -&gt; \"RandomPool\":\n        \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n        if self.text and self.texts:\n            raise ValueError(\"text and texts cannot be set together\")\n        if self.image and self.images:\n            raise ValueError(\"image and images cannot be set together\")\n        if self.audio and self.audios:\n            raise ValueError(\"audio and audios cannot be set together\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_at_least_one_modality(self) -&gt; \"RandomPool\":\n        \"\"\"Ensure at least one modality is provided\"\"\"\n        if not any(\n            [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n        ):\n            raise ValueError(\"At least one modality must be provided\")\n        return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.RandomPool.validate_at_least_one_modality","title":"<code>validate_at_least_one_modality()</code>","text":"<p>Ensure at least one modality is provided</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_at_least_one_modality(self) -&gt; \"RandomPool\":\n    \"\"\"Ensure at least one modality is provided\"\"\"\n    if not any(\n        [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n    ):\n        raise ValueError(\"At least one modality must be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.RandomPool.validate_mutually_exclusive_fields","title":"<code>validate_mutually_exclusive_fields()</code>","text":"<p>Ensure mutually exclusive fields are not set together</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_mutually_exclusive_fields(self) -&gt; \"RandomPool\":\n    \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n    if self.text and self.texts:\n        raise ValueError(\"text and texts cannot be set together\")\n    if self.image and self.images:\n        raise ValueError(\"image and images cannot be set together\")\n    if self.audio and self.audios:\n        raise ValueError(\"audio and audios cannot be set together\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.SingleTurn","title":"<code>SingleTurn</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for single-turn data.</p> <p>User can use this format to quickly provide a custom single turn dataset. Each line in the file will be treated as a single turn conversation.</p> <p>The single turn type   - supports multi-modal (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch_size &gt; 1)   - DOES NOT support multi-turn features (e.g. session_id)</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>class SingleTurn(AIPerfBaseModel):\n    \"\"\"Defines the schema for single-turn data.\n\n    User can use this format to quickly provide a custom single turn dataset.\n    Each line in the file will be treated as a single turn conversation.\n\n    The single turn type\n      - supports multi-modal (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch_size &gt; 1)\n      - DOES NOT support multi-turn features (e.g. session_id)\n    \"\"\"\n\n    type: Literal[CustomDatasetType.SINGLE_TURN] = CustomDatasetType.SINGLE_TURN\n\n    text: str | None = Field(None, description=\"Simple text string content\")\n    texts: list[str] | list[Text] | None = Field(\n        None,\n        description=\"List of text strings or Text objects format\",\n    )\n    image: str | None = Field(None, description=\"Simple image string content\")\n    images: list[str] | list[Image] | None = Field(\n        None,\n        description=\"List of image strings or Image objects format\",\n    )\n    audio: str | None = Field(None, description=\"Simple audio string content\")\n    audios: list[str] | list[Audio] | None = Field(\n        None,\n        description=\"List of audio strings or Audio objects format\",\n    )\n    timestamp: int | None = Field(\n        default=None, description=\"Timestamp of the turn in milliseconds.\"\n    )\n    delay: int | None = Field(\n        default=None,\n        description=\"Amount of milliseconds to wait before sending the turn.\",\n    )\n    role: str | None = Field(default=None, description=\"Role of the turn.\")\n\n    @model_validator(mode=\"after\")\n    def validate_mutually_exclusive_fields(self) -&gt; \"SingleTurn\":\n        \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n        if self.text and self.texts:\n            raise ValueError(\"text and texts cannot be set together\")\n        if self.image and self.images:\n            raise ValueError(\"image and images cannot be set together\")\n        if self.audio and self.audios:\n            raise ValueError(\"audio and audios cannot be set together\")\n        if self.timestamp and self.delay:\n            raise ValueError(\"timestamp and delay cannot be set together\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_at_least_one_modality(self) -&gt; \"SingleTurn\":\n        \"\"\"Ensure at least one modality is provided\"\"\"\n        if not any(\n            [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n        ):\n            raise ValueError(\"At least one modality must be provided\")\n        return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.SingleTurn.validate_at_least_one_modality","title":"<code>validate_at_least_one_modality()</code>","text":"<p>Ensure at least one modality is provided</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_at_least_one_modality(self) -&gt; \"SingleTurn\":\n    \"\"\"Ensure at least one modality is provided\"\"\"\n    if not any(\n        [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n    ):\n        raise ValueError(\"At least one modality must be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.models.SingleTurn.validate_mutually_exclusive_fields","title":"<code>validate_mutually_exclusive_fields()</code>","text":"<p>Ensure mutually exclusive fields are not set together</p> Source code in <code>aiperf/services/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_mutually_exclusive_fields(self) -&gt; \"SingleTurn\":\n    \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n    if self.text and self.texts:\n        raise ValueError(\"text and texts cannot be set together\")\n    if self.image and self.images:\n        raise ValueError(\"image and images cannot be set together\")\n    if self.audio and self.audios:\n        raise ValueError(\"audio and audios cannot be set together\")\n    if self.timestamp and self.delay:\n        raise ValueError(\"timestamp and delay cannot be set together\")\n    return self\n</code></pre>"},{"location":"api/#aiperfservicesdatasetloadermooncake_trace","title":"aiperf.services.dataset.loader.mooncake_trace","text":""},{"location":"api/#aiperf.services.dataset.loader.mooncake_trace.MooncakeTraceDatasetLoader","title":"<code>MooncakeTraceDatasetLoader</code>","text":"<p>A dataset loader that loads Mooncake trace data from a file.</p> <p>Loads Mooncake trace data from a file and converts the data into a list of conversations for dataset manager.</p> <p>Each line in the file represents a single trace entry and will be converted to a separate conversation with a unique session ID.</p> <p>Example: Fixed schedule version (Each line is a distinct session. Multi-turn is NOT supported)</p> <pre><code>{\"timestamp\": 1000, \"input_length\": 300, \"output_length\": 40, \"hash_ids\": [123, 456]}\n</code></pre> Source code in <code>aiperf/services/dataset/loader/mooncake_trace.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.MOONCAKE_TRACE)\nclass MooncakeTraceDatasetLoader:\n    \"\"\"A dataset loader that loads Mooncake trace data from a file.\n\n    Loads Mooncake trace data from a file and converts the data into\n    a list of conversations for dataset manager.\n\n    Each line in the file represents a single trace entry and will be\n    converted to a separate conversation with a unique session ID.\n\n    Example:\n    Fixed schedule version (Each line is a distinct session. Multi-turn is NOT supported)\n    ```json\n    {\"timestamp\": 1000, \"input_length\": 300, \"output_length\": 40, \"hash_ids\": [123, 456]}\n    ```\n    \"\"\"\n\n    def __init__(self, filename: str, prompt_generator: PromptGenerator):\n        self.filename = filename\n        self.prompt_generator = prompt_generator\n\n    def load_dataset(self) -&gt; dict[str, list[MooncakeTrace]]:\n        \"\"\"Load Mooncake trace data from a file.\n\n        Returns:\n            A dictionary of session_id and list of Mooncake trace data.\n        \"\"\"\n        data: dict[str, list[MooncakeTrace]] = defaultdict(list)\n\n        with open(self.filename) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                trace_data = MooncakeTrace.model_validate_json(line)\n                session_id = str(uuid.uuid4())\n                data[session_id].append(trace_data)\n\n        return data\n\n    def convert_to_conversations(\n        self, data: dict[str, list[MooncakeTrace]]\n    ) -&gt; list[Conversation]:\n        \"\"\"Convert all the Mooncake trace data to conversation objects.\n\n        Args:\n            data: A dictionary of session_id and list of Mooncake trace data.\n\n        Returns:\n            A list of conversations.\n        \"\"\"\n        conversations = []\n        for session_id, traces in data.items():\n            conversation = Conversation(session_id=session_id)\n            for trace in traces:\n                prompt = self.prompt_generator.generate(\n                    mean=trace.input_length,\n                    stddev=0,\n                    hash_ids=trace.hash_ids,\n                )\n                turn = Turn(\n                    timestamp=trace.timestamp,\n                    texts=[Text(name=\"text\", contents=[prompt])],\n                )\n                conversation.turns.append(turn)\n            conversations.append(conversation)\n        return conversations\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.mooncake_trace.MooncakeTraceDatasetLoader.convert_to_conversations","title":"<code>convert_to_conversations(data)</code>","text":"<p>Convert all the Mooncake trace data to conversation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[MooncakeTrace]]</code> <p>A dictionary of session_id and list of Mooncake trace data.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>A list of conversations.</p> Source code in <code>aiperf/services/dataset/loader/mooncake_trace.py</code> <pre><code>def convert_to_conversations(\n    self, data: dict[str, list[MooncakeTrace]]\n) -&gt; list[Conversation]:\n    \"\"\"Convert all the Mooncake trace data to conversation objects.\n\n    Args:\n        data: A dictionary of session_id and list of Mooncake trace data.\n\n    Returns:\n        A list of conversations.\n    \"\"\"\n    conversations = []\n    for session_id, traces in data.items():\n        conversation = Conversation(session_id=session_id)\n        for trace in traces:\n            prompt = self.prompt_generator.generate(\n                mean=trace.input_length,\n                stddev=0,\n                hash_ids=trace.hash_ids,\n            )\n            turn = Turn(\n                timestamp=trace.timestamp,\n                texts=[Text(name=\"text\", contents=[prompt])],\n            )\n            conversation.turns.append(turn)\n        conversations.append(conversation)\n    return conversations\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.mooncake_trace.MooncakeTraceDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load Mooncake trace data from a file.</p> <p>Returns:</p> Type Description <code>dict[str, list[MooncakeTrace]]</code> <p>A dictionary of session_id and list of Mooncake trace data.</p> Source code in <code>aiperf/services/dataset/loader/mooncake_trace.py</code> <pre><code>def load_dataset(self) -&gt; dict[str, list[MooncakeTrace]]:\n    \"\"\"Load Mooncake trace data from a file.\n\n    Returns:\n        A dictionary of session_id and list of Mooncake trace data.\n    \"\"\"\n    data: dict[str, list[MooncakeTrace]] = defaultdict(list)\n\n    with open(self.filename) as f:\n        for line in f:\n            if (line := line.strip()) == \"\":\n                continue  # Skip empty lines\n\n            trace_data = MooncakeTrace.model_validate_json(line)\n            session_id = str(uuid.uuid4())\n            data[session_id].append(trace_data)\n\n    return data\n</code></pre>"},{"location":"api/#aiperfservicesdatasetloadermulti_turn","title":"aiperf.services.dataset.loader.multi_turn","text":""},{"location":"api/#aiperf.services.dataset.loader.multi_turn.MultiTurnDatasetLoader","title":"<code>MultiTurnDatasetLoader</code>","text":"<p>A dataset loader that loads multi-turn data from a file.</p> <p>The multi-turn type   - supports multi-modal data (e.g. text, image, audio)   - supports multi-turn features (e.g. delay, sessions, etc.)   - supports client-side batching for each data (e.g. batch_size &gt; 1)</p> <p>NOTE: If the user specifies multiple multi-turn entries with same session ID, the loader will group them together. If the timestamps are specified, they will be sorted in ascending order later in the timing manager.</p> <p>Examples: 1. Simple version</p> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"text\": \"Hello\", \"image\": \"url\", \"delay\": 0},\n        {\"text\": \"Hi there\", \"delay\": 1000}\n    ]\n}\n</code></pre> <ol> <li>Batched version</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/1.png\", \"/path/2.png\"]},\n        {\"texts\": [\"What is in the image?\", \"What is AI?\"], \"images\": [\"/path/3.png\", \"/path/4.png\"]}\n    ]\n}\n</code></pre> <ol> <li>Fixed schedule version</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n        {\"timestamp\": 1000, \"text\": \"Who are you?\"}\n    ]\n}\n</code></pre> <ol> <li>Time delayed version</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"delay\": 0, \"text\": \"What is deep learning?\"},\n        {\"delay\": 1000, \"text\": \"Who are you?\"}\n    ]\n}\n</code></pre> <ol> <li>full-featured version (multi-batch, multi-modal, multi-fielded, session-based, etc.)</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\n            \"timestamp\": 1234,\n            \"texts\": [\n                {\"name\": \"text_field_a\", \"contents\": [\"hello\", \"world\"]},\n                {\"name\": \"text_field_b\", \"contents\": [\"hi there\"]}\n            ],\n            \"images\": [\n                {\"name\": \"image_field_a\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n                {\"name\": \"image_field_b\", \"contents\": [\"/path/3.png\"]}\n            ]\n        }\n    ]\n}\n</code></pre> Source code in <code>aiperf/services/dataset/loader/multi_turn.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.MULTI_TURN)\nclass MultiTurnDatasetLoader:\n    \"\"\"A dataset loader that loads multi-turn data from a file.\n\n    The multi-turn type\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports multi-turn features (e.g. delay, sessions, etc.)\n      - supports client-side batching for each data (e.g. batch_size &gt; 1)\n\n    NOTE: If the user specifies multiple multi-turn entries with same session ID,\n    the loader will group them together. If the timestamps are specified, they will\n    be sorted in ascending order later in the timing manager.\n\n    Examples:\n    1. Simple version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"text\": \"Hello\", \"image\": \"url\", \"delay\": 0},\n            {\"text\": \"Hi there\", \"delay\": 1000}\n        ]\n    }\n    ```\n\n    2. Batched version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/1.png\", \"/path/2.png\"]},\n            {\"texts\": [\"What is in the image?\", \"What is AI?\"], \"images\": [\"/path/3.png\", \"/path/4.png\"]}\n        ]\n    }\n    ```\n\n    3. Fixed schedule version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n            {\"timestamp\": 1000, \"text\": \"Who are you?\"}\n        ]\n    }\n    ```\n\n    4. Time delayed version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"delay\": 0, \"text\": \"What is deep learning?\"},\n            {\"delay\": 1000, \"text\": \"Who are you?\"}\n        ]\n    }\n    ```\n\n    5. full-featured version (multi-batch, multi-modal, multi-fielded, session-based, etc.)\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\n                \"timestamp\": 1234,\n                \"texts\": [\n                    {\"name\": \"text_field_a\", \"contents\": [\"hello\", \"world\"]},\n                    {\"name\": \"text_field_b\", \"contents\": [\"hi there\"]}\n                ],\n                \"images\": [\n                    {\"name\": \"image_field_a\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n                    {\"name\": \"image_field_b\", \"contents\": [\"/path/3.png\"]}\n                ]\n            }\n        ]\n    }\n    ```\n    \"\"\"\n\n    def __init__(self, filename: str):\n        self.filename = filename\n\n    def load_dataset(self) -&gt; dict[str, list[MultiTurn]]:\n        \"\"\"Load multi-turn data from a JSONL file.\n\n        Each line represents a complete multi-turn conversation with its own\n        session_id and multiple turns.\n\n        Returns:\n            A dictionary mapping session_id to list of CustomData (containing the MultiTurn).\n        \"\"\"\n        data: dict[str, list[MultiTurn]] = defaultdict(list)\n\n        with open(self.filename) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                multi_turn_data = MultiTurn.model_validate_json(line)\n                session_id = multi_turn_data.session_id or str(uuid.uuid4())\n                data[session_id].append(multi_turn_data)\n\n        return data\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.multi_turn.MultiTurnDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load multi-turn data from a JSONL file.</p> <p>Each line represents a complete multi-turn conversation with its own session_id and multiple turns.</p> <p>Returns:</p> Type Description <code>dict[str, list[MultiTurn]]</code> <p>A dictionary mapping session_id to list of CustomData (containing the MultiTurn).</p> Source code in <code>aiperf/services/dataset/loader/multi_turn.py</code> <pre><code>def load_dataset(self) -&gt; dict[str, list[MultiTurn]]:\n    \"\"\"Load multi-turn data from a JSONL file.\n\n    Each line represents a complete multi-turn conversation with its own\n    session_id and multiple turns.\n\n    Returns:\n        A dictionary mapping session_id to list of CustomData (containing the MultiTurn).\n    \"\"\"\n    data: dict[str, list[MultiTurn]] = defaultdict(list)\n\n    with open(self.filename) as f:\n        for line in f:\n            if (line := line.strip()) == \"\":\n                continue  # Skip empty lines\n\n            multi_turn_data = MultiTurn.model_validate_json(line)\n            session_id = multi_turn_data.session_id or str(uuid.uuid4())\n            data[session_id].append(multi_turn_data)\n\n    return data\n</code></pre>"},{"location":"api/#aiperfservicesdatasetloaderprotocol","title":"aiperf.services.dataset.loader.protocol","text":""},{"location":"api/#aiperfservicesdatasetloaderrandom_pool","title":"aiperf.services.dataset.loader.random_pool","text":""},{"location":"api/#aiperf.services.dataset.loader.random_pool.RandomPoolDatasetLoader","title":"<code>RandomPoolDatasetLoader</code>","text":"<p>A dataset loader that loads data from a single file or a directory.</p> <p>Each line in the file represents single-turn conversation data, and files create individual pools for random sampling:   - Single file: All lines form one single pool (to be randomly sampled from)   - Directory: Each file becomes a separate pool, then pools are randomly sampled                and merged into conversations later.</p> <p>The random pool custom dataset   - supports multi-modal data (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch size &gt; 1)   - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)   - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)</p> <p>Example:</p> <ol> <li>Single file</li> </ol> <pre><code>{\"text\": \"Who are you?\", \"image\": \"/path/to/image1.png\"}\n{\"text\": \"Explain what is the meaning of life.\", \"image\": \"/path/to/image2.png\"}\n...\n</code></pre> <p>The file will form a single pool of text and image data that will be used to generate conversations.</p> <ol> <li>Directory</li> </ol> <p>Directory will be useful if user wants to   - create multiple pools of different modalities separately (e.g. text, image)   - specify different field names for the same modality.</p> <p>data/queries.jsonl</p> <pre><code>{\"texts\": [{\"name\": \"query\", \"contents\": [\"Who are you?\"]}]}\n{\"texts\": [{\"name\": \"query\", \"contents\": [\"What is the meaning of life?\"]}]}\n...\n</code></pre> <p>data/passages.jsonl</p> <pre><code>{\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a cat.\"]}]}\n{\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a dog.\"]}]}\n...\n</code></pre> <p>The loader will create two separate pools for each file: queries and passages. Each pool is a text dataset with a different field name (e.g. query, passage), and loader will later sample from these two pools to create conversations.</p> Source code in <code>aiperf/services/dataset/loader/random_pool.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.RANDOM_POOL)\nclass RandomPoolDatasetLoader:\n    \"\"\"A dataset loader that loads data from a single file or a directory.\n\n    Each line in the file represents single-turn conversation data,\n    and files create individual pools for random sampling:\n      - Single file: All lines form one single pool (to be randomly sampled from)\n      - Directory: Each file becomes a separate pool, then pools are randomly sampled\n                   and merged into conversations later.\n\n    The random pool custom dataset\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch size &gt; 1)\n      - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)\n      - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)\n\n    Example:\n\n    1. Single file\n    ```jsonl\n    {\"text\": \"Who are you?\", \"image\": \"/path/to/image1.png\"}\n    {\"text\": \"Explain what is the meaning of life.\", \"image\": \"/path/to/image2.png\"}\n    ...\n    ```\n    The file will form a single pool of text and image data that will be used\n    to generate conversations.\n\n    2. Directory\n\n    Directory will be useful if user wants to\n      - create multiple pools of different modalities separately (e.g. text, image)\n      - specify different field names for the same modality.\n\n    data/queries.jsonl\n    ```jsonl\n    {\"texts\": [{\"name\": \"query\", \"contents\": [\"Who are you?\"]}]}\n    {\"texts\": [{\"name\": \"query\", \"contents\": [\"What is the meaning of life?\"]}]}\n    ...\n    ```\n\n    data/passages.jsonl\n    ```jsonl\n    {\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a cat.\"]}]}\n    {\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a dog.\"]}]}\n    ...\n    ```\n\n    The loader will create two separate pools for each file: queries and passages.\n    Each pool is a text dataset with a different field name (e.g. query, passage),\n    and loader will later sample from these two pools to create conversations.\n    \"\"\"\n\n    def __init__(self, filename: str):\n        self.filename = filename\n\n    def load_dataset(self) -&gt; dict[Filename, list[RandomPool]]:\n        \"\"\"Load random pool data from a file or directory.\n\n        If filename is a file, reads and parses using the RandomPool model.\n        If filename is a directory, reads each file in the directory and merges\n        items with different modality names into combined RandomPool objects.\n\n        Returns:\n            A dictionary mapping filename to list of RandomPool objects.\n        \"\"\"\n        path = Path(self.filename)\n\n        if path.is_file():\n            dataset_pool = self._load_dataset_from_file(path)\n            return {path.name: dataset_pool}\n\n        return self._load_dataset_from_dir(path)\n\n    def _load_dataset_from_file(self, file_path: Path) -&gt; list[RandomPool]:\n        \"\"\"Load random pool data from a single file.\n\n        Args:\n            file_path: The path to the file containing the data.\n\n        Returns:\n            A list of RandomPool objects.\n        \"\"\"\n        dataset_pool: list[RandomPool] = []\n\n        with open(file_path) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                random_pool_data = RandomPool.model_validate_json(line)\n                dataset_pool.append(random_pool_data)\n\n        return dataset_pool\n\n    def _load_dataset_from_dir(\n        self, dir_path: Path\n    ) -&gt; dict[Filename, list[RandomPool]]:\n        \"\"\"Load random pool data from all files in a directory.\n\n        Args:\n            dir_path: The path to the directory containing the files.\n\n        Returns:\n            A dictionary mapping filename to list of RandomPool objects.\n        \"\"\"\n        data: dict[Filename, list[RandomPool]] = defaultdict(list)\n\n        for file_path in dir_path.iterdir():\n            if file_path.is_file():\n                dataset_pool = self._load_dataset_from_file(file_path)\n                data[file_path.name].extend(dataset_pool)\n\n        return data\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.random_pool.RandomPoolDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load random pool data from a file or directory.</p> <p>If filename is a file, reads and parses using the RandomPool model. If filename is a directory, reads each file in the directory and merges items with different modality names into combined RandomPool objects.</p> <p>Returns:</p> Type Description <code>dict[Filename, list[RandomPool]]</code> <p>A dictionary mapping filename to list of RandomPool objects.</p> Source code in <code>aiperf/services/dataset/loader/random_pool.py</code> <pre><code>def load_dataset(self) -&gt; dict[Filename, list[RandomPool]]:\n    \"\"\"Load random pool data from a file or directory.\n\n    If filename is a file, reads and parses using the RandomPool model.\n    If filename is a directory, reads each file in the directory and merges\n    items with different modality names into combined RandomPool objects.\n\n    Returns:\n        A dictionary mapping filename to list of RandomPool objects.\n    \"\"\"\n    path = Path(self.filename)\n\n    if path.is_file():\n        dataset_pool = self._load_dataset_from_file(path)\n        return {path.name: dataset_pool}\n\n    return self._load_dataset_from_dir(path)\n</code></pre>"},{"location":"api/#aiperfservicesdatasetloadersingle_turn","title":"aiperf.services.dataset.loader.single_turn","text":""},{"location":"api/#aiperf.services.dataset.loader.single_turn.SingleTurnDatasetLoader","title":"<code>SingleTurnDatasetLoader</code>","text":"<p>A dataset loader that loads single turn data from a file.</p> <p>The single turn type   - supports multi-modal data (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch_size &gt; 1)   - DOES NOT support multi-turn features (e.g. delay, sessions, etc.)</p> <p>Examples: 1. Single-batch, text only</p> <pre><code>{\"text\": \"What is deep learning?\"}\n</code></pre> <ol> <li>Single-batch, multi-modal</li> </ol> <pre><code>{\"text\": \"What is in the image?\", \"image\": \"/path/to/image.png\"}\n</code></pre> <ol> <li>Multi-batch, multi-modal</li> </ol> <pre><code>{\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/to/image.png\", \"/path/to/image2.png\"]}\n</code></pre> <ol> <li>Fixed schedule version</li> </ol> <pre><code>{\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n{\"timestamp\": 1000, \"text\": \"Who are you?\"},\n{\"timestamp\": 2000, \"text\": \"What is AI?\"}\n</code></pre> <ol> <li>Time delayed version</li> </ol> <pre><code>{\"delay\": 0, \"text\": \"What is deep learning?\"},\n{\"delay\": 1234, \"text\": \"Who are you?\"}\n</code></pre> <ol> <li>Full-featured version (Multi-batch, multi-modal, multi-fielded)</li> </ol> <pre><code>{\n    \"texts\": [\n        {\"name\": \"text_field_A\", \"contents\": [\"Hello\", \"World\"]},\n        {\"name\": \"text_field_B\", \"contents\": [\"Hi there\"]}\n    ],\n    \"images\": [\n        {\"name\": \"image_field_A\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n        {\"name\": \"image_field_B\", \"contents\": [\"/path/3.png\"]}\n    ]\n}\n</code></pre> Source code in <code>aiperf/services/dataset/loader/single_turn.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.SINGLE_TURN)\nclass SingleTurnDatasetLoader:\n    \"\"\"A dataset loader that loads single turn data from a file.\n\n    The single turn type\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch_size &gt; 1)\n      - DOES NOT support multi-turn features (e.g. delay, sessions, etc.)\n\n    Examples:\n    1. Single-batch, text only\n    ```json\n    {\"text\": \"What is deep learning?\"}\n    ```\n\n    2. Single-batch, multi-modal\n    ```json\n    {\"text\": \"What is in the image?\", \"image\": \"/path/to/image.png\"}\n    ```\n\n    3. Multi-batch, multi-modal\n    ```json\n    {\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/to/image.png\", \"/path/to/image2.png\"]}\n    ```\n\n    4. Fixed schedule version\n    ```json\n    {\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n    {\"timestamp\": 1000, \"text\": \"Who are you?\"},\n    {\"timestamp\": 2000, \"text\": \"What is AI?\"}\n    ```\n\n    5. Time delayed version\n    ```json\n    {\"delay\": 0, \"text\": \"What is deep learning?\"},\n    {\"delay\": 1234, \"text\": \"Who are you?\"}\n    ```\n\n    6. Full-featured version (Multi-batch, multi-modal, multi-fielded)\n    ```json\n    {\n        \"texts\": [\n            {\"name\": \"text_field_A\", \"contents\": [\"Hello\", \"World\"]},\n            {\"name\": \"text_field_B\", \"contents\": [\"Hi there\"]}\n        ],\n        \"images\": [\n            {\"name\": \"image_field_A\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n            {\"name\": \"image_field_B\", \"contents\": [\"/path/3.png\"]}\n        ]\n    }\n    ```\n    \"\"\"\n\n    def __init__(self, filename: str):\n        self.filename = filename\n\n    def load_dataset(self) -&gt; dict[str, list[SingleTurn]]:\n        \"\"\"Load single-turn data from a JSONL file.\n\n        Each line represents a single turn conversation. Multiple turns with\n        the same session_id (or generated UUID) are grouped together.\n\n        Returns:\n            A dictionary mapping session_id to list of CustomData.\n        \"\"\"\n        data: dict[str, list[SingleTurn]] = defaultdict(list)\n\n        with open(self.filename) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                single_turn_data = SingleTurn.model_validate_json(line)\n                session_id = str(uuid.uuid4())\n                data[session_id].append(single_turn_data)\n\n        return data\n</code></pre>"},{"location":"api/#aiperf.services.dataset.loader.single_turn.SingleTurnDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load single-turn data from a JSONL file.</p> <p>Each line represents a single turn conversation. Multiple turns with the same session_id (or generated UUID) are grouped together.</p> <p>Returns:</p> Type Description <code>dict[str, list[SingleTurn]]</code> <p>A dictionary mapping session_id to list of CustomData.</p> Source code in <code>aiperf/services/dataset/loader/single_turn.py</code> <pre><code>def load_dataset(self) -&gt; dict[str, list[SingleTurn]]:\n    \"\"\"Load single-turn data from a JSONL file.\n\n    Each line represents a single turn conversation. Multiple turns with\n    the same session_id (or generated UUID) are grouped together.\n\n    Returns:\n        A dictionary mapping session_id to list of CustomData.\n    \"\"\"\n    data: dict[str, list[SingleTurn]] = defaultdict(list)\n\n    with open(self.filename) as f:\n        for line in f:\n            if (line := line.strip()) == \"\":\n                continue  # Skip empty lines\n\n            single_turn_data = SingleTurn.model_validate_json(line)\n            session_id = str(uuid.uuid4())\n            data[session_id].append(single_turn_data)\n\n    return data\n</code></pre>"},{"location":"api/#aiperfservicesdatasetutils","title":"aiperf.services.dataset.utils","text":""},{"location":"api/#aiperf.services.dataset.utils.check_file_exists","title":"<code>check_file_exists(filename)</code>","text":"<p>Verifies that the file exists.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>The file path to verify.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def check_file_exists(filename: Path) -&gt; None:\n    \"\"\"Verifies that the file exists.\n\n    Args:\n        filename : The file path to verify.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    if not filename.exists():\n        raise FileNotFoundError(f\"The file '{filename}' does not exist.\")\n</code></pre>"},{"location":"api/#aiperf.services.dataset.utils.encode_image","title":"<code>encode_image(img, format)</code>","text":"<p>Encodes an image into base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Image</code> <p>The PIL Image object to encode.</p> required <code>format</code> <code>str</code> <p>The image format to use (e.g., \"JPEG\", \"PNG\").</p> required <p>Returns:</p> Type Description <code>str</code> <p>A base64 encoded string representation of the image.</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def encode_image(img: Image, format: str) -&gt; str:\n    \"\"\"Encodes an image into base64 encoded string.\n\n    Args:\n        img: The PIL Image object to encode.\n        format: The image format to use (e.g., \"JPEG\", \"PNG\").\n\n    Returns:\n        A base64 encoded string representation of the image.\n    \"\"\"\n    # JPEG does not support P or RGBA mode (commonly used for PNG) so it needs\n    # to be converted to RGB before an image can be saved as JPEG format.\n    if format == \"JPEG\" and img.mode != \"RGB\":\n        img = img.convert(\"RGB\")\n\n    buffer = BytesIO()\n    img.save(buffer, format=format)\n    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n</code></pre>"},{"location":"api/#aiperf.services.dataset.utils.load_json_str","title":"<code>load_json_str(json_str, func=lambda x: x)</code>","text":"<p>Deserializes JSON encoded string into Python object.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>JSON encoded string</p> required <code>func</code> <code>Callable</code> <p>A function that takes deserialized JSON object. This can be used to run validation checks on the object. Defaults to identity function.</p> <code>lambda x: x</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The deserialized JSON object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the JSON string is invalid.</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def load_json_str(json_str: str, func: Callable = lambda x: x) -&gt; dict[str, Any]:\n    \"\"\"Deserializes JSON encoded string into Python object.\n\n    Args:\n        json_str: JSON encoded string\n        func: A function that takes deserialized JSON object. This can be used to\n            run validation checks on the object. Defaults to identity function.\n\n    Returns:\n        The deserialized JSON object.\n\n    Raises:\n        RuntimeError: If the JSON string is invalid.\n    \"\"\"\n    try:\n        # TODO: Consider using orjson for faster JSON parsing\n        return func(json.loads(json_str))\n    except json.JSONDecodeError as e:\n        snippet = json_str[:200] + (\"...\" if len(json_str) &gt; 200 else \"\")\n        raise RuntimeError(f\"Failed to parse JSON string: '{snippet}'\") from e\n</code></pre>"},{"location":"api/#aiperf.services.dataset.utils.open_image","title":"<code>open_image(filename)</code>","text":"<p>Opens an image file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>The file path to open.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The opened PIL Image object.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def open_image(filename: str) -&gt; Image:\n    \"\"\"Opens an image file.\n\n    Args:\n        filename : The file path to open.\n\n    Returns:\n        The opened PIL Image object.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    check_file_exists(Path(filename))\n    img = Image.open(filename)\n\n    if img.format is None:\n        raise RuntimeError(f\"Failed to determine image format of '{filename}'.\")\n\n    if img.format.upper() not in list(ImageFormat):\n        raise RuntimeError(\n            f\"'{img.format}' is not one of the supported image formats: \"\n            f\"{', '.join(ImageFormat)}\"\n        )\n    return img\n</code></pre>"},{"location":"api/#aiperf.services.dataset.utils.sample_normal","title":"<code>sample_normal(mean, stddev, lower=-np.inf, upper=np.inf)</code>","text":"<p>Sample from a normal distribution with support for bounds using rejection sampling.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>stddev</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <code>lower</code> <code>float</code> <p>The lower bound of the distribution.</p> <code>-inf</code> <code>upper</code> <code>float</code> <p>The upper bound of the distribution.</p> <code>inf</code> <p>Returns:</p> Type Description <code>int</code> <p>An integer sampled from the distribution.</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def sample_normal(\n    mean: float, stddev: float, lower: float = -np.inf, upper: float = np.inf\n) -&gt; int:\n    \"\"\"Sample from a normal distribution with support for bounds using rejection sampling.\n\n    Args:\n        mean: The mean of the normal distribution.\n        stddev: The standard deviation of the normal distribution.\n        lower: The lower bound of the distribution.\n        upper: The upper bound of the distribution.\n\n    Returns:\n        An integer sampled from the distribution.\n    \"\"\"\n    while True:\n        n = np.random.normal(mean, stddev)\n        if lower &lt;= n &lt;= upper:\n            return n\n</code></pre>"},{"location":"api/#aiperf.services.dataset.utils.sample_positive_normal","title":"<code>sample_positive_normal(mean, stddev)</code>","text":"<p>Sample from a normal distribution ensuring positive values without distorting the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>Mean value for the normal distribution</p> required <code>stddev</code> <code>float</code> <p>Standard deviation for the normal distribution</p> required <p>Returns:</p> Type Description <code>float</code> <p>A positive sample from the normal distribution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean is less than 0</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def sample_positive_normal(mean: float, stddev: float) -&gt; float:\n    \"\"\"Sample from a normal distribution ensuring positive values\n    without distorting the distribution.\n\n    Args:\n        mean: Mean value for the normal distribution\n        stddev: Standard deviation for the normal distribution\n\n    Returns:\n        A positive sample from the normal distribution\n\n    Raises:\n        ValueError: If mean is less than 0\n    \"\"\"\n    if mean &lt; 0:\n        raise ValueError(f\"Mean value ({mean}) should be greater than 0\")\n    return sample_normal(mean, stddev, lower=0)\n</code></pre>"},{"location":"api/#aiperf.services.dataset.utils.sample_positive_normal_integer","title":"<code>sample_positive_normal_integer(mean, stddev)</code>","text":"<p>Sample a random positive integer from a normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>stddev</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <p>Returns:</p> Type Description <code>int</code> <p>A positive integer sampled from the distribution. If the sampled</p> <code>int</code> <p>number is less than 1, it returns 1.</p> Source code in <code>aiperf/services/dataset/utils.py</code> <pre><code>def sample_positive_normal_integer(mean: float, stddev: float) -&gt; int:\n    \"\"\"Sample a random positive integer from a normal distribution.\n\n    Args:\n        mean: The mean of the normal distribution.\n        stddev: The standard deviation of the normal distribution.\n\n    Returns:\n        A positive integer sampled from the distribution. If the sampled\n        number is less than 1, it returns 1.\n    \"\"\"\n    return math.ceil(sample_positive_normal(mean, stddev))\n</code></pre>"},{"location":"api/#aiperfservicesinference_result_parserinference_result_parser","title":"aiperf.services.inference_result_parser.inference_result_parser","text":""},{"location":"api/#aiperf.services.inference_result_parser.inference_result_parser.InferenceResultParser","title":"<code>InferenceResultParser</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code></p> <p>InferenceResultParser is responsible for parsing the inference results and pushing them to the RecordsManager.</p> Source code in <code>aiperf/services/inference_result_parser/inference_result_parser.py</code> <pre><code>@ServiceFactory.register(ServiceType.INFERENCE_RESULT_PARSER)\nclass InferenceResultParser(PullClientMixin, BaseComponentService):\n    \"\"\"InferenceResultParser is responsible for parsing the inference results\n    and pushing them to the RecordsManager.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.RAW_INFERENCE_PROXY_BACKEND,\n            pull_client_bind=False,\n            pull_client_max_concurrency=DEFAULT_PULL_CLIENT_MAX_CONCURRENCY,\n        )\n        self.debug(\"Initializing inference result parser\")\n\n        self.records_push_client: PushClientProtocol = self.comms.create_push_client(\n            CommAddress.RECORDS,\n        )\n        self.conversation_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n        self.tokenizers: dict[str, Tokenizer] = {}\n        self.user_config: UserConfig = user_config\n        self.tokenizer_lock: asyncio.Lock = asyncio.Lock()\n        self.model_endpoint: ModelEndpointInfo = ModelEndpointInfo.from_user_config(\n            user_config\n        )\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize inference result parser-specific components.\"\"\"\n        self.debug(\"Initializing inference result parser\")\n\n        self.extractor = ResponseExtractorFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n            model_endpoint=self.model_endpoint,\n        )\n\n        async with self.tokenizer_lock:\n            self.tokenizers = {\n                model.name: Tokenizer.from_pretrained(\n                    self.user_config.tokenizer.name or model.name,\n                    trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                    revision=self.user_config.tokenizer.revision,\n                )\n                for model in self.model_endpoint.models.models\n            }\n            self.info(\"Initialized tokenizers for %d models\", len(self.tokenizers))\n\n    async def get_tokenizer(self, model: str) -&gt; Tokenizer:\n        \"\"\"Get the tokenizer for a given model.\"\"\"\n        async with self.tokenizer_lock:\n            if model not in self.tokenizers:\n                self.tokenizers[model] = Tokenizer.from_pretrained(\n                    self.user_config.tokenizer.name or model,\n                    trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                    revision=self.user_config.tokenizer.revision,\n                )\n            return self.tokenizers[model]\n\n    @on_pull_message(MessageType.INFERENCE_RESULTS)\n    async def _on_inference_results(self, message: InferenceResultsMessage) -&gt; None:\n        \"\"\"Handle an inference results message.\"\"\"\n        self.debug(lambda: f\"Received inference results message: {message}\")\n\n        if message.record.has_error:\n            await self.records_push_client.push(\n                ParsedInferenceResultsMessage(\n                    service_id=self.service_id,\n                    record=ParsedResponseRecord(\n                        worker_id=message.service_id,\n                        request=message.record,\n                        responses=[],\n                    ),\n                )\n            )\n\n        elif message.record.valid:\n            try:\n                record = await self.process_valid_record(message)\n                self.debug(\n                    lambda: f\"Received {len(record.request.responses)} responses, input_token_count: {record.input_token_count}, output_token_count: {record.output_token_count}\"\n                )\n                await self.records_push_client.push(\n                    ParsedInferenceResultsMessage(\n                        service_id=self.service_id,\n                        record=record,\n                    )\n                )\n            except Exception as e:\n                self.exception(f\"Error processing valid record: {e}\")\n                await self.records_push_client.push(\n                    ParsedInferenceResultsMessage(\n                        service_id=self.service_id,\n                        record=ParsedResponseRecord(\n                            worker_id=message.service_id,\n                            request=message.record,\n                            responses=[],\n                        ),\n                    )\n                )\n        else:\n            self.warning(f\"Received invalid inference results: {message.record}\")\n            message.record.error = ErrorDetails(\n                code=None,\n                message=\"Invalid inference results\",\n                type=\"InvalidInferenceResults\",\n            )\n            await self.records_push_client.push(\n                ParsedInferenceResultsMessage(\n                    service_id=self.service_id,\n                    record=ParsedResponseRecord(\n                        worker_id=message.service_id,\n                        request=message.record,\n                        responses=[],\n                    ),\n                )\n            )\n\n    async def process_valid_record(\n        self, message: InferenceResultsMessage\n    ) -&gt; ParsedResponseRecord:\n        \"\"\"Process a valid request record.\"\"\"\n        if message.record.model_name is None:\n            self.warning(\n                lambda: f\"Model name is None, unable to process record: {message.record}\"\n            )\n            return ParsedResponseRecord(\n                worker_id=message.service_id,\n                request=message.record,\n                responses=[],\n                input_token_count=None,\n                output_token_count=None,\n            )\n\n        tokenizer = await self.get_tokenizer(message.record.model_name)\n        resp = await self.extractor.extract_response_data(message.record, tokenizer)\n        input_token_count = await self.compute_input_token_count(\n            message.record, tokenizer\n        )\n        output_token_count = sum(\n            response.token_count\n            for response in resp\n            if response.token_count is not None\n        )\n\n        return ParsedResponseRecord(\n            worker_id=message.service_id,\n            request=message.record,\n            responses=resp,\n            input_token_count=input_token_count,\n            output_token_count=output_token_count,\n        )\n\n    async def compute_input_token_count(\n        self, record: RequestRecord, tokenizer: Tokenizer\n    ) -&gt; int | None:\n        \"\"\"Compute the number of tokens in the input for a given request record.\"\"\"\n        if record.conversation_id is None or record.turn_index is None:\n            self.warning(\n                lambda: f\"Conversation ID or turn index is None: {record.conversation_id=} {record.turn_index=}\"\n            )\n            return None\n\n        turn_response: ConversationTurnResponseMessage = (\n            await self.conversation_request_client.request(\n                ConversationTurnRequestMessage(\n                    service_id=self.service_id,\n                    conversation_id=record.conversation_id,\n                    turn_index=record.turn_index,\n                )\n            )\n        )\n        if isinstance(turn_response, ErrorMessage):\n            self.error(lambda: f\"Error getting turn response: {turn_response}\")\n            return None\n\n        turn = turn_response.turn\n        return sum(\n            len(tokenizer.encode(content))\n            for text in turn.texts\n            for content in text.contents\n        )\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.inference_result_parser.InferenceResultParser.compute_input_token_count","title":"<code>compute_input_token_count(record, tokenizer)</code>  <code>async</code>","text":"<p>Compute the number of tokens in the input for a given request record.</p> Source code in <code>aiperf/services/inference_result_parser/inference_result_parser.py</code> <pre><code>async def compute_input_token_count(\n    self, record: RequestRecord, tokenizer: Tokenizer\n) -&gt; int | None:\n    \"\"\"Compute the number of tokens in the input for a given request record.\"\"\"\n    if record.conversation_id is None or record.turn_index is None:\n        self.warning(\n            lambda: f\"Conversation ID or turn index is None: {record.conversation_id=} {record.turn_index=}\"\n        )\n        return None\n\n    turn_response: ConversationTurnResponseMessage = (\n        await self.conversation_request_client.request(\n            ConversationTurnRequestMessage(\n                service_id=self.service_id,\n                conversation_id=record.conversation_id,\n                turn_index=record.turn_index,\n            )\n        )\n    )\n    if isinstance(turn_response, ErrorMessage):\n        self.error(lambda: f\"Error getting turn response: {turn_response}\")\n        return None\n\n    turn = turn_response.turn\n    return sum(\n        len(tokenizer.encode(content))\n        for text in turn.texts\n        for content in text.contents\n    )\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.inference_result_parser.InferenceResultParser.get_tokenizer","title":"<code>get_tokenizer(model)</code>  <code>async</code>","text":"<p>Get the tokenizer for a given model.</p> Source code in <code>aiperf/services/inference_result_parser/inference_result_parser.py</code> <pre><code>async def get_tokenizer(self, model: str) -&gt; Tokenizer:\n    \"\"\"Get the tokenizer for a given model.\"\"\"\n    async with self.tokenizer_lock:\n        if model not in self.tokenizers:\n            self.tokenizers[model] = Tokenizer.from_pretrained(\n                self.user_config.tokenizer.name or model,\n                trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                revision=self.user_config.tokenizer.revision,\n            )\n        return self.tokenizers[model]\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.inference_result_parser.InferenceResultParser.process_valid_record","title":"<code>process_valid_record(message)</code>  <code>async</code>","text":"<p>Process a valid request record.</p> Source code in <code>aiperf/services/inference_result_parser/inference_result_parser.py</code> <pre><code>async def process_valid_record(\n    self, message: InferenceResultsMessage\n) -&gt; ParsedResponseRecord:\n    \"\"\"Process a valid request record.\"\"\"\n    if message.record.model_name is None:\n        self.warning(\n            lambda: f\"Model name is None, unable to process record: {message.record}\"\n        )\n        return ParsedResponseRecord(\n            worker_id=message.service_id,\n            request=message.record,\n            responses=[],\n            input_token_count=None,\n            output_token_count=None,\n        )\n\n    tokenizer = await self.get_tokenizer(message.record.model_name)\n    resp = await self.extractor.extract_response_data(message.record, tokenizer)\n    input_token_count = await self.compute_input_token_count(\n        message.record, tokenizer\n    )\n    output_token_count = sum(\n        response.token_count\n        for response in resp\n        if response.token_count is not None\n    )\n\n    return ParsedResponseRecord(\n        worker_id=message.service_id,\n        request=message.record,\n        responses=resp,\n        input_token_count=input_token_count,\n        output_token_count=output_token_count,\n    )\n</code></pre>"},{"location":"api/#aiperfservicesinference_result_parseropenai_parsers","title":"aiperf.services.inference_result_parser.openai_parsers","text":""},{"location":"api/#aiperf.services.inference_result_parser.openai_parsers.OpenAIObject","title":"<code>OpenAIObject</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Types of OpenAI objects.</p> Source code in <code>aiperf/services/inference_result_parser/openai_parsers.py</code> <pre><code>class OpenAIObject(CaseInsensitiveStrEnum):\n    \"\"\"Types of OpenAI objects.\"\"\"\n\n    CHAT_COMPLETION = \"chat.completion\"\n    CHAT_COMPLETION_CHUNK = \"chat.completion.chunk\"\n    COMPLETION = \"completion\"\n    EMBEDDING = \"embedding\"\n    LIST = \"list\"\n    RESPONSE = \"response\"\n    TEXT_COMPLETION = \"text_completion\"\n\n    @classmethod\n    def parse(cls, text: str) -&gt; BaseModel:\n        \"\"\"Attempt to parse a string into an OpenAI object.\n\n        Raises:\n            ValueError: If the object is invalid.\n        \"\"\"\n        try:\n            obj = load_json_str(text)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid OpenAI object: {text}\") from e\n\n        # Mapping of OpenAI object types to their corresponding Pydantic models.\n        _object_mapping: dict[str, type[BaseModel]] = {\n            cls.CHAT_COMPLETION: ChatCompletion,\n            cls.CHAT_COMPLETION_CHUNK: ChatCompletionChunk,\n            cls.COMPLETION: Completion,\n            cls.RESPONSE: ResponsesModel,\n            cls.TEXT_COMPLETION: Completion,  # Alias for vLLM compatibility\n        }\n\n        obj_type = obj.get(\"object\")\n        if obj_type is None:\n            raise ValueError(f\"Invalid OpenAI object: {obj}\")\n        if obj_type == cls.LIST:\n            return cls.parse_list(obj)\n        if obj_type not in _object_mapping:\n            raise ValueError(f\"Invalid OpenAI object type: {obj_type}\")\n        try:\n            # Hotfix: vLLM does not always include a finish_reason, which Pydantic requires.\n            # Without this code, model_validate will raise an objection due to the missing finish_reason.\n            if obj_type == cls.TEXT_COMPLETION:\n                for choice in obj.get(\"choices\", []):\n                    if choice.get(\"finish_reason\") is None:\n                        choice[\"finish_reason\"] = \"stop\"\n            return _object_mapping[obj_type].model_validate(obj)\n        except Exception as e:\n            raise ValueError(f\"Invalid OpenAI object: {text}\") from e\n\n    @classmethod\n    def parse_list(cls, obj: Any) -&gt; BaseModel:\n        \"\"\"Attempt to parse a string into an OpenAI object from a list.\n\n        Raises:\n            ValueError: If the object is invalid.\n        \"\"\"\n        data = obj.get(\"data\", [])\n        if all(\n            isinstance(item, dict) and item.get(\"object\") == cls.EMBEDDING\n            for item in data\n        ):\n            return CreateEmbeddingResponse.model_validate(obj)\n        else:\n            raise ValueError(f\"Receive invalid list in response: {obj}\")\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.openai_parsers.OpenAIObject.parse","title":"<code>parse(text)</code>  <code>classmethod</code>","text":"<p>Attempt to parse a string into an OpenAI object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object is invalid.</p> Source code in <code>aiperf/services/inference_result_parser/openai_parsers.py</code> <pre><code>@classmethod\ndef parse(cls, text: str) -&gt; BaseModel:\n    \"\"\"Attempt to parse a string into an OpenAI object.\n\n    Raises:\n        ValueError: If the object is invalid.\n    \"\"\"\n    try:\n        obj = load_json_str(text)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid OpenAI object: {text}\") from e\n\n    # Mapping of OpenAI object types to their corresponding Pydantic models.\n    _object_mapping: dict[str, type[BaseModel]] = {\n        cls.CHAT_COMPLETION: ChatCompletion,\n        cls.CHAT_COMPLETION_CHUNK: ChatCompletionChunk,\n        cls.COMPLETION: Completion,\n        cls.RESPONSE: ResponsesModel,\n        cls.TEXT_COMPLETION: Completion,  # Alias for vLLM compatibility\n    }\n\n    obj_type = obj.get(\"object\")\n    if obj_type is None:\n        raise ValueError(f\"Invalid OpenAI object: {obj}\")\n    if obj_type == cls.LIST:\n        return cls.parse_list(obj)\n    if obj_type not in _object_mapping:\n        raise ValueError(f\"Invalid OpenAI object type: {obj_type}\")\n    try:\n        # Hotfix: vLLM does not always include a finish_reason, which Pydantic requires.\n        # Without this code, model_validate will raise an objection due to the missing finish_reason.\n        if obj_type == cls.TEXT_COMPLETION:\n            for choice in obj.get(\"choices\", []):\n                if choice.get(\"finish_reason\") is None:\n                    choice[\"finish_reason\"] = \"stop\"\n        return _object_mapping[obj_type].model_validate(obj)\n    except Exception as e:\n        raise ValueError(f\"Invalid OpenAI object: {text}\") from e\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.openai_parsers.OpenAIObject.parse_list","title":"<code>parse_list(obj)</code>  <code>classmethod</code>","text":"<p>Attempt to parse a string into an OpenAI object from a list.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object is invalid.</p> Source code in <code>aiperf/services/inference_result_parser/openai_parsers.py</code> <pre><code>@classmethod\ndef parse_list(cls, obj: Any) -&gt; BaseModel:\n    \"\"\"Attempt to parse a string into an OpenAI object from a list.\n\n    Raises:\n        ValueError: If the object is invalid.\n    \"\"\"\n    data = obj.get(\"data\", [])\n    if all(\n        isinstance(item, dict) and item.get(\"object\") == cls.EMBEDDING\n        for item in data\n    ):\n        return CreateEmbeddingResponse.model_validate(obj)\n    else:\n        raise ValueError(f\"Receive invalid list in response: {obj}\")\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.openai_parsers.OpenAIResponseExtractor","title":"<code>OpenAIResponseExtractor</code>","text":"<p>Extractor for OpenAI responses.</p> Source code in <code>aiperf/services/inference_result_parser/openai_parsers.py</code> <pre><code>@ResponseExtractorFactory.register_all(\n    EndpointType.OPENAI_CHAT_COMPLETIONS,\n    EndpointType.OPENAI_COMPLETIONS,\n    EndpointType.OPENAI_EMBEDDINGS,\n    EndpointType.OPENAI_RESPONSES,\n)\nclass OpenAIResponseExtractor:\n    \"\"\"Extractor for OpenAI responses.\"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfo) -&gt; None:\n        \"\"\"Create a new response extractor based on the provided configuration.\"\"\"\n        self.model_endpoint = model_endpoint\n\n    def _parse_text_response(self, response: TextResponse) -&gt; ResponseData | None:\n        \"\"\"Parse a TextResponse into a ResponseData object.\"\"\"\n        raw = response.text\n        parsed = self._parse_text(raw)\n        if parsed is None:\n            return None\n\n        return ResponseData(\n            perf_ns=response.perf_ns,\n            raw_text=[raw],\n            parsed_text=[parsed],\n            metadata={},\n        )\n\n    def _parse_sse_response(self, response: SSEMessage) -&gt; ResponseData | None:\n        \"\"\"Parse a SSEMessage into a ResponseData object.\"\"\"\n        raw = response.extract_data_content()\n        parsed = self._parse_sse(raw)\n        if parsed is None or len(parsed) == 0:\n            return None\n\n        return ResponseData(\n            perf_ns=response.perf_ns,\n            raw_text=raw,\n            parsed_text=parsed,\n            metadata={},\n        )\n\n    def _parse_response(self, response: InferenceServerResponse) -&gt; ResponseData | None:\n        \"\"\"Parse a response into a ResponseData object.\"\"\"\n        if isinstance(response, TextResponse):\n            return self._parse_text_response(response)\n        elif isinstance(response, SSEMessage):\n            return self._parse_sse_response(response)\n\n    async def extract_response_data(\n        self, record: RequestRecord, tokenizer: Tokenizer | None\n    ) -&gt; list[ResponseData]:\n        \"\"\"Extract the text from a server response message.\"\"\"\n        results = []\n        for response in record.responses:\n            response_data = self._parse_response(response)\n            if response_data is None:\n                continue\n\n            if tokenizer is not None:\n                response_data.token_count = sum(\n                    len(tokenizer.encode(text))\n                    for text in response_data.parsed_text\n                    if text is not None\n                )\n            results.append(response_data)\n        return results\n\n    def _parse_text(self, raw_text: str) -&gt; Any | None:\n        \"\"\"Parse the text of the response.\"\"\"\n        if raw_text in (\"\", None, \"[DONE]\"):\n            return None\n\n        obj = OpenAIObject.parse(raw_text)\n\n        # Dictionary mapping object types to their value extraction functions\n        type_to_extractor = {\n            # TODO: how to support multiple choices?\n            ChatCompletion: lambda obj: obj.choices[0].message.content,\n            # TODO: how to support multiple choices?\n            ChatCompletionChunk: lambda obj: obj.choices[0].delta.content,\n            # TODO: how to support multiple choices?\n            Completion: lambda obj: obj.choices[0].text,\n            CreateEmbeddingResponse: lambda obj: \"\",  # Don't store embedding data\n            ResponsesModel: lambda obj: obj.output_text,\n        }\n\n        for obj_type, extractor in type_to_extractor.items():\n            if isinstance(obj, obj_type):\n                return extractor(obj)\n\n        raise ValueError(f\"Invalid OpenAI object: {raw_text}\")\n\n    def _parse_sse(self, raw_sse: list[str]) -&gt; list[Any]:\n        \"\"\"Parse the SSE of the response.\"\"\"\n        result = []\n        for sse in raw_sse:\n            parsed = self._parse_text(sse)\n            if parsed is None:\n                continue\n            result.append(parsed)\n        return result\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.openai_parsers.OpenAIResponseExtractor.__init__","title":"<code>__init__(model_endpoint)</code>","text":"<p>Create a new response extractor based on the provided configuration.</p> Source code in <code>aiperf/services/inference_result_parser/openai_parsers.py</code> <pre><code>def __init__(self, model_endpoint: ModelEndpointInfo) -&gt; None:\n    \"\"\"Create a new response extractor based on the provided configuration.\"\"\"\n    self.model_endpoint = model_endpoint\n</code></pre>"},{"location":"api/#aiperf.services.inference_result_parser.openai_parsers.OpenAIResponseExtractor.extract_response_data","title":"<code>extract_response_data(record, tokenizer)</code>  <code>async</code>","text":"<p>Extract the text from a server response message.</p> Source code in <code>aiperf/services/inference_result_parser/openai_parsers.py</code> <pre><code>async def extract_response_data(\n    self, record: RequestRecord, tokenizer: Tokenizer | None\n) -&gt; list[ResponseData]:\n    \"\"\"Extract the text from a server response message.\"\"\"\n    results = []\n    for response in record.responses:\n        response_data = self._parse_response(response)\n        if response_data is None:\n            continue\n\n        if tokenizer is not None:\n            response_data.token_count = sum(\n                len(tokenizer.encode(text))\n                for text in response_data.parsed_text\n                if text is not None\n            )\n        results.append(response_data)\n    return results\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricsbase_metric","title":"aiperf.services.records_manager.metrics.base_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.base_metric.BaseMetric","title":"<code>BaseMetric</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all metrics with automatic subclass registration.</p> Source code in <code>aiperf/services/records_manager/metrics/base_metric.py</code> <pre><code>class BaseMetric(ABC):\n    \"\"\"Base class for all metrics with automatic subclass registration.\"\"\"\n\n    # Class attributes that subclasses must override\n    tag: ClassVar[MetricTagT] = \"\"\n    unit: ClassVar[MetricTimeType | None] = None\n    larger_is_better: ClassVar[bool] = True\n    header: ClassVar[str] = \"\"\n    streaming_only: ClassVar[bool] = False\n    required_metrics: ClassVar[set[MetricTagT]] = set()\n\n    base_metrics: set[MetricTagT] = set()\n    metric_interfaces: dict[MetricTagT, type[\"BaseMetric\"]] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"\n        This method is called when a class is subclassed from Metric.\n        It automatically registers the subclass in the metric_interfaces\n        dictionary using the `tag` class attribute.\n        The `tag` attribute must be a non-empty string that uniquely identifies the\n        metric type. Only concrete (non-abstract) classes will be registered.\n        \"\"\"\n\n        super().__init_subclass__(**kwargs)\n\n        # Only register concrete classes (not abstract ones)\n        if inspect.isabstract(cls):\n            return\n\n        # Enforce that subclasses define a non-empty tag\n        if not cls.tag or not isinstance(cls.tag, str):\n            raise TypeError(\n                f\"Concrete metric class {cls.__name__} must define a non-empty 'tag' class attribute\"\n            )\n\n        # Check for duplicate tags\n        if cls.tag in cls.metric_interfaces:\n            raise ValueError(\n                f\"Metric tag '{cls.tag}' is already registered by {cls.metric_interfaces[cls.tag].__name__}\"\n            )\n\n        cls.metric_interfaces[cls.tag] = cls\n\n    @classmethod\n    def get_all(cls) -&gt; dict[str, type[\"BaseMetric\"]]:\n        \"\"\"\n        Returns the dictionary of all registered metric interfaces.\n\n        This method dynamically imports all metric type modules from the 'types'\n        directory to ensure all metric classes are registered via __init_subclass__.\n\n        Returns:\n            dict[str, type[Metric]]: Mapping of metric tags to their corresponding classes\n\n        Raises:\n            MetricTypeError: If there's an error importing metric type modules\n        \"\"\"\n        # Get the types directory path\n        types_dir = Path(__file__).parent / \"types\"\n\n        # Import all metric type modules to trigger registration\n        if types_dir.exists():\n            for python_file in types_dir.glob(\"*.py\"):\n                if python_file.name != \"__init__.py\":\n                    module_name = python_file.stem  # Get filename without extension\n                    try:\n                        importlib.import_module(\n                            f\"aiperf.services.records_manager.metrics.types.{module_name}\"\n                        )\n                    except ImportError as err:\n                        raise MetricTypeError(\n                            f\"Error importing metric type module '{module_name}'\"\n                        ) from err\n\n        return cls.metric_interfaces\n\n    @abstractmethod\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Updates the metric value based on the provided record and dictionary of other metrics.\n\n        Args:\n            record (Optional[Record]): The record to update the metric with.\n            metrics (Optional[dict[BaseMetric]]): A dictionary of other metrics that may be needed for calculation.\n        \"\"\"\n\n    @abstractmethod\n    def values(self) -&gt; Any:\n        \"\"\"\n        Returns the list of calculated metrics.\n        \"\"\"\n\n    @abstractmethod\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Checks if the record is valid for metric calculation.\n\n        Raises:\n            ValueError: If the record does not meet the required conditions.\n        \"\"\"\n\n    def get_converted_metrics(self, unit: MetricTimeType | None) -&gt; list[Any]:\n        if not isinstance(unit, MetricTimeType) or not isinstance(\n            self.unit, MetricTimeType\n        ):\n            raise MetricTypeError(\"Invalid metric time type for conversion.\")\n\n        scale_factor = self._get_conversion_factor(self.unit, unit)\n\n        return [metric / 10**scale_factor for metric in self.values()]\n\n    def _check_metrics(self, metrics: dict[MetricTagT, \"BaseMetric\"] | None) -&gt; None:\n        \"\"\"\n        Validates that the required dependent metrics are available.\n\n        Raises:\n            ValueError: If required metrics are missing.\n        \"\"\"\n        if not metrics:\n            raise ValueError(\"Metrics dictionary is missing.\")\n\n        for tag in self.required_metrics:\n            if tag not in metrics:\n                raise ValueError(f\"Missing required metric: '{tag}'\")\n\n    def _get_conversion_factor(\n        self, from_unit: MetricTimeType, to_unit: MetricTimeType\n    ) -&gt; int:\n        unit_scales = {\n            MetricTimeType.NANOSECONDS: 9,\n            MetricTimeType.MILLISECONDS: 3,\n            MetricTimeType.SECONDS: 0,\n        }\n\n        return unit_scales[from_unit] - unit_scales[to_unit]\n\n    def _require_valid_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Ensures the given record is not None and is marked as valid.\n\n        Raises:\n            ValueError: If the record is None or invalid.\n        \"\"\"\n        if not record or not record.valid:\n            raise ValueError(\"Invalid Record\")\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.base_metric.BaseMetric.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>This method is called when a class is subclassed from Metric. It automatically registers the subclass in the metric_interfaces dictionary using the <code>tag</code> class attribute. The <code>tag</code> attribute must be a non-empty string that uniquely identifies the metric type. Only concrete (non-abstract) classes will be registered.</p> Source code in <code>aiperf/services/records_manager/metrics/base_metric.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"\n    This method is called when a class is subclassed from Metric.\n    It automatically registers the subclass in the metric_interfaces\n    dictionary using the `tag` class attribute.\n    The `tag` attribute must be a non-empty string that uniquely identifies the\n    metric type. Only concrete (non-abstract) classes will be registered.\n    \"\"\"\n\n    super().__init_subclass__(**kwargs)\n\n    # Only register concrete classes (not abstract ones)\n    if inspect.isabstract(cls):\n        return\n\n    # Enforce that subclasses define a non-empty tag\n    if not cls.tag or not isinstance(cls.tag, str):\n        raise TypeError(\n            f\"Concrete metric class {cls.__name__} must define a non-empty 'tag' class attribute\"\n        )\n\n    # Check for duplicate tags\n    if cls.tag in cls.metric_interfaces:\n        raise ValueError(\n            f\"Metric tag '{cls.tag}' is already registered by {cls.metric_interfaces[cls.tag].__name__}\"\n        )\n\n    cls.metric_interfaces[cls.tag] = cls\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.base_metric.BaseMetric.get_all","title":"<code>get_all()</code>  <code>classmethod</code>","text":"<p>Returns the dictionary of all registered metric interfaces.</p> <p>This method dynamically imports all metric type modules from the 'types' directory to ensure all metric classes are registered via init_subclass.</p> <p>Returns:</p> Type Description <code>dict[str, type[BaseMetric]]</code> <p>dict[str, type[Metric]]: Mapping of metric tags to their corresponding classes</p> <p>Raises:</p> Type Description <code>MetricTypeError</code> <p>If there's an error importing metric type modules</p> Source code in <code>aiperf/services/records_manager/metrics/base_metric.py</code> <pre><code>@classmethod\ndef get_all(cls) -&gt; dict[str, type[\"BaseMetric\"]]:\n    \"\"\"\n    Returns the dictionary of all registered metric interfaces.\n\n    This method dynamically imports all metric type modules from the 'types'\n    directory to ensure all metric classes are registered via __init_subclass__.\n\n    Returns:\n        dict[str, type[Metric]]: Mapping of metric tags to their corresponding classes\n\n    Raises:\n        MetricTypeError: If there's an error importing metric type modules\n    \"\"\"\n    # Get the types directory path\n    types_dir = Path(__file__).parent / \"types\"\n\n    # Import all metric type modules to trigger registration\n    if types_dir.exists():\n        for python_file in types_dir.glob(\"*.py\"):\n            if python_file.name != \"__init__.py\":\n                module_name = python_file.stem  # Get filename without extension\n                try:\n                    importlib.import_module(\n                        f\"aiperf.services.records_manager.metrics.types.{module_name}\"\n                    )\n                except ImportError as err:\n                    raise MetricTypeError(\n                        f\"Error importing metric type module '{module_name}'\"\n                    ) from err\n\n    return cls.metric_interfaces\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.base_metric.BaseMetric.update_value","title":"<code>update_value(record=None, metrics=None)</code>  <code>abstractmethod</code>","text":"<p>Updates the metric value based on the provided record and dictionary of other metrics.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Optional[Record]</code> <p>The record to update the metric with.</p> <code>None</code> <code>metrics</code> <code>Optional[dict[BaseMetric]]</code> <p>A dictionary of other metrics that may be needed for calculation.</p> <code>None</code> Source code in <code>aiperf/services/records_manager/metrics/base_metric.py</code> <pre><code>@abstractmethod\ndef update_value(\n    self,\n    record: ParsedResponseRecord | None = None,\n    metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Updates the metric value based on the provided record and dictionary of other metrics.\n\n    Args:\n        record (Optional[Record]): The record to update the metric with.\n        metrics (Optional[dict[BaseMetric]]): A dictionary of other metrics that may be needed for calculation.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.base_metric.BaseMetric.values","title":"<code>values()</code>  <code>abstractmethod</code>","text":"<p>Returns the list of calculated metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/base_metric.py</code> <pre><code>@abstractmethod\ndef values(self) -&gt; Any:\n    \"\"\"\n    Returns the list of calculated metrics.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesbenchmark_duration_metric","title":"aiperf.services.records_manager.metrics.types.benchmark_duration_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.benchmark_duration_metric.BenchmarkDurationMetric","title":"<code>BenchmarkDurationMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating the Benchmark Duration metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/benchmark_duration_metric.py</code> <pre><code>class BenchmarkDurationMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating the Benchmark Duration metric.\n    \"\"\"\n\n    tag = MetricTag.BENCHMARK_DURATION\n    unit = MetricTimeType.NANOSECONDS\n    larger_is_better = False\n    header = \"Benchmark Duration\"\n    type = MetricType.METRIC_OF_METRICS\n    required_metrics = {MetricTag.MIN_REQUEST, MetricTag.MAX_RESPONSE}\n\n    def __init__(self):\n        self.metric: float = 0.0\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        self._check_metrics(metrics)\n        min_req_time = metrics[MetricTag.MIN_REQUEST].values()\n        max_res_time = metrics[MetricTag.MAX_RESPONSE].values()\n        benchmark_duration = max_res_time - min_req_time\n        self.metric = benchmark_duration\n\n    def values(self) -&gt; float:\n        \"\"\"\n        Returns the BenchmarkDuration metric.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.benchmark_duration_metric.BenchmarkDurationMetric.values","title":"<code>values()</code>","text":"<p>Returns the BenchmarkDuration metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/benchmark_duration_metric.py</code> <pre><code>def values(self) -&gt; float:\n    \"\"\"\n    Returns the BenchmarkDuration metric.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesinput_sequence_length_metric","title":"aiperf.services.records_manager.metrics.types.input_sequence_length_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.input_sequence_length_metric.InputSequenceLengthMetric","title":"<code>InputSequenceLengthMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating Input Sequence Length (ISL) metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/input_sequence_length_metric.py</code> <pre><code>class InputSequenceLengthMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating Input Sequence Length (ISL) metrics from records.\n    \"\"\"\n\n    tag = MetricTag.ISL\n    unit = None\n    larger_is_better = False\n    header = \"Input Sequence Length\"\n    type = MetricType.METRIC_OF_RECORDS\n    streaming_only = False\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: list[int] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ):\n        self._check_record(record)\n        input_token_count = record.input_token_count\n        self.metric.append(input_token_count)\n\n    def values(self) -&gt; list[int]:\n        \"\"\"\n        Returns the list of Input Sequence Length (ISL) metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord):\n        \"\"\"\n        Checks if the record is valid for ISL calculation.\n\n        Raises:\n            ValueError: If the record is not valid or doesn't have input_token_count.\n        \"\"\"\n        self._require_valid_record(record)\n        if record.input_token_count is None:\n            raise ValueError(\"Input Token Count is not available for the record.\")\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.input_sequence_length_metric.InputSequenceLengthMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Input Sequence Length (ISL) metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/input_sequence_length_metric.py</code> <pre><code>def values(self) -&gt; list[int]:\n    \"\"\"\n    Returns the list of Input Sequence Length (ISL) metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesinter_token_latency_metric","title":"aiperf.services.records_manager.metrics.types.inter_token_latency_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.inter_token_latency_metric.InterTokenLatencyMetric","title":"<code>InterTokenLatencyMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post Processor for calculating Inter Token Latency (ITL) metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/inter_token_latency_metric.py</code> <pre><code>class InterTokenLatencyMetric(BaseMetric):\n    \"\"\"\n    Post Processor for calculating Inter Token Latency (ITL) metric.\n    \"\"\"\n\n    tag = MetricTag.INTER_TOKEN_LATENCY\n    unit = MetricTimeType.MILLISECONDS\n    larger_is_better = False\n    header = \"Inter Token Latency (ITL)\"\n    type = MetricType.METRIC_OF_METRICS\n    streaming_only = True\n    required_metrics = {\n        MetricTag.REQUEST_LATENCY,\n        MetricTag.TTFT,\n        MetricTag.OUTPUT_TOKEN_COUNT,\n    }\n\n    def __init__(self):\n        self.metric: list[float] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ):\n        self._check_metrics(metrics)\n        # Clear the current value because we re-compute it each time\n        self.metric.clear()\n\n        latencies = metrics[MetricTag.REQUEST_LATENCY].values()\n        ttfts = metrics[MetricTag.TTFT].values()\n        output_token_counts = metrics[MetricTag.OUTPUT_TOKEN_COUNT].values()\n\n        for latency, ttft, output_tokens in zip(\n            latencies, ttfts, output_token_counts, strict=False\n        ):\n            itl = (latency - ttft) / (output_tokens - 1)\n            self.metric.append(itl)\n\n    def values(self) -&gt; list[float]:\n        \"\"\"\n        Returns the list of Inter Token Latency (ITL) metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record):\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.inter_token_latency_metric.InterTokenLatencyMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Inter Token Latency (ITL) metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/inter_token_latency_metric.py</code> <pre><code>def values(self) -&gt; list[float]:\n    \"\"\"\n    Returns the list of Inter Token Latency (ITL) metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesmax_response_metric","title":"aiperf.services.records_manager.metrics.types.max_response_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.max_response_metric.MaxResponseMetric","title":"<code>MaxResponseMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating the maximum response time stamp metric from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/max_response_metric.py</code> <pre><code>class MaxResponseMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating the maximum response time stamp metric from records.\n    \"\"\"\n\n    tag = MetricTag.MAX_RESPONSE\n    unit = MetricTimeType.NANOSECONDS\n    type = MetricType.METRIC_OF_RECORDS\n    larger_is_better = False\n    header = \"Maximum Response Timestamp\"\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: float = 0\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a new record and calculates the maximum response timestamp metric.\n\n        \"\"\"\n        self._check_record(record)\n        if record.responses[-1].perf_ns &gt; self.metric:\n            self.metric = record.responses[-1].perf_ns\n\n    def values(self) -&gt; float:\n        \"\"\"\n        Returns the Max Response Timestamp metric.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Checks if the record is valid for calculations.\n        \"\"\"\n        self._require_valid_record(record)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.max_response_metric.MaxResponseMetric.update_value","title":"<code>update_value(record=None, metrics=None)</code>","text":"<p>Adds a new record and calculates the maximum response timestamp metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/max_response_metric.py</code> <pre><code>def update_value(\n    self,\n    record: ParsedResponseRecord | None = None,\n    metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Adds a new record and calculates the maximum response timestamp metric.\n\n    \"\"\"\n    self._check_record(record)\n    if record.responses[-1].perf_ns &gt; self.metric:\n        self.metric = record.responses[-1].perf_ns\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.max_response_metric.MaxResponseMetric.values","title":"<code>values()</code>","text":"<p>Returns the Max Response Timestamp metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/max_response_metric.py</code> <pre><code>def values(self) -&gt; float:\n    \"\"\"\n    Returns the Max Response Timestamp metric.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesmin_request_metric","title":"aiperf.services.records_manager.metrics.types.min_request_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.min_request_metric.MinRequestMetric","title":"<code>MinRequestMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating the minimum request time stamp metric from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/min_request_metric.py</code> <pre><code>class MinRequestMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating the minimum request time stamp metric from records.\n    \"\"\"\n\n    tag = MetricTag.MIN_REQUEST\n    unit = MetricTimeType.NANOSECONDS\n    type = MetricType.METRIC_OF_RECORDS\n    larger_is_better = False\n    header = \"Minimum Request Timestamp\"\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: float = float(\"inf\")\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a new record and calculates the minimum request timestamp metric.\n\n        \"\"\"\n        self._check_record(record)\n        if record.start_perf_ns &lt; self.metric:\n            self.metric = record.start_perf_ns\n\n    def values(self) -&gt; float:\n        \"\"\"\n        Returns the Minimum Request Timestamp metric.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Checks if the record is valid for calculations.\n\n        \"\"\"\n        self._require_valid_record(record)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.min_request_metric.MinRequestMetric.update_value","title":"<code>update_value(record=None, metrics=None)</code>","text":"<p>Adds a new record and calculates the minimum request timestamp metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/min_request_metric.py</code> <pre><code>def update_value(\n    self,\n    record: ParsedResponseRecord | None = None,\n    metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Adds a new record and calculates the minimum request timestamp metric.\n\n    \"\"\"\n    self._check_record(record)\n    if record.start_perf_ns &lt; self.metric:\n        self.metric = record.start_perf_ns\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.min_request_metric.MinRequestMetric.values","title":"<code>values()</code>","text":"<p>Returns the Minimum Request Timestamp metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/min_request_metric.py</code> <pre><code>def values(self) -&gt; float:\n    \"\"\"\n    Returns the Minimum Request Timestamp metric.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesoutput_sequence_length_metric","title":"aiperf.services.records_manager.metrics.types.output_sequence_length_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.output_sequence_length_metric.OutputSequenceLengthMetric","title":"<code>OutputSequenceLengthMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating Output Sequence Length (OSL) metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_sequence_length_metric.py</code> <pre><code>class OutputSequenceLengthMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating Output Sequence Length (OSL) metrics from records.\n    \"\"\"\n\n    tag = MetricTag.OSL\n    unit = None\n    larger_is_better = False\n    header = \"Output Sequence Length\"\n    type = MetricType.METRIC_OF_RECORDS\n    streaming_only = False\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: list[int] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ):\n        self._check_record(record)\n        self.metric.append(record.output_token_count)\n\n    def values(self):\n        \"\"\"\n        Returns the list of Output Sequence Length (OSL) metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord):\n        \"\"\"\n        Checks if the record is valid for OSL calculation.\n\n        Raises:\n            ValueError: If record is not valid or output_token_count is missing.\n        \"\"\"\n        self._require_valid_record(record)\n        if record.output_token_count is None:\n            raise ValueError(\"Output token count is missing in the record.\")\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.output_sequence_length_metric.OutputSequenceLengthMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Output Sequence Length (OSL) metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_sequence_length_metric.py</code> <pre><code>def values(self):\n    \"\"\"\n    Returns the list of Output Sequence Length (OSL) metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesoutput_token_count_metric","title":"aiperf.services.records_manager.metrics.types.output_token_count_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.output_token_count_metric.OutputTokenCountMetric","title":"<code>OutputTokenCountMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post Processor for calculating Output Token Count Metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_token_count_metric.py</code> <pre><code>class OutputTokenCountMetric(BaseMetric):\n    \"\"\"\n    Post Processor for calculating Output Token Count Metric.\n    \"\"\"\n\n    tag = MetricTag.OUTPUT_TOKEN_COUNT\n    unit = None\n    larger_is_better = True\n    header = \"Output Token Count\"\n    type = MetricType.METRIC_OF_RECORDS\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: list[int] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ):\n        self._check_record(record)\n        self.metric.append(record.output_token_count)\n\n    def values(self) -&gt; list[int]:\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord):\n        self._require_valid_record(record)\n        if record.output_token_count is None or record.output_token_count &lt; 0:\n            raise ValueError(\"Record has invalid output token count.\")\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesoutput_token_throughput_metric","title":"aiperf.services.records_manager.metrics.types.output_token_throughput_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.output_token_throughput_metric.OutputTokenThroughputMetric","title":"<code>OutputTokenThroughputMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post Processor for calculating Output Token Throughput Metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_token_throughput_metric.py</code> <pre><code>class OutputTokenThroughputMetric(BaseMetric):\n    \"\"\"\n    Post Processor for calculating Output Token Throughput Metric.\n    \"\"\"\n\n    tag = MetricTag.OUTPUT_TOKEN_THROUGHPUT\n    unit = None\n    larger_is_better = True\n    header = \"Output Token Throughput (tokens/sec)\"\n    type = MetricType.METRIC_OF_METRICS\n    required_metrics = {\n        MetricTag.OUTPUT_TOKEN_COUNT,\n        MetricTag.BENCHMARK_DURATION,\n    }\n\n    def __init__(self):\n        self.metric: float = 0.0\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ):\n        self._check_metrics(metrics)\n        tokens = metrics[MetricTag.OUTPUT_TOKEN_COUNT].values()\n        total_tokens = sum(tokens)\n\n        duration_ns = metrics[MetricTag.BENCHMARK_DURATION].values()\n        self.metric = total_tokens / (duration_ns / NANOS_PER_SECOND)\n\n    def values(self) -&gt; float:\n        \"\"\"\n        Returns the OutputTokenThroughput metric.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record):\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.output_token_throughput_metric.OutputTokenThroughputMetric.values","title":"<code>values()</code>","text":"<p>Returns the OutputTokenThroughput metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_token_throughput_metric.py</code> <pre><code>def values(self) -&gt; float:\n    \"\"\"\n    Returns the OutputTokenThroughput metric.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesoutput_token_throughput_per_user_metric","title":"aiperf.services.records_manager.metrics.types.output_token_throughput_per_user_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.output_token_throughput_per_user_metric.OutputTokenThroughputPerUserMetric","title":"<code>OutputTokenThroughputPerUserMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post Processor for calculating Output Token Throughput per user metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_token_throughput_per_user_metric.py</code> <pre><code>class OutputTokenThroughputPerUserMetric(BaseMetric):\n    \"\"\"\n    Post Processor for calculating Output Token Throughput per user metrics from records.\n    \"\"\"\n\n    tag = MetricTag.OUTPUT_TOKEN_THROUGHPUT_PER_USER\n    unit = MetricTimeType.SECONDS\n    larger_is_better = True\n    header = \"Output Token Throughput Per User\"\n    type = MetricType.METRIC_OF_METRICS\n    streaming_only = True\n    required_metrics = {MetricTag.INTER_TOKEN_LATENCY}\n\n    def __init__(self):\n        self.metric: list[float] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ):\n        self._check_metrics(metrics)\n        # Clear the current value because we re-compute it each time\n        self.metric.clear()\n        inter_token_latencies = metrics[MetricTag.INTER_TOKEN_LATENCY].values()\n        for inter_token_latency in inter_token_latencies:\n            inter_token_latency_s = inter_token_latency / NANOS_PER_SECOND\n            if inter_token_latency_s &lt;= 0:\n                raise ValueError(\"Inter-token latency must be greater than 0.\")\n            self.metric.append(1 / inter_token_latency_s)\n\n    def values(self):\n        \"\"\"\n        Returns the list of Output Token Throughput Per User metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record):\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.output_token_throughput_per_user_metric.OutputTokenThroughputPerUserMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Output Token Throughput Per User metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/output_token_throughput_per_user_metric.py</code> <pre><code>def values(self):\n    \"\"\"\n    Returns the list of Output Token Throughput Per User metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesrequest_count_metric","title":"aiperf.services.records_manager.metrics.types.request_count_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.request_count_metric.RequestCountMetric","title":"<code>RequestCountMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for counting the number of valid requests.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_count_metric.py</code> <pre><code>class RequestCountMetric(BaseMetric):\n    \"\"\"\n    Post-processor for counting the number of valid requests.\n    \"\"\"\n\n    tag = MetricTag.REQUEST_COUNT\n    unit = None\n    larger_is_better = True\n    header = \"Request Count\"\n    type = MetricType.METRIC_OF_RECORDS\n    streaming_only = False\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: int = 0\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        self._check_record(record)\n        self.metric += 1\n\n    def values(self) -&gt; int:\n        \"\"\"\n        Returns the Request Count metric.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        self._require_valid_record(record)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.request_count_metric.RequestCountMetric.values","title":"<code>values()</code>","text":"<p>Returns the Request Count metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_count_metric.py</code> <pre><code>def values(self) -&gt; int:\n    \"\"\"\n    Returns the Request Count metric.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesrequest_latency_metric","title":"aiperf.services.records_manager.metrics.types.request_latency_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.request_latency_metric.RequestLatencyMetric","title":"<code>RequestLatencyMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating Request Latency metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_latency_metric.py</code> <pre><code>class RequestLatencyMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating Request Latency metrics from records.\n    \"\"\"\n\n    tag = MetricTag.REQUEST_LATENCY\n    unit = MetricTimeType.NANOSECONDS\n    type = MetricType.METRIC_OF_RECORDS\n    larger_is_better = False\n    header = \"Request Latency\"\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: list[int] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a new record and calculates the Request Latency metric.\n\n        This method extracts the request and last response timestamps, calculates the differences in time, and\n        appends the result to the metric list.\n        \"\"\"\n        self._check_record(record)\n        request_ts = record.start_perf_ns\n        final_response_ts = record.responses[-1].perf_ns\n        request_latency = final_response_ts - request_ts\n        self.metric.append(request_latency)\n\n    def values(self) -&gt; list[int]:\n        \"\"\"\n        Returns the list of Request Latency metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        self._require_valid_record(record)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.request_latency_metric.RequestLatencyMetric.update_value","title":"<code>update_value(record=None, metrics=None)</code>","text":"<p>Adds a new record and calculates the Request Latency metric.</p> <p>This method extracts the request and last response timestamps, calculates the differences in time, and appends the result to the metric list.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_latency_metric.py</code> <pre><code>def update_value(\n    self,\n    record: ParsedResponseRecord | None = None,\n    metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Adds a new record and calculates the Request Latency metric.\n\n    This method extracts the request and last response timestamps, calculates the differences in time, and\n    appends the result to the metric list.\n    \"\"\"\n    self._check_record(record)\n    request_ts = record.start_perf_ns\n    final_response_ts = record.responses[-1].perf_ns\n    request_latency = final_response_ts - request_ts\n    self.metric.append(request_latency)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.request_latency_metric.RequestLatencyMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Request Latency metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_latency_metric.py</code> <pre><code>def values(self) -&gt; list[int]:\n    \"\"\"\n    Returns the list of Request Latency metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesrequest_throughput_metric","title":"aiperf.services.records_manager.metrics.types.request_throughput_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.request_throughput_metric.RequestThroughputMetric","title":"<code>RequestThroughputMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post Processor for calculating Request throughput metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_throughput_metric.py</code> <pre><code>class RequestThroughputMetric(BaseMetric):\n    \"\"\"\n    Post Processor for calculating Request throughput metrics from records.\n    \"\"\"\n\n    tag = MetricTag.REQUEST_THROUGHPUT\n    unit = MetricTimeType.SECONDS\n    larger_is_better = True\n    header = \"Request Throughput\"\n    type = MetricType.METRIC_OF_METRICS\n    streaming_only = False\n    required_metrics = {MetricTag.REQUEST_COUNT, MetricTag.BENCHMARK_DURATION}\n\n    def __init__(self):\n        self.metric: float = 0.0\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        self._check_metrics(metrics)\n        total_requests = metrics[MetricTag.REQUEST_COUNT].values()\n        benchmark_duration = metrics[MetricTag.BENCHMARK_DURATION].values()\n        self.metric = total_requests / (benchmark_duration / NANOS_PER_SECOND)\n\n    def values(self) -&gt; float:\n        \"\"\"\n        Returns the Request Throughput metric.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Checks if the record is valid.\n\n        Raises:\n            ValueError: If the record is None or is invalid.\n        \"\"\"\n        self._require_valid_record(record)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.request_throughput_metric.RequestThroughputMetric.values","title":"<code>values()</code>","text":"<p>Returns the Request Throughput metric.</p> Source code in <code>aiperf/services/records_manager/metrics/types/request_throughput_metric.py</code> <pre><code>def values(self) -&gt; float:\n    \"\"\"\n    Returns the Request Throughput metric.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesttft_metric","title":"aiperf.services.records_manager.metrics.types.ttft_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.ttft_metric.TTFTMetric","title":"<code>TTFTMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating Time to First Token (TTFT) metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/ttft_metric.py</code> <pre><code>class TTFTMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating Time to First Token (TTFT) metrics from records.\n    \"\"\"\n\n    tag = MetricTag.TTFT\n    unit = MetricTimeType.NANOSECONDS\n    larger_is_better = False\n    header = \"Time to First Token (TTFT)\"\n    type = MetricType.METRIC_OF_RECORDS\n    streaming_only = True\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: list[int] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a new record and calculates the Time To First Token (TTFT) metric.\n\n        This method extracts the timestamp from the request and the first response in the given\n        RequestRecord object, computes the difference (TTFT), and appends the result to the metric list.\n        \"\"\"\n        self._check_record(record)\n        request_ts = record.request.start_perf_ns\n        response_ts = record.responses[0].perf_ns\n        ttft = response_ts - request_ts\n        self.metric.append(ttft)\n\n    def values(self) -&gt; list[int]:\n        \"\"\"\n        Returns the list of Time to First Token (TTFT) metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Checks if the record is valid for TTFT calculation.\n\n        Raises:\n            ValueError: If record is None or record is not valid\n        \"\"\"\n        self._require_valid_record(record)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.ttft_metric.TTFTMetric.update_value","title":"<code>update_value(record=None, metrics=None)</code>","text":"<p>Adds a new record and calculates the Time To First Token (TTFT) metric.</p> <p>This method extracts the timestamp from the request and the first response in the given RequestRecord object, computes the difference (TTFT), and appends the result to the metric list.</p> Source code in <code>aiperf/services/records_manager/metrics/types/ttft_metric.py</code> <pre><code>def update_value(\n    self,\n    record: ParsedResponseRecord | None = None,\n    metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Adds a new record and calculates the Time To First Token (TTFT) metric.\n\n    This method extracts the timestamp from the request and the first response in the given\n    RequestRecord object, computes the difference (TTFT), and appends the result to the metric list.\n    \"\"\"\n    self._check_record(record)\n    request_ts = record.request.start_perf_ns\n    response_ts = record.responses[0].perf_ns\n    ttft = response_ts - request_ts\n    self.metric.append(ttft)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.ttft_metric.TTFTMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Time to First Token (TTFT) metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/ttft_metric.py</code> <pre><code>def values(self) -&gt; list[int]:\n    \"\"\"\n    Returns the list of Time to First Token (TTFT) metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managermetricstypesttst_metric","title":"aiperf.services.records_manager.metrics.types.ttst_metric","text":""},{"location":"api/#aiperf.services.records_manager.metrics.types.ttst_metric.TTSTMetric","title":"<code>TTSTMetric</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Post-processor for calculating Time to Second Token (TTST) metrics from records.</p> Source code in <code>aiperf/services/records_manager/metrics/types/ttst_metric.py</code> <pre><code>class TTSTMetric(BaseMetric):\n    \"\"\"\n    Post-processor for calculating Time to Second Token (TTST) metrics from records.\n    \"\"\"\n\n    tag = MetricTag.TTST\n    unit = MetricTimeType.NANOSECONDS\n    larger_is_better = False\n    header = \"Time to Second Token (TTST)\"\n    type = MetricType.METRIC_OF_RECORDS\n    streaming_only = True\n    required_metrics = set()\n\n    def __init__(self):\n        self.metric: list[int] = []\n\n    def update_value(\n        self,\n        record: ParsedResponseRecord | None = None,\n        metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a new record and calculates the Time To Second Token (TTST) metric.\n\n        This method extracts the timestamp from the first and second response in the given\n        Record object, computes the difference (TTST), and appends the result to the metric list.\n        \"\"\"\n        self._check_record(record)\n        first_reponse_ts = record.responses[0].perf_ns\n        second_response_ts = record.responses[1].perf_ns\n        ttst = second_response_ts - first_reponse_ts\n        self.metric.append(ttst)\n\n    def values(self) -&gt; list[int]:\n        \"\"\"\n        Returns the list of Time to First Token (TTST) metrics.\n        \"\"\"\n        return self.metric\n\n    def _check_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"\n        Checks if the record is valid for TTST calculation.\n\n        Raises:\n            ValueError: If the record does not have at least two responses.\n        \"\"\"\n        self._require_valid_record(record)\n        if len(record.responses) &lt; 2:\n            raise ValueError(\n                \"Record must have at least two responses to calculate TTST.\"\n            )\n        if record.responses[1].perf_ns &lt; record.responses[0].perf_ns:\n            raise ValueError(\n                \"Second response timestamp must be greater than or equal to the first response timestamp.\"\n            )\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.ttst_metric.TTSTMetric.update_value","title":"<code>update_value(record=None, metrics=None)</code>","text":"<p>Adds a new record and calculates the Time To Second Token (TTST) metric.</p> <p>This method extracts the timestamp from the first and second response in the given Record object, computes the difference (TTST), and appends the result to the metric list.</p> Source code in <code>aiperf/services/records_manager/metrics/types/ttst_metric.py</code> <pre><code>def update_value(\n    self,\n    record: ParsedResponseRecord | None = None,\n    metrics: dict[MetricTagT, \"BaseMetric\"] | None = None,\n) -&gt; None:\n    \"\"\"\n    Adds a new record and calculates the Time To Second Token (TTST) metric.\n\n    This method extracts the timestamp from the first and second response in the given\n    Record object, computes the difference (TTST), and appends the result to the metric list.\n    \"\"\"\n    self._check_record(record)\n    first_reponse_ts = record.responses[0].perf_ns\n    second_response_ts = record.responses[1].perf_ns\n    ttst = second_response_ts - first_reponse_ts\n    self.metric.append(ttst)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.metrics.types.ttst_metric.TTSTMetric.values","title":"<code>values()</code>","text":"<p>Returns the list of Time to First Token (TTST) metrics.</p> Source code in <code>aiperf/services/records_manager/metrics/types/ttst_metric.py</code> <pre><code>def values(self) -&gt; list[int]:\n    \"\"\"\n    Returns the list of Time to First Token (TTST) metrics.\n    \"\"\"\n    return self.metric\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managerpost_processorsbasic_metrics_streamer","title":"aiperf.services.records_manager.post_processors.basic_metrics_streamer","text":""},{"location":"api/#aiperf.services.records_manager.post_processors.basic_metrics_streamer.BasicMetricsStreamer","title":"<code>BasicMetricsStreamer</code>","text":"<p>               Bases: <code>BaseStreamingPostProcessor</code></p> <p>Streamer for basic metrics.</p> Source code in <code>aiperf/services/records_manager/post_processors/basic_metrics_streamer.py</code> <pre><code>@StreamingPostProcessorFactory.register(StreamingPostProcessorType.BASIC_METRICS)\nclass BasicMetricsStreamer(BaseStreamingPostProcessor):\n    \"\"\"Streamer for basic metrics.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.valid_count: int = 0\n        self.error_count: int = 0\n        self.start_time_ns: int = time.time_ns()\n        self.error_summary: dict[ErrorDetails, int] = {}\n        self.end_time_ns: int | None = None\n        self.total_expected: int | None = None\n        self.metric_summary = PostProcessorFactory.create_instance(\n            PostProcessorType.METRIC_SUMMARY,\n            endpoint_type=self.user_config.endpoint.type,\n        )\n\n    async def stream_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"Stream a record.\"\"\"\n        if record.request.valid:\n            self.valid_count += 1\n            self.metric_summary.process_record(record)\n        else:\n            self.error_count += 1\n            self.warning(f\"Received invalid inference results: {record.request.error}\")\n            if record.request.error is not None:\n                self.error_summary.setdefault(record.request.error, 0)\n                self.error_summary[record.request.error] += 1\n\n    def get_error_summary(self) -&gt; list[ErrorDetailsCount]:\n        \"\"\"Generate a summary of the error records.\"\"\"\n        return [\n            ErrorDetailsCount(error_details=error_details, count=count)\n            for error_details, count in self.error_summary.items()\n        ]\n\n    @on_message(MessageType.CREDIT_PHASE_START)\n    async def _on_credit_phase_start(\n        self, phase_start_msg: CreditPhaseStartMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase start message.\"\"\"\n        if phase_start_msg.phase != CreditPhase.PROFILING:\n            return\n        self.start_time_ns = phase_start_msg.start_ns\n        self.total_expected = phase_start_msg.total_expected_requests\n        self.info(\n            f\"Credit phase start: {phase_start_msg.phase} with {self.total_expected} expected requests\"\n        )\n\n    @on_message(MessageType.CREDIT_PHASE_COMPLETE)\n    async def _on_credit_phase_complete(\n        self, phase_complete_msg: CreditPhaseCompleteMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase complete message.\"\"\"\n        if phase_complete_msg.phase != CreditPhase.PROFILING:\n            return\n        self.end_time_ns = phase_complete_msg.end_ns\n        if self.total_expected is None:\n            self.total_expected = phase_complete_msg.completed\n\n    async def process_records(\n        self, cancelled: bool\n    ) -&gt; ProfileResults | ErrorDetails | None:\n        \"\"\"Process the records.\n\n        This method is called when the records manager receives a command to process the records.\n        \"\"\"\n        if self.valid_count + self.error_count == 0:\n            self.warning(\"No records to process\")\n            return None\n\n        self.notice(\"Processing records\")\n        try:\n            self.info(\n                f\"Processing {self.valid_count} successful records and {self.error_count} error records\"\n            )\n            return ProfileResults(\n                total_expected=self.total_expected,\n                completed=self.valid_count + self.error_count,\n                start_ns=self.start_time_ns,\n                end_ns=self.end_time_ns or time.time_ns(),\n                records=self.metric_summary.post_process(),\n                error_summary=self.get_error_summary(),\n                was_cancelled=cancelled,\n            )\n        except Exception as e:\n            self.exception(f\"Error processing records: {e}\")\n            return ErrorDetails.from_exception(e)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.basic_metrics_streamer.BasicMetricsStreamer.get_error_summary","title":"<code>get_error_summary()</code>","text":"<p>Generate a summary of the error records.</p> Source code in <code>aiperf/services/records_manager/post_processors/basic_metrics_streamer.py</code> <pre><code>def get_error_summary(self) -&gt; list[ErrorDetailsCount]:\n    \"\"\"Generate a summary of the error records.\"\"\"\n    return [\n        ErrorDetailsCount(error_details=error_details, count=count)\n        for error_details, count in self.error_summary.items()\n    ]\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.basic_metrics_streamer.BasicMetricsStreamer.process_records","title":"<code>process_records(cancelled)</code>  <code>async</code>","text":"<p>Process the records.</p> <p>This method is called when the records manager receives a command to process the records.</p> Source code in <code>aiperf/services/records_manager/post_processors/basic_metrics_streamer.py</code> <pre><code>async def process_records(\n    self, cancelled: bool\n) -&gt; ProfileResults | ErrorDetails | None:\n    \"\"\"Process the records.\n\n    This method is called when the records manager receives a command to process the records.\n    \"\"\"\n    if self.valid_count + self.error_count == 0:\n        self.warning(\"No records to process\")\n        return None\n\n    self.notice(\"Processing records\")\n    try:\n        self.info(\n            f\"Processing {self.valid_count} successful records and {self.error_count} error records\"\n        )\n        return ProfileResults(\n            total_expected=self.total_expected,\n            completed=self.valid_count + self.error_count,\n            start_ns=self.start_time_ns,\n            end_ns=self.end_time_ns or time.time_ns(),\n            records=self.metric_summary.post_process(),\n            error_summary=self.get_error_summary(),\n            was_cancelled=cancelled,\n        )\n    except Exception as e:\n        self.exception(f\"Error processing records: {e}\")\n        return ErrorDetails.from_exception(e)\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.basic_metrics_streamer.BasicMetricsStreamer.stream_record","title":"<code>stream_record(record)</code>  <code>async</code>","text":"<p>Stream a record.</p> Source code in <code>aiperf/services/records_manager/post_processors/basic_metrics_streamer.py</code> <pre><code>async def stream_record(self, record: ParsedResponseRecord) -&gt; None:\n    \"\"\"Stream a record.\"\"\"\n    if record.request.valid:\n        self.valid_count += 1\n        self.metric_summary.process_record(record)\n    else:\n        self.error_count += 1\n        self.warning(f\"Received invalid inference results: {record.request.error}\")\n        if record.request.error is not None:\n            self.error_summary.setdefault(record.request.error, 0)\n            self.error_summary[record.request.error] += 1\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managerpost_processorsmetric_summary","title":"aiperf.services.records_manager.post_processors.metric_summary","text":""},{"location":"api/#aiperf.services.records_manager.post_processors.metric_summary.MetricSummary","title":"<code>MetricSummary</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>MetricSummary is a post-processor that generates a summary of metrics from the records. It processes the records to extract relevant metrics and returns them in a structured format.</p> Source code in <code>aiperf/services/records_manager/post_processors/metric_summary.py</code> <pre><code>@PostProcessorFactory.register(PostProcessorType.METRIC_SUMMARY)\nclass MetricSummary(AIPerfLoggerMixin):\n    \"\"\"\n    MetricSummary is a post-processor that generates a summary of metrics from the records.\n    It processes the records to extract relevant metrics and returns them in a structured format.\n    \"\"\"\n\n    def __init__(self, endpoint_type: EndpointType | None = None, **kwargs):\n        super().__init__(endpoint_type=endpoint_type, **kwargs)\n        self.debug(\"Initializing MetricSummary post-processor\")\n\n        # Only include latency and throughput metrics for embeddings endpoint\n        allowed_tags = None\n        if (\n            endpoint_type is not None\n            and endpoint_type == EndpointType.OPENAI_EMBEDDINGS\n        ):\n            allowed_tags = {\n                MetricTag.REQUEST_LATENCY,\n                MetricTag.REQUEST_THROUGHPUT,\n                MetricTag.BENCHMARK_DURATION,\n                MetricTag.REQUEST_COUNT,\n                MetricTag.MIN_REQUEST,\n                MetricTag.MAX_RESPONSE,\n            }\n\n        self._metrics = []\n        for metric_cls in BaseMetric.get_all().values():\n            if (\n                allowed_tags is not None\n                and getattr(metric_cls, \"tag\", None) not in allowed_tags\n            ):\n                continue\n            self._metrics.append(metric_cls())\n\n    def process_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"Process a single record.\n\n        Classifies and computes metrics in dependency order to ensure correctness.\n        The metrics are categorized based on their dependency types:\n\n        1. METRIC_OF_RECORDS:\n            - Depend solely on each individual record.\n            - Computed first, as they have no dependencies.\n\n        2. METRIC_OF_BOTH:\n            - Depend on both:\n                - the current record, and\n                - previously computed metrics (specifically, METRIC_OF_RECORDS).\n            - Computed after all METRIC_OF_RECORDS have been processed.\n            - Must not depend on other METRIC_OF_BOTH or METRIC_OF_METRICS.\n\n        3. METRIC_OF_METRICS:\n            Computed once after all records have been processed.\n            see: :meth:`post_process`\n        \"\"\"\n        if not record.valid:\n            return\n\n        # METRIC_OF_RECORDS\n        for metric in self._metrics:\n            if metric.type == MetricType.METRIC_OF_RECORDS:\n                metric.update_value(record=record)\n\n        # METRIC_OF_BOTH\n        for metric in self._metrics:\n            if metric.type == MetricType.METRIC_OF_BOTH:\n                metric.update_value(\n                    record=record, metrics={m.tag: m for m in self._metrics}\n                )\n\n    def post_process(self) -&gt; list[MetricResult]:\n        \"\"\"\n        Classifies and computes metrics in dependency order to ensure correctness.\n        The metrics are categorized based on their dependency types:\n\n        1. METRIC_OF_RECORDS:\n            - Computed for each individual record.\n            see: :meth:`process_record`\n\n        2. METRIC_OF_BOTH:\n            - Computed for each individual record.\n            see: :meth:`process_record`\n\n        3. METRIC_OF_METRICS:\n            - Computed based only on other metrics (not records).\n            - May depend on any combination of:\n                - METRIC_OF_RECORDS,\n                - METRIC_OF_BOTH,\n                - other METRIC_OF_METRICS (if dependency order is respected).\n            - Computed using a dependency-resolution loop.\n\n        This process ensures:\n            - All metrics are computed exactly once, after dependencies are satisfied.\n            - Misconfigured or cyclic dependencies will raise an explicit runtime error.\n        \"\"\"\n        # METRIC_OF_METRICS\n        # Precompute tags of all metrics already processed\n        computed_tags = {\n            m.tag\n            for m in self._metrics\n            if m.type in {MetricType.METRIC_OF_RECORDS, MetricType.METRIC_OF_BOTH}\n        }\n\n        remaining = [m for m in self._metrics if m.type == MetricType.METRIC_OF_METRICS]\n\n        # Resolve dependencies: loop until all metrics are computed or a circular dependency is found\n        while remaining:\n            progress = False\n            for metric in remaining[:]:\n                # If required dependencies are all satisfied, compute this metric\n                if metric.required_metrics.issubset(computed_tags):\n                    metric.update_value(metrics={m.tag: m for m in self._metrics})\n                    computed_tags.add(metric.tag)\n                    remaining.remove(metric)\n                    progress = True\n\n            if not progress:\n                # Circular dependencies\n                missing = {m.tag: m.required_metrics - computed_tags for m in remaining}\n                raise ValueError(\n                    f\"Circular or unsatisfiable dependencies detected in METRIC_OF_METRICS: {missing}\"\n                )\n\n        df = pd.DataFrame({metric.tag: metric.values() for metric in self._metrics})\n        return [record_from_dataframe(df, metric) for metric in self._metrics]\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.metric_summary.MetricSummary.post_process","title":"<code>post_process()</code>","text":"<p>Classifies and computes metrics in dependency order to ensure correctness. The metrics are categorized based on their dependency types:</p> <ol> <li> <p>METRIC_OF_RECORDS:</p> <ul> <li>Computed for each individual record. see: :meth:<code>process_record</code></li> </ul> </li> <li> <p>METRIC_OF_BOTH:</p> <ul> <li>Computed for each individual record. see: :meth:<code>process_record</code></li> </ul> </li> <li> <p>METRIC_OF_METRICS:</p> <ul> <li>Computed based only on other metrics (not records).</li> <li>May depend on any combination of:<ul> <li>METRIC_OF_RECORDS,</li> <li>METRIC_OF_BOTH,</li> <li>other METRIC_OF_METRICS (if dependency order is respected).</li> </ul> </li> <li>Computed using a dependency-resolution loop.</li> </ul> </li> </ol> This process ensures <ul> <li>All metrics are computed exactly once, after dependencies are satisfied.</li> <li>Misconfigured or cyclic dependencies will raise an explicit runtime error.</li> </ul> Source code in <code>aiperf/services/records_manager/post_processors/metric_summary.py</code> <pre><code>def post_process(self) -&gt; list[MetricResult]:\n    \"\"\"\n    Classifies and computes metrics in dependency order to ensure correctness.\n    The metrics are categorized based on their dependency types:\n\n    1. METRIC_OF_RECORDS:\n        - Computed for each individual record.\n        see: :meth:`process_record`\n\n    2. METRIC_OF_BOTH:\n        - Computed for each individual record.\n        see: :meth:`process_record`\n\n    3. METRIC_OF_METRICS:\n        - Computed based only on other metrics (not records).\n        - May depend on any combination of:\n            - METRIC_OF_RECORDS,\n            - METRIC_OF_BOTH,\n            - other METRIC_OF_METRICS (if dependency order is respected).\n        - Computed using a dependency-resolution loop.\n\n    This process ensures:\n        - All metrics are computed exactly once, after dependencies are satisfied.\n        - Misconfigured or cyclic dependencies will raise an explicit runtime error.\n    \"\"\"\n    # METRIC_OF_METRICS\n    # Precompute tags of all metrics already processed\n    computed_tags = {\n        m.tag\n        for m in self._metrics\n        if m.type in {MetricType.METRIC_OF_RECORDS, MetricType.METRIC_OF_BOTH}\n    }\n\n    remaining = [m for m in self._metrics if m.type == MetricType.METRIC_OF_METRICS]\n\n    # Resolve dependencies: loop until all metrics are computed or a circular dependency is found\n    while remaining:\n        progress = False\n        for metric in remaining[:]:\n            # If required dependencies are all satisfied, compute this metric\n            if metric.required_metrics.issubset(computed_tags):\n                metric.update_value(metrics={m.tag: m for m in self._metrics})\n                computed_tags.add(metric.tag)\n                remaining.remove(metric)\n                progress = True\n\n        if not progress:\n            # Circular dependencies\n            missing = {m.tag: m.required_metrics - computed_tags for m in remaining}\n            raise ValueError(\n                f\"Circular or unsatisfiable dependencies detected in METRIC_OF_METRICS: {missing}\"\n            )\n\n    df = pd.DataFrame({metric.tag: metric.values() for metric in self._metrics})\n    return [record_from_dataframe(df, metric) for metric in self._metrics]\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.metric_summary.MetricSummary.process_record","title":"<code>process_record(record)</code>","text":"<p>Process a single record.</p> <p>Classifies and computes metrics in dependency order to ensure correctness. The metrics are categorized based on their dependency types:</p> <ol> <li> <p>METRIC_OF_RECORDS:</p> <ul> <li>Depend solely on each individual record.</li> <li>Computed first, as they have no dependencies.</li> </ul> </li> <li> <p>METRIC_OF_BOTH:</p> <ul> <li>Depend on both:<ul> <li>the current record, and</li> <li>previously computed metrics (specifically, METRIC_OF_RECORDS).</li> </ul> </li> <li>Computed after all METRIC_OF_RECORDS have been processed.</li> <li>Must not depend on other METRIC_OF_BOTH or METRIC_OF_METRICS.</li> </ul> </li> <li> <p>METRIC_OF_METRICS:     Computed once after all records have been processed.     see: :meth:<code>post_process</code></p> </li> </ol> Source code in <code>aiperf/services/records_manager/post_processors/metric_summary.py</code> <pre><code>def process_record(self, record: ParsedResponseRecord) -&gt; None:\n    \"\"\"Process a single record.\n\n    Classifies and computes metrics in dependency order to ensure correctness.\n    The metrics are categorized based on their dependency types:\n\n    1. METRIC_OF_RECORDS:\n        - Depend solely on each individual record.\n        - Computed first, as they have no dependencies.\n\n    2. METRIC_OF_BOTH:\n        - Depend on both:\n            - the current record, and\n            - previously computed metrics (specifically, METRIC_OF_RECORDS).\n        - Computed after all METRIC_OF_RECORDS have been processed.\n        - Must not depend on other METRIC_OF_BOTH or METRIC_OF_METRICS.\n\n    3. METRIC_OF_METRICS:\n        Computed once after all records have been processed.\n        see: :meth:`post_process`\n    \"\"\"\n    if not record.valid:\n        return\n\n    # METRIC_OF_RECORDS\n    for metric in self._metrics:\n        if metric.type == MetricType.METRIC_OF_RECORDS:\n            metric.update_value(record=record)\n\n    # METRIC_OF_BOTH\n    for metric in self._metrics:\n        if metric.type == MetricType.METRIC_OF_BOTH:\n            metric.update_value(\n                record=record, metrics={m.tag: m for m in self._metrics}\n            )\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.metric_summary.record_from_dataframe","title":"<code>record_from_dataframe(df, metric)</code>","text":"<p>Create a Record from a DataFrame.</p> Source code in <code>aiperf/services/records_manager/post_processors/metric_summary.py</code> <pre><code>def record_from_dataframe(df: pd.DataFrame, metric: BaseMetric) -&gt; MetricResult:\n    \"\"\"Create a Record from a DataFrame.\"\"\"\n\n    column = df[metric.tag]\n    quantiles = column.quantile([0.01, 0.05, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99])\n\n    return MetricResult(\n        tag=metric.tag,\n        header=metric.header,\n        unit=metric.unit.short_name() if metric.unit else \"\",\n        avg=column.mean(),\n        min=column.min(),\n        max=column.max(),\n        p1=quantiles[0.01],\n        p5=quantiles[0.05],\n        p25=quantiles[0.25],\n        p50=quantiles[0.50],\n        p75=quantiles[0.75],\n        p90=quantiles[0.90],\n        p95=quantiles[0.95],\n        p99=quantiles[0.99],\n        std=column.std(),\n        count=int(column.count()),\n        streaming_only=metric.streaming_only,\n    )\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managerpost_processorsprocessing_stats_streamer","title":"aiperf.services.records_manager.post_processors.processing_stats_streamer","text":""},{"location":"api/#aiperf.services.records_manager.post_processors.processing_stats_streamer.ProcessingStatsStreamer","title":"<code>ProcessingStatsStreamer</code>","text":"<p>               Bases: <code>BaseStreamingPostProcessor</code></p> <p>This streamer is used to track the number of records processed and the number of errors. It is also used to track the number of requests expected and the number of requests completed. It will send a notification message when all expected requests have been received.</p> Source code in <code>aiperf/services/records_manager/post_processors/processing_stats_streamer.py</code> <pre><code>@StreamingPostProcessorFactory.register(StreamingPostProcessorType.PROCESSING_STATS)\nclass ProcessingStatsStreamer(BaseStreamingPostProcessor):\n    \"\"\"This streamer is used to track the number of records processed and the number of errors.\n    It is also used to track the number of requests expected and the number of requests completed.\n    It will send a notification message when all expected requests have been received.\n    \"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.start_time_ns: int | None = None\n        self.processing_stats: PhaseProcessingStats = PhaseProcessingStats()\n        self.final_request_count: int | None = None\n        self.end_time_ns: int | None = None\n\n        # Track per-worker statistics\n        self.worker_stats: dict[str, PhaseProcessingStats] = {}\n\n    async def stream_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"Stream a record.\"\"\"\n        self.trace(lambda: f\"Received parsed inference results: {record}\")\n        if record.request.credit_phase != CreditPhase.PROFILING:\n            self.debug(\n                lambda: f\"Skipping non-profiling record: {record.request.credit_phase}\"\n            )\n            return\n\n        worker_id = record.worker_id\n        if worker_id not in self.worker_stats:\n            self.worker_stats[worker_id] = PhaseProcessingStats()\n\n        if record.request.valid:\n            self.worker_stats[worker_id].processed += 1\n            self.processing_stats.processed += 1\n        else:\n            self.warning(f\"Received invalid inference results: {record}\")\n            self.worker_stats[worker_id].errors += 1\n            self.processing_stats.errors += 1\n\n        if (\n            self.final_request_count is not None\n            and self.processing_stats.total_records &gt;= self.final_request_count\n        ):\n            self.info(\n                lambda: f\"Processed {self.processing_stats.processed} valid requests and {self.processing_stats.errors} errors ({self.processing_stats.total_records} total).\"\n            )\n            # Make sure everyone knows the final stats, including the worker stats\n            await self.publish_processing_stats()\n\n            # Send a message to the event bus to signal that we received all the records\n            await self.publish(\n                AllRecordsReceivedMessage(\n                    service_id=self.service_id,\n                    request_ns=time.time_ns(),\n                    final_processing_stats=self.processing_stats,\n                )\n            )\n\n    @on_message(MessageType.CREDIT_PHASE_START)\n    async def _on_credit_phase_start(\n        self, phase_start_msg: CreditPhaseStartMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase start message.\"\"\"\n        if phase_start_msg.phase == CreditPhase.PROFILING:\n            self.processing_stats.total_expected_requests = (\n                phase_start_msg.total_expected_requests\n            )\n\n    @on_message(MessageType.CREDIT_PHASE_COMPLETE)\n    async def _on_credit_phase_complete(\n        self, phase_complete_msg: CreditPhaseCompleteMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase complete message.\"\"\"\n        if phase_complete_msg.phase == CreditPhase.PROFILING:\n            # This will equate to how many records we expect to receive,\n            # and once we receive that many records, we know to stop.\n            self.final_request_count = phase_complete_msg.completed\n            self.end_time_ns = phase_complete_msg.end_ns\n            self.info(f\"Updating final request count to {self.final_request_count}\")\n\n    @background_task(\n        interval=lambda self: self.service_config.progress_report_interval,\n        immediate=False,\n    )\n    async def _report_records_task(self) -&gt; None:\n        \"\"\"Report the records.\"\"\"\n        if self.processing_stats.processed &gt; 0 or self.processing_stats.errors &gt; 0:\n            # Only publish stats if there are records to report\n            await self.publish_processing_stats()\n\n    async def publish_processing_stats(self) -&gt; None:\n        \"\"\"Publish the profile processing stats.\"\"\"\n        await self.publish(\n            RecordsProcessingStatsMessage(\n                service_id=self.service_id,\n                request_ns=time.time_ns(),\n                processing_stats=self.processing_stats,\n                worker_stats=self.worker_stats,\n            ),\n        )\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.processing_stats_streamer.ProcessingStatsStreamer.publish_processing_stats","title":"<code>publish_processing_stats()</code>  <code>async</code>","text":"<p>Publish the profile processing stats.</p> Source code in <code>aiperf/services/records_manager/post_processors/processing_stats_streamer.py</code> <pre><code>async def publish_processing_stats(self) -&gt; None:\n    \"\"\"Publish the profile processing stats.\"\"\"\n    await self.publish(\n        RecordsProcessingStatsMessage(\n            service_id=self.service_id,\n            request_ns=time.time_ns(),\n            processing_stats=self.processing_stats,\n            worker_stats=self.worker_stats,\n        ),\n    )\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.processing_stats_streamer.ProcessingStatsStreamer.stream_record","title":"<code>stream_record(record)</code>  <code>async</code>","text":"<p>Stream a record.</p> Source code in <code>aiperf/services/records_manager/post_processors/processing_stats_streamer.py</code> <pre><code>async def stream_record(self, record: ParsedResponseRecord) -&gt; None:\n    \"\"\"Stream a record.\"\"\"\n    self.trace(lambda: f\"Received parsed inference results: {record}\")\n    if record.request.credit_phase != CreditPhase.PROFILING:\n        self.debug(\n            lambda: f\"Skipping non-profiling record: {record.request.credit_phase}\"\n        )\n        return\n\n    worker_id = record.worker_id\n    if worker_id not in self.worker_stats:\n        self.worker_stats[worker_id] = PhaseProcessingStats()\n\n    if record.request.valid:\n        self.worker_stats[worker_id].processed += 1\n        self.processing_stats.processed += 1\n    else:\n        self.warning(f\"Received invalid inference results: {record}\")\n        self.worker_stats[worker_id].errors += 1\n        self.processing_stats.errors += 1\n\n    if (\n        self.final_request_count is not None\n        and self.processing_stats.total_records &gt;= self.final_request_count\n    ):\n        self.info(\n            lambda: f\"Processed {self.processing_stats.processed} valid requests and {self.processing_stats.errors} errors ({self.processing_stats.total_records} total).\"\n        )\n        # Make sure everyone knows the final stats, including the worker stats\n        await self.publish_processing_stats()\n\n        # Send a message to the event bus to signal that we received all the records\n        await self.publish(\n            AllRecordsReceivedMessage(\n                service_id=self.service_id,\n                request_ns=time.time_ns(),\n                final_processing_stats=self.processing_stats,\n            )\n        )\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managerpost_processorsstreaming_post_processor","title":"aiperf.services.records_manager.post_processors.streaming_post_processor","text":""},{"location":"api/#aiperf.services.records_manager.post_processors.streaming_post_processor.BaseStreamingPostProcessor","title":"<code>BaseStreamingPostProcessor</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code>, <code>ABC</code></p> <p>BaseStreamingPostProcessor is a base class for all classes that wish to stream the incoming ParsedResponseRecords.</p> Source code in <code>aiperf/services/records_manager/post_processors/streaming_post_processor.py</code> <pre><code>@implements_protocol(StreamingPostProcessorProtocol)\nclass BaseStreamingPostProcessor(MessageBusClientMixin, ABC):\n    \"\"\"\n    BaseStreamingPostProcessor is a base class for all classes that wish to stream the incoming\n    ParsedResponseRecords.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_id: str,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        max_queue_size: int = DEFAULT_STREAMING_MAX_QUEUE_SIZE,\n        **kwargs,\n    ) -&gt; None:\n        self.service_id = service_id\n        self.user_config = user_config\n        self.service_config = service_config\n        super().__init__(\n            user_config=user_config,\n            service_config=service_config,\n            **kwargs,\n        )\n        self.info(\n            lambda: f\"Created streaming post processor: {self.__class__.__name__} with max_queue_size: {max_queue_size:,}\"\n        )\n        self.records_queue: asyncio.Queue[ParsedResponseRecord] = asyncio.Queue(\n            maxsize=max_queue_size\n        )\n        self.cancellation_event = asyncio.Event()\n\n    @background_task(immediate=True, interval=None)\n    async def _stream_records_task(self) -&gt; None:\n        \"\"\"Task that streams records from the queue to the post processor's stream_record method.\"\"\"\n        while not self.stop_requested and not self.cancellation_event.is_set():\n            try:\n                record = await self.records_queue.get()\n                await self.stream_record(record)\n                self.records_queue.task_done()\n            except asyncio.CancelledError:\n                break\n\n        if self.cancellation_event.is_set():\n            self.debug(\n                lambda: f\"Streaming post processor {self.__class__.__name__} task cancelled, draining queue\"\n            )\n            # Drain the rest of the queue\n            while not self.records_queue.empty():\n                _ = self.records_queue.get_nowait()\n                self.records_queue.task_done()\n\n        self.debug(\n            lambda: f\"Streaming post processor {self.__class__.__name__} task completed\"\n        )\n\n    @abstractmethod\n    async def stream_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"Handle the incoming record. This method should be implemented by the subclass.\"\"\"\n        raise NotImplementedError(\n            \"BaseStreamingPostProcessor.stream_record method must be implemented by the subclass.\"\n        )\n\n    async def process_records(self, cancelled: bool) -&gt; Any:\n        \"\"\"Handle the process records command. This method is called when the records manager receives\n        a command to process the records, and can be handled by the subclass. The results will be\n        returned by the records manager to the caller.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.streaming_post_processor.BaseStreamingPostProcessor.process_records","title":"<code>process_records(cancelled)</code>  <code>async</code>","text":"<p>Handle the process records command. This method is called when the records manager receives a command to process the records, and can be handled by the subclass. The results will be returned by the records manager to the caller.</p> Source code in <code>aiperf/services/records_manager/post_processors/streaming_post_processor.py</code> <pre><code>async def process_records(self, cancelled: bool) -&gt; Any:\n    \"\"\"Handle the process records command. This method is called when the records manager receives\n    a command to process the records, and can be handled by the subclass. The results will be\n    returned by the records manager to the caller.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.post_processors.streaming_post_processor.BaseStreamingPostProcessor.stream_record","title":"<code>stream_record(record)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Handle the incoming record. This method should be implemented by the subclass.</p> Source code in <code>aiperf/services/records_manager/post_processors/streaming_post_processor.py</code> <pre><code>@abstractmethod\nasync def stream_record(self, record: ParsedResponseRecord) -&gt; None:\n    \"\"\"Handle the incoming record. This method should be implemented by the subclass.\"\"\"\n    raise NotImplementedError(\n        \"BaseStreamingPostProcessor.stream_record method must be implemented by the subclass.\"\n    )\n</code></pre>"},{"location":"api/#aiperfservicesrecords_managerrecords_manager","title":"aiperf.services.records_manager.records_manager","text":""},{"location":"api/#aiperf.services.records_manager.records_manager.RecordsManager","title":"<code>RecordsManager</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code></p> <p>The RecordsManager service is primarily responsible for holding the results returned from the workers.</p> Source code in <code>aiperf/services/records_manager/records_manager.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ServiceFactory.register(ServiceType.RECORDS_MANAGER)\nclass RecordsManager(PullClientMixin, BaseComponentService):\n    \"\"\"\n    The RecordsManager service is primarily responsible for holding the\n    results returned from the workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.RECORDS,\n            pull_client_bind=True,\n            pull_client_max_concurrency=DEFAULT_PULL_CLIENT_MAX_CONCURRENCY,\n        )\n        self._profile_cancelled = False\n        self.streaming_post_processors: list[StreamingPostProcessorProtocol] = []\n        for streamer_type in StreamingPostProcessorFactory.get_all_class_types():\n            streamer = StreamingPostProcessorFactory.create_instance(\n                class_type=streamer_type,\n                service_id=self.service_id,\n                service_config=self.service_config,\n                user_config=self.user_config,\n            )\n            self.debug(\n                f\"Created streaming post processor: {streamer_type}: {streamer.__class__.__name__}\"\n            )\n            self.streaming_post_processors.append(streamer)\n            self.attach_child_lifecycle(streamer)\n\n    @on_pull_message(MessageType.PARSED_INFERENCE_RESULTS)\n    async def _on_parsed_inference_results(\n        self, message: ParsedInferenceResultsMessage\n    ) -&gt; None:\n        \"\"\"Handle a parsed inference results message.\"\"\"\n        self.trace(lambda: f\"Received parsed inference results: {message}\")\n\n        if self._profile_cancelled:\n            self.debug(\"Skipping record because profiling is cancelled\")\n            return\n\n        if message.record.request.credit_phase != CreditPhase.PROFILING:\n            self.debug(\n                lambda: f\"Skipping non-profiling record: {message.record.request.credit_phase}\"\n            )\n            return\n\n        # Stream the record to all of the streaming post processors\n        for streamer in self.streaming_post_processors:\n            try:\n                self.debug(\n                    lambda name=streamer.__class__.__name__: f\"Putting record into queue for streamer {name}\"\n                )\n                streamer.records_queue.put_nowait(message.record)\n            except asyncio.QueueFull:\n                self.error(\n                    f\"Streaming post processor {streamer.__class__.__name__} is unable to keep up with the rate of incoming records.\"\n                )\n                self.warning(\n                    f\"Waiting for queue to be available for streamer {streamer.__class__.__name__}. This will cause back pressure on the records manager.\"\n                )\n                await streamer.records_queue.put(message.record)\n\n    @on_command(CommandType.PROCESS_RECORDS)\n    async def _on_process_records_command(\n        self, message: ProcessRecordsCommand\n    ) -&gt; ProcessRecordsResult:\n        \"\"\"Handle the process records command by forwarding it to all of the streaming post processors, and returning the results.\"\"\"\n        self.debug(lambda: f\"Received process records command: {message}\")\n        return await self._process_records(cancelled=message.cancelled)\n\n    @on_command(CommandType.PROFILE_CANCEL)\n    async def _on_profile_cancel_command(\n        self, message: ProfileCancelCommand\n    ) -&gt; ProcessRecordsResult:\n        \"\"\"Handle the profile cancel command by cancelling the streaming post processors.\"\"\"\n        self.debug(lambda: f\"Received profile cancel command: {message}\")\n        self._profile_cancelled = True\n        for streamer in self.streaming_post_processors:\n            streamer.cancellation_event.set()\n        return await self._process_records(cancelled=True)\n\n    @on_message(MessageType.ALL_RECORDS_RECEIVED)\n    async def _on_all_records_received(\n        self, message: AllRecordsReceivedMessage\n    ) -&gt; None:\n        \"\"\"Handle a all records received message.\"\"\"\n        self.debug(lambda: f\"Received all records: {message}, processing now...\")\n        await self._process_records(cancelled=self._profile_cancelled)\n\n    async def _process_records(self, cancelled: bool) -&gt; ProcessRecordsResult:\n        \"\"\"Process the records.\"\"\"\n        self.debug(lambda: f\"Processing records (cancelled: {cancelled})\")\n\n        # Even though all the records have been received, we need to ensure that\n        # all the records have been processed through the streaming post processors.\n        await asyncio.gather(\n            *[\n                streamer.records_queue.join()\n                for streamer in self.streaming_post_processors\n            ]\n        )\n\n        # Process the records through the streaming post processors\n        results = await asyncio.gather(\n            *[\n                streamer.process_records(cancelled)\n                for streamer in self.streaming_post_processors\n            ],\n            return_exceptions=True,\n        )\n        self.debug(lambda: f\"Processed records results: {results}\")\n\n        records_results = [\n            result for result in results if isinstance(result, ProfileResults)\n        ]\n        error_results = [\n            result for result in results if isinstance(result, ErrorDetails)\n        ]\n\n        result = ProcessRecordsResult(records=records_results, errors=error_results)\n        self.debug(lambda: f\"Processed records result: {result}\")\n        await self.publish(\n            ProcessRecordsResultMessage(\n                service_id=self.service_id,\n                process_records_result=result,\n            )\n        )\n        return result\n</code></pre>"},{"location":"api/#aiperf.services.records_manager.records_manager.main","title":"<code>main()</code>","text":"<p>Main entry point for the records manager.</p> Source code in <code>aiperf/services/records_manager/records_manager.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the records manager.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(RecordsManager)\n</code></pre>"},{"location":"api/#aiperfservicessystem_controllerbase_service_manager","title":"aiperf.services.system_controller.base_service_manager","text":""},{"location":"api/#aiperf.services.system_controller.base_service_manager.BaseServiceManager","title":"<code>BaseServiceManager</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>Base class for service managers. It provides a common interface for managing services.</p> Source code in <code>aiperf/services/system_controller/base_service_manager.py</code> <pre><code>@implements_protocol(ServiceManagerProtocol)\nclass BaseServiceManager(AIPerfLifecycleMixin, ABC):\n    \"\"\"\n    Base class for service managers. It provides a common interface for managing services.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        **kwargs,\n    ):\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            **kwargs,\n        )\n        self.required_services = required_services\n        self.service_config = service_config\n        self.user_config = user_config\n        self.kwargs = kwargs\n        # Maps to track service information\n        self.service_map: dict[ServiceTypeT, list[ServiceRunInfo]] = {}\n\n        # Create service ID map for component lookups\n        self.service_id_map: dict[str, ServiceRunInfo] = {}\n\n    @on_start\n    async def _start_service_manager(self) -&gt; None:\n        await self.run_required_services()\n\n    @on_stop\n    async def _stop_service_manager(self) -&gt; None:\n        await self.shutdown_all_services()\n\n    async def run_services(\n        self, service_types: dict[ServiceTypeT, int]\n    ) -&gt; list[BaseException | None]:\n        return await asyncio.gather(\n            *[\n                self.run_service(service_type, num_replicas)\n                for service_type, num_replicas in service_types.items()\n            ],\n            return_exceptions=True,\n        )\n\n    @abstractmethod\n    async def stop_service(\n        self, service_type: ServiceTypeT, service_id: str | None = None\n    ) -&gt; list[BaseException | None]: ...\n\n    # TODO: This stuff needs some major cleanup\n\n    async def stop_services_by_type(\n        self, service_types: list[ServiceTypeT]\n    ) -&gt; list[BaseException | None]:\n        \"\"\"Stop a set of services.\"\"\"\n        results = await asyncio.gather(\n            *[self.stop_service(service_type) for service_type in service_types],\n            return_exceptions=True,\n        )\n        output: list[BaseException | None] = []\n        for result in results:\n            if isinstance(result, list):\n                output.extend(result)\n            else:\n                output.append(result)\n        return output\n\n    async def run_required_services(self) -&gt; None:\n        await self.run_services(self.required_services)\n\n    @abstractmethod\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n        pass\n\n    @abstractmethod\n    async def kill_all_services(self) -&gt; list[BaseException | None]:\n        pass\n\n    @abstractmethod\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None:\n        pass\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.base_service_manager.BaseServiceManager.stop_services_by_type","title":"<code>stop_services_by_type(service_types)</code>  <code>async</code>","text":"<p>Stop a set of services.</p> Source code in <code>aiperf/services/system_controller/base_service_manager.py</code> <pre><code>async def stop_services_by_type(\n    self, service_types: list[ServiceTypeT]\n) -&gt; list[BaseException | None]:\n    \"\"\"Stop a set of services.\"\"\"\n    results = await asyncio.gather(\n        *[self.stop_service(service_type) for service_type in service_types],\n        return_exceptions=True,\n    )\n    output: list[BaseException | None] = []\n    for result in results:\n        if isinstance(result, list):\n            output.extend(result)\n        else:\n            output.append(result)\n    return output\n</code></pre>"},{"location":"api/#aiperfservicessystem_controllerkubernetes_service_manager","title":"aiperf.services.system_controller.kubernetes_service_manager","text":""},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.KubernetesServiceManager","title":"<code>KubernetesServiceManager</code>","text":"<p>               Bases: <code>BaseServiceManager</code></p> <p>Service Manager for starting and stopping services in a Kubernetes cluster.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>@implements_protocol(ServiceManagerProtocol)\n@ServiceManagerFactory.register(ServiceRunType.KUBERNETES)\nclass KubernetesServiceManager(BaseServiceManager):\n    \"\"\"\n    Service Manager for starting and stopping services in a Kubernetes cluster.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        **kwargs,\n    ):\n        super().__init__(required_services, service_config, user_config, **kwargs)\n\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None:\n        \"\"\"Run a service as a Kubernetes pod.\"\"\"\n        self.logger.debug(f\"Running service {service_type} as a Kubernetes pod\")\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.run_service not implemented\"\n        )\n\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Stop all required services as Kubernetes pods.\"\"\"\n        self.logger.debug(\"Stopping all required services as Kubernetes pods\")\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.stop_all_services not implemented\"\n        )\n\n    async def kill_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Kill all required services as Kubernetes pods.\"\"\"\n        self.logger.debug(\"Killing all required services as Kubernetes pods\")\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.kill_all_services not implemented\"\n        )\n\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be registered in Kubernetes.\"\"\"\n        self.logger.debug(\n            \"Waiting for all required services to be registered in Kubernetes\"\n        )\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.wait_for_all_services_registration not implemented\"\n        )\n\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be started in Kubernetes.\"\"\"\n        self.logger.debug(\n            \"Waiting for all required services to be started in Kubernetes\"\n        )\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.wait_for_all_services_start not implemented\"\n        )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.KubernetesServiceManager.kill_all_services","title":"<code>kill_all_services()</code>  <code>async</code>","text":"<p>Kill all required services as Kubernetes pods.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>async def kill_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Kill all required services as Kubernetes pods.\"\"\"\n    self.logger.debug(\"Killing all required services as Kubernetes pods\")\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.kill_all_services not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.KubernetesServiceManager.run_service","title":"<code>run_service(service_type, num_replicas=1)</code>  <code>async</code>","text":"<p>Run a service as a Kubernetes pod.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>async def run_service(\n    self, service_type: ServiceTypeT, num_replicas: int = 1\n) -&gt; None:\n    \"\"\"Run a service as a Kubernetes pod.\"\"\"\n    self.logger.debug(f\"Running service {service_type} as a Kubernetes pod\")\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.run_service not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.KubernetesServiceManager.shutdown_all_services","title":"<code>shutdown_all_services()</code>  <code>async</code>","text":"<p>Stop all required services as Kubernetes pods.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Stop all required services as Kubernetes pods.\"\"\"\n    self.logger.debug(\"Stopping all required services as Kubernetes pods\")\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.stop_all_services not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.KubernetesServiceManager.wait_for_all_services_registration","title":"<code>wait_for_all_services_registration(stop_event, timeout_seconds=DEFAULT_SERVICE_REGISTRATION_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be registered in Kubernetes.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>async def wait_for_all_services_registration(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be registered in Kubernetes.\"\"\"\n    self.logger.debug(\n        \"Waiting for all required services to be registered in Kubernetes\"\n    )\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.wait_for_all_services_registration not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.KubernetesServiceManager.wait_for_all_services_start","title":"<code>wait_for_all_services_start(stop_event, timeout_seconds=DEFAULT_SERVICE_START_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be started in Kubernetes.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>async def wait_for_all_services_start(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be started in Kubernetes.\"\"\"\n    self.logger.debug(\n        \"Waiting for all required services to be started in Kubernetes\"\n    )\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.wait_for_all_services_start not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.kubernetes_service_manager.ServiceKubernetesRunInfo","title":"<code>ServiceKubernetesRunInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a service running in a Kubernetes pod.</p> Source code in <code>aiperf/services/system_controller/kubernetes_service_manager.py</code> <pre><code>class ServiceKubernetesRunInfo(BaseModel):\n    \"\"\"Information about a service running in a Kubernetes pod.\"\"\"\n\n    pod_name: str\n    node_name: str\n    namespace: str\n</code></pre>"},{"location":"api/#aiperfservicessystem_controllermultiprocess_service_manager","title":"aiperf.services.system_controller.multiprocess_service_manager","text":""},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessRunInfo","title":"<code>MultiProcessRunInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a service running as a multiprocessing process.</p> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>class MultiProcessRunInfo(BaseModel):\n    \"\"\"Information about a service running as a multiprocessing process.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    process: Process | SpawnProcess | ForkProcess | None = Field(default=None)\n    service_type: ServiceTypeT = Field(\n        ...,\n        description=\"Type of service running in the process\",\n    )\n    service_id: str = Field(\n        ...,\n        description=\"ID of the service running in the process\",\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessServiceManager","title":"<code>MultiProcessServiceManager</code>","text":"<p>               Bases: <code>BaseServiceManager</code></p> <p>Service Manager for starting and stopping services as multiprocessing processes.</p> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>@implements_protocol(ServiceManagerProtocol)\n@ServiceManagerFactory.register(ServiceRunType.MULTIPROCESSING)\nclass MultiProcessServiceManager(BaseServiceManager):\n    \"\"\"\n    Service Manager for starting and stopping services as multiprocessing processes.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        log_queue: \"multiprocessing.Queue | None\" = None,\n        **kwargs,\n    ):\n        super().__init__(required_services, service_config, user_config, **kwargs)\n        self.multi_process_info: list[MultiProcessRunInfo] = []\n        self.log_queue = log_queue\n\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None:\n        \"\"\"Run a service with the given number of replicas.\"\"\"\n        service_class = ServiceFactory.get_class_from_type(service_type)\n\n        for _ in range(num_replicas):\n            service_id = f\"{service_type}_{uuid.uuid4().hex[:8]}\"\n            process = Process(\n                target=bootstrap_and_run_service,\n                name=f\"{service_type}_process\",\n                kwargs={\n                    \"service_class\": service_class,\n                    \"service_id\": service_id,\n                    \"service_config\": self.service_config,\n                    \"user_config\": self.user_config,\n                    \"log_queue\": self.log_queue,\n                },\n                daemon=True,\n            )\n\n            process.start()\n\n            self.debug(\n                lambda pid=process.pid,\n                type=service_type: f\"Service {type} started as process (pid: {pid})\"\n            )\n\n            self.multi_process_info.append(\n                MultiProcessRunInfo(\n                    process=process,\n                    service_type=service_type,\n                    service_id=service_id,\n                )\n            )\n\n    async def stop_service(\n        self, service_type: ServiceTypeT, service_id: str | None = None\n    ) -&gt; list[BaseException | None]:\n        self.debug(lambda: f\"Stopping {service_type} process(es) with id: {service_id}\")\n        tasks = []\n        for info in list(self.multi_process_info):\n            if info.service_type == service_type and (\n                service_id is None or info.service_id == service_id\n            ):\n                task = asyncio.create_task(self._wait_for_process(info))\n                task.add_done_callback(\n                    lambda _, info=info: self.multi_process_info.remove(info)\n                )\n                tasks.append(task)\n        return await asyncio.gather(*tasks, return_exceptions=True)\n\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Stop all required services as multiprocessing processes.\"\"\"\n        self.debug(\"Stopping all service processes\")\n\n        # Wait for all to finish in parallel\n        return await asyncio.gather(\n            *[self._wait_for_process(info) for info in self.multi_process_info],\n            return_exceptions=True,\n        )\n\n    async def kill_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Kill all required services as multiprocessing processes.\"\"\"\n        self.debug(\"Killing all service processes\")\n\n        # Kill all processes\n        for info in self.multi_process_info:\n            if info.process:\n                info.process.kill()\n\n        # Wait for all to finish in parallel\n        return await asyncio.gather(\n            *[self._wait_for_process(info) for info in self.multi_process_info],\n            return_exceptions=True,\n        )\n\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be registered.\n\n        Args:\n            stop_event: Event to check if operation should be cancelled\n            timeout_seconds: Maximum time to wait in seconds\n\n        Raises:\n            Exception if any service failed to register, None otherwise\n        \"\"\"\n        self.debug(\"Waiting for all required services to register...\")\n\n        # Get the set of required service types for checking completion\n        required_types = set(self.required_services.keys())\n\n        # TODO: Can this be done better by using asyncio.Event()?\n\n        async def _wait_for_registration():\n            while not stop_event.is_set():\n                # Get all registered service types from the id map\n                registered_types = {\n                    service_info.service_type\n                    for service_info in self.service_id_map.values()\n                    if service_info.registration_status\n                    == ServiceRegistrationStatus.REGISTERED\n                }\n\n                # Check if all required types are registered\n                if required_types.issubset(registered_types):\n                    return\n\n                # Wait a bit before checking again\n                await asyncio.sleep(0.5)\n\n        try:\n            await asyncio.wait_for(_wait_for_registration(), timeout=timeout_seconds)\n        except asyncio.TimeoutError as e:\n            # Log which services didn't register in time\n            registered_types_set = set(\n                service_info.service_type\n                for service_info in self.service_id_map.values()\n                if service_info.registration_status\n                == ServiceRegistrationStatus.REGISTERED\n            )\n\n            for service_type in required_types:\n                if service_type not in registered_types_set:\n                    self.error(\n                        f\"Service {service_type} failed to register within timeout\"\n                    )\n\n            raise AIPerfError(\"Some services failed to register within timeout\") from e\n\n    async def _wait_for_process(self, info: MultiProcessRunInfo) -&gt; None:\n        \"\"\"Wait for a process to terminate with timeout handling.\"\"\"\n        if not info.process or not info.process.is_alive():\n            return\n\n        try:\n            info.process.terminate()\n            await asyncio.to_thread(\n                info.process.join, timeout=TASK_CANCEL_TIMEOUT_SHORT\n            )\n            self.debug(\n                f\"Service {info.service_type} process stopped (pid: {info.process.pid})\"\n            )\n        except asyncio.TimeoutError:\n            self.warning(\n                f\"Service {info.service_type} process (pid: {info.process.pid}) did not terminate gracefully, killing\"\n            )\n            info.process.kill()\n\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be started.\"\"\"\n        self.debug(\"Waiting for all required services to start...\")\n        self.warning(\n            \"Waiting for all required services to start is not implemented for multiprocessing\"\n        )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessServiceManager.kill_all_services","title":"<code>kill_all_services()</code>  <code>async</code>","text":"<p>Kill all required services as multiprocessing processes.</p> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>async def kill_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Kill all required services as multiprocessing processes.\"\"\"\n    self.debug(\"Killing all service processes\")\n\n    # Kill all processes\n    for info in self.multi_process_info:\n        if info.process:\n            info.process.kill()\n\n    # Wait for all to finish in parallel\n    return await asyncio.gather(\n        *[self._wait_for_process(info) for info in self.multi_process_info],\n        return_exceptions=True,\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessServiceManager.run_service","title":"<code>run_service(service_type, num_replicas=1)</code>  <code>async</code>","text":"<p>Run a service with the given number of replicas.</p> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>async def run_service(\n    self, service_type: ServiceTypeT, num_replicas: int = 1\n) -&gt; None:\n    \"\"\"Run a service with the given number of replicas.\"\"\"\n    service_class = ServiceFactory.get_class_from_type(service_type)\n\n    for _ in range(num_replicas):\n        service_id = f\"{service_type}_{uuid.uuid4().hex[:8]}\"\n        process = Process(\n            target=bootstrap_and_run_service,\n            name=f\"{service_type}_process\",\n            kwargs={\n                \"service_class\": service_class,\n                \"service_id\": service_id,\n                \"service_config\": self.service_config,\n                \"user_config\": self.user_config,\n                \"log_queue\": self.log_queue,\n            },\n            daemon=True,\n        )\n\n        process.start()\n\n        self.debug(\n            lambda pid=process.pid,\n            type=service_type: f\"Service {type} started as process (pid: {pid})\"\n        )\n\n        self.multi_process_info.append(\n            MultiProcessRunInfo(\n                process=process,\n                service_type=service_type,\n                service_id=service_id,\n            )\n        )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessServiceManager.shutdown_all_services","title":"<code>shutdown_all_services()</code>  <code>async</code>","text":"<p>Stop all required services as multiprocessing processes.</p> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Stop all required services as multiprocessing processes.\"\"\"\n    self.debug(\"Stopping all service processes\")\n\n    # Wait for all to finish in parallel\n    return await asyncio.gather(\n        *[self._wait_for_process(info) for info in self.multi_process_info],\n        return_exceptions=True,\n    )\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessServiceManager.wait_for_all_services_registration","title":"<code>wait_for_all_services_registration(stop_event, timeout_seconds=DEFAULT_SERVICE_REGISTRATION_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be registered.</p> <p>Parameters:</p> Name Type Description Default <code>stop_event</code> <code>Event</code> <p>Event to check if operation should be cancelled</p> required <code>timeout_seconds</code> <code>float</code> <p>Maximum time to wait in seconds</p> <code>DEFAULT_SERVICE_REGISTRATION_TIMEOUT</code> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>async def wait_for_all_services_registration(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be registered.\n\n    Args:\n        stop_event: Event to check if operation should be cancelled\n        timeout_seconds: Maximum time to wait in seconds\n\n    Raises:\n        Exception if any service failed to register, None otherwise\n    \"\"\"\n    self.debug(\"Waiting for all required services to register...\")\n\n    # Get the set of required service types for checking completion\n    required_types = set(self.required_services.keys())\n\n    # TODO: Can this be done better by using asyncio.Event()?\n\n    async def _wait_for_registration():\n        while not stop_event.is_set():\n            # Get all registered service types from the id map\n            registered_types = {\n                service_info.service_type\n                for service_info in self.service_id_map.values()\n                if service_info.registration_status\n                == ServiceRegistrationStatus.REGISTERED\n            }\n\n            # Check if all required types are registered\n            if required_types.issubset(registered_types):\n                return\n\n            # Wait a bit before checking again\n            await asyncio.sleep(0.5)\n\n    try:\n        await asyncio.wait_for(_wait_for_registration(), timeout=timeout_seconds)\n    except asyncio.TimeoutError as e:\n        # Log which services didn't register in time\n        registered_types_set = set(\n            service_info.service_type\n            for service_info in self.service_id_map.values()\n            if service_info.registration_status\n            == ServiceRegistrationStatus.REGISTERED\n        )\n\n        for service_type in required_types:\n            if service_type not in registered_types_set:\n                self.error(\n                    f\"Service {service_type} failed to register within timeout\"\n                )\n\n        raise AIPerfError(\"Some services failed to register within timeout\") from e\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.multiprocess_service_manager.MultiProcessServiceManager.wait_for_all_services_start","title":"<code>wait_for_all_services_start(stop_event, timeout_seconds=DEFAULT_SERVICE_START_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be started.</p> Source code in <code>aiperf/services/system_controller/multiprocess_service_manager.py</code> <pre><code>async def wait_for_all_services_start(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be started.\"\"\"\n    self.debug(\"Waiting for all required services to start...\")\n    self.warning(\n        \"Waiting for all required services to start is not implemented for multiprocessing\"\n    )\n</code></pre>"},{"location":"api/#aiperfservicessystem_controllerproxy_manager","title":"aiperf.services.system_controller.proxy_manager","text":""},{"location":"api/#aiperfservicessystem_controllersystem_controller","title":"aiperf.services.system_controller.system_controller","text":""},{"location":"api/#aiperf.services.system_controller.system_controller.SystemController","title":"<code>SystemController</code>","text":"<p>               Bases: <code>SignalHandlerMixin</code>, <code>BaseService</code></p> <p>System Controller service.</p> <p>This service is responsible for managing the lifecycle of all other services. It will start, stop, and configure all other services.</p> Source code in <code>aiperf/services/system_controller/system_controller.py</code> <pre><code>@ServiceFactory.register(ServiceType.SYSTEM_CONTROLLER)\nclass SystemController(SignalHandlerMixin, BaseService):\n    \"\"\"System Controller service.\n\n    This service is responsible for managing the lifecycle of all other services.\n    It will start, stop, and configure all other services.\n    \"\"\"\n\n    def __init__(\n        self,\n        user_config: UserConfig,\n        service_config: ServiceConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n        )\n        self.debug(\"Creating System Controller\")\n        # List of required service types, in no particular order\n        # These are services that must be running before the system controller can start profiling\n        self.required_services: dict[ServiceTypeT, int] = {\n            ServiceType.DATASET_MANAGER: 1,\n            ServiceType.TIMING_MANAGER: 1,\n            ServiceType.WORKER_MANAGER: 1,\n            ServiceType.RECORDS_MANAGER: 1,\n            ServiceType.INFERENCE_RESULT_PARSER: service_config.result_parser_service_count,\n        }\n\n        self.proxy_manager: ProxyManager = ProxyManager(\n            service_config=self.service_config\n        )\n        self.service_manager: ServiceManagerProtocol = (\n            ServiceManagerFactory.create_instance(\n                self.service_config.service_run_type.value,\n                required_services=self.required_services,\n                user_config=self.user_config,\n                service_config=self.service_config,\n                log_queue=get_global_log_queue(),\n            )\n        )\n        self._stop_tasks: set[asyncio.Task] = set()\n        self.debug(\"System Controller created\")\n\n    async def initialize(self) -&gt; None:\n        \"\"\"We need to override the initialize method to run the proxy manager before the base service initialize.\n        This is because the proxies need to be running before we can subscribe to the message bus.\n        \"\"\"\n        self.debug(\"Running ZMQ Proxy Manager Before Initialize\")\n        await self.proxy_manager.initialize_and_start()\n        # Once the proxies are running, call the original initialize method\n        await super().initialize()\n\n    @on_init\n    async def _initialize_system_controller(self) -&gt; None:\n        self.debug(\"Initializing System Controller\")\n\n        self.setup_signal_handlers(self._handle_signal)\n        self.debug(\"Setup signal handlers\")\n        await self.service_manager.initialize()\n\n    @on_start\n    async def _start_services(self) -&gt; None:\n        \"\"\"Bootstrap the system services.\n\n        This method will:\n        - Initialize all required services\n        - Wait for all required services to be registered\n        - Start all required services\n        \"\"\"\n        self.debug(\"System Controller is bootstrapping services\")\n\n        # Start all required services\n        try:\n            await self.service_manager.start()\n        except Exception as e:\n            raise self._service_error(\n                \"Failed to initialize all services\",\n            ) from e\n        await self.service_manager.wait_for_all_services_registration(\n            stop_event=self._stop_requested_event,\n        )\n\n        # TODO: HACK: Wait for 1 second to ensure registrations made. This needs to be\n        # removed once we have the ability to track registrations of services and their state before\n        # starting the profiling.\n        await asyncio.sleep(1)\n\n        self.info(\"AIPerf System is READY\")\n\n        await self._start_profiling_all_services()\n\n        self.debug(\"All required services started successfully\")\n        self.info(\"AIPerf System is RUNNING\")\n\n    async def _start_profiling_all_services(self) -&gt; None:\n        \"\"\"Tell all services to start profiling.\"\"\"\n        # TODO: HACK: Wait for 1 second to ensure services are ready\n        await asyncio.sleep(1)\n\n        self.debug(\"Sending START_PROFILING command to all services\")\n        await self.publish(\n            ProfileStartCommand(service_id=self.service_id),\n        )\n\n    @on_message(MessageType.REGISTRATION)\n    async def _process_registration_message(self, message: RegistrationMessage) -&gt; None:\n        \"\"\"Process a registration message from a service. It will\n        add the service to the service manager and send a configure command\n        to the service.\n\n        Args:\n            message: The registration message to process\n        \"\"\"\n        service_id = message.service_id\n        service_type = message.service_type\n\n        self.debug(\n            lambda: f\"Processing registration from {service_type} with ID: {service_id}\"\n        )\n\n        service_info = ServiceRunInfo(\n            registration_status=ServiceRegistrationStatus.REGISTERED,\n            service_type=service_type,\n            service_id=service_id,\n            first_seen=time.time_ns(),\n            state=message.state,\n            last_seen=time.time_ns(),\n        )\n\n        self.service_manager.service_id_map[service_id] = service_info\n        if service_type not in self.service_manager.service_map:\n            self.service_manager.service_map[service_type] = []\n        self.service_manager.service_map[service_type].append(service_info)\n\n        self.info(lambda: f\"Registered service: {service_type=} with ID: {service_id=}\")\n\n        # Send configure command to the newly registered service\n        try:\n            await self.publish(\n                ProfileConfigureCommand(service_id=service_id, config=self.user_config)\n            )\n        except Exception as e:\n            raise self._service_error(\n                f\"Failed to send configure command to {service_type} (ID: {service_id})\",\n            ) from e\n\n        self.debug(\n            lambda: f\"Sent configure command to {service_type} (ID: {service_id})\"\n        )\n\n    @on_message(MessageType.HEARTBEAT)\n    async def _process_heartbeat_message(self, message: HeartbeatMessage) -&gt; None:\n        \"\"\"Process a heartbeat message from a service. It will\n        update the last seen timestamp and state of the service.\n\n        Args:\n            message: The heartbeat message to process\n        \"\"\"\n        service_id = message.service_id\n        service_type = message.service_type\n        timestamp = message.request_ns\n\n        self.debug(lambda: f\"Received heartbeat from {service_type} (ID: {service_id})\")\n\n        # Update the last heartbeat timestamp if the component exists\n        try:\n            service_info = self.service_manager.service_id_map[service_id]\n            service_info.last_seen = timestamp\n            service_info.state = message.state\n            self.debug(f\"Updated heartbeat for {service_id} to {timestamp}\")\n        except Exception:\n            self.warning(\n                f\"Received heartbeat from unknown service: {service_id} ({service_type})\"\n            )\n\n    @on_message(MessageType.CREDITS_COMPLETE)\n    async def _process_credits_complete_message(\n        self, message: CreditsCompleteMessage\n    ) -&gt; None:\n        \"\"\"Process a credits complete message from a service. It will\n        update the state of the service with the service manager.\n\n        Args:\n            message: The credits complete message to process\n        \"\"\"\n        service_id = message.service_id\n        self.info(f\"Received credits complete from {service_id}\")\n\n    @on_message(MessageType.STATUS)\n    async def _process_status_message(self, message: StatusMessage) -&gt; None:\n        \"\"\"Process a status message from a service. It will\n        update the state of the service with the service manager.\n\n        Args:\n            message: The status message to process\n        \"\"\"\n        service_id = message.service_id\n        service_type = message.service_type\n        state = message.state\n\n        self.debug(\n            lambda: f\"Received status update from {service_type} (ID: {service_id}): {state}\"\n        )\n\n        # Update the component state if the component exists\n        if service_id not in self.service_manager.service_id_map:\n            self.debug(\n                lambda: f\"Received status update from un-registered service: {service_id} ({service_type})\"\n            )\n            return\n\n        service_info = self.service_manager.service_id_map.get(service_id)\n        if service_info is None:\n            return\n\n        service_info.state = message.state\n\n        self.debug(f\"Updated state for {service_id} to {message.state}\")\n\n    @on_message(MessageType.NOTIFICATION)\n    async def _process_notification_message(self, message: NotificationMessage) -&gt; None:\n        \"\"\"Process a notification message.\"\"\"\n        self.info(f\"Received notification message: {message}\")\n\n    @on_message(MessageType.COMMAND_RESPONSE)\n    async def _process_command_response_message(self, message: CommandResponse) -&gt; None:\n        \"\"\"Process a command response message.\"\"\"\n        self.debug(lambda: f\"Received command response message: {message}\")\n        if message.status == CommandResponseStatus.SUCCESS:\n            self.debug(f\"Command {message.command} succeeded from {message.service_id}\")\n        elif message.status == CommandResponseStatus.ACKNOWLEDGED:\n            self.debug(\n                f\"Command {message.command} acknowledged from {message.service_id}\"\n            )\n        elif message.status == CommandResponseStatus.UNHANDLED:\n            self.debug(f\"Command {message.command} unhandled from {message.service_id}\")\n        elif message.status == CommandResponseStatus.FAILURE:\n            message = cast(CommandErrorResponse, message)\n            self.error(\n                f\"Command {message.command} failed from {message.service_id}: {message.error}\"\n            )\n\n    @on_command(CommandType.SPAWN_WORKERS)\n    async def _handle_spawn_workers_command(self, message: SpawnWorkersCommand) -&gt; None:\n        \"\"\"Handle a spawn workers command.\"\"\"\n        self.debug(lambda: f\"Received spawn workers command: {message}\")\n        await self.service_manager.run_service(ServiceType.WORKER, message.num_workers)\n\n    @on_command(CommandType.SHUTDOWN_WORKERS)\n    async def _handle_shutdown_workers_command(\n        self, message: ShutdownWorkersCommand\n    ) -&gt; None:\n        \"\"\"Handle a shutdown workers command.\"\"\"\n        self.debug(lambda: f\"Received shutdown workers command: {message}\")\n        # TODO: Handle individual worker shutdowns via worker id\n        await self.service_manager.stop_service(ServiceType.WORKER)\n\n    @on_message(MessageType.PROCESS_RECORDS_RESULT)\n    async def _on_process_records_result_message(\n        self, message: ProcessRecordsResultMessage\n    ) -&gt; None:\n        \"\"\"Handle a profile results message.\"\"\"\n        self.debug(lambda: f\"Received profile results message: {message}\")\n        if message.process_records_result.errors:\n            self.error(\n                f\"Received process records result message with errors: {message.process_records_result.errors}\"\n            )\n            return\n        if not message.process_records_result.records:\n            self.error(\n                f\"Received process records result message with no records: {message.process_records_result.records}\"\n            )\n            return\n\n        await ExporterManager(\n            results=message.process_records_result.records[0],\n            input_config=self.user_config,\n        ).export_all()\n\n        # TODO: HACK: Stop the system controller after exporting the records\n        self.debug(\"Stopping system controller after exporting records\")\n        await asyncio.shield(self.stop())\n\n    async def _handle_signal(self, sig: int) -&gt; None:\n        \"\"\"Handle received signals by triggering graceful shutdown.\n\n        Args:\n            sig: The signal number received\n        \"\"\"\n        if self.stop_requested:\n            # If we are already in a stopping state, we need to kill the process to be safe.\n            self.warning(lambda: f\"Received signal {sig}, killing\")\n            await self._kill()\n            return\n\n        self.debug(lambda: f\"Received signal {sig}, initiating graceful shutdown\")\n        await self._cancel_profiling()\n\n    async def _cancel_profiling(self) -&gt; None:\n        self.debug(\"Cancelling profiling of all services\")\n        await self.publish(ProfileCancelCommand(service_id=self.service_id))\n\n        # TODO: HACK: Wait for 2 seconds to ensure the profiling is cancelled\n        # Wait for the profiling to be cancelled\n        await asyncio.sleep(2)\n        self.debug(\"Stopping system controller after profiling cancelled\")\n        await asyncio.shield(self.stop())\n\n    @on_stop\n    async def _stop_system_controller(self) -&gt; None:\n        \"\"\"Stop the system controller and all running services.\"\"\"\n        # Broadcast a shutdown command to all services\n        await self.publish(ShutdownCommand(service_id=self.service_id))\n\n        # TODO: HACK: Wait for 0.5 seconds to ensure the shutdown command is received\n        await asyncio.sleep(0.5)\n\n        await self.service_manager.shutdown_all_services()\n        await self.comms.stop()\n        await self.proxy_manager.stop()\n\n    async def _kill(self):\n        \"\"\"Kill the system controller.\"\"\"\n        try:\n            await self.service_manager.kill_all_services()\n        except Exception as e:\n            raise self._service_error(\"Failed to stop all services\") from e\n\n        await super()._kill()\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.system_controller.SystemController.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>We need to override the initialize method to run the proxy manager before the base service initialize. This is because the proxies need to be running before we can subscribe to the message bus.</p> Source code in <code>aiperf/services/system_controller/system_controller.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"We need to override the initialize method to run the proxy manager before the base service initialize.\n    This is because the proxies need to be running before we can subscribe to the message bus.\n    \"\"\"\n    self.debug(\"Running ZMQ Proxy Manager Before Initialize\")\n    await self.proxy_manager.initialize_and_start()\n    # Once the proxies are running, call the original initialize method\n    await super().initialize()\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.system_controller.main","title":"<code>main()</code>","text":"<p>Main entry point for the system controller.</p> Source code in <code>aiperf/services/system_controller/system_controller.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the system controller.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(SystemController)\n</code></pre>"},{"location":"api/#aiperfservicessystem_controllersystem_mixins","title":"aiperf.services.system_controller.system_mixins","text":""},{"location":"api/#aiperf.services.system_controller.system_mixins.SignalHandlerMixin","title":"<code>SignalHandlerMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Mixin for services that need to handle system signals.</p> Source code in <code>aiperf/services/system_controller/system_mixins.py</code> <pre><code>class SignalHandlerMixin(AIPerfLoggerMixin):\n    \"\"\"Mixin for services that need to handle system signals.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        # Set to store signal handler tasks to prevent them from being garbage collected\n        self._signal_tasks = set()\n        super().__init__(**kwargs)\n\n    def setup_signal_handlers(self, callback: Callable[[int], Coroutine]) -&gt; None:\n        \"\"\"This method will set up signal handlers for the SIGTERM and SIGINT signals\n        in order to trigger a graceful shutdown of the service.\n\n        Args:\n            callback: The callback to call when a signal is received\n        \"\"\"\n        loop = asyncio.get_running_loop()\n\n        def signal_handler(sig: int) -&gt; None:\n            task = asyncio.create_task(callback(sig))\n            self._signal_tasks.add(task)\n            task.add_done_callback(self._signal_tasks.discard)\n\n        loop.add_signal_handler(signal.SIGINT, signal_handler, signal.SIGINT)\n</code></pre>"},{"location":"api/#aiperf.services.system_controller.system_mixins.SignalHandlerMixin.setup_signal_handlers","title":"<code>setup_signal_handlers(callback)</code>","text":"<p>This method will set up signal handlers for the SIGTERM and SIGINT signals in order to trigger a graceful shutdown of the service.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[int], Coroutine]</code> <p>The callback to call when a signal is received</p> required Source code in <code>aiperf/services/system_controller/system_mixins.py</code> <pre><code>def setup_signal_handlers(self, callback: Callable[[int], Coroutine]) -&gt; None:\n    \"\"\"This method will set up signal handlers for the SIGTERM and SIGINT signals\n    in order to trigger a graceful shutdown of the service.\n\n    Args:\n        callback: The callback to call when a signal is received\n    \"\"\"\n    loop = asyncio.get_running_loop()\n\n    def signal_handler(sig: int) -&gt; None:\n        task = asyncio.create_task(callback(sig))\n        self._signal_tasks.add(task)\n        task.add_done_callback(self._signal_tasks.discard)\n\n    loop.add_signal_handler(signal.SIGINT, signal_handler, signal.SIGINT)\n</code></pre>"},{"location":"api/#aiperfservicestiming_managerconcurrency_strategy","title":"aiperf.services.timing_manager.concurrency_strategy","text":""},{"location":"api/#aiperf.services.timing_manager.concurrency_strategy.ConcurrencyStrategy","title":"<code>ConcurrencyStrategy</code>","text":"<p>               Bases: <code>CreditIssuingStrategy</code></p> <p>Class for concurrency credit issuing strategy.</p> Source code in <code>aiperf/services/timing_manager/concurrency_strategy.py</code> <pre><code>@CreditIssuingStrategyFactory.register(TimingMode.CONCURRENCY)\nclass ConcurrencyStrategy(CreditIssuingStrategy):\n    \"\"\"Class for concurrency credit issuing strategy.\"\"\"\n\n    def __init__(\n        self, config: TimingManagerConfig, credit_manager: CreditManagerProtocol\n    ):\n        super().__init__(config=config, credit_manager=credit_manager)\n\n        # If the concurrency is larger than the total number of requests, it does not matter\n        # as it is simply an upper bound that will never be reached\n        self._semaphore = asyncio.Semaphore(value=config.concurrency)\n\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute a single credit phase. This will not return until the phase sending is complete.\"\"\"\n        if phase_stats.is_time_based:\n            await self._execute_time_based_phase(phase_stats)\n        elif phase_stats.is_request_count_based:\n            await self._execute_request_count_based_phase(phase_stats)\n        else:\n            raise InvalidStateError(\n                \"Phase must have either a valid total or expected_duration_ns set\"\n            )\n\n    async def _execute_time_based_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute a time-based phase.\"\"\"\n\n        # Start the internal loop in a task so that we can cancel it when the time expires\n        time_task = asyncio.create_task(\n            self._execute_time_based_phase_internal(phase_stats)\n        )\n\n        # Calculate how long until the phase expires\n        sleep_time_sec = (\n            (phase_stats.start_ns / NANOS_PER_SECOND)  # type: ignore\n            + phase_stats.expected_duration_sec\n            - time.time()\n        )\n        self.trace(\n            lambda: f\"Time-based phase will expire in {sleep_time_sec} seconds: {phase_stats}\"\n        )\n\n        # Sleep until the phase expires, and then cancel the task\n        await asyncio.sleep(sleep_time_sec)\n        time_task.cancel()\n        self.debug(lambda: f\"Time-based phase execution expired: {phase_stats}\")\n        # Note, not awaiting the task here as we do not want to block moving to the next phase\n\n    async def _execute_time_based_phase_internal(\n        self, phase_stats: CreditPhaseStats\n    ) -&gt; None:\n        \"\"\"Execute a the internal loop for a time-based phase. This will be called within a task and cancelled when the time expires.\"\"\"\n\n        self.trace(\n            lambda: f\"_execute_time_based_phase_internal loop entered: {phase_stats}\"\n        )\n\n        # This will loop until the task is cancelled\n        while True:\n            try:\n                # Acquire the semaphore. Once we hit the concurrency limit, this will block until a credit is returned\n                await self._semaphore.acquire()\n                self.execute_async(\n                    self.credit_manager.drop_credit(\n                        credit_phase=phase_stats.type,\n                    )\n                )\n                phase_stats.sent += 1\n            except asyncio.CancelledError:\n                self.trace(\n                    lambda: f\"_execute_time_based_phase_internal loop exited: {phase_stats}\"\n                )\n                self.debug(\"Time-based phase execution expired\")\n                break\n\n    async def _execute_request_count_based_phase(\n        self, phase_stats: CreditPhaseStats\n    ) -&gt; None:\n        self.trace(\n            lambda: f\"_execute_request_count_based_phase loop entered: {phase_stats}\"\n        )\n\n        total: int = phase_stats.total_expected_requests  # type: ignore\n\n        while phase_stats.sent &lt; total:\n            await self._semaphore.acquire()\n            self.execute_async(\n                self.credit_manager.drop_credit(\n                    credit_phase=phase_stats.type,\n                )\n            )\n            phase_stats.sent += 1\n\n        self.trace(\n            lambda: f\"_execute_request_count_based_phase loop exited: {phase_stats}\"\n        )\n\n    async def _on_credit_return(self, message: CreditReturnMessage) -&gt; None:\n        \"\"\"Process a credit return message.\"\"\"\n\n        # Release the semaphore to allow another credit to be issued,\n        # then call the superclass to handle the credit return like normal\n        self._semaphore.release()\n        self.trace(lambda: f\"Credit return released semaphore: {self._semaphore}\")\n        await super()._on_credit_return(message)\n</code></pre>"},{"location":"api/#aiperfservicestiming_managerconfig","title":"aiperf.services.timing_manager.config","text":""},{"location":"api/#aiperf.services.timing_manager.config.TimingManagerConfig","title":"<code>TimingManagerConfig</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Configuration for the timing manager.</p> Source code in <code>aiperf/services/timing_manager/config.py</code> <pre><code>class TimingManagerConfig(AIPerfBaseModel):\n    \"\"\"Configuration for the timing manager.\"\"\"\n\n    timing_mode: TimingMode = LoadGeneratorDefaults.TIMING_MODE\n    concurrency: int = LoadGeneratorDefaults.CONCURRENCY\n    request_rate: float | None = LoadGeneratorDefaults.REQUEST_RATE\n    request_rate_mode: RequestRateMode = LoadGeneratorDefaults.REQUEST_RATE_MODE\n    request_count: int = LoadGeneratorDefaults.REQUEST_COUNT\n    warmup_request_count: int = LoadGeneratorDefaults.WARMUP_REQUEST_COUNT\n    random_seed: int | None = None\n    progress_report_interval_sec: float = ServiceDefaults.PROGRESS_REPORT_INTERVAL\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"TimingManagerConfig\":\n        \"\"\"Create a TimingManagerConfig from a UserConfig.\"\"\"\n\n        if user_config.input.fixed_schedule:\n            timing_mode = TimingMode.FIXED_SCHEDULE\n        elif user_config.loadgen.request_rate is not None:\n            timing_mode = TimingMode.REQUEST_RATE\n        else:\n            # Default to concurrency mode if no request rate or schedule is provided\n            timing_mode = TimingMode.CONCURRENCY\n\n        return cls(\n            timing_mode=timing_mode,\n            concurrency=user_config.loadgen.concurrency,\n            request_rate=user_config.loadgen.request_rate,\n            request_rate_mode=user_config.loadgen.request_rate_mode,\n            request_count=user_config.loadgen.request_count,\n            warmup_request_count=user_config.loadgen.warmup_request_count,\n            random_seed=user_config.input.random_seed,\n        )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.config.TimingManagerConfig.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create a TimingManagerConfig from a UserConfig.</p> Source code in <code>aiperf/services/timing_manager/config.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"TimingManagerConfig\":\n    \"\"\"Create a TimingManagerConfig from a UserConfig.\"\"\"\n\n    if user_config.input.fixed_schedule:\n        timing_mode = TimingMode.FIXED_SCHEDULE\n    elif user_config.loadgen.request_rate is not None:\n        timing_mode = TimingMode.REQUEST_RATE\n    else:\n        # Default to concurrency mode if no request rate or schedule is provided\n        timing_mode = TimingMode.CONCURRENCY\n\n    return cls(\n        timing_mode=timing_mode,\n        concurrency=user_config.loadgen.concurrency,\n        request_rate=user_config.loadgen.request_rate,\n        request_rate_mode=user_config.loadgen.request_rate_mode,\n        request_count=user_config.loadgen.request_count,\n        warmup_request_count=user_config.loadgen.warmup_request_count,\n        random_seed=user_config.input.random_seed,\n    )\n</code></pre>"},{"location":"api/#aiperfservicestiming_managercredit_issuing_strategy","title":"aiperf.services.timing_manager.credit_issuing_strategy","text":""},{"location":"api/#aiperf.services.timing_manager.credit_issuing_strategy.CreditIssuingStrategy","title":"<code>CreditIssuingStrategy</code>","text":"<p>               Bases: <code>TaskManagerMixin</code>, <code>ABC</code></p> <p>Base class for credit issuing strategies.</p> Source code in <code>aiperf/services/timing_manager/credit_issuing_strategy.py</code> <pre><code>class CreditIssuingStrategy(TaskManagerMixin, ABC):\n    \"\"\"\n    Base class for credit issuing strategies.\n    \"\"\"\n\n    def __init__(\n        self, config: TimingManagerConfig, credit_manager: CreditManagerProtocol\n    ):\n        super().__init__()\n        self.config = config\n        self.credit_manager = credit_manager\n\n        # This event is set when all phases are complete\n        self.all_phases_complete_event = asyncio.Event()\n\n        # The running stats for each phase, keyed by phase type.\n        self.phase_stats: dict[CreditPhase, CreditPhaseStats] = {}\n\n        # The phases to run including their configuration, in order of execution.\n        self.ordered_phase_configs: list[CreditPhaseConfig] = []\n\n        self._setup_phase_configs()\n        self._validate_phase_configs()\n\n    def _setup_phase_configs(self) -&gt; None:\n        \"\"\"Setup the phases for the strategy. This can be overridden in subclasses to modify the phases.\"\"\"\n        self._setup_warmup_phase_config()\n        self._setup_profiling_phase_config()\n        self.info(\n            lambda: f\"Credit issuing strategy {self.__class__.__name__} initialized with {len(self.ordered_phase_configs)} \"\n            f\"phase(s): {self.ordered_phase_configs}\"\n        )\n\n    def _setup_warmup_phase_config(self) -&gt; None:\n        \"\"\"Setup the warmup phase. This can be overridden in subclasses to modify the warmup phase.\"\"\"\n        if self.config.warmup_request_count &gt; 0:\n            self.ordered_phase_configs.append(\n                CreditPhaseConfig(\n                    type=CreditPhase.WARMUP,\n                    total_expected_requests=self.config.warmup_request_count,\n                )\n            )\n\n    def _setup_profiling_phase_config(self) -&gt; None:\n        \"\"\"Setup the profiling phase. This can be overridden in subclasses to modify the profiling phase.\"\"\"\n        self.ordered_phase_configs.append(\n            CreditPhaseConfig(\n                type=CreditPhase.PROFILING,\n                total_expected_requests=self.config.request_count,\n            )\n        )\n\n    def _validate_phase_configs(self) -&gt; None:\n        \"\"\"Validate the phase configs.\"\"\"\n        for phase_config in self.ordered_phase_configs:\n            if not phase_config.is_valid:\n                raise ConfigurationError(\n                    f\"Phase {phase_config.type} is not valid. It must have either a valid total_expected_requests or expected_duration_sec set\"\n                )\n\n    async def start(self) -&gt; None:\n        \"\"\"Start the credit issuing strategy. This will launch the progress reporting loop, the\n        warmup phase (if applicable), and the profiling phase, all in the background.\"\"\"\n        self.debug(\n            lambda: f\"Starting credit issuing strategy {self.__class__.__name__}\"\n        )\n        self.all_phases_complete_event.clear()\n\n        # Start the progress reporting loop in the background\n        self.execute_async(self._progress_report_loop())\n\n        # Execute the phases in the background\n        self.execute_async(self._execute_phases())\n\n        self.debug(\n            lambda: f\"Waiting for all credit phases to complete for {self.__class__.__name__}\"\n        )\n        # Wait for all phases to complete before returning\n        await self.all_phases_complete_event.wait()\n        self.debug(lambda: f\"All credit phases completed for {self.__class__.__name__}\")\n\n    async def _execute_phases(self) -&gt; None:\n        \"\"\"Execute the all of the credit phases sequentially. This can be overridden in subclasses to modify the execution of the phases.\"\"\"\n        for phase_config in self.ordered_phase_configs:\n            phase_stats = CreditPhaseStats.from_phase_config(phase_config)\n            phase_stats.start_ns = time.time_ns()\n            self.phase_stats[phase_config.type] = phase_stats\n\n            self.execute_async(\n                self.credit_manager.publish_phase_start(\n                    phase_config.type,\n                    phase_stats.start_ns,\n                    # Only one of the below will be set, this is already validated in the strategy\n                    phase_config.total_expected_requests,\n                    phase_config.expected_duration_sec,\n                )\n            )\n\n            # This is implemented in subclasses\n            await self._execute_single_phase(phase_stats)\n\n            # We have sent all the credits for this phase. We must continue to the next\n            # phase even though not all the credits have been returned. This is because\n            # we do not want a gap in the credit issuing.\n            phase_stats.sent_end_ns = time.time_ns()\n            self.execute_async(\n                self.credit_manager.publish_phase_sending_complete(\n                    phase_config.type, phase_stats.sent_end_ns\n                )\n            )\n\n    @abstractmethod\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute a single phase. Should not return until the phase sending is complete. Must be implemented in subclasses.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    async def stop(self) -&gt; None:\n        \"\"\"Stop the credit issuing strategy.\"\"\"\n        await self.cancel_all_tasks()\n\n    async def _on_credit_return(self, message: CreditReturnMessage) -&gt; None:\n        \"\"\"This is called by the credit manager when a credit is returned. It can be\n        overridden in subclasses to handle the credit return.\"\"\"\n        if message.phase not in self.phase_stats:\n            # self.warning(\n            #     lambda: f\"Credit return message received for phase {message.phase} but no phase stats found\"\n            # )\n            return\n\n        phase_stats = self.phase_stats[message.phase]\n        phase_stats.completed += 1\n\n        if (\n            # If we have sent all the credits, check if this is the last one to be returned\n            phase_stats.is_sending_complete\n            and phase_stats.completed &gt;= phase_stats.total_expected_requests  # type: ignore[operator]\n        ):\n            phase_stats.end_ns = time.time_ns()\n            self.info(lambda: f\"Phase completed: {phase_stats}\")\n\n            self.execute_async(\n                self.credit_manager.publish_phase_complete(\n                    message.phase, phase_stats.completed, phase_stats.end_ns\n                )\n            )\n\n            if phase_stats.type == CreditPhase.PROFILING:\n                self.execute_async(self.credit_manager.publish_credits_complete())\n                self.all_phases_complete_event.set()\n\n            # We don't need to keep track of the phase stats anymore\n            self.notice(\n                lambda: f\"Phase {message.phase} completed, removing phase stats\"\n            )\n            self.phase_stats.pop(message.phase)\n\n    async def _progress_report_loop(self) -&gt; None:\n        \"\"\"Report the progress at a fixed interval.\"\"\"\n        self.debug(\"Starting progress reporting loop\")\n        while not self.all_phases_complete_event.is_set():\n            await asyncio.sleep(self.config.progress_report_interval_sec)\n\n            for phase, stats in self.phase_stats.items():\n                try:\n                    await self.credit_manager.publish_progress(\n                        phase, stats.sent, stats.completed\n                    )\n                except Exception as e:\n                    self.error(f\"Error publishing credit progress: {e}\")\n                except asyncio.CancelledError:\n                    self.debug(\"Credit progress reporting loop cancelled\")\n                    return\n\n        self.debug(\"All credits completed, stopping credit progress reporting loop\")\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_issuing_strategy.CreditIssuingStrategy.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the credit issuing strategy. This will launch the progress reporting loop, the warmup phase (if applicable), and the profiling phase, all in the background.</p> Source code in <code>aiperf/services/timing_manager/credit_issuing_strategy.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the credit issuing strategy. This will launch the progress reporting loop, the\n    warmup phase (if applicable), and the profiling phase, all in the background.\"\"\"\n    self.debug(\n        lambda: f\"Starting credit issuing strategy {self.__class__.__name__}\"\n    )\n    self.all_phases_complete_event.clear()\n\n    # Start the progress reporting loop in the background\n    self.execute_async(self._progress_report_loop())\n\n    # Execute the phases in the background\n    self.execute_async(self._execute_phases())\n\n    self.debug(\n        lambda: f\"Waiting for all credit phases to complete for {self.__class__.__name__}\"\n    )\n    # Wait for all phases to complete before returning\n    await self.all_phases_complete_event.wait()\n    self.debug(lambda: f\"All credit phases completed for {self.__class__.__name__}\")\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_issuing_strategy.CreditIssuingStrategy.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the credit issuing strategy.</p> Source code in <code>aiperf/services/timing_manager/credit_issuing_strategy.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the credit issuing strategy.\"\"\"\n    await self.cancel_all_tasks()\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_issuing_strategy.CreditIssuingStrategyFactory","title":"<code>CreditIssuingStrategyFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[TimingMode, CreditIssuingStrategy]</code></p> <p>Factory for creating credit issuing strategies based on the timing mode.</p> Source code in <code>aiperf/services/timing_manager/credit_issuing_strategy.py</code> <pre><code>class CreditIssuingStrategyFactory(AIPerfFactory[TimingMode, CreditIssuingStrategy]):\n    \"\"\"Factory for creating credit issuing strategies based on the timing mode.\"\"\"\n</code></pre>"},{"location":"api/#aiperfservicestiming_managercredit_manager","title":"aiperf.services.timing_manager.credit_manager","text":""},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditManagerProtocol","title":"<code>CreditManagerProtocol</code>","text":"<p>               Bases: <code>PubClientProtocol</code>, <code>Protocol</code></p> <p>Defines the interface for a CreditManager.</p> <p>This is used to allow the credit issuing strategy to interact with the TimingManager in a decoupled way.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>@runtime_checkable\nclass CreditManagerProtocol(PubClientProtocol, Protocol):\n    \"\"\"Defines the interface for a CreditManager.\n\n    This is used to allow the credit issuing strategy to interact with the TimingManager\n    in a decoupled way.\n    \"\"\"\n\n    async def drop_credit(\n        self,\n        credit_phase: CreditPhase,\n        conversation_id: str | None = None,\n        credit_drop_ns: int | None = None,\n    ) -&gt; None: ...\n\n    async def publish_progress(\n        self, phase: CreditPhase, sent: int, completed: int\n    ) -&gt; None: ...\n\n    async def publish_credits_complete(self) -&gt; None: ...\n\n    async def publish_phase_start(\n        self,\n        phase: CreditPhase,\n        start_ns: int,\n        total_expected_requests: int | None,\n        expected_duration_sec: float | None,\n    ) -&gt; None: ...\n\n    async def publish_phase_sending_complete(\n        self, phase: CreditPhase, sent_end_ns: int\n    ) -&gt; None: ...\n\n    async def publish_phase_complete(\n        self, phase: CreditPhase, completed: int, end_ns: int\n    ) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesMixin","title":"<code>CreditPhaseMessagesMixin</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code>, <code>CreditPhaseMessagesRequirements</code></p> <p>Mixin for services to implement the CreditManagerProtocol.</p> Requirements <p>This mixin must be used with a class that provides: - pub_client: PubClientProtocol - service_id: str</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>class CreditPhaseMessagesMixin(MessageBusClientMixin, CreditPhaseMessagesRequirements):\n    \"\"\"Mixin for services to implement the CreditManagerProtocol.\n\n    Requirements:\n        This mixin must be used with a class that provides:\n        - pub_client: PubClientProtocol\n        - service_id: str\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if not isinstance(self, CreditPhaseMessagesRequirements):\n            raise TypeError(\n                \"CreditPhaseMessagesMixin must be used with a class that provides CreditPhaseMessagesRequirements\"\n            )\n\n    async def publish_phase_start(\n        self,\n        phase: CreditPhase,\n        start_ns: int,\n        total_expected_requests: int | None,\n        expected_duration_sec: float | None,\n    ) -&gt; None:\n        \"\"\"Publish the phase start message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseStartMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    start_ns=start_ns,\n                    # Only one of the below will be set, this is already validated in the strategy\n                    total_expected_requests=total_expected_requests,\n                    expected_duration_sec=expected_duration_sec,\n                )\n            )\n        )\n\n    async def publish_phase_sending_complete(\n        self, phase: CreditPhase, sent_end_ns: int\n    ) -&gt; None:\n        \"\"\"Publish the phase sending complete message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseSendingCompleteMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    sent_end_ns=sent_end_ns,\n                )\n            )\n        )\n\n    async def publish_phase_complete(\n        self, phase: CreditPhase, completed: int, end_ns: int\n    ) -&gt; None:\n        \"\"\"Publish the phase complete message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseCompleteMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    completed=completed,\n                    end_ns=end_ns,\n                )\n            )\n        )\n\n    async def publish_progress(\n        self, phase: CreditPhase, sent: int, completed: int\n    ) -&gt; None:\n        \"\"\"Publish the progress message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseProgressMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    sent=sent,\n                    completed=completed,\n                )\n            )\n        )\n\n    async def publish_credits_complete(self) -&gt; None:\n        \"\"\"Publish the credits complete message.\"\"\"\n        self.debug(\"Publishing credits complete message\")\n        self.execute_async(\n            self.publish(CreditsCompleteMessage(service_id=self.service_id))\n        )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesMixin.publish_credits_complete","title":"<code>publish_credits_complete()</code>  <code>async</code>","text":"<p>Publish the credits complete message.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>async def publish_credits_complete(self) -&gt; None:\n    \"\"\"Publish the credits complete message.\"\"\"\n    self.debug(\"Publishing credits complete message\")\n    self.execute_async(\n        self.publish(CreditsCompleteMessage(service_id=self.service_id))\n    )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesMixin.publish_phase_complete","title":"<code>publish_phase_complete(phase, completed, end_ns)</code>  <code>async</code>","text":"<p>Publish the phase complete message.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>async def publish_phase_complete(\n    self, phase: CreditPhase, completed: int, end_ns: int\n) -&gt; None:\n    \"\"\"Publish the phase complete message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseCompleteMessage(\n                service_id=self.service_id,\n                phase=phase,\n                completed=completed,\n                end_ns=end_ns,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesMixin.publish_phase_sending_complete","title":"<code>publish_phase_sending_complete(phase, sent_end_ns)</code>  <code>async</code>","text":"<p>Publish the phase sending complete message.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>async def publish_phase_sending_complete(\n    self, phase: CreditPhase, sent_end_ns: int\n) -&gt; None:\n    \"\"\"Publish the phase sending complete message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseSendingCompleteMessage(\n                service_id=self.service_id,\n                phase=phase,\n                sent_end_ns=sent_end_ns,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesMixin.publish_phase_start","title":"<code>publish_phase_start(phase, start_ns, total_expected_requests, expected_duration_sec)</code>  <code>async</code>","text":"<p>Publish the phase start message.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>async def publish_phase_start(\n    self,\n    phase: CreditPhase,\n    start_ns: int,\n    total_expected_requests: int | None,\n    expected_duration_sec: float | None,\n) -&gt; None:\n    \"\"\"Publish the phase start message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseStartMessage(\n                service_id=self.service_id,\n                phase=phase,\n                start_ns=start_ns,\n                # Only one of the below will be set, this is already validated in the strategy\n                total_expected_requests=total_expected_requests,\n                expected_duration_sec=expected_duration_sec,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesMixin.publish_progress","title":"<code>publish_progress(phase, sent, completed)</code>  <code>async</code>","text":"<p>Publish the progress message.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>async def publish_progress(\n    self, phase: CreditPhase, sent: int, completed: int\n) -&gt; None:\n    \"\"\"Publish the progress message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseProgressMessage(\n                service_id=self.service_id,\n                phase=phase,\n                sent=sent,\n                completed=completed,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.credit_manager.CreditPhaseMessagesRequirements","title":"<code>CreditPhaseMessagesRequirements</code>","text":"<p>               Bases: <code>AIPerfLoggerProtocol</code>, <code>Protocol</code></p> <p>Requirements for the CreditPhaseMessagesMixin. This is the list of attributes that must be provided by the class that uses this mixin.</p> Source code in <code>aiperf/services/timing_manager/credit_manager.py</code> <pre><code>@runtime_checkable\nclass CreditPhaseMessagesRequirements(AIPerfLoggerProtocol, Protocol):\n    \"\"\"Requirements for the CreditPhaseMessagesMixin. This is the list of attributes that must\n    be provided by the class that uses this mixin.\"\"\"\n\n    service_id: str\n</code></pre>"},{"location":"api/#aiperfservicestiming_managerfixed_schedule_strategy","title":"aiperf.services.timing_manager.fixed_schedule_strategy","text":""},{"location":"api/#aiperf.services.timing_manager.fixed_schedule_strategy.FixedScheduleStrategy","title":"<code>FixedScheduleStrategy</code>","text":"<p>               Bases: <code>CreditIssuingStrategy</code></p> <p>Class for fixed schedule credit issuing strategy.</p> Source code in <code>aiperf/services/timing_manager/fixed_schedule_strategy.py</code> <pre><code>@CreditIssuingStrategyFactory.register(TimingMode.FIXED_SCHEDULE)\nclass FixedScheduleStrategy(CreditIssuingStrategy):\n    \"\"\"\n    Class for fixed schedule credit issuing strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: TimingManagerConfig,\n        credit_manager: CreditManagerProtocol,\n        schedule: list[tuple[int, str]],\n    ):\n        super().__init__(config=config, credit_manager=credit_manager)\n\n        self._schedule: list[tuple[int, str]] = schedule\n\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        # TODO: Convert this code to work with the new CreditPhase logic and base classes\n\n        if not self._schedule:\n            self.warning(\"No schedule loaded, no credits will be dropped\")\n            return\n\n        start_time_ns = time.time_ns()\n\n        timestamp_groups = defaultdict(list)\n\n        for timestamp, conversation_id in self._schedule:\n            timestamp_groups[timestamp].append((timestamp, conversation_id))\n\n        schedule_unique_sorted = sorted(timestamp_groups.keys())\n\n        for unique_timestamp in schedule_unique_sorted:\n            wait_duration_ns = max(0, start_time_ns + unique_timestamp - time.time_ns())\n            wait_duration_sec = wait_duration_ns / 1_000_000_000\n\n            if wait_duration_sec &gt; 0:\n                await asyncio.sleep(wait_duration_sec)\n\n            for _, conversation_id in timestamp_groups[unique_timestamp]:\n                self.execute_async(\n                    self.credit_manager.drop_credit(\n                        credit_phase=CreditPhase.PROFILING,\n                        conversation_id=conversation_id,\n                        # We already waited, so it can be sent ASAP\n                        credit_drop_ns=None,\n                    )\n                )\n\n        self.info(\"Completed all scheduled credit drops\")\n</code></pre>"},{"location":"api/#aiperfservicestiming_managerrequest_rate_strategy","title":"aiperf.services.timing_manager.request_rate_strategy","text":""},{"location":"api/#aiperf.services.timing_manager.request_rate_strategy.RequestRateStrategy","title":"<code>RequestRateStrategy</code>","text":"<p>               Bases: <code>CreditIssuingStrategy</code></p> <p>Strategy for issuing credits based on a specified request rate.</p> <p>Supports two modes: - CONSTANT: Issues credits at a constant rate with fixed intervals - POISSON: Issues credits using a Poisson process with exponentially distributed intervals</p> Source code in <code>aiperf/services/timing_manager/request_rate_strategy.py</code> <pre><code>@CreditIssuingStrategyFactory.register(TimingMode.REQUEST_RATE)\nclass RequestRateStrategy(CreditIssuingStrategy):\n    \"\"\"\n    Strategy for issuing credits based on a specified request rate.\n\n    Supports two modes:\n    - CONSTANT: Issues credits at a constant rate with fixed intervals\n    - POISSON: Issues credits using a Poisson process with exponentially distributed intervals\n    \"\"\"\n\n    def __init__(\n        self, config: TimingManagerConfig, credit_manager: CreditManagerProtocol\n    ):\n        super().__init__(config=config, credit_manager=credit_manager)\n\n        if config.request_rate is None:\n            raise InvalidStateError(\"Request rate is not set\")\n        if config.request_count &lt; 1:\n            raise InvalidStateError(\"Request count must be at least 1\")\n\n        self._request_rate = config.request_rate\n        self._request_rate_mode = config.request_rate_mode\n\n        # Initialize random number generator for reproducibility\n        self._random = (\n            random.Random(config.random_seed) if config.random_seed else random.Random()\n        )\n\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute a single phase. This will not return until the phase sending is complete.\"\"\"\n        # Issue credit drops at the specified rate\n        if self._request_rate_mode == RequestRateMode.CONSTANT:\n            await self._execute_constant_rate(phase_stats)\n        elif self._request_rate_mode == RequestRateMode.POISSON:\n            await self._execute_poisson_rate(phase_stats)\n        else:\n            raise InvalidStateError(\n                f\"Unsupported request rate mode: {self._request_rate_mode}\"\n            )\n\n    async def _execute_constant_rate(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute credit drops at a constant rate.\"\"\"\n\n        # The effective time between each credit drop is the inverse of the request rate.\n        period_sec = 1.0 / self._request_rate\n\n        # We start by sending the first credit immediately.\n        next_drop_at = time.perf_counter()\n\n        while phase_stats.should_send:\n            wait_sec = next_drop_at - time.perf_counter()\n            if wait_sec &gt; 0:\n                await asyncio.sleep(wait_sec)\n\n            self.execute_async(\n                self.credit_manager.drop_credit(credit_phase=phase_stats.type)\n            )\n            phase_stats.sent += 1\n\n            # Instead of naively sleeping for a constant period_sec, we are scheduling the\n            # next drop to happen at exactly (next_drop_at + period_sec). This ensures that\n            # we do not slowly drift over time based on slight variances in the asyncio.sleep\n            # or executing the credit drop task.\n            next_drop_at += period_sec\n\n    async def _execute_poisson_rate(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute credit drops using Poisson process (exponential inter-arrival times).\n\n        In a Poisson process with rate \u03bb (requests per second), the inter-arrival times\n        are exponentially distributed with parameter \u03bb. This models realistic traffic\n        patterns where requests arrive randomly but at a consistent average rate.\n        \"\"\"\n        while phase_stats.should_send:\n            # For Poisson process, inter-arrival times are exponentially distributed.\n            # random.expovariate(lambd) generates exponentially distributed random numbers\n            # where lambd is the rate parameter (requests per second)\n            wait_duration_sec = self._random.expovariate(self._request_rate)\n\n            if wait_duration_sec &gt; 0:\n                await asyncio.sleep(wait_duration_sec)\n\n            self.execute_async(\n                self.credit_manager.drop_credit(credit_phase=phase_stats.type)\n            )\n            phase_stats.sent += 1\n</code></pre>"},{"location":"api/#aiperfservicestiming_managertiming_manager","title":"aiperf.services.timing_manager.timing_manager","text":""},{"location":"api/#aiperf.services.timing_manager.timing_manager.TimingManager","title":"<code>TimingManager</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code>, <code>CreditPhaseMessagesMixin</code></p> <p>The TimingManager service is responsible to generate the schedule and issuing timing credits for requests.</p> Source code in <code>aiperf/services/timing_manager/timing_manager.py</code> <pre><code>@ServiceFactory.register(ServiceType.TIMING_MANAGER)\nclass TimingManager(PullClientMixin, BaseComponentService, CreditPhaseMessagesMixin):\n    \"\"\"\n    The TimingManager service is responsible to generate the schedule and issuing\n    timing credits for requests.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.CREDIT_RETURN,\n            pull_client_bind=True,\n        )\n        self.config = TimingManagerConfig.from_user_config(self.user_config)\n\n        self.dataset_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n        self.credit_drop_push_client: PushClientProtocol = (\n            self.comms.create_push_client(\n                CommAddress.CREDIT_DROP,\n                bind=True,\n            )\n        )\n\n        self._credit_issuing_strategy: CreditIssuingStrategy | None = None\n\n    @on_init\n    async def _timing_manager_initialize(self) -&gt; None:\n        \"\"\"Initialize timing manager-specific components.\"\"\"\n        self.debug(\"Initializing timing manager\")\n        await self._configure()\n\n    async def _configure(self) -&gt; None:\n        \"\"\"Configure the timing manager.\"\"\"\n        self.debug(\"Configuring timing manager\")\n\n        if self.config.timing_mode == TimingMode.FIXED_SCHEDULE:\n            # This will block until the dataset is ready and the timing response is received\n            dataset_timing_response: DatasetTimingResponse = (\n                await self.dataset_request_client.request(\n                    message=DatasetTimingRequest(\n                        service_id=self.service_id,\n                    ),\n                )\n            )\n            self.debug(\n                lambda: f\"TM: Received dataset timing response: {dataset_timing_response}\"\n            )\n            self.info(\"Using fixed schedule strategy\")\n            self._credit_issuing_strategy = (\n                CreditIssuingStrategyFactory.create_instance(\n                    TimingMode.FIXED_SCHEDULE,\n                    config=self.config,\n                    credit_manager=self,\n                    schedule=dataset_timing_response.timing_data,\n                )\n            )\n        elif self.config.timing_mode == TimingMode.CONCURRENCY:\n            self.info(\"Using concurrency strategy\")\n            self._credit_issuing_strategy = (\n                CreditIssuingStrategyFactory.create_instance(\n                    TimingMode.CONCURRENCY,\n                    config=self.config,\n                    credit_manager=self,\n                )\n            )\n        elif self.config.timing_mode == TimingMode.REQUEST_RATE:\n            self.info(\"Using request rate strategy\")\n            self._credit_issuing_strategy = (\n                CreditIssuingStrategyFactory.create_instance(\n                    TimingMode.REQUEST_RATE,\n                    config=self.config,\n                    credit_manager=self,\n                )\n            )\n\n        if not self._credit_issuing_strategy:\n            raise InvalidStateError(\"No credit issuing strategy configured\")\n        self.debug(\n            lambda: f\"Timing manager configured with credit issuing strategy: {self._credit_issuing_strategy}\"\n        )\n\n    @on_command(CommandType.PROFILE_START)\n    async def _on_start_profiling(self, message: CommandMessage) -&gt; None:\n        \"\"\"Start the timing manager and issue credit drops according to the configured strategy.\"\"\"\n        self.debug(\"Starting profiling\")\n\n        self.debug(\"Waiting for timing manager to be initialized\")\n        await self.initialized_event.wait()\n        self.debug(\"Timing manager initialized, starting profiling\")\n\n        if not self._credit_issuing_strategy:\n            raise InvalidStateError(\"No credit issuing strategy configured\")\n\n        self.execute_async(self._credit_issuing_strategy.start())\n        self.info(\"Profiling started\")\n\n    @on_command(CommandType.PROFILE_CANCEL)\n    async def _handle_profile_cancel_command(\n        self, message: ProfileCancelCommand\n    ) -&gt; None:\n        self.debug(lambda: f\"Received profile cancel command: {message}\")\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n        if self._credit_issuing_strategy:\n            await self._credit_issuing_strategy.stop()\n\n    @on_stop\n    async def _timing_manager_stop(self) -&gt; None:\n        \"\"\"Stop the timing manager.\"\"\"\n        self.debug(\"Stopping timing manager\")\n        if self._credit_issuing_strategy:\n            await self._credit_issuing_strategy.stop()\n        await self.cancel_all_tasks()\n\n    @on_pull_message(MessageType.CREDIT_RETURN)\n    async def _on_credit_return(self, message: CreditReturnMessage) -&gt; None:\n        \"\"\"Handle the credit return message.\"\"\"\n        self.debug(lambda: f\"Timing manager received credit return message: {message}\")\n        if self._credit_issuing_strategy:\n            await self._credit_issuing_strategy._on_credit_return(message)\n\n    async def drop_credit(\n        self,\n        credit_phase: CreditPhase,\n        conversation_id: str | None = None,\n        credit_drop_ns: int | None = None,\n    ) -&gt; None:\n        \"\"\"Drop a credit.\"\"\"\n        self.execute_async(\n            self.credit_drop_push_client.push(\n                message=CreditDropMessage(\n                    service_id=self.service_id,\n                    phase=credit_phase,\n                    credit_drop_ns=credit_drop_ns,\n                    conversation_id=conversation_id,\n                ),\n            )\n        )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.timing_manager.TimingManager.drop_credit","title":"<code>drop_credit(credit_phase, conversation_id=None, credit_drop_ns=None)</code>  <code>async</code>","text":"<p>Drop a credit.</p> Source code in <code>aiperf/services/timing_manager/timing_manager.py</code> <pre><code>async def drop_credit(\n    self,\n    credit_phase: CreditPhase,\n    conversation_id: str | None = None,\n    credit_drop_ns: int | None = None,\n) -&gt; None:\n    \"\"\"Drop a credit.\"\"\"\n    self.execute_async(\n        self.credit_drop_push_client.push(\n            message=CreditDropMessage(\n                service_id=self.service_id,\n                phase=credit_phase,\n                credit_drop_ns=credit_drop_ns,\n                conversation_id=conversation_id,\n            ),\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.services.timing_manager.timing_manager.main","title":"<code>main()</code>","text":"<p>Main entry point for the timing manager.</p> Source code in <code>aiperf/services/timing_manager/timing_manager.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the timing manager.\"\"\"\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(TimingManager)\n</code></pre>"},{"location":"api/#aiperfservicesworkerscredit_processor_mixin","title":"aiperf.services.workers.credit_processor_mixin","text":""},{"location":"api/#aiperf.services.workers.credit_processor_mixin.CreditProcessorMixin","title":"<code>CreditProcessorMixin</code>","text":"<p>               Bases: <code>CreditProcessorMixinRequirements</code></p> <p>CreditProcessorMixin is a mixin that provides a method to process credit drops.</p> Source code in <code>aiperf/services/workers/credit_processor_mixin.py</code> <pre><code>class CreditProcessorMixin(CreditProcessorMixinRequirements):\n    \"\"\"CreditProcessorMixin is a mixin that provides a method to process credit drops.\"\"\"\n\n    def __init__(self, **kwargs):\n        if not isinstance(self, CreditProcessorMixinRequirements):\n            raise ValueError(\n                \"CreditProcessorMixin must be used with CreditProcessorMixinRequirements\"\n            )\n\n    async def _process_credit_drop_internal(\n        self, message: CreditDropMessage\n    ) -&gt; CreditReturnMessage:\n        \"\"\"Process a credit drop message. Return the CreditReturnMessage.\n\n        - Every credit must be returned after processing\n        - All results or errors should be converted to a RequestRecord and pushed to the inference results client.\n\n        NOTE: This function MUST NOT return until the credit drop is fully processed.\n        This is to ensure that the max concurrency is respected via the semaphore of the pull client.\n        The way this is enforced is by requiring that this method returns a CreditReturnMessage.\n        \"\"\"\n        # TODO: Add tests to ensure that the above note is never violated in the future\n\n        self.trace(lambda: f\"Processing credit drop: {message}\")\n        drop_perf_ns = time.perf_counter_ns()  # The time the credit was received\n\n        if message.phase not in self.task_stats:\n            self.task_stats[message.phase] = WorkerPhaseTaskStats()\n        self.task_stats[message.phase].total += 1\n\n        record: RequestRecord = RequestRecord()\n        try:\n            record = await self._execute_single_credit_internal(message)\n\n        except Exception as e:\n            self.exception(f\"Error processing credit drop: {e}\")\n            record.error = ErrorDetails.from_exception(e)\n            record.end_perf_ns = time.perf_counter_ns()\n\n        finally:\n            record.credit_phase = message.phase\n            msg = InferenceResultsMessage(\n                service_id=self.service_id,\n                record=record,\n            )\n\n            # Note that we already ensured that the phase exists in the task_stats dict in the above code.\n            if not record.valid:\n                self.task_stats[message.phase].failed += 1\n            else:\n                self.task_stats[message.phase].completed += 1\n\n            try:\n                await self.inference_results_push_client.push(msg)\n            except Exception as e:\n                # If we fail to push the record, log the error and continue\n                self.exception(f\"Error pushing request record: {e}\")\n            finally:\n                # Calculate the latency of the credit drop (from when the credit was dropped to when the request was sent)\n                pre_inference_ns = record.start_perf_ns - drop_perf_ns\n                # Always return the credits\n                return_message = CreditReturnMessage(\n                    service_id=self.service_id,\n                    delayed_ns=record.delayed_ns,\n                    pre_inference_ns=pre_inference_ns,\n                    phase=message.phase,\n                )\n                self.trace(lambda: f\"Returning credit {return_message}\")\n                return return_message  # noqa: B012\n\n    async def _execute_single_credit_internal(\n        self, message: CreditDropMessage\n    ) -&gt; RequestRecord:\n        \"\"\"Run a credit task for a single credit.\"\"\"\n\n        if not self.inference_client:\n            raise NotInitializedError(\"Inference server client not initialized.\")\n\n        # retrieve the prompt from the dataset\n        conversation_response: ConversationResponseMessage = (\n            await self.conversation_request_client.request(\n                ConversationRequestMessage(\n                    service_id=self.service_id,\n                    conversation_id=message.conversation_id,\n                    credit_phase=message.phase,\n                )\n            )\n        )\n        self.trace(lambda: f\"Received response message: {conversation_response}\")\n\n        if isinstance(conversation_response, ErrorMessage):\n            return RequestRecord(\n                model_name=self.model_endpoint.primary_model_name,\n                conversation_id=message.conversation_id,\n                turn_index=0,\n                timestamp_ns=time.time_ns(),\n                start_perf_ns=time.perf_counter_ns(),\n                end_perf_ns=time.perf_counter_ns(),\n                error=conversation_response.error,\n            )\n\n        record = await self._call_inference_api_internal(\n            message, conversation_response.conversation.turns[0]\n        )\n        record.model_name = self.model_endpoint.primary_model_name\n        record.conversation_id = conversation_response.conversation.session_id\n        record.turn_index = 0\n        return record\n\n    async def _call_inference_api_internal(\n        self,\n        message: CreditDropMessage,\n        turn: Turn,\n    ) -&gt; RequestRecord:\n        \"\"\"Make a single call to the inference API. Will return an error record if the call fails.\"\"\"\n        self.trace(lambda: f\"Calling inference API for turn: {turn}\")\n        formatted_payload = None\n        pre_send_perf_ns = None\n        timestamp_ns = None\n        try:\n            # Format payload for the API request\n            formatted_payload = await self.request_converter.format_payload(\n                model_endpoint=self.model_endpoint,\n                turn=turn,\n            )\n\n            # NOTE: Current implementation of the TimingManager bypasses this, it is for future use.\n            # Wait for the credit drop time if it is in the future.\n            # Note that we check this after we have retrieved the data from the dataset, to ensure\n            # that we are fully ready to go.\n            delayed_ns = None\n            drop_ns = message.credit_drop_ns\n            now_ns = time.time_ns()\n            if drop_ns and drop_ns &gt; now_ns:\n                self.trace(\n                    lambda: f\"Waiting for credit drop expected time: {(drop_ns - now_ns) / NANOS_PER_SECOND:.2f} s\"\n                )\n                await asyncio.sleep((drop_ns - now_ns) / NANOS_PER_SECOND)\n            elif drop_ns and drop_ns &lt; now_ns:\n                delayed_ns = now_ns - drop_ns\n\n            # Save the current perf_ns before sending the request so it can be used to calculate\n            # the start_perf_ns of the request in case of an exception.\n            pre_send_perf_ns = time.perf_counter_ns()\n            timestamp_ns = time.time_ns()\n\n            # Send the request to the Inference Server API and wait for the response\n            result: RequestRecord = await self.inference_client.send_request(\n                model_endpoint=self.model_endpoint,\n                payload=formatted_payload,\n            )\n\n            self.debug(\n                lambda: f\"pre_send_perf_ns to start_perf_ns latency: {result.start_perf_ns - pre_send_perf_ns} ns\"\n            )\n\n            result.delayed_ns = delayed_ns\n            return result\n\n        except Exception as e:\n            self.exception(\n                f\"Error calling inference server API at {self.model_endpoint.url}: {e}\"\n            )\n            return RequestRecord(\n                request=formatted_payload,\n                timestamp_ns=timestamp_ns or time.time_ns(),\n                # Try and use the pre_send_perf_ns if it is available, otherwise use the current time.\n                start_perf_ns=pre_send_perf_ns or time.perf_counter_ns(),\n                end_perf_ns=time.perf_counter_ns(),\n                error=ErrorDetails.from_exception(e),\n            )\n</code></pre>"},{"location":"api/#aiperf.services.workers.credit_processor_mixin.CreditProcessorMixinRequirements","title":"<code>CreditProcessorMixinRequirements</code>","text":"<p>               Bases: <code>AIPerfLoggerProtocol</code>, <code>Protocol</code></p> <p>CreditProcessorMixinRequirements is a protocol that provides the requirements needed for the CreditProcessorMixin.</p> Source code in <code>aiperf/services/workers/credit_processor_mixin.py</code> <pre><code>@runtime_checkable\nclass CreditProcessorMixinRequirements(AIPerfLoggerProtocol, Protocol):\n    \"\"\"CreditProcessorMixinRequirements is a protocol that provides the requirements needed for the CreditProcessorMixin.\"\"\"\n\n    service_id: str\n    inference_client: InferenceClientProtocol\n    conversation_request_client: RequestClientProtocol\n    inference_results_push_client: PushClientProtocol\n    request_converter: RequestConverterProtocol\n    model_endpoint: ModelEndpointInfo\n    task_stats: dict[CreditPhase, WorkerPhaseTaskStats]\n\n    async def _process_credit_drop_internal(\n        self, message: CreditDropMessage\n    ) -&gt; CreditReturnMessage:\n        \"\"\"Process a credit drop message. Return the CreditReturnMessage.\"\"\"\n        ...\n\n    async def _execute_single_credit_internal(\n        self, message: CreditDropMessage\n    ) -&gt; RequestRecord:\n        \"\"\"Execute a single credit drop. Return the RequestRecord.\"\"\"\n        ...\n\n    async def _call_inference_api_internal(\n        self,\n        message: CreditDropMessage,\n        turn: Turn,\n    ) -&gt; RequestRecord:\n        \"\"\"Make a single call to the inference API. Will return an error record if the call fails.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.services.workers.credit_processor_mixin.CreditProcessorProtocol","title":"<code>CreditProcessorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>CreditProcessorProtocol is a protocol that provides a method to process credit drops.</p> Source code in <code>aiperf/services/workers/credit_processor_mixin.py</code> <pre><code>@runtime_checkable\nclass CreditProcessorProtocol(Protocol):\n    \"\"\"CreditProcessorProtocol is a protocol that provides a method to process credit drops.\"\"\"\n\n    async def _process_credit_drop_internal(\n        self, message: CreditDropMessage\n    ) -&gt; CreditReturnMessage:\n        \"\"\"Process a credit drop message. Return the CreditReturnMessage.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperfservicesworkersworker","title":"aiperf.services.workers.worker","text":""},{"location":"api/#aiperf.services.workers.worker.Worker","title":"<code>Worker</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code>, <code>ProcessHealthMixin</code>, <code>CreditProcessorMixin</code></p> <p>Worker is primarily responsible for making API calls to the inference server. It also manages the conversation between turns and returns the results to the Inference Results Parsers.</p> Source code in <code>aiperf/services/workers/worker.py</code> <pre><code>@ServiceFactory.register(ServiceType.WORKER)\nclass Worker(\n    PullClientMixin, BaseComponentService, ProcessHealthMixin, CreditProcessorMixin\n):\n    \"\"\"Worker is primarily responsible for making API calls to the inference server.\n    It also manages the conversation between turns and returns the results to the Inference Results Parsers.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ):\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.CREDIT_DROP,\n            pull_client_bind=False,\n            **kwargs,\n        )\n\n        self.debug(lambda: f\"Worker process __init__ (pid: {self.process.pid})\")\n\n        self.health_check_interval = self.service_config.workers.health_check_interval\n\n        self.task_stats: dict[CreditPhase, WorkerPhaseTaskStats] = {}\n\n        self.credit_return_push_client: PushClientProtocol = (\n            self.comms.create_push_client(\n                CommAddress.CREDIT_RETURN,\n            )\n        )\n        self.inference_results_push_client: PushClientProtocol = (\n            self.comms.create_push_client(\n                CommAddress.RAW_INFERENCE_PROXY_FRONTEND,\n            )\n        )\n        self.conversation_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n\n        self.model_endpoint = ModelEndpointInfo.from_user_config(self.user_config)\n\n        self.debug(\n            lambda: f\"Creating inference client for {self.model_endpoint.endpoint.type}, \"\n            f\"class: {InferenceClientFactory.get_class_from_type(self.model_endpoint.endpoint.type).__name__}\",\n        )\n        self.request_converter = RequestConverterFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n        )\n        self.inference_client = InferenceClientFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n            model_endpoint=self.model_endpoint,\n        )\n\n    @on_pull_message(MessageType.CREDIT_DROP)\n    async def _credit_drop_callback(self, message: CreditDropMessage) -&gt; None:\n        \"\"\"Handle an incoming credit drop message from the timing manager. Every credit must be returned after processing.\"\"\"\n\n        # Create a default credit return message in case of an exception\n        credit_return_message = CreditReturnMessage(\n            service_id=self.service_id,\n            phase=message.phase,\n        )\n\n        try:\n            # NOTE: This must be awaited to ensure that the max concurrency is respected\n            credit_return_message = await self._process_credit_drop_internal(message)\n        except Exception as e:\n            self.exception(f\"Error processing credit drop: {e}\")\n        finally:\n            # It is fine to execute the push asynchronously here because the worker is technically\n            # ready to process the next credit drop.\n            self.execute_async(\n                self.credit_return_push_client.push(credit_return_message)\n            )\n\n    @on_stop\n    async def _shutdown_worker(self) -&gt; None:\n        self.debug(\"Shutting down worker\")\n        if self.inference_client:\n            await self.inference_client.close()\n\n    @background_task(\n        immediate=False,\n        interval=lambda self: self.health_check_interval,\n    )\n    async def _health_check_task(self) -&gt; None:\n        \"\"\"Task to report the health of the worker to the worker manager.\"\"\"\n        await self.publish(self.create_health_message())\n\n    def create_health_message(self) -&gt; WorkerHealthMessage:\n        return WorkerHealthMessage(\n            service_id=self.service_id,\n            process=self.get_process_health(),\n            task_stats=self.task_stats,\n        )\n\n    @on_command(CommandType.PROFILE_CANCEL)\n    async def _handle_profile_cancel_command(\n        self, message: ProfileCancelCommand\n    ) -&gt; None:\n        self.debug(lambda: f\"Received profile cancel command: {message}\")\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n        await self.stop()\n</code></pre>"},{"location":"api/#aiperfservicesworkersworker_manager","title":"aiperf.services.workers.worker_manager","text":""},{"location":"api/#aiperf.services.workers.worker_manager.WorkerManager","title":"<code>WorkerManager</code>","text":"<p>               Bases: <code>BaseComponentService</code></p> <p>The WorkerManager service is primary responsibility to manage the worker processes. It will spawn the workers, monitor their health, and stop them when the service is stopped. In the future it will also be responsible for the auto-scaling of the workers.</p> Source code in <code>aiperf/services/workers/worker_manager.py</code> <pre><code>@ServiceFactory.register(ServiceType.WORKER_MANAGER)\nclass WorkerManager(BaseComponentService):\n    \"\"\"\n    The WorkerManager service is primary responsibility to manage the worker processes.\n    It will spawn the workers, monitor their health, and stop them when the service is stopped.\n    In the future it will also be responsible for the auto-scaling of the workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ):\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            **kwargs,\n        )\n\n        self.trace(\"WorkerManager.__init__\")\n        self.workers: dict[str, WorkerProcessInfo] = {}\n        self.worker_health: dict[str, WorkerHealthMessage] = {}\n\n        self.cpu_count = multiprocessing.cpu_count()\n        self.debug(lambda: f\"Detected {self.cpu_count} CPU cores/threads\")\n\n        self.max_concurrency = self.user_config.loadgen.concurrency\n        self.max_workers = self.service_config.workers.max\n        if self.max_workers is None:\n            # Default to the number of CPU cores - 1\n            self.max_workers = self.cpu_count - 1\n\n        # Cap the worker count to the max concurrency + 1, but only if the user is in concurrency mode.\n        if self.max_concurrency &gt; 1:\n            self.max_workers = min(\n                self.max_concurrency + 1,\n                self.max_workers,\n            )\n\n        # Ensure we have at least the min workers\n        self.max_workers = max(\n            self.max_workers,\n            self.service_config.workers.min or 0,\n        )\n        self.initial_workers = self.max_workers\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize worker manager-specific components.\"\"\"\n        self.debug(\"WorkerManager initializing\")\n\n        await self.publish(\n            SpawnWorkersCommand(\n                service_id=self.service_id,\n                num_workers=self.initial_workers,\n            )\n        )\n\n    @on_stop\n    async def _stop(self) -&gt; None:\n        self.debug(\"WorkerManager stopping\")\n\n        await self.publish(\n            ShutdownWorkersCommand(\n                service_id=self.service_id,\n                all_workers=True,\n            )\n        )\n\n    @on_message(MessageType.WORKER_HEALTH)\n    async def _on_worker_health(self, message: WorkerHealthMessage) -&gt; None:\n        self.debug(lambda: f\"Received worker health message: {message}\")\n        self.worker_health[message.service_id] = message\n</code></pre>"},{"location":"api/#aiperf.services.workers.worker_manager.WorkerProcessInfo","title":"<code>WorkerProcessInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a worker process.</p> Source code in <code>aiperf/services/workers/worker_manager.py</code> <pre><code>class WorkerProcessInfo(AIPerfBaseModel):\n    \"\"\"Information about a worker process.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    worker_id: str = Field(..., description=\"ID of the worker process\")\n    process: Any = Field(None, description=\"Process object or task\")\n</code></pre>"},{"location":"hook-system/","title":"Hook system","text":""},{"location":"hook-system/#aiperf-hook-system","title":"AIPerf Hook System","text":"<p>TODO: Once we create a Mixin for self.stop_event, we can avoid having the user to call <code>while not self.stop_event.is_set()</code></p> <p>The AIPerf Hook System provides a powerful, extensible mechanism for implementing lifecycle management and event-driven programming patterns. It enables clean separation of concerns by allowing components to register callbacks that execute at specific points during service execution.</p>"},{"location":"hook-system/#core-components","title":"Core Components","text":""},{"location":"hook-system/#1-hook-types-aiperfhook","title":"1. Hook Types (<code>AIPerfHook</code>)","text":"<p>The system defines standard lifecycle hooks:</p> <ul> <li><code>ON_INIT</code>: Initialization phase</li> <li><code>ON_RUN</code>: Main execution phase</li> <li><code>ON_CONFIGURE</code>: Configuration updates</li> <li><code>ON_START</code>: Service startup</li> <li><code>ON_STOP</code>: Service shutdown</li> <li><code>ON_CLEANUP</code>: Resource cleanup</li> </ul> <p>And additional usability hooks:</p> <ul> <li><code>ON_SET_STATE</code>: State transitions</li> <li><code>AIPERF_TASK</code>: Background task registration</li> </ul>"},{"location":"hook-system/#2-hook-system-hooksystem","title":"2. Hook System (<code>HookSystem</code>)","text":"<p>Manages hook registration and execution:</p> <pre><code>class HookSystem:\n    def __init__(self, supported_hooks: set[HookType]):\n        self.supported_hooks = supported_hooks\n        self._hooks: dict[HookType, list[Callable]] = {}\n</code></pre>"},{"location":"hook-system/#3-hooks-mixin-hooksmixin","title":"3. Hooks Mixin (<code>HooksMixin</code>)","text":"<p>Provides the interface for hook-enabled classes:</p> <pre><code>class HooksMixin:\n    supported_hooks: set[HookType] = set()\n\n    def __init__(self):\n        self._hook_system = HookSystem(self.supported_hooks)\n        # Auto-register decorated methods\n</code></pre>"},{"location":"hook-system/#usage-patterns","title":"Usage Patterns","text":""},{"location":"hook-system/#basic-implementation-self-contained","title":"Basic Implementation - Self Contained","text":"<p>Hooks can be defined and used by the same class</p> <pre><code>import asyncio\nfrom aiperf.common.hooks import supports_hooks, on_init, on_cleanup, AIPerfHook\nfrom aiperf.common.mixins import HooksMixin\n\n\n@supports_hooks(AIPerfHook.ON_INIT, AIPerfHook.ON_CLEANUP)\nclass MyService(HooksMixin):\n    def __init__(self):\n        self.resources = []\n        # Make sure to call __init__ on the HooksMixin\n        super().__init__()\n\n    # Hook definitions\n\n    @on_init\n    async def _setup_database(self):\n        \"\"\"Initialize database connection.\"\"\"\n        self.db = await connect_to_database()\n        self.resources.append(self.db)\n\n    @on_init\n    async def _setup_cache(self):\n        \"\"\"Initialize cache system.\"\"\"\n        self.cache = await setup_redis_cache()\n        self.resources.append(self.cache)\n\n    @on_cleanup\n    async def _cleanup_resources(self):\n        \"\"\"Clean up all resources.\"\"\"\n        await asyncio.gather(*[\n            resource.close() for resource in self.resources\n        ])\n\n    # Top-level functions that will call the hooks\n\n    async def initialize(self):\n        await self.run_hooks_async(AIPerfHook.ON_INIT)\n\n    async def cleanup(self):\n        await self.run_hooks_async(AIPerfHook.ON_CLEANUP)\n</code></pre>"},{"location":"hook-system/#basic-implementation-inheritance","title":"Basic Implementation - Inheritance","text":"<p>Hooks can also be used to call additional functionality defined in subclasses. By calling <code>await self.run_hooks_async(AIPerfHook.ON_INIT)</code>, the base class is able to call all registered init functions no matter the subclass that defined it.</p> <pre><code>from aiperf.common.hooks import supports_hooks, on_init, on_cleanup, AIPerfHook\nfrom aiperf.common.mixins import HooksMixin\n\n\n@supports_hooks(AIPerfHook.ON_INIT, AIPerfHook.ON_CLEANUP)\nclass MyHookService(HooksMixin):\n    \"\"\"Defines the top-level functionality that will call the registered hooks.\"\"\"\n\n    def __init__(self):\n        # Make sure to call __init__ on the HooksMixin\n        super().__init__()\n\n    async def initialize(self):\n        \"\"\"Runs all of the registered ON_INIT hooks\"\"\"\n        # Note: Using run_hooks without the _async will run them serially\n        await self.run_hooks(AIPerfHook.ON_INIT)\n\n    async def cleanup(self):\n        \"\"\"Runs all of the registered ON_CLEANUP hooks\"\"\"\n        await self.run_hooks_async(AIPerfHook.ON_CLEANUP)\n\n\nclass CustomService(MyHookService):\n    \"\"\"Defines functions that will be called by the lifecycle hooks\"\"\"\n\n    @on_init\n    async def _setup_database(self):\n        \"\"\"Initialize database connection.\"\"\"\n        self.db = await connect_to_database()\n        self.resources.append(self.db)\n\n    @on_init\n    async def _setup_cache(self):\n        \"\"\"Initialize cache system.\"\"\"\n        self.cache = await setup_redis_cache()\n        self.resources.append(self.cache)\n\n    @on_cleanup\n    async def _cleanup_cache(self):\n        await self.cache.close()\n\n    @on_cleanup\n    async def _cleanup_database(self):\n        await self.db.close()\n</code></pre>"},{"location":"hook-system/#hook-execution-flow","title":"Hook Execution Flow","text":"<pre><code>sequenceDiagram\n    participant C as \ud83d\udcf1 Client\n    participant CS as \ud83d\udd27 CustomService\n    participant MHS as \u2699\ufe0f MyHookService\n    participant HM as \ud83c\udfaf HooksMixin\n\n    Note over C, HM: Hook System Setup &amp; Registration\n    MHS--&gt;&gt;HM: \ud83d\udccb @supports_hooks(AIPerfHook.ON_INIT, ...)\n    CS--&gt;&gt;HM: \ud83d\udd17 @on_init (registration)\n\n    Note over C, HM: Hook Execution Flow\n    C-&gt;&gt;+MHS: \ud83d\ude80 initialize()\n    MHS-&gt;&gt;+HM: \u26a1 run_hooks_async(ON_INIT)\n\n    Note over HM: Parallel Hook Execution\n    loop \ud83d\udd04 For each registered hook\n        HM-&gt;&gt;+CS: \ud83c\udfac Execute hook function\n        CS--&gt;&gt;-HM: \u2705 Hook completed\n    end\n\n    HM--&gt;&gt;-MHS: \ud83c\udfc1 All hooks completed\n    MHS--&gt;&gt;-C: \u2728 Initialization complete\n\n    %% Custom styling for better visibility\n    %%{init: {\n        'theme': 'dark',\n        'themeVariables': {\n            'primaryColor': '#2196f3',\n            'primaryTextColor': '#ffffff',\n            'primaryBorderColor': '#1976d2',\n            'lineColor': '#90a4ae',\n            'secondaryColor': '#9c27b0',\n            'tertiaryColor': '#4caf50',\n            'background': '#263238',\n            'noteTextColor': '#ffffff',\n            'noteBkgColor': '#37474f',\n            'noteBorderColor': '#546e7a'\n        }\n    }}%%\n</code></pre>"},{"location":"hook-system/#inheritance-and-hook-composition","title":"Inheritance and Hook Composition","text":"<pre><code>@supports_hooks(AIPerfHook.ON_INIT, AIPerfHook.ON_CLEANUP)\nclass BaseService(HooksMixin):\n    @on_init\n    async def base_init(self):\n        self.logger.info(\"Base service initializing\")\n\n@supports_hooks(AIPerfHook.ON_START)  # Adds ON_START to inherited hooks\nclass WebService(BaseService):\n    @on_init\n    async def web_init(self):\n        self.logger.info(\"Web service initializing\")\n\n    @on_start\n    async def start_server(self):\n        self.server = await start_web_server()\n</code></pre> <p>Hook inheritance flow:</p> <pre><code>graph TD\n    A[\"**BaseService**\"] --&gt; B[\"*WebService*\"]\n    A --&gt; C[\"&lt;b&gt;Hooks:&lt;/b&gt;&lt;br/&gt;\u2022 ON_INIT&lt;br/&gt;\u2022 ON_CLEANUP\"]\n    B --&gt; D[\"&lt;b&gt;Inherited Hooks:&lt;/b&gt;&lt;br/&gt;\u2022 ON_INIT&lt;br/&gt;\u2022 ON_CLEANUP&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Added Hook:&lt;/b&gt;&lt;br/&gt;\u2022 ON_START\"]\n\n    E[\"Hook Execution&lt;br/&gt;Order\"] --&gt; F[\"base_init()\"]\n    F --&gt; G[\"web_init()\"]\n\n    %% Styling for better visibility\n    style A fill:#bbdefb,stroke:#1976d2,stroke-width:2px,color:#000\n    style B fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,color:#000\n    style C fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    style F fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    style G fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n\n    %% Better arrow styling\n    linkStyle 0 stroke:#666,stroke-width:2px\n    linkStyle 1 stroke:#666,stroke-width:2px\n    linkStyle 2 stroke:#666,stroke-width:2px\n    linkStyle 3 stroke:#666,stroke-width:2px\n    linkStyle 4 stroke:#666,stroke-width:2px\n</code></pre>"},{"location":"hook-system/#hook-registration-and-execution-order","title":"Hook Registration and Execution Order","text":"<p>Hooks are registered in a predictable, deterministic order that ensures proper initialization flow:</p>"},{"location":"hook-system/#1-across-classes-base-derived","title":"1. Across Classes: Base \u2192 Derived","text":"<pre><code>class BaseService(HooksMixin):\n    @on_init\n    async def base_setup(self):        # Registered 1st\n        pass\n\nclass MyService(BaseService):\n    @on_init\n    async def service_setup(self):     # Registered 2nd\n        pass\n</code></pre>"},{"location":"hook-system/#2-within-classes-definition-order","title":"2. Within Classes: Definition Order","text":"<pre><code>class MyService(BaseService):\n    @on_init\n    async def setup_database(self):    # Registered 1st\n        pass\n\n    @on_init\n    async def setup_cache(self):       # Registered 2nd\n        pass\n\n    @on_init\n    async def setup_metrics(self):     # Registered 3rd\n        pass\n</code></pre> <p>\ud83d\udca1 Key Point: Base class hooks always run before derived class hooks, ensuring that foundational components (communication, signals) are initialized before service-specific functionality.</p> <p>\u26a0\ufe0f Important: To maintain this execution order, you must use <code>run_hooks()</code> (serial execution). Using <code>run_hooks_async()</code> runs hooks concurrently and does not guarantee execution order, even though registration order is still deterministic.</p> <pre><code># \u2705 Preserves execution order (serial)\nawait self.run_hooks(AIPerfHook.ON_INIT)\n\n# \u274c No execution order guarantee (concurrent)\nawait self.run_hooks_async(AIPerfHook.ON_INIT)\n</code></pre>"},{"location":"hook-system/#advanced-features","title":"Advanced Features","text":""},{"location":"hook-system/#runtime-hook-registration","title":"Runtime Hook Registration","text":"<pre><code>service = MyService()\n\nasync def custom_monitoring_hook():\n    await send_metrics_to_monitoring_system()\n\n# Register hook at runtime using class instance\nservice.register_hook(AIPerfHook.ON_START, custom_monitoring_hook)\n</code></pre>"},{"location":"hook-system/#serial-vs-concurrent-execution","title":"Serial vs Concurrent Execution","text":"<pre><code># Serial execution (hooks run one after another). Each one is awaited individually.\nawait self.run_hooks(AIPerfHook.ON_INIT)\n\n# Concurrent execution (all hooks run simultaneously and are gathered at the end)\nawait self.run_hooks_async(AIPerfHook.ON_INIT)\n</code></pre>"},{"location":"hook-system/#error-handling","title":"Error Handling","text":""},{"location":"hook-system/#unsupported-hook-error","title":"Unsupported Hook Error","text":"<p>When a hook decorator is defined on a function within a class that does not support that hook type, an exception is raised. The reason for this is to cause traceability and prevent users from trying to hook into a functionality that is not implemented.</p> <pre><code>@supports_hooks(AIPerfHook.ON_INIT)\nclass LimitedService(HooksMixin):\n    @on_start  # This will raise UnsupportedHookError\n    async def invalid_hook(self):\n        pass\n</code></pre>"},{"location":"hook-system/#multi-error-handling","title":"Multi-Error Handling","text":"<p>When multiple hooks fail, the system collects all errors:</p> <pre><code>try:\n    await self.run_hooks(AIPerfHook.ON_INIT)\nexcept AIPerfMultiError as e:\n    for error in e.errors:\n        self.logger.error(f\"Hook failed: {error}\")\n</code></pre>"},{"location":"hook-system/#best-practices","title":"Best Practices","text":""},{"location":"hook-system/#1-hook-naming-convention","title":"1. Hook Naming Convention","text":"<pre><code>class MyService(BaseService):\n    @on_init\n    async def _initialize_database(self):  # Prefix with underscore\n        pass\n\n    @on_cleanup\n    async def _cleanup_connections(self):  # Descriptive names\n        pass\n</code></pre>"},{"location":"hook-system/#2-resource-management","title":"2. Resource Management","text":"<pre><code>@on_init\nasync def _setup_resources(self):\n    self.resources = []\n\n@on_cleanup\nasync def _cleanup_resources(self):\n    for resource in reversed(self.resources):  # LIFO cleanup\n        await resource.close()\n</code></pre>"},{"location":"hook-system/#3-error-isolation","title":"3. Error Isolation","text":"<pre><code>@on_init\nasync def _safe_initialization(self):\n    try:\n        await risky_operation()\n    except Exception as e:\n        self.logger.error(f\"Non-critical init failed: {e}\")\n        # Don't re-raise if operation is optional\n</code></pre>"},{"location":"hook-system/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Concurrent execution: Use <code>run_hooks_async()</code> for independent hooks</li> <li>Serial execution: Use <code>run_hooks()</code> when hooks have dependencies (ie. base class hooks must be called before subclass hooks)</li> <li>Hook registration: Happens once during <code>__init__</code>, minimal overhead</li> <li>Memory usage: Hooks are stored as method references bound to self, not duplicated</li> </ul> <p>The AIPerf Hook System provides a robust foundation for building extensible, maintainable services with clear lifecycle management and event-driven architecture patterns.</p>"},{"location":"hook-system/#the-special-aiperf_task-decorator","title":"The Special <code>@aiperf_task</code> Decorator","text":"<p>The <code>@aiperf_task</code> decorator is unique among the AIPerf hooks because it doesn't follow the typical hook execution pattern. Instead of being executed at specific lifecycle moments like other hooks, functions decorated with <code>@aiperf_task</code> are automatically registered as long-running background tasks that start when the service initializes and run continuously until the service shuts down.</p>"},{"location":"hook-system/#how-aiperf_task-works","title":"How <code>@aiperf_task</code> Works","text":"<p>The <code>@aiperf_task</code> decorator works through the <code>AIPerfTaskMixin</code> class, which provides automatic task lifecycle management:</p> <ol> <li>Discovery: All methods decorated with <code>@aiperf_task</code> are discovered during class initialization</li> <li>Automatic Startup: Tasks are automatically started during the <code>ON_INIT</code> hook phase</li> <li>Registration: Each task is created using <code>asyncio.create_task()</code> and stored in <code>registered_tasks</code></li> <li>Automatic Shutdown: Tasks are cancelled and cleaned up during the <code>ON_STOP</code> hook phase</li> </ol>"},{"location":"hook-system/#using-aiperftaskmixin","title":"Using <code>AIPerfTaskMixin</code>","text":"<p>To use <code>@aiperf_task</code> decorated methods, your class must inherit from <code>AIPerfTaskMixin</code>:</p> <pre><code>from aiperf.common.hooks import aiperf_task\nfrom aiperf.common.mixins import AIPerfTaskMixin\nimport asyncio\n\n\nclass BackgroundService(AIPerfTaskMixin):\n    def __init__(self):\n        self.stop_event = asyncio.Event()\n        self.metrics = {}\n        super().__init__()  # Important: call super().__init__()\n\n    @aiperf_task\n    async def _monitor_system_health(self):\n        \"\"\"Continuously monitor system health metrics.\"\"\"\n        while not self.stop_event.is_set():\n            try:\n                # Collect system metrics\n                cpu_usage = await get_cpu_usage()\n                memory_usage = await get_memory_usage()\n\n                self.metrics.update({\n                    'cpu': cpu_usage,\n                    'memory': memory_usage,\n                    'timestamp': time.time()\n                })\n\n                # Check if metrics exceed thresholds\n                if cpu_usage &gt; 90:\n                    self.logger.warning(f\"High CPU usage: {cpu_usage}%\")\n\n                await asyncio.sleep(5)  # Poll every 5 seconds\n\n            except asyncio.CancelledError:\n                self.logger.info(\"Health monitoring task cancelled\")\n                break\n            except Exception as e:\n                self.logger.error(f\"Error in health monitoring: {e}\")\n                await asyncio.sleep(1)\n\n    # Manual lifecycle control\n    async def start_service(self):\n        \"\"\"Start the service and all background tasks.\"\"\"\n        await self.run_hooks(AIPerfHook.ON_INIT)  # This starts all @aiperf_task methods\n\n    async def stop_service(self):\n        \"\"\"Stop the service and all background tasks.\"\"\"\n        self.stop_event.set()  # Signal tasks to stop\n        await self.run_hooks(AIPerfHook.ON_STOP)  # This cancels and waits for all tasks\n</code></pre>"},{"location":"hook-system/#key-differences-from-other-hooks","title":"Key Differences from Other Hooks","text":"Aspect Regular Hooks (<code>@on_init</code>, <code>@on_start</code>, etc.) <code>@aiperf_task</code> Execution Called once at specific lifecycle events Run continuously as background tasks Lifecycle Short-lived, return after completion Long-lived, run until service shutdown Cancellation Not applicable Automatically cancelled on service stop Purpose Setup, teardown, event handling Background processing, monitoring, polling Mixin Required <code>HooksMixin</code> <code>AIPerfTaskMixin</code>"},{"location":"hook-system/#task-lifecycle-management","title":"Task Lifecycle Management","text":"<p>The <code>AIPerfTaskMixin</code> handles the complete lifecycle of <code>@aiperf_task</code> decorated methods:</p> <pre><code>class AIPerfTaskMixin(HooksMixin):\n    def __init__(self):\n        super().__init__()\n        self.registered_tasks: dict[str, asyncio.Task] = {}\n\n    @on_init\n    async def _start_tasks(self):\n        \"\"\"Start all the registered tasks.\"\"\"\n        for hook in self.get_hooks(AIPerfHook.AIPERF_TASK):\n            self.registered_tasks[hook.__name__] = asyncio.create_task(hook())\n\n    @on_stop\n    async def _stop_tasks(self):\n        \"\"\"Stop all the registered tasks.\"\"\"\n        for task in self.registered_tasks.values():\n            task.cancel()\n\n        # Wait for all tasks to complete\n        with contextlib.suppress(asyncio.CancelledError):\n            await asyncio.gather(*self.registered_tasks.values())\n</code></pre>"},{"location":"hook-system/#best-practices-for-aiperf_task","title":"Best Practices for <code>@aiperf_task</code>","text":""},{"location":"hook-system/#1-always-handle-cancellation","title":"1. Always Handle Cancellation","text":"<pre><code>@aiperf_task\nasync def _background_worker(self):\n    try:\n        while not self.stop_event.is_set():\n            await do_work()\n            await asyncio.sleep(1)\n    except asyncio.CancelledError:\n        # Perform cleanup if necessary\n        await cleanup_resources()\n        raise  # Re-raise to properly cancel the task\n</code></pre>"},{"location":"hook-system/#2-use-stop-events-for-graceful-shutdown","title":"2. Use Stop Events for Graceful Shutdown","text":"<pre><code>@aiperf_task\nasync def _poller(self):\n    while not self.stop_event.is_set():\n        try:\n            await poll_external_service()\n            await asyncio.sleep(10)\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"hook-system/#3-include-error-handling-and-recovery","title":"3. Include Error Handling and Recovery","text":"<pre><code>@aiperf_task\nasync def _resilient_worker(self):\n    retry_count = 0\n    max_retries = 3\n\n    while not self.stop_event.is_set():\n        try:\n            await potentially_failing_operation()\n            retry_count = 0  # Reset on success\n\n        except asyncio.CancelledError:\n            break\n        except Exception as e:\n            retry_count += 1\n            if retry_count &gt; max_retries:\n                self.logger.error(f\"Task failed {max_retries} times, stopping\")\n                break\n\n            backoff_time = min(2 ** retry_count, 60)  # Exponential backoff\n            await asyncio.sleep(backoff_time)\n</code></pre>"},{"location":"hook-system/#4-avoid-blocking-operations","title":"4. Avoid Blocking Operations","text":"<pre><code>@aiperf_task\nasync def _file_processor(self):\n    while not self.stop_event.is_set():\n        try:\n            # Use asyncio.to_thread for blocking I/O\n            result = await asyncio.to_thread(cpu_intensive_operation)\n            await process_result(result)\n\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"hook-system/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"hook-system/#network-communication-tasks","title":"Network Communication Tasks","text":"<pre><code>@aiperf_task\nasync def _message_receiver(self):\n    \"\"\"Continuously receive messages from ZMQ socket.\"\"\"\n    while not self.stop_event.is_set():\n        try:\n            message = await self.socket.recv_string()\n            await self.handle_message(message)\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"hook-system/#periodic-maintenance-tasks","title":"Periodic Maintenance Tasks","text":"<pre><code>@aiperf_task\nasync def _cleanup_old_files(self):\n    \"\"\"Clean up old log files every hour.\"\"\"\n    while not self.stop_event.is_set():\n        try:\n            await cleanup_logs_older_than(days=7)\n            await asyncio.sleep(3600)  # Wait 1 hour\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"hook-system/#health-check-and-heartbeat-tasks","title":"Health Check and Heartbeat Tasks","text":"<pre><code>@aiperf_task\nasync def _send_heartbeat(self):\n    \"\"\"Send periodic heartbeat messages.\"\"\"\n    while not self.stop_event.is_set():\n        try:\n            heartbeat = HeartbeatMessage(\n                service_id=self.service_id,\n                timestamp=time.time(),\n                status=\"healthy\"\n            )\n            await self.publish_heartbeat(heartbeat)\n            await asyncio.sleep(self.heartbeat_interval)\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"hook-system/#task-registry-and-debugging","title":"Task Registry and Debugging","text":"<p>All <code>@aiperf_task</code> decorated methods are stored in <code>self.registered_tasks</code> with their function name as the key:</p> <pre><code># Access running tasks programmatically\nfor task_name, task in self.registered_tasks.items():\n    print(f\"Task {task_name}: {'running' if not task.done() else 'finished'}\")\n\n# Check if a specific task is running\nif '_monitor_system_health' in self.registered_tasks:\n    task = self.registered_tasks['_monitor_system_health']\n    if not task.done():\n        print(\"Health monitoring is active\")\n</code></pre>"},{"location":"hook-system/#integration-with-services","title":"Integration with Services","text":"<p>When using with AIPerf services that inherit from <code>BaseService</code>, the lifecycle is automatically managed:</p> <pre><code>from aiperf.services.base_service import BaseService\n\n\nclass MyService(BaseService):  # BaseService inherits from AIPerfTaskMixin\n    @aiperf_task\n    async def _background_processor(self):\n        while not self.stop_event.is_set():\n            await self.process_work()\n            await asyncio.sleep(1)\n\n    # Tasks will automatically start when service initializes\n    # Tasks will automatically stop when service shuts down\n</code></pre>"},{"location":"diagrams/Service%20Class%20Diagram/","title":"Service Class Diagram","text":"<pre><code>classDiagram\n    direction LR\n\n    %% Abstract classes\n    class AbstractBaseService {\n        &lt;&lt;abstract&gt;&gt;\n    }\n\n    %% Concrete base classes\n    class BaseService {\n        &lt;&lt;abstract&gt;&gt;\n    }\n\n    %% Specialized service types\n    class BaseComponentService {\n        &lt;&lt;abstract&gt;&gt;\n    }\n\n    class BaseControllerService {\n        &lt;&lt;abstract&gt;&gt;\n    }\n\n    %% Concrete service implementations\n    class SystemController {\n    }\n\n    class Worker {\n    }\n\n    class WorkerManager {\n    }\n\n    class DatasetManager {\n    }\n\n    class RecordsManager {\n    }\n\n    class TimingManager {\n    }\n\n    class PostProcessorManager {\n    }\n\n    %% Service management classes\n    class BaseServiceManager {\n        &lt;&lt;abstract&gt;&gt;\n    }\n\n    class MultiProcessServiceManager {\n    }\n\n    class KubernetesServiceManager {\n    }\n\n    %% Relationships\n    AbstractBaseService &lt;|-- BaseService\n    BaseService &lt;|-- BaseComponentService\n    BaseService &lt;|-- BaseControllerService\n    BaseService &lt;|-- Worker\n    BaseControllerService &lt;|-- SystemController\n    BaseComponentService &lt;|-- WorkerManager\n    BaseComponentService &lt;|-- DatasetManager\n    BaseComponentService &lt;|-- RecordsManager\n    BaseComponentService &lt;|-- TimingManager\n    BaseComponentService &lt;|-- PostProcessorManager\n\n\n    SystemController ..&gt; BaseServiceManager: uses\n    BaseServiceManager &lt;|-- MultiProcessServiceManager\n    BaseServiceManager &lt;|-- KubernetesServiceManager\n    WorkerManager --|&gt; Worker: spawns\n</code></pre>"},{"location":"diagrams/mixins/","title":"Mixins","text":"<pre><code>flowchart TD\n    %% Core Mixins Hierarchy\n    A[\"BaseMixin&lt;br/&gt;&lt;em&gt;Ensures proper inheritance chain&lt;/em&gt;\"] --&gt; B[\"AIPerfLoggerMixin&lt;br/&gt;&lt;em&gt;Lazy-evaluated logging with f-strings&lt;/em&gt;\"]\n    B --&gt; C[\"HooksMixin&lt;br/&gt;&lt;em&gt;Extensible hook system for behavior&lt;/em&gt;\"]\n    B --&gt; D[\"TaskManagerMixin&lt;br/&gt;&lt;em&gt;Async task and background operations&lt;/em&gt;\"]\n\n    C --&gt; E[\"AIPerfLifecycleMixin&lt;br/&gt;&lt;em&gt;Component lifecycle state management&lt;/em&gt;\"]\n    D --&gt; E\n\n    E --&gt; F[\"MessageBusClientMixin&lt;br/&gt;&lt;em&gt;Message bus communication capabilities&lt;/em&gt;\"]\n\n    %% Service Base Classes\n    F --&gt; G[\"BaseService&lt;br/&gt;&lt;em&gt;Foundation for AIPerf services&lt;/em&gt;\"]\n    G --&gt; H[\"BaseComponentService&lt;br/&gt;&lt;em&gt;Component services with status reporting&lt;/em&gt;\"]\n\n    %% Special SystemController path\n    G --&gt; I[SystemController]\n\n    %% Main Component Services\n    H --&gt; J[DatasetManager]\n    H --&gt; K[TimingManager]\n    H --&gt; L[RecordsManager]\n    H --&gt; M[InferenceResultParser]\n    H --&gt; N[WorkerManager]\n    H --&gt; O[Worker]\n\n    %% Modern styling with better colors and shapes\n    classDef baseMixin fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000000,font-weight:bold\n    classDef loggerMixin fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000000,font-weight:bold\n    classDef hooksMixin fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000000,font-weight:bold\n    classDef taskMixin fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000000,font-weight:bold\n    classDef lifecycleMixin fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000000,font-weight:bold\n    classDef messageMixin fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000000,font-weight:bold\n    classDef baseService fill:#fff8e1,stroke:#ffa000,stroke-width:2px,color:#000000,font-weight:bold\n    classDef componentService fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000000,font-weight:bold\n    classDef services fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000000\n    classDef systemController fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000000,font-weight:bold\n\n    %% Apply styles\n    class A baseMixin\n    class B loggerMixin\n    class C hooksMixin\n    class D taskMixin\n    class E lifecycleMixin\n    class F messageMixin\n    class G baseService\n    class H componentService\n    class I systemController\n    class J,K,L,M,N,O services\n</code></pre>"}]}