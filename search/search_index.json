{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-aiperf-documentation","title":"Welcome to AIPerf Documentation","text":"<p>AIPerf is a package for performance testing AI models.</p>"},{"location":"#overview","title":"Overview","text":"<ul> <li>Explore the documentation using the navigation menu.</li> <li>See the Development page for contributing and setup instructions.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies.</li> <li>Run the CLI or use the Python API.</li> </ol> <p>For details, see the rest of the documentation.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page contains the API documentation for all Python modules in the codebase (excluding init.py files).</p>"},{"location":"api/#aiperfcli","title":"aiperf.cli","text":"<p>Main CLI entry point for the AIPerf system.</p>"},{"location":"api/#aiperf.cli.profile","title":"<code>profile(user_config, service_config=None)</code>","text":"<p>Run the Profile subcommand.</p> <p>Parameters:</p> Name Type Description Default <code>user_config</code> <code>UserConfig</code> <p>User configuration for the benchmark</p> required <code>service_config</code> <code>ServiceConfig | None</code> <p>Service configuration options</p> <code>None</code> Source code in <code>aiperf/cli.py</code> <pre><code>@app.command(name=\"profile\")\ndef profile(\n    user_config: UserConfig,\n    service_config: ServiceConfig | None = None,\n) -&gt; None:\n    \"\"\"Run the Profile subcommand.\n\n    Args:\n        user_config: User configuration for the benchmark\n        service_config: Service configuration options\n    \"\"\"\n    with exit_on_error(title=\"Error Running AIPerf System\"):\n        from aiperf.cli_runner import run_system_controller\n        from aiperf.common.config import load_service_config\n\n        service_config = service_config or load_service_config()\n\n        run_system_controller(user_config, service_config)\n</code></pre>"},{"location":"api/#aiperfcli_runner","title":"aiperf.cli_runner","text":""},{"location":"api/#aiperf.cli_runner.run_system_controller","title":"<code>run_system_controller(user_config, service_config)</code>","text":"<p>Run the system controller with the given configuration.</p> Source code in <code>aiperf/cli_runner.py</code> <pre><code>def run_system_controller(\n    user_config: UserConfig,\n    service_config: ServiceConfig,\n) -&gt; None:\n    \"\"\"Run the system controller with the given configuration.\"\"\"\n\n    from aiperf.common.aiperf_logger import AIPerfLogger\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n    from aiperf.common.logging import get_global_log_queue\n    from aiperf.controller import SystemController\n    from aiperf.module_loader import ensure_modules_loaded\n\n    logger = AIPerfLogger(__name__)\n\n    log_queue = None\n    if service_config.ui_type == AIPerfUIType.DASHBOARD:\n        log_queue = get_global_log_queue()\n    else:\n        from aiperf.common.logging import setup_rich_logging\n\n        setup_rich_logging(user_config, service_config)\n\n    # Create and start the system controller\n    logger.info(\"Starting AIPerf System\")\n\n    try:\n        ensure_modules_loaded()\n    except Exception as e:\n        raise_startup_error_and_exit(\n            f\"Error loading modules: {e}\",\n            title=\"Error Loading Modules\",\n        )\n\n    try:\n        bootstrap_and_run_service(\n            SystemController,\n            service_id=\"system_controller\",\n            service_config=service_config,\n            user_config=user_config,\n            log_queue=log_queue,\n        )\n    except Exception:\n        logger.exception(\"Error running AIPerf System\")\n        raise\n    finally:\n        logger.debug(\"AIPerf System exited\")\n</code></pre>"},{"location":"api/#aiperfcli_utils","title":"aiperf.cli_utils","text":""},{"location":"api/#aiperf.cli_utils.exit_on_error","title":"<code>exit_on_error</code>","text":"<p>               Bases: <code>AbstractContextManager</code></p> <p>Context manager that exits the program if an error occurs.</p> <p>Parameters:</p> Name Type Description Default <code>*exceptions</code> <code>type[BaseException]</code> <p>The exceptions to exit on. If no exceptions are provided, all exceptions will be caught.</p> <code>()</code> <code>message</code> <code>RenderableType</code> <p>The message to display. Can be a string or a rich renderable. Will be formatted with the exception as <code>{e}</code>.</p> <code>'{e}'</code> <code>text_color</code> <code>StyleType | None</code> <p>The text color to use.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the error.</p> <code>'Error'</code> <code>exit_code</code> <code>int</code> <p>The exit code to use.</p> <code>1</code> Source code in <code>aiperf/cli_utils.py</code> <pre><code>class exit_on_error(AbstractContextManager):\n    \"\"\"Context manager that exits the program if an error occurs.\n\n    Args:\n        *exceptions: The exceptions to exit on. If no exceptions are provided, all exceptions will be caught.\n        message: The message to display. Can be a string or a rich renderable. Will be formatted with the exception as `{e}`.\n        text_color: The text color to use.\n        title: The title of the error.\n        exit_code: The exit code to use.\n    \"\"\"\n\n    def __init__(\n        self,\n        *exceptions: type[BaseException],\n        message: \"RenderableType\" = \"{e}\",\n        text_color: \"StyleType | None\" = None,\n        title: str = \"Error\",\n        exit_code: int = 1,\n    ):\n        self.message: RenderableType = message\n        self.text_color: StyleType | None = text_color\n        self.title: str = title\n        self.exit_code: int = exit_code\n        self.exceptions: tuple[type[BaseException], ...] = exceptions\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is None:\n            return\n\n        if (\n            not self.exceptions\n            and not isinstance(exc_value, (SystemExit | KeyboardInterrupt))\n        ) or issubclass(exc_type, self.exceptions):\n            from rich.console import Console\n\n            console = Console()\n            console.print_exception(\n                show_locals=True,\n                max_frames=10,\n                word_wrap=True,\n                width=console.width,\n            )\n            console.file.flush()\n            message = (\n                self.message.format(e=exc_value)\n                if isinstance(self.message, str)\n                else self.message\n            )\n            raise_startup_error_and_exit(\n                message,\n                text_color=self.text_color,\n                title=self.title,\n                exit_code=self.exit_code,\n            )\n</code></pre>"},{"location":"api/#aiperf.cli_utils.raise_startup_error_and_exit","title":"<code>raise_startup_error_and_exit(message, text_color=None, title='Error', exit_code=1, border_style='bold red', title_align='left')</code>","text":"<p>Raise a startup error and exit the program.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>RenderableType</code> <p>The message to display. Can be a string or a rich renderable.</p> required <code>text_color</code> <code>StyleType | None</code> <p>The text color to use.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the error.</p> <code>'Error'</code> <code>exit_code</code> <code>int</code> <p>The exit code to use.</p> <code>1</code> <code>border_style</code> <code>StyleType</code> <p>The border style to use.</p> <code>'bold red'</code> <code>title_align</code> <code>AlignMethod</code> <p>The alignment of the title.</p> <code>'left'</code> Source code in <code>aiperf/cli_utils.py</code> <pre><code>def raise_startup_error_and_exit(\n    message: \"RenderableType\",\n    text_color: \"StyleType | None\" = None,\n    title: str = \"Error\",\n    exit_code: int = 1,\n    border_style: \"StyleType\" = \"bold red\",\n    title_align: \"AlignMethod\" = \"left\",\n) -&gt; None:\n    \"\"\"Raise a startup error and exit the program.\n\n    Args:\n        message: The message to display. Can be a string or a rich renderable.\n        text_color: The text color to use.\n        title: The title of the error.\n        exit_code: The exit code to use.\n        border_style: The border style to use.\n        title_align: The alignment of the title.\n    \"\"\"\n    import sys\n\n    from rich.console import Console\n    from rich.panel import Panel\n\n    if isinstance(message, str):\n        message = f\"[{text_color}]{message}[/{text_color}]\" if text_color else message\n\n    console = Console()\n    console.print(\n        Panel(\n            renderable=message,\n            title=title,\n            title_align=title_align,\n            border_style=border_style,\n        )\n    )\n\n    sys.exit(exit_code)\n</code></pre>"},{"location":"api/#aiperf.cli_utils.warn_command_not_implemented","title":"<code>warn_command_not_implemented(command)</code>","text":"<p>Warn the user that the subcommand is not implemented.</p> Source code in <code>aiperf/cli_utils.py</code> <pre><code>def warn_command_not_implemented(command: str) -&gt; None:\n    \"\"\"Warn the user that the subcommand is not implemented.\"\"\"\n    raise_startup_error_and_exit(\n        f\"Command [bold red]{command}[/bold red] is not yet implemented\",\n        title=\"Not Implemented\",\n    )\n</code></pre>"},{"location":"api/#aiperfclientshttpaiohttp_client","title":"aiperf.clients.http.aiohttp_client","text":""},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin","title":"<code>AioHttpClientMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>A high-performance HTTP client for communicating with HTTP based REST APIs using aiohttp.</p> <p>This class is optimized for maximum performance and accurate timing measurements, making it ideal for benchmarking scenarios.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>class AioHttpClientMixin(AIPerfLoggerMixin):\n    \"\"\"A high-performance HTTP client for communicating with HTTP based REST APIs using aiohttp.\n\n    This class is optimized for maximum performance and accurate timing measurements,\n    making it ideal for benchmarking scenarios.\n    \"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfo, **kwargs) -&gt; None:\n        self.model_endpoint = model_endpoint\n        super().__init__(model_endpoint=model_endpoint, **kwargs)\n        self.tcp_connector = create_tcp_connector()\n\n        # For now, just set all timeouts to the same value.\n        # TODO: Add support for different timeouts for different parts of the request.\n        self.timeout = aiohttp.ClientTimeout(\n            total=self.model_endpoint.endpoint.timeout,\n            connect=self.model_endpoint.endpoint.timeout,\n            sock_connect=self.model_endpoint.endpoint.timeout,\n            sock_read=self.model_endpoint.endpoint.timeout,\n            ceil_threshold=self.model_endpoint.endpoint.timeout,\n        )\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the client.\"\"\"\n        if self.tcp_connector:\n            await self.tcp_connector.close()\n            self.tcp_connector = None\n\n    async def _request(\n        self,\n        method: str,\n        url: str,\n        headers: dict[str, str],\n        data: str | None = None,\n        **kwargs: Any,\n    ) -&gt; RequestRecord:\n        \"\"\"Generic request method that handles common logic for all HTTP methods.\n\n        Args:\n            method: HTTP method (GET, POST, etc.)\n            url: The URL to send the request to\n            headers: Request headers\n            data: Request payload (for POST, PUT, etc.)\n            **kwargs: Additional arguments to pass to the request\n\n        Returns:\n            RequestRecord with the response data\n        \"\"\"\n        self.debug(lambda: f\"Sending {method} request to {url}\")\n\n        record: RequestRecord = RequestRecord(\n            start_perf_ns=time.perf_counter_ns(),\n        )\n\n        try:\n            # Make raw HTTP request with precise timing using aiohttp\n            async with aiohttp.ClientSession(\n                connector=self.tcp_connector,\n                timeout=self.timeout,\n                headers=headers,\n                skip_auto_headers=[\n                    *list(headers.keys()),\n                    \"User-Agent\",\n                    \"Accept-Encoding\",\n                ],\n                connector_owner=False,\n            ) as session:\n                record.start_perf_ns = time.perf_counter_ns()\n                async with session.request(\n                    method, url, data=data, headers=headers, **kwargs\n                ) as response:\n                    record.status = response.status\n                    # Check for HTTP errors\n                    if response.status != 200:\n                        error_text = await response.text()\n                        record.error = ErrorDetails(\n                            code=response.status,\n                            type=response.reason,\n                            message=error_text,\n                        )\n                        return record\n\n                    record.recv_start_perf_ns = time.perf_counter_ns()\n\n                    if (\n                        method == \"POST\"\n                        and response.content_type == \"text/event-stream\"\n                    ):\n                        # Parse SSE stream with optimal performance\n                        messages = await AioHttpSSEStreamReader(\n                            response\n                        ).read_complete_stream()\n                        record.responses.extend(messages)\n                    else:\n                        raw_response = await response.text()\n                        record.end_perf_ns = time.perf_counter_ns()\n                        record.responses.append(\n                            TextResponse(\n                                perf_ns=record.end_perf_ns,\n                                content_type=response.content_type,\n                                text=raw_response,\n                            )\n                        )\n                    record.end_perf_ns = time.perf_counter_ns()\n\n        except Exception as e:\n            record.end_perf_ns = time.perf_counter_ns()\n            self.error(f\"Error in aiohttp request: {e!r}\")\n            record.error = ErrorDetails(type=e.__class__.__name__, message=str(e))\n\n        return record\n\n    async def post_request(\n        self,\n        url: str,\n        payload: str,\n        headers: dict[str, str],\n        **kwargs: Any,\n    ) -&gt; RequestRecord:\n        \"\"\"Send a streaming or non-streaming POST request to the specified URL with the given payload and headers.\n\n        If the response is an SSE stream, the response will be parsed into a list of SSE messages.\n        Otherwise, the response will be parsed into a TextResponse object.\n        \"\"\"\n        return await self._request(\"POST\", url, headers, data=payload, **kwargs)\n\n    async def get_request(\n        self, url: str, headers: dict[str, str], **kwargs: Any\n    ) -&gt; RequestRecord:\n        \"\"\"Send a GET request to the specified URL with the given headers.\n\n        The response will be parsed into a TextResponse object.\n        \"\"\"\n        return await self._request(\"GET\", url, headers, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the client.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the client.\"\"\"\n    if self.tcp_connector:\n        await self.tcp_connector.close()\n        self.tcp_connector = None\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin.get_request","title":"<code>get_request(url, headers, **kwargs)</code>  <code>async</code>","text":"<p>Send a GET request to the specified URL with the given headers.</p> <p>The response will be parsed into a TextResponse object.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def get_request(\n    self, url: str, headers: dict[str, str], **kwargs: Any\n) -&gt; RequestRecord:\n    \"\"\"Send a GET request to the specified URL with the given headers.\n\n    The response will be parsed into a TextResponse object.\n    \"\"\"\n    return await self._request(\"GET\", url, headers, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpClientMixin.post_request","title":"<code>post_request(url, payload, headers, **kwargs)</code>  <code>async</code>","text":"<p>Send a streaming or non-streaming POST request to the specified URL with the given payload and headers.</p> <p>If the response is an SSE stream, the response will be parsed into a list of SSE messages. Otherwise, the response will be parsed into a TextResponse object.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def post_request(\n    self,\n    url: str,\n    payload: str,\n    headers: dict[str, str],\n    **kwargs: Any,\n) -&gt; RequestRecord:\n    \"\"\"Send a streaming or non-streaming POST request to the specified URL with the given payload and headers.\n\n    If the response is an SSE stream, the response will be parsed into a list of SSE messages.\n    Otherwise, the response will be parsed into a TextResponse object.\n    \"\"\"\n    return await self._request(\"POST\", url, headers, data=payload, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpSSEStreamReader","title":"<code>AioHttpSSEStreamReader</code>","text":"<p>A helper class for reading an SSE stream from an aiohttp.ClientResponse object.</p> <p>This class is optimized for maximum performance and accurate timing measurements, making it ideal for benchmarking scenarios.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>class AioHttpSSEStreamReader:\n    \"\"\"A helper class for reading an SSE stream from an aiohttp.ClientResponse object.\n\n    This class is optimized for maximum performance and accurate timing measurements,\n    making it ideal for benchmarking scenarios.\n    \"\"\"\n\n    def __init__(self, response: aiohttp.ClientResponse):\n        self.response = response\n\n    async def read_complete_stream(self) -&gt; list[SSEMessage]:\n        \"\"\"Read the complete SSE stream in a performant manner and return a list of\n        SSE messages that contain the most accurate timestamp data possible.\n\n        Returns:\n            A list of SSE messages.\n        \"\"\"\n        messages: list[SSEMessage] = []\n\n        async for raw_message, first_byte_ns in self.__aiter__():\n            # Parse the raw SSE message into a SSEMessage object\n            message = parse_sse_message(raw_message, first_byte_ns)\n            messages.append(message)\n\n        return messages\n\n    async def __aiter__(self) -&gt; typing.AsyncIterator[tuple[str, int]]:\n        \"\"\"Iterate over the SSE stream in a performant manner and return a tuple of the\n        raw SSE message, the perf_counter_ns of the first byte, and the perf_counter_ns of the last byte.\n        This provides the most accurate timing information possible without any delays due to the nature of\n        the aiohttp library. The first byte is read immediately to capture the timestamp of the first byte,\n        and the last byte is read after the rest of the chunk is read to capture the timestamp of the last byte.\n\n        Returns:\n            An async iterator of tuples of the raw SSE message, and the perf_counter_ns of the first byte\n        \"\"\"\n\n        while not self.response.content.at_eof():\n            # Read the first byte of the SSE stream\n            first_byte = await self.response.content.read(1)\n            chunk_ns_first_byte = time.perf_counter_ns()\n            if not first_byte:\n                break\n\n            chunk = await self.response.content.readuntil(b\"\\n\\n\")\n\n            if not chunk:\n                break\n            chunk = first_byte + chunk\n\n            try:\n                # Use the fastest available decoder\n                yield (\n                    chunk.decode(\"utf-8\").strip(),\n                    chunk_ns_first_byte,\n                )\n            except UnicodeDecodeError:\n                # Handle potential encoding issues gracefully\n                yield (\n                    chunk.decode(\"utf-8\", errors=\"replace\").strip(),\n                    chunk_ns_first_byte,\n                )\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpSSEStreamReader.__aiter__","title":"<code>__aiter__()</code>  <code>async</code>","text":"<p>Iterate over the SSE stream in a performant manner and return a tuple of the raw SSE message, the perf_counter_ns of the first byte, and the perf_counter_ns of the last byte. This provides the most accurate timing information possible without any delays due to the nature of the aiohttp library. The first byte is read immediately to capture the timestamp of the first byte, and the last byte is read after the rest of the chunk is read to capture the timestamp of the last byte.</p> <p>Returns:</p> Type Description <code>AsyncIterator[tuple[str, int]]</code> <p>An async iterator of tuples of the raw SSE message, and the perf_counter_ns of the first byte</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def __aiter__(self) -&gt; typing.AsyncIterator[tuple[str, int]]:\n    \"\"\"Iterate over the SSE stream in a performant manner and return a tuple of the\n    raw SSE message, the perf_counter_ns of the first byte, and the perf_counter_ns of the last byte.\n    This provides the most accurate timing information possible without any delays due to the nature of\n    the aiohttp library. The first byte is read immediately to capture the timestamp of the first byte,\n    and the last byte is read after the rest of the chunk is read to capture the timestamp of the last byte.\n\n    Returns:\n        An async iterator of tuples of the raw SSE message, and the perf_counter_ns of the first byte\n    \"\"\"\n\n    while not self.response.content.at_eof():\n        # Read the first byte of the SSE stream\n        first_byte = await self.response.content.read(1)\n        chunk_ns_first_byte = time.perf_counter_ns()\n        if not first_byte:\n            break\n\n        chunk = await self.response.content.readuntil(b\"\\n\\n\")\n\n        if not chunk:\n            break\n        chunk = first_byte + chunk\n\n        try:\n            # Use the fastest available decoder\n            yield (\n                chunk.decode(\"utf-8\").strip(),\n                chunk_ns_first_byte,\n            )\n        except UnicodeDecodeError:\n            # Handle potential encoding issues gracefully\n            yield (\n                chunk.decode(\"utf-8\", errors=\"replace\").strip(),\n                chunk_ns_first_byte,\n            )\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.AioHttpSSEStreamReader.read_complete_stream","title":"<code>read_complete_stream()</code>  <code>async</code>","text":"<p>Read the complete SSE stream in a performant manner and return a list of SSE messages that contain the most accurate timestamp data possible.</p> <p>Returns:</p> Type Description <code>list[SSEMessage]</code> <p>A list of SSE messages.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>async def read_complete_stream(self) -&gt; list[SSEMessage]:\n    \"\"\"Read the complete SSE stream in a performant manner and return a list of\n    SSE messages that contain the most accurate timestamp data possible.\n\n    Returns:\n        A list of SSE messages.\n    \"\"\"\n    messages: list[SSEMessage] = []\n\n    async for raw_message, first_byte_ns in self.__aiter__():\n        # Parse the raw SSE message into a SSEMessage object\n        message = parse_sse_message(raw_message, first_byte_ns)\n        messages.append(message)\n\n    return messages\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.create_tcp_connector","title":"<code>create_tcp_connector(**kwargs)</code>","text":"<p>Create a new connector with the given configuration.</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>def create_tcp_connector(**kwargs) -&gt; aiohttp.TCPConnector:\n    \"\"\"Create a new connector with the given configuration.\"\"\"\n\n    def socket_factory(addr_info):\n        \"\"\"Custom socket factory optimized for SSE streaming performance.\"\"\"\n        family, sock_type, proto, _, _ = addr_info\n        sock = socket.socket(family=family, type=sock_type, proto=proto)\n        SocketDefaults.apply_to_socket(sock)\n        return sock\n\n    default_kwargs: dict[str, Any] = {\n        \"limit\": AioHttpDefaults.LIMIT,\n        \"limit_per_host\": AioHttpDefaults.LIMIT_PER_HOST,\n        \"ttl_dns_cache\": AioHttpDefaults.TTL_DNS_CACHE,\n        \"use_dns_cache\": AioHttpDefaults.USE_DNS_CACHE,\n        \"enable_cleanup_closed\": AioHttpDefaults.ENABLE_CLEANUP_CLOSED,\n        \"force_close\": AioHttpDefaults.FORCE_CLOSE,\n        \"keepalive_timeout\": AioHttpDefaults.KEEPALIVE_TIMEOUT,\n        \"happy_eyeballs_delay\": AioHttpDefaults.HAPPY_EYEBALLS_DELAY,\n        \"family\": AioHttpDefaults.SOCKET_FAMILY,\n        \"socket_factory\": socket_factory,\n    }\n\n    default_kwargs.update(kwargs)\n\n    return aiohttp.TCPConnector(\n        **default_kwargs,\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.http.aiohttp_client.parse_sse_message","title":"<code>parse_sse_message(raw_message, perf_ns)</code>","text":"<p>Parse a raw SSE message into an SSEMessage object.</p> <p>Parsing logic based on official HTML SSE Living Standard: https://html.spec.whatwg.org/multipage/server-sent-events.html#parsing-an-event-stream</p> Source code in <code>aiperf/clients/http/aiohttp_client.py</code> <pre><code>def parse_sse_message(raw_message: str, perf_ns: int) -&gt; SSEMessage:\n    \"\"\"Parse a raw SSE message into an SSEMessage object.\n\n    Parsing logic based on official HTML SSE Living Standard:\n    https://html.spec.whatwg.org/multipage/server-sent-events.html#parsing-an-event-stream\n    \"\"\"\n\n    message = SSEMessage(perf_ns=perf_ns)\n    for line in raw_message.split(\"\\n\"):\n        if not (line := line.strip()):\n            continue\n\n        parts = line.split(\":\", 1)\n        if len(parts) &lt; 2:\n            # Fields without a colon have no value, so the whole line is the field name\n            message.packets.append(SSEField(name=parts[0].strip(), value=None))\n            continue\n\n        field_name, value = parts\n\n        if field_name == \"\":\n            # Field name is empty, so this is a comment\n            field_name = SSEFieldType.COMMENT\n\n        message.packets.append(SSEField(name=field_name.strip(), value=value.strip()))\n\n    return message\n</code></pre>"},{"location":"api/#aiperfclientshttpdefaults","title":"aiperf.clients.http.defaults","text":""},{"location":"api/#aiperf.clients.http.defaults.AioHttpDefaults","title":"<code>AioHttpDefaults</code>  <code>dataclass</code>","text":"<p>Default values for aiohttp.ClientSession.</p> Source code in <code>aiperf/clients/http/defaults.py</code> <pre><code>@dataclass(frozen=True)\nclass AioHttpDefaults:\n    \"\"\"Default values for aiohttp.ClientSession.\"\"\"\n\n    LIMIT = 2500  # Maximum number of concurrent connections\n    LIMIT_PER_HOST = 2500  # Maximum number of concurrent connections per host\n    TTL_DNS_CACHE = 300  # Time to live for DNS cache\n    USE_DNS_CACHE = True  # Enable DNS cache\n    ENABLE_CLEANUP_CLOSED = False  # Disable cleanup of closed connections\n    FORCE_CLOSE = False  # Disable force close connections\n    KEEPALIVE_TIMEOUT = 300  # Keepalive timeout\n    HAPPY_EYEBALLS_DELAY = None  # Happy eyeballs delay (None = disabled)\n    SOCKET_FAMILY = socket.AF_INET  # Family of the socket (IPv4)\n</code></pre>"},{"location":"api/#aiperf.clients.http.defaults.SocketDefaults","title":"<code>SocketDefaults</code>  <code>dataclass</code>","text":"<p>Default values for socket options.</p> Source code in <code>aiperf/clients/http/defaults.py</code> <pre><code>@dataclass(frozen=True)\nclass SocketDefaults:\n    \"\"\"\n    Default values for socket options.\n    \"\"\"\n\n    TCP_NODELAY = 1  # Disable Nagle's algorithm\n    TCP_QUICKACK = 1  # Quick ACK mode\n\n    SO_KEEPALIVE = 1  # Enable keepalive\n    TCP_KEEPIDLE = 60  # Start keepalive after 1 min idle\n    TCP_KEEPINTVL = 30  # Keepalive interval: 30 seconds\n    TCP_KEEPCNT = 1  # 1 failed keepalive probes = dead\n\n    SO_LINGER = 0  # Disable linger\n    SO_REUSEADDR = 1  # Enable reuse address\n    SO_REUSEPORT = 1  # Enable reuse port\n\n    SO_RCVBUF = 1024 * 1024 * 10  # 10MB receive buffer\n    SO_SNDBUF = 1024 * 1024 * 10  # 10MB send buffer\n\n    SO_RCVTIMEO = 30  # 30 second receive timeout\n    SO_SNDTIMEO = 30  # 30 second send timeout\n    TCP_USER_TIMEOUT = 30000  # 30 sec user timeout\n\n    @classmethod\n    def apply_to_socket(cls, sock: socket.socket) -&gt; None:\n        \"\"\"Apply the default socket options to the given socket.\"\"\"\n\n        # Low-latency optimizations for streaming\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, cls.TCP_NODELAY)\n\n        # Connection keepalive settings for long-lived SSE connections\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, cls.SO_KEEPALIVE)\n\n        # Fine-tune keepalive timing (Linux-specific)\n        if hasattr(socket, \"TCP_KEEPIDLE\"):\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPIDLE, cls.TCP_KEEPIDLE)\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPINTVL, cls.TCP_KEEPINTVL)\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPCNT, cls.TCP_KEEPCNT)\n\n        # Buffer size optimizations for streaming\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, cls.SO_RCVBUF)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, cls.SO_SNDBUF)\n\n        # Linux-specific TCP optimizations\n        if hasattr(socket, \"TCP_QUICKACK\"):\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_QUICKACK, cls.TCP_QUICKACK)\n\n        if hasattr(socket, \"TCP_USER_TIMEOUT\"):\n            sock.setsockopt(\n                socket.SOL_TCP, socket.TCP_USER_TIMEOUT, cls.TCP_USER_TIMEOUT\n            )\n</code></pre>"},{"location":"api/#aiperf.clients.http.defaults.SocketDefaults.apply_to_socket","title":"<code>apply_to_socket(sock)</code>  <code>classmethod</code>","text":"<p>Apply the default socket options to the given socket.</p> Source code in <code>aiperf/clients/http/defaults.py</code> <pre><code>@classmethod\ndef apply_to_socket(cls, sock: socket.socket) -&gt; None:\n    \"\"\"Apply the default socket options to the given socket.\"\"\"\n\n    # Low-latency optimizations for streaming\n    sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, cls.TCP_NODELAY)\n\n    # Connection keepalive settings for long-lived SSE connections\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, cls.SO_KEEPALIVE)\n\n    # Fine-tune keepalive timing (Linux-specific)\n    if hasattr(socket, \"TCP_KEEPIDLE\"):\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPIDLE, cls.TCP_KEEPIDLE)\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPINTVL, cls.TCP_KEEPINTVL)\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_KEEPCNT, cls.TCP_KEEPCNT)\n\n    # Buffer size optimizations for streaming\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, cls.SO_RCVBUF)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, cls.SO_SNDBUF)\n\n    # Linux-specific TCP optimizations\n    if hasattr(socket, \"TCP_QUICKACK\"):\n        sock.setsockopt(socket.SOL_TCP, socket.TCP_QUICKACK, cls.TCP_QUICKACK)\n\n    if hasattr(socket, \"TCP_USER_TIMEOUT\"):\n        sock.setsockopt(\n            socket.SOL_TCP, socket.TCP_USER_TIMEOUT, cls.TCP_USER_TIMEOUT\n        )\n</code></pre>"},{"location":"api/#aiperfclientsmodel_endpoint_info","title":"aiperf.clients.model_endpoint_info","text":"<p>Model endpoint information.</p> <p>This module contains the pydantic models that encapsulate the information needed to send requests to an inference server, primarily around the model, endpoint, and additional request payload information.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.EndpointInfo","title":"<code>EndpointInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about an endpoint.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class EndpointInfo(AIPerfBaseModel):\n    \"\"\"Information about an endpoint.\"\"\"\n\n    type: EndpointType = Field(\n        default=EndpointType.OPENAI_CHAT_COMPLETIONS,\n        description=\"The type of request payload to use for the endpoint.\",\n    )\n    base_url: str | None = Field(\n        default=None,\n        description=\"URL of the endpoint.\",\n    )\n    custom_endpoint: str | None = Field(\n        default=None,\n        description=\"Custom endpoint to use for the models.\",\n    )\n    url_params: dict[str, Any] | None = Field(\n        default=None, description=\"Custom URL parameters to use for the endpoint.\"\n    )\n    streaming: bool = Field(\n        default=False,\n        description=\"Whether the endpoint supports streaming.\",\n    )\n    headers: list[tuple[str, str]] | None = Field(\n        default=None,\n        description=\"Custom URL headers to use for the endpoint.\",\n    )\n    api_key: str | None = Field(\n        default=None,\n        description=\"API key to use for the endpoint.\",\n    )\n    ssl_options: dict[str, Any] | None = Field(\n        default=None,\n        description=\"SSL options to use for the endpoint.\",\n    )\n    timeout: float = Field(\n        default=EndpointDefaults.TIMEOUT,\n        description=\"The timeout in seconds for each request to the endpoint.\",\n    )\n    extra: list[tuple[str, Any]] | None = Field(\n        default=None,\n        description=\"Additional inputs to include with every request. \"\n        \"You can repeat this flag for multiple inputs. Inputs should be in an 'input_name:value' format. \"\n        \"Alternatively, a string representing a json formatted dict can be provided.\",\n    )\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"EndpointInfo\":\n        \"\"\"Create an HttpEndpointInfo from a UserConfig.\"\"\"\n        return cls(\n            type=EndpointType(user_config.endpoint.type),\n            custom_endpoint=user_config.endpoint.custom_endpoint,\n            streaming=user_config.endpoint.streaming,\n            base_url=user_config.endpoint.url,\n            headers=user_config.input.headers,\n            extra=user_config.input.extra,\n            timeout=user_config.endpoint.timeout_seconds,\n            api_key=user_config.endpoint.api_key,\n        )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.EndpointInfo.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create an HttpEndpointInfo from a UserConfig.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"EndpointInfo\":\n    \"\"\"Create an HttpEndpointInfo from a UserConfig.\"\"\"\n    return cls(\n        type=EndpointType(user_config.endpoint.type),\n        custom_endpoint=user_config.endpoint.custom_endpoint,\n        streaming=user_config.endpoint.streaming,\n        base_url=user_config.endpoint.url,\n        headers=user_config.input.headers,\n        extra=user_config.input.extra,\n        timeout=user_config.endpoint.timeout_seconds,\n        api_key=user_config.endpoint.api_key,\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo","title":"<code>ModelEndpointInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a model endpoint.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class ModelEndpointInfo(AIPerfBaseModel):\n    \"\"\"Information about a model endpoint.\"\"\"\n\n    models: ModelListInfo = Field(\n        ...,\n        description=\"The models to use for the endpoint.\",\n    )\n    endpoint: EndpointInfo = Field(\n        ...,\n        description=\"The endpoint to use for the models.\",\n    )\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"ModelEndpointInfo\":\n        \"\"\"Create a ModelEndpointInfo from a UserConfig.\"\"\"\n        return cls(\n            models=ModelListInfo.from_user_config(user_config),\n            endpoint=EndpointInfo.from_user_config(user_config),\n        )\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"Get the full URL for the endpoint.\"\"\"\n        url = self.endpoint.base_url.rstrip(\"/\") if self.endpoint.base_url else \"\"\n        if self.endpoint.custom_endpoint:\n            url += \"/\" + self.endpoint.custom_endpoint.lstrip(\"/\")\n        elif path := self.endpoint.type.endpoint_path:\n            url += \"/\" + path.lstrip(\"/\")\n        return url\n\n    @property\n    def primary_model(self) -&gt; ModelInfo:\n        \"\"\"Get the primary model.\"\"\"\n        return self.models.models[0]\n\n    @property\n    def primary_model_name(self) -&gt; str:\n        \"\"\"Get the primary model name.\"\"\"\n        return self.primary_model.name\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.primary_model","title":"<code>primary_model</code>  <code>property</code>","text":"<p>Get the primary model.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.primary_model_name","title":"<code>primary_model_name</code>  <code>property</code>","text":"<p>Get the primary model name.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.url","title":"<code>url</code>  <code>property</code>","text":"<p>Get the full URL for the endpoint.</p>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelEndpointInfo.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create a ModelEndpointInfo from a UserConfig.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"ModelEndpointInfo\":\n    \"\"\"Create a ModelEndpointInfo from a UserConfig.\"\"\"\n    return cls(\n        models=ModelListInfo.from_user_config(user_config),\n        endpoint=EndpointInfo.from_user_config(user_config),\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelInfo","title":"<code>ModelInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a model.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class ModelInfo(AIPerfBaseModel):\n    \"\"\"Information about a model.\"\"\"\n\n    name: str = Field(\n        ...,\n        min_length=1,\n        description=\"The name of the model. This is used to identify the model.\",\n    )\n    version: str | None = Field(\n        default=None,\n        description=\"The version of the model.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelListInfo","title":"<code>ModelListInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Information about a list of models.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>class ModelListInfo(AIPerfBaseModel):\n    \"\"\"Information about a list of models.\"\"\"\n\n    models: list[ModelInfo] = Field(\n        ...,\n        min_length=1,\n        description=\"The models to use for the endpoint.\",\n    )\n    model_selection_strategy: ModelSelectionStrategy = Field(\n        ...,\n        description=\"The strategy to use for selecting the model to use for the endpoint.\",\n    )\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"ModelListInfo\":\n        \"\"\"Create a ModelListInfo from a UserConfig.\"\"\"\n        return cls(\n            models=[\n                ModelInfo(name=model) for model in user_config.endpoint.model_names\n            ],\n            model_selection_strategy=user_config.endpoint.model_selection_strategy,\n        )\n</code></pre>"},{"location":"api/#aiperf.clients.model_endpoint_info.ModelListInfo.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create a ModelListInfo from a UserConfig.</p> Source code in <code>aiperf/clients/model_endpoint_info.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"ModelListInfo\":\n    \"\"\"Create a ModelListInfo from a UserConfig.\"\"\"\n    return cls(\n        models=[\n            ModelInfo(name=model) for model in user_config.endpoint.model_names\n        ],\n        model_selection_strategy=user_config.endpoint.model_selection_strategy,\n    )\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_aiohttp","title":"aiperf.clients.openai.openai_aiohttp","text":""},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp","title":"<code>OpenAIClientAioHttp</code>","text":"<p>               Bases: <code>AioHttpClientMixin</code>, <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> <p>Inference client for OpenAI based requests using aiohttp.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>@InferenceClientFactory.register_all(\n    EndpointType.OPENAI_CHAT_COMPLETIONS,\n    EndpointType.OPENAI_COMPLETIONS,\n    EndpointType.OPENAI_EMBEDDINGS,\n    EndpointType.RANKINGS,\n    EndpointType.OPENAI_RESPONSES,\n)\nclass OpenAIClientAioHttp(AioHttpClientMixin, AIPerfLoggerMixin, ABC):\n    \"\"\"Inference client for OpenAI based requests using aiohttp.\"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfo, **kwargs) -&gt; None:\n        super().__init__(model_endpoint, **kwargs)\n        self.model_endpoint = model_endpoint\n\n    def get_headers(self, model_endpoint: ModelEndpointInfo) -&gt; dict[str, str]:\n        \"\"\"Get the headers for the given endpoint.\"\"\"\n\n        accept = (\n            \"text/event-stream\"\n            if model_endpoint.endpoint.streaming\n            else \"application/json\"\n        )\n\n        headers = {\n            \"User-Agent\": \"aiperf/1.0\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": accept,\n        }\n        if model_endpoint.endpoint.api_key:\n            headers[\"Authorization\"] = f\"Bearer {model_endpoint.endpoint.api_key}\"\n        if model_endpoint.endpoint.headers:\n            headers.update(model_endpoint.endpoint.headers)\n        return headers\n\n    def get_url(self, model_endpoint: ModelEndpointInfo) -&gt; str:\n        \"\"\"Get the URL for the given endpoint.\"\"\"\n        url = model_endpoint.url\n        if not url.startswith(\"http\"):\n            url = f\"http://{url}\"\n        return url\n\n    async def send_request(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        payload: dict[str, Any],\n    ) -&gt; RequestRecord:\n        \"\"\"Send OpenAI request using aiohttp.\"\"\"\n\n        # capture start time before request is sent in the case of an error\n        start_perf_ns = time.perf_counter_ns()\n        try:\n            self.debug(\n                lambda: f\"Sending OpenAI request to {model_endpoint.url}, payload: {payload}\"\n            )\n\n            record = await self.post_request(\n                self.get_url(model_endpoint),\n                json.dumps(payload),\n                self.get_headers(model_endpoint),\n            )\n\n        except Exception as e:\n            record = RequestRecord(\n                start_perf_ns=start_perf_ns,\n                end_perf_ns=time.perf_counter_ns(),\n                error=ErrorDetails(type=e.__class__.__name__, message=str(e)),\n            )\n            self.exception(f\"Error in OpenAI request: {e.__class__.__name__} {str(e)}\")\n\n        return record\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp.get_headers","title":"<code>get_headers(model_endpoint)</code>","text":"<p>Get the headers for the given endpoint.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>def get_headers(self, model_endpoint: ModelEndpointInfo) -&gt; dict[str, str]:\n    \"\"\"Get the headers for the given endpoint.\"\"\"\n\n    accept = (\n        \"text/event-stream\"\n        if model_endpoint.endpoint.streaming\n        else \"application/json\"\n    )\n\n    headers = {\n        \"User-Agent\": \"aiperf/1.0\",\n        \"Content-Type\": \"application/json\",\n        \"Accept\": accept,\n    }\n    if model_endpoint.endpoint.api_key:\n        headers[\"Authorization\"] = f\"Bearer {model_endpoint.endpoint.api_key}\"\n    if model_endpoint.endpoint.headers:\n        headers.update(model_endpoint.endpoint.headers)\n    return headers\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp.get_url","title":"<code>get_url(model_endpoint)</code>","text":"<p>Get the URL for the given endpoint.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>def get_url(self, model_endpoint: ModelEndpointInfo) -&gt; str:\n    \"\"\"Get the URL for the given endpoint.\"\"\"\n    url = model_endpoint.url\n    if not url.startswith(\"http\"):\n        url = f\"http://{url}\"\n    return url\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_aiohttp.OpenAIClientAioHttp.send_request","title":"<code>send_request(model_endpoint, payload)</code>  <code>async</code>","text":"<p>Send OpenAI request using aiohttp.</p> Source code in <code>aiperf/clients/openai/openai_aiohttp.py</code> <pre><code>async def send_request(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    payload: dict[str, Any],\n) -&gt; RequestRecord:\n    \"\"\"Send OpenAI request using aiohttp.\"\"\"\n\n    # capture start time before request is sent in the case of an error\n    start_perf_ns = time.perf_counter_ns()\n    try:\n        self.debug(\n            lambda: f\"Sending OpenAI request to {model_endpoint.url}, payload: {payload}\"\n        )\n\n        record = await self.post_request(\n            self.get_url(model_endpoint),\n            json.dumps(payload),\n            self.get_headers(model_endpoint),\n        )\n\n    except Exception as e:\n        record = RequestRecord(\n            start_perf_ns=start_perf_ns,\n            end_perf_ns=time.perf_counter_ns(),\n            error=ErrorDetails(type=e.__class__.__name__, message=str(e)),\n        )\n        self.exception(f\"Error in OpenAI request: {e.__class__.__name__} {str(e)}\")\n\n    return record\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_chat","title":"aiperf.clients.openai.openai_chat","text":""},{"location":"api/#aiperf.clients.openai.openai_chat.OpenAIChatCompletionRequestConverter","title":"<code>OpenAIChatCompletionRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI chat completion requests.</p> Source code in <code>aiperf/clients/openai/openai_chat.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_CHAT_COMPLETIONS)\nclass OpenAIChatCompletionRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI chat completion requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a chat completion request.\"\"\"\n\n        messages = self._create_messages(turn)\n\n        payload = {\n            \"messages\": messages,\n            \"model\": turn.model or model_endpoint.primary_model_name,\n            \"stream\": model_endpoint.endpoint.streaming,\n        }\n\n        if turn.max_tokens is not None:\n            payload[\"max_completion_tokens\"] = turn.max_tokens\n\n        if model_endpoint.endpoint.extra:\n            payload.update(model_endpoint.endpoint.extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n\n    def _create_messages(self, turn: Turn) -&gt; list[dict[str, Any]]:\n        message = {\n            \"role\": turn.role or DEFAULT_ROLE,\n        }\n\n        if (\n            len(turn.texts) == 1\n            and len(turn.texts[0].contents) == 1\n            and len(turn.images) == 0\n            and len(turn.audios) == 0\n        ):\n            # Hotfix for Dynamo API which does not yet support a list of messages\n            message[\"name\"] = turn.texts[0].name\n            message[\"content\"] = (\n                turn.texts[0].contents[0] if turn.texts[0].contents else \"\"\n            )\n            return [message]\n\n        message_content = []\n\n        for text in turn.texts:\n            for content in text.contents:\n                if not content:\n                    continue\n                message_content.append({\"type\": \"text\", \"text\": content})\n\n        for image in turn.images:\n            for content in image.contents:\n                if not content:\n                    continue\n                message_content.append(\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": content}}\n                )\n\n        for audio in turn.audios:\n            for content in audio.contents:\n                if not content:\n                    continue\n                if \",\" not in content:\n                    raise ValueError(\n                        \"Audio content must be in the format 'format,b64_audio'.\"\n                    )\n                format, b64_audio = content.split(\",\", 1)\n                message_content.append(\n                    {\n                        \"type\": \"input_audio\",\n                        \"input_audio\": {\n                            \"data\": b64_audio,\n                            \"format\": format,\n                        },\n                    }\n                )\n\n        message[\"content\"] = message_content\n\n        return [message]\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_chat.OpenAIChatCompletionRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a chat completion request.</p> Source code in <code>aiperf/clients/openai/openai_chat.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a chat completion request.\"\"\"\n\n    messages = self._create_messages(turn)\n\n    payload = {\n        \"messages\": messages,\n        \"model\": turn.model or model_endpoint.primary_model_name,\n        \"stream\": model_endpoint.endpoint.streaming,\n    }\n\n    if turn.max_tokens is not None:\n        payload[\"max_completion_tokens\"] = turn.max_tokens\n\n    if model_endpoint.endpoint.extra:\n        payload.update(model_endpoint.endpoint.extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_completions","title":"aiperf.clients.openai.openai_completions","text":""},{"location":"api/#aiperf.clients.openai.openai_completions.OpenAICompletionRequestConverter","title":"<code>OpenAICompletionRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI completion requests.</p> Source code in <code>aiperf/clients/openai/openai_completions.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_COMPLETIONS)\nclass OpenAICompletionRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI completion requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a completion request.\"\"\"\n\n        prompts = [\n            content for text in turn.texts for content in text.contents if content\n        ]\n\n        extra = model_endpoint.endpoint.extra or []\n\n        payload = {\n            \"prompt\": prompts,\n            \"model\": turn.model or model_endpoint.primary_model_name,\n            \"stream\": model_endpoint.endpoint.streaming,\n        }\n\n        if turn.max_tokens:\n            payload[\"max_tokens\"] = turn.max_tokens\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_completions.OpenAICompletionRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a completion request.</p> Source code in <code>aiperf/clients/openai/openai_completions.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a completion request.\"\"\"\n\n    prompts = [\n        content for text in turn.texts for content in text.contents if content\n    ]\n\n    extra = model_endpoint.endpoint.extra or []\n\n    payload = {\n        \"prompt\": prompts,\n        \"model\": turn.model or model_endpoint.primary_model_name,\n        \"stream\": model_endpoint.endpoint.streaming,\n    }\n\n    if turn.max_tokens:\n        payload[\"max_tokens\"] = turn.max_tokens\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_embeddings","title":"aiperf.clients.openai.openai_embeddings","text":""},{"location":"api/#aiperf.clients.openai.openai_embeddings.OpenAIEmbeddingsRequestConverter","title":"<code>OpenAIEmbeddingsRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI embeddings requests.</p> Source code in <code>aiperf/clients/openai/openai_embeddings.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_EMBEDDINGS)\nclass OpenAIEmbeddingsRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI embeddings requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for an embeddings request.\"\"\"\n\n        if turn.max_tokens:\n            self.error(\"Max_tokens is provided but is not supported for embeddings.\")\n\n        prompts = [\n            content for text in turn.texts for content in text.contents if content\n        ]\n\n        extra = model_endpoint.endpoint.extra or []\n\n        payload = {\n            \"model\": turn.model or model_endpoint.primary_model_name,\n            \"input\": prompts,\n        }\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_embeddings.OpenAIEmbeddingsRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for an embeddings request.</p> Source code in <code>aiperf/clients/openai/openai_embeddings.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for an embeddings request.\"\"\"\n\n    if turn.max_tokens:\n        self.error(\"Max_tokens is provided but is not supported for embeddings.\")\n\n    prompts = [\n        content for text in turn.texts for content in text.contents if content\n    ]\n\n    extra = model_endpoint.endpoint.extra or []\n\n    payload = {\n        \"model\": turn.model or model_endpoint.primary_model_name,\n        \"input\": prompts,\n    }\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenaiopenai_responses","title":"aiperf.clients.openai.openai_responses","text":""},{"location":"api/#aiperf.clients.openai.openai_responses.OpenAIResponsesRequestConverter","title":"<code>OpenAIResponsesRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for OpenAI Responses requests.</p> Source code in <code>aiperf/clients/openai/openai_responses.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.OPENAI_RESPONSES)\nclass OpenAIResponsesRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for OpenAI Responses requests.\"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a responses request.\"\"\"\n\n        # TODO: Add support for image and audio inputs.\n        prompts = [\n            content for text in turn.texts for content in text.contents if content\n        ]\n\n        extra = model_endpoint.endpoint.extra or []\n\n        payload = {\n            \"input\": prompts,\n            \"model\": turn.model or model_endpoint.primary_model_name,\n            \"stream\": model_endpoint.endpoint.streaming,\n        }\n        if turn.max_tokens:\n            payload[\"max_output_tokens\"] = turn.max_tokens\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.openai_responses.OpenAIResponsesRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a responses request.</p> Source code in <code>aiperf/clients/openai/openai_responses.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a responses request.\"\"\"\n\n    # TODO: Add support for image and audio inputs.\n    prompts = [\n        content for text in turn.texts for content in text.contents if content\n    ]\n\n    extra = model_endpoint.endpoint.extra or []\n\n    payload = {\n        \"input\": prompts,\n        \"model\": turn.model or model_endpoint.primary_model_name,\n        \"stream\": model_endpoint.endpoint.streaming,\n    }\n    if turn.max_tokens:\n        payload[\"max_output_tokens\"] = turn.max_tokens\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfclientsopenairankings","title":"aiperf.clients.openai.rankings","text":""},{"location":"api/#aiperf.clients.openai.rankings.RankingsRequestConverter","title":"<code>RankingsRequestConverter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Request converter for rankings requests.</p> <p>Expects texts with specific names: - 'query': Single text containing the query to rank against - 'passages': Multiple texts containing passages to be ranked</p> Source code in <code>aiperf/clients/openai/rankings.py</code> <pre><code>@RequestConverterFactory.register(EndpointType.RANKINGS)\nclass RankingsRequestConverter(AIPerfLoggerMixin):\n    \"\"\"Request converter for rankings requests.\n\n    Expects texts with specific names:\n    - 'query': Single text containing the query to rank against\n    - 'passages': Multiple texts containing passages to be ranked\n    \"\"\"\n\n    async def format_payload(\n        self,\n        model_endpoint: ModelEndpointInfo,\n        turn: Turn,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Format payload for a rankings request.\"\"\"\n\n        if turn.max_tokens:\n            self.warning(\"Max_tokens is provided but is not supported for rankings.\")\n        query_texts = []\n        passage_texts = []\n\n        for text in turn.texts:\n            if text.name == \"query\":\n                query_texts.extend(text.contents)\n            elif text.name == \"passages\":\n                passage_texts.extend(text.contents)\n            else:\n                self.warning(\n                    f\"Ignoring text with name '{text.name}' - rankings expects 'query' and 'passages'\"\n                )\n\n        if not query_texts:\n            raise ValueError(\n                \"Rankings request requires a text with name 'query'. \"\n                \"Provide a Text object with name='query' containing the search query.\"\n            )\n\n        if len(query_texts) &gt; 1:\n            self.warning(\n                f\"Multiple query texts found, using the first one. Found {len(query_texts)} queries.\"\n            )\n\n        query_text = query_texts[0]\n\n        if not passage_texts:\n            self.warning(\n                \"Rankings request has query but no passages to rank. \"\n                \"Consider adding a Text object with name='passages' containing texts to rank.\"\n            )\n\n        extra = model_endpoint.endpoint.extra or []\n\n        payload = {\n            \"model\": turn.model or model_endpoint.primary_model_name,\n            \"query\": {\"text\": query_text},\n            \"passages\": [{\"text\": passage} for passage in passage_texts],\n        }\n\n        if extra:\n            payload.update(extra)\n\n        self.debug(lambda: f\"Formatted rankings payload: {payload}\")\n        return payload\n</code></pre>"},{"location":"api/#aiperf.clients.openai.rankings.RankingsRequestConverter.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format payload for a rankings request.</p> Source code in <code>aiperf/clients/openai/rankings.py</code> <pre><code>async def format_payload(\n    self,\n    model_endpoint: ModelEndpointInfo,\n    turn: Turn,\n) -&gt; dict[str, Any]:\n    \"\"\"Format payload for a rankings request.\"\"\"\n\n    if turn.max_tokens:\n        self.warning(\"Max_tokens is provided but is not supported for rankings.\")\n    query_texts = []\n    passage_texts = []\n\n    for text in turn.texts:\n        if text.name == \"query\":\n            query_texts.extend(text.contents)\n        elif text.name == \"passages\":\n            passage_texts.extend(text.contents)\n        else:\n            self.warning(\n                f\"Ignoring text with name '{text.name}' - rankings expects 'query' and 'passages'\"\n            )\n\n    if not query_texts:\n        raise ValueError(\n            \"Rankings request requires a text with name 'query'. \"\n            \"Provide a Text object with name='query' containing the search query.\"\n        )\n\n    if len(query_texts) &gt; 1:\n        self.warning(\n            f\"Multiple query texts found, using the first one. Found {len(query_texts)} queries.\"\n        )\n\n    query_text = query_texts[0]\n\n    if not passage_texts:\n        self.warning(\n            \"Rankings request has query but no passages to rank. \"\n            \"Consider adding a Text object with name='passages' containing texts to rank.\"\n        )\n\n    extra = model_endpoint.endpoint.extra or []\n\n    payload = {\n        \"model\": turn.model or model_endpoint.primary_model_name,\n        \"query\": {\"text\": query_text},\n        \"passages\": [{\"text\": passage} for passage in passage_texts],\n    }\n\n    if extra:\n        payload.update(extra)\n\n    self.debug(lambda: f\"Formatted rankings payload: {payload}\")\n    return payload\n</code></pre>"},{"location":"api/#aiperfcommonaiperf_logger","title":"aiperf.common.aiperf_logger","text":""},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger","title":"<code>AIPerfLogger</code>","text":"<p>Logger for AIPerf messages with lazy evaluation support for f-strings.</p> <p>This logger supports lazy evaluation of f-strings through lambdas to avoid expensive string formatting operations when the log level is not enabled.</p> It also extends the standard logging module with additional log levels <ul> <li>TRACE    (TRACE &lt; DEBUG)</li> <li>NOTICE   (INFO &lt; NOTICE &lt; WARNING)</li> <li>SUCCESS  (WARNING &lt; SUCCESS &lt; ERROR)</li> </ul> Usage <p>logger = AIPerfLogger(\"my_logger\") logger.debug(lambda: f\"Processing {item} with {count} items\") logger.info(\"Simple string message\") logger.notice(\"Notice message\") logger.success(\"Benchmark completed successfully\")</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>class AIPerfLogger:\n    \"\"\"Logger for AIPerf messages with lazy evaluation support for f-strings.\n\n    This logger supports lazy evaluation of f-strings through lambdas to avoid\n    expensive string formatting operations when the log level is not enabled.\n\n    It also extends the standard logging module with additional log levels:\n        - TRACE    (TRACE &lt; DEBUG)\n        - NOTICE   (INFO &lt; NOTICE &lt; WARNING)\n        - SUCCESS  (WARNING &lt; SUCCESS &lt; ERROR)\n\n    Usage:\n        logger = AIPerfLogger(\"my_logger\")\n        logger.debug(lambda: f\"Processing {item} with {count} items\")\n        logger.info(\"Simple string message\")\n        logger.notice(\"Notice message\")\n        logger.success(\"Benchmark completed successfully\")\n        # Need to pass local variables to the lambda to avoid them going out of scope\n        logger.debug(lambda i=i: f\"Binding loop variable: {i}\")\n        logger.exception(f\"Direct f-string usage: {e}\")\n    \"\"\"\n\n    def __init__(self, logger_name: str):\n        self.logger_name = logger_name\n        self._logger = logging.getLogger(logger_name)\n\n        # Cache the internal logging module's _log method\n        self._internal_log = self._logger._log\n\n        # Forward the internal findCaller method to our custom method\n        self._logger.findCaller = self.find_caller\n\n        # Python style method names\n        self.is_enabled_for = self._logger.isEnabledFor\n        self.set_level = self._logger.setLevel\n        self.get_effective_level = self._logger.getEffectiveLevel\n\n        # Legacy logging method compatibility / passthrough\n        self.isEnabledFor = self._logger.isEnabledFor\n        self.setLevel = self._logger.setLevel\n        self.getEffectiveLevel = self._logger.getEffectiveLevel\n        self.handlers = self._logger.handlers\n        self.addHandler = self._logger.addHandler\n        self.removeHandler = self._logger.removeHandler\n        self.hasHandlers = self._logger.hasHandlers\n        self.root = self._logger.root\n\n    @property\n    def is_debug_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_DEBUG)\n\n    @property\n    def is_trace_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_TRACE)\n\n    def _log(self, level: int, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Internal log method that handles lazy evaluation of f-strings.\"\"\"\n        if callable(msg):\n            # NOTE: Internal python logging _log method requires a tuple for the args, even if there are no args\n            self._internal_log(level, msg(*args), (), **kwargs)\n        else:\n            self._internal_log(level, msg, args, **kwargs)\n\n    @classmethod\n    def is_valid_level(cls, level: int | str) -&gt; bool:\n        \"\"\"Check if the given level is a valid level.\"\"\"\n        if isinstance(level, str):\n            return level in [\n                \"TRACE\",\n                \"DEBUG\",\n                \"INFO\",\n                \"NOTICE\",\n                \"WARNING\",\n                \"SUCCESS\",\n                \"ERROR\",\n                \"CRITICAL\",\n            ]\n        else:\n            return level in [\n                _TRACE,\n                _DEBUG,\n                _INFO,\n                _NOTICE,\n                _WARNING,\n                _SUCCESS,\n                _ERROR,\n                _CRITICAL,\n            ]\n\n    @classmethod\n    def get_level_number(cls, level: int | str) -&gt; int:\n        \"\"\"Get the numeric level for the given level.\"\"\"\n        if isinstance(level, str):\n            return getattr(cls, level.upper())\n        else:\n            return level\n\n    def find_caller(\n        self, stack_info=False, stacklevel=1\n    ) -&gt; tuple[str, int, str, str | None]:\n        \"\"\"\n        NOTE: This is a modified version of the findCaller method in the logging module,\n        in order to allow us to add custom ignored files.\n\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        # On some versions of IronPython, currentframe() returns None if\n        # IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        orig_f = f\n        while f and stacklevel &gt; 1:\n            f = f.f_back\n            stacklevel -= 1\n        if not f:\n            f = orig_f\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\n        while f and hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            # NOTE: The if-statement below was modified to use our own list of ignored files (_ignored_files).\n            # This is required to avoid it appearing as all logs are coming from this file.\n            if filename in _ignored_files:\n                f = f.f_back\n                continue\n            sinfo = None\n            if stack_info:\n                sio = io.StringIO()\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == \"\\n\":\n                    sinfo = sinfo[:-1]\n                sio.close()\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n            break\n        return rv\n\n    def log(self, level: int, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(level):\n            self._log(level, msg, args, **kwargs)\n\n    def trace(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a trace message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_TRACE):\n            self._log(_TRACE, msg, *args, **kwargs)\n\n    def debug(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a debug message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_DEBUG):\n            self._log(_DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an info message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_INFO):\n            self._log(_INFO, msg, *args, **kwargs)\n\n    def notice(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a notice message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_NOTICE):\n            self._log(_NOTICE, msg, *args, **kwargs)\n\n    def warning(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a warning message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_WARNING):\n            self._log(_WARNING, msg, *args, **kwargs)\n\n    def success(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a success message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_SUCCESS):\n            self._log(_SUCCESS, msg, *args, **kwargs)\n\n    def error(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an error message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an exception message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, msg, *args, exc_info=True, **kwargs)\n\n    def critical(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a critical message with support for lazy evaluation using lambdas.\"\"\"\n        if self.is_enabled_for(_CRITICAL):\n            self._log(_CRITICAL, msg, *args, **kwargs)\n\n    def trace_or_debug(\n        self,\n        trace_msg: str | Callable[..., str],\n        debug_msg: str | Callable[..., str],\n    ) -&gt; None:\n        \"\"\"Log different messages depending on the level of the logger.\n\n        This method is used to log a message at the trace level if the trace level is enabled,\n        otherwise it will log a debug message. It enables us to use a single method to log\n        different messages depending on the level of the logger. Use this method to provide\n        full dumps of data when the logger is in trace mode, and a more concise message when\n        the logger is in debug mode.\n\n        Example:\n        ```python\n        self.trace_or_debug(\n            lambda: f\"Received request: {request}\",\n            lambda: f\"Received request id: {request.id}\",\n        )\n        ```\n        \"\"\"\n        if self.is_enabled_for(_TRACE):\n            self._log(_TRACE, trace_msg)\n        elif self.is_enabled_for(_DEBUG):\n            self._log(_DEBUG, debug_msg)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger--need-to-pass-local-variables-to-the-lambda-to-avoid-them-going-out-of-scope","title":"Need to pass local variables to the lambda to avoid them going out of scope","text":"<p>logger.debug(lambda i=i: f\"Binding loop variable: {i}\") logger.exception(f\"Direct f-string usage: {e}\")</p>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.critical","title":"<code>critical(msg, *args, **kwargs)</code>","text":"<p>Log a critical message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def critical(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a critical message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_CRITICAL):\n        self._log(_CRITICAL, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.debug","title":"<code>debug(msg, *args, **kwargs)</code>","text":"<p>Log a debug message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def debug(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a debug message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_DEBUG):\n        self._log(_DEBUG, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.error","title":"<code>error(msg, *args, **kwargs)</code>","text":"<p>Log an error message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def error(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an error message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.exception","title":"<code>exception(msg, *args, **kwargs)</code>","text":"<p>Log an exception message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def exception(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an exception message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, msg, *args, exc_info=True, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.find_caller","title":"<code>find_caller(stack_info=False, stacklevel=1)</code>","text":"<p>NOTE: This is a modified version of the findCaller method in the logging module, in order to allow us to add custom ignored files.</p> <p>Find the stack frame of the caller so that we can note the source file name, line number and function name.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def find_caller(\n    self, stack_info=False, stacklevel=1\n) -&gt; tuple[str, int, str, str | None]:\n    \"\"\"\n    NOTE: This is a modified version of the findCaller method in the logging module,\n    in order to allow us to add custom ignored files.\n\n    Find the stack frame of the caller so that we can note the source\n    file name, line number and function name.\n    \"\"\"\n    f = currentframe()\n    # On some versions of IronPython, currentframe() returns None if\n    # IronPython isn't run with -X:Frames.\n    if f is not None:\n        f = f.f_back\n    orig_f = f\n    while f and stacklevel &gt; 1:\n        f = f.f_back\n        stacklevel -= 1\n    if not f:\n        f = orig_f\n    rv = \"(unknown file)\", 0, \"(unknown function)\", None\n    while f and hasattr(f, \"f_code\"):\n        co = f.f_code\n        filename = os.path.normcase(co.co_filename)\n        # NOTE: The if-statement below was modified to use our own list of ignored files (_ignored_files).\n        # This is required to avoid it appearing as all logs are coming from this file.\n        if filename in _ignored_files:\n            f = f.f_back\n            continue\n        sinfo = None\n        if stack_info:\n            sio = io.StringIO()\n            sio.write(\"Stack (most recent call last):\\n\")\n            traceback.print_stack(f, file=sio)\n            sinfo = sio.getvalue()\n            if sinfo[-1] == \"\\n\":\n                sinfo = sinfo[:-1]\n            sio.close()\n        rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n        break\n    return rv\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.get_level_number","title":"<code>get_level_number(level)</code>  <code>classmethod</code>","text":"<p>Get the numeric level for the given level.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>@classmethod\ndef get_level_number(cls, level: int | str) -&gt; int:\n    \"\"\"Get the numeric level for the given level.\"\"\"\n    if isinstance(level, str):\n        return getattr(cls, level.upper())\n    else:\n        return level\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.info","title":"<code>info(msg, *args, **kwargs)</code>","text":"<p>Log an info message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def info(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_INFO):\n        self._log(_INFO, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.is_valid_level","title":"<code>is_valid_level(level)</code>  <code>classmethod</code>","text":"<p>Check if the given level is a valid level.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>@classmethod\ndef is_valid_level(cls, level: int | str) -&gt; bool:\n    \"\"\"Check if the given level is a valid level.\"\"\"\n    if isinstance(level, str):\n        return level in [\n            \"TRACE\",\n            \"DEBUG\",\n            \"INFO\",\n            \"NOTICE\",\n            \"WARNING\",\n            \"SUCCESS\",\n            \"ERROR\",\n            \"CRITICAL\",\n        ]\n    else:\n        return level in [\n            _TRACE,\n            _DEBUG,\n            _INFO,\n            _NOTICE,\n            _WARNING,\n            _SUCCESS,\n            _ERROR,\n            _CRITICAL,\n        ]\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.log","title":"<code>log(level, msg, *args, **kwargs)</code>","text":"<p>Log a message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def log(self, level: int, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(level):\n        self._log(level, msg, args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.notice","title":"<code>notice(msg, *args, **kwargs)</code>","text":"<p>Log a notice message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def notice(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a notice message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_NOTICE):\n        self._log(_NOTICE, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.success","title":"<code>success(msg, *args, **kwargs)</code>","text":"<p>Log a success message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def success(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a success message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_SUCCESS):\n        self._log(_SUCCESS, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.trace","title":"<code>trace(msg, *args, **kwargs)</code>","text":"<p>Log a trace message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def trace(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a trace message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_TRACE):\n        self._log(_TRACE, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.trace_or_debug","title":"<code>trace_or_debug(trace_msg, debug_msg)</code>","text":"<p>Log different messages depending on the level of the logger.</p> <p>This method is used to log a message at the trace level if the trace level is enabled, otherwise it will log a debug message. It enables us to use a single method to log different messages depending on the level of the logger. Use this method to provide full dumps of data when the logger is in trace mode, and a more concise message when the logger is in debug mode.</p> <p>Example:</p> <pre><code>self.trace_or_debug(\n    lambda: f\"Received request: {request}\",\n    lambda: f\"Received request id: {request.id}\",\n)\n</code></pre> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def trace_or_debug(\n    self,\n    trace_msg: str | Callable[..., str],\n    debug_msg: str | Callable[..., str],\n) -&gt; None:\n    \"\"\"Log different messages depending on the level of the logger.\n\n    This method is used to log a message at the trace level if the trace level is enabled,\n    otherwise it will log a debug message. It enables us to use a single method to log\n    different messages depending on the level of the logger. Use this method to provide\n    full dumps of data when the logger is in trace mode, and a more concise message when\n    the logger is in debug mode.\n\n    Example:\n    ```python\n    self.trace_or_debug(\n        lambda: f\"Received request: {request}\",\n        lambda: f\"Received request id: {request.id}\",\n    )\n    ```\n    \"\"\"\n    if self.is_enabled_for(_TRACE):\n        self._log(_TRACE, trace_msg)\n    elif self.is_enabled_for(_DEBUG):\n        self._log(_DEBUG, debug_msg)\n</code></pre>"},{"location":"api/#aiperf.common.aiperf_logger.AIPerfLogger.warning","title":"<code>warning(msg, *args, **kwargs)</code>","text":"<p>Log a warning message with support for lazy evaluation using lambdas.</p> Source code in <code>aiperf/common/aiperf_logger.py</code> <pre><code>def warning(self, msg: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a warning message with support for lazy evaluation using lambdas.\"\"\"\n    if self.is_enabled_for(_WARNING):\n        self._log(_WARNING, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperfcommonbase_comms","title":"aiperf.common.base_comms","text":""},{"location":"api/#aiperf.common.base_comms.BaseCommunication","title":"<code>BaseCommunication</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>Base class for specifying the base communication layer for AIPerf components.</p> Source code in <code>aiperf/common/base_comms.py</code> <pre><code>@implements_protocol(CommunicationProtocol)\nclass BaseCommunication(AIPerfLifecycleMixin, ABC):\n    \"\"\"Base class for specifying the base communication layer for AIPerf components.\"\"\"\n\n    @abstractmethod\n    def get_address(self, address_type: CommAddressType) -&gt; str:\n        \"\"\"Get the address for a given address type.\n\n        Args:\n            address_type: The type of address to get the address for, or the address itself.\n\n        Returns:\n            The address for the given address type, or the address itself if it is a string.\n        \"\"\"\n\n    @abstractmethod\n    def create_client(\n        self,\n        client_type: CommClientType,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; CommunicationClientProtocol:\n        \"\"\"Create a communication client for a given client type and address.\n\n        Args:\n            client_type: The type of client to create.\n            address: The type of address to use when looking up in the communication config, or the address itself.\n            bind: Whether to bind or connect the socket.\n            socket_ops: Additional socket options to set.\n            max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n        \"\"\"\n\n    def create_pub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PubClientProtocol:\n        return cast(\n            PubClientProtocol,\n            self.create_client(CommClientType.PUB, address, bind, socket_ops),\n        )\n\n    def create_sub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; SubClientProtocol:\n        return cast(\n            SubClientProtocol,\n            self.create_client(CommClientType.SUB, address, bind, socket_ops),\n        )\n\n    def create_push_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PushClientProtocol:\n        return cast(\n            PushClientProtocol,\n            self.create_client(CommClientType.PUSH, address, bind, socket_ops),\n        )\n\n    def create_pull_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; PullClientProtocol:\n        return cast(\n            PullClientProtocol,\n            self.create_client(\n                CommClientType.PULL,\n                address,\n                bind,\n                socket_ops,\n                max_pull_concurrency=max_pull_concurrency,\n            ),\n        )\n\n    def create_request_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; RequestClientProtocol:\n        return cast(\n            RequestClientProtocol,\n            self.create_client(CommClientType.REQUEST, address, bind, socket_ops),\n        )\n\n    def create_reply_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; ReplyClientProtocol:\n        return cast(\n            ReplyClientProtocol,\n            self.create_client(CommClientType.REPLY, address, bind, socket_ops),\n        )\n</code></pre>"},{"location":"api/#aiperf.common.base_comms.BaseCommunication.create_client","title":"<code>create_client(client_type, address, bind=False, socket_ops=None, max_pull_concurrency=None)</code>  <code>abstractmethod</code>","text":"<p>Create a communication client for a given client type and address.</p> <p>Parameters:</p> Name Type Description Default <code>client_type</code> <code>CommClientType</code> <p>The type of client to create.</p> required <code>address</code> <code>CommAddressType</code> <p>The type of address to use when looking up in the communication config, or the address itself.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> <code>False</code> <code>socket_ops</code> <code>dict | None</code> <p>Additional socket options to set.</p> <code>None</code> <code>max_pull_concurrency</code> <code>int | None</code> <p>The maximum number of concurrent pull requests to allow. (Only used for pull clients)</p> <code>None</code> Source code in <code>aiperf/common/base_comms.py</code> <pre><code>@abstractmethod\ndef create_client(\n    self,\n    client_type: CommClientType,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n) -&gt; CommunicationClientProtocol:\n    \"\"\"Create a communication client for a given client type and address.\n\n    Args:\n        client_type: The type of client to create.\n        address: The type of address to use when looking up in the communication config, or the address itself.\n        bind: Whether to bind or connect the socket.\n        socket_ops: Additional socket options to set.\n        max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.base_comms.BaseCommunication.get_address","title":"<code>get_address(address_type)</code>  <code>abstractmethod</code>","text":"<p>Get the address for a given address type.</p> <p>Parameters:</p> Name Type Description Default <code>address_type</code> <code>CommAddressType</code> <p>The type of address to get the address for, or the address itself.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The address for the given address type, or the address itself if it is a string.</p> Source code in <code>aiperf/common/base_comms.py</code> <pre><code>@abstractmethod\ndef get_address(self, address_type: CommAddressType) -&gt; str:\n    \"\"\"Get the address for a given address type.\n\n    Args:\n        address_type: The type of address to get the address for, or the address itself.\n\n    Returns:\n        The address for the given address type, or the address itself if it is a string.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperfcommonbase_component_service","title":"aiperf.common.base_component_service","text":""},{"location":"api/#aiperf.common.base_component_service.BaseComponentService","title":"<code>BaseComponentService</code>","text":"<p>               Bases: <code>BaseService</code></p> <p>Base class for all Component services.</p> <p>This class provides a common interface for all Component services in the AIPerf framework such as the Timing Manager, Dataset Manager, etc.</p> <p>It extends the BaseService by adding heartbeat and registration functionality, as well as publishing the current state of the service to the system controller.</p> Source code in <code>aiperf/common/base_component_service.py</code> <pre><code>@implements_protocol(ServiceProtocol)\nclass BaseComponentService(BaseService):\n    \"\"\"Base class for all Component services.\n\n    This class provides a common interface for all Component services in the AIPerf\n    framework such as the Timing Manager, Dataset Manager, etc.\n\n    It extends the BaseService by adding heartbeat and registration functionality, as well as\n    publishing the current state of the service to the system controller.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            **kwargs,\n        )\n\n    @background_task(interval=DEFAULT_HEARTBEAT_INTERVAL, immediate=False)\n    async def _heartbeat_task(self) -&gt; None:\n        \"\"\"Send a heartbeat notification to the system controller.\"\"\"\n        await self.publish(\n            HeartbeatMessage(\n                service_id=self.service_id,\n                service_type=self.service_type,\n                state=self.state,\n            )\n        )\n\n    @on_start\n    async def _register_service_on_start(self) -&gt; None:\n        \"\"\"Register the service with the system controller on startup.\"\"\"\n        self.debug(\n            lambda: f\"Attempting to register service {self} ({self.service_id}) with system controller\"\n        )\n        result = None\n        command_message = RegisterServiceCommand(\n            command_id=str(uuid.uuid4()),\n            service_id=self.service_id,\n            service_type=self.service_type,\n            # Target the system controller directly to avoid broadcasting to all services.\n            target_service_type=ServiceType.SYSTEM_CONTROLLER,\n            state=self.state,\n        )\n        for _ in range(DEFAULT_MAX_REGISTRATION_ATTEMPTS):\n            result = await self.send_command_and_wait_for_response(\n                # NOTE: We keep the command id the same each time to ensure that the system controller\n                #       can ignore duplicate registration requests.\n                command_message,\n                timeout=DEFAULT_REGISTRATION_INTERVAL,\n            )\n            if isinstance(result, CommandResponse):\n                self.debug(\n                    lambda: f\"Service {self.service_id} registered with system controller\"\n                )\n                break\n        if isinstance(result, ErrorDetails):\n            self.error(\n                f\"Failed to register service {self} ({self.service_id}): {result}\"\n            )\n            raise self._service_error(\n                f\"Failed to register service {self} ({self.service_id}): {result}\"\n            )\n\n    @on_state_change\n    async def _on_state_change(\n        self, old_state: LifecycleState, new_state: LifecycleState\n    ) -&gt; None:\n        \"\"\"Action to take when the service state is set.\n\n        This method will also publish the status message to the status message_type if the\n        communications are initialized.\n        \"\"\"\n        if self.stop_requested:\n            return\n        if not self.comms.was_initialized:\n            return\n        await self.publish(\n            StatusMessage(\n                service_id=self.service_id,\n                service_type=self.service_type,\n                state=new_state,\n            )\n        )\n\n    @on_command(CommandType.SHUTDOWN)\n    async def _on_shutdown_command(self, message: CommandMessage) -&gt; None:\n        self.debug(f\"Received shutdown command: {message}, {self.service_id}\")\n        try:\n            await self.stop()\n        except Exception as e:\n            self.warning(\n                f\"Failed to stop service {self} ({self.service_id}) after receiving shutdown command: {e}. Killing.\"\n            )\n            await self._kill()\n        raise asyncio.CancelledError()\n</code></pre>"},{"location":"api/#aiperfcommonbase_service","title":"aiperf.common.base_service","text":""},{"location":"api/#aiperf.common.base_service.BaseService","title":"<code>BaseService</code>","text":"<p>               Bases: <code>CommandHandlerMixin</code>, <code>ProcessHealthMixin</code>, <code>ABC</code></p> <p>Base class for all AIPerf services, providing common functionality for communication, state management, and lifecycle operations. This class inherits from the MessageBusClientMixin, which provides the message bus client functionality.</p> <p>This class provides the foundation for implementing the various services of the AIPerf system. Some of the abstract methods are implemented here, while others are still required to be implemented by derived classes.</p> Source code in <code>aiperf/common/base_service.py</code> <pre><code>class BaseService(CommandHandlerMixin, ProcessHealthMixin, ABC):\n    \"\"\"Base class for all AIPerf services, providing common functionality for\n    communication, state management, and lifecycle operations.\n    This class inherits from the MessageBusClientMixin, which provides the\n    message bus client functionality.\n\n    This class provides the foundation for implementing the various services of the\n    AIPerf system. Some of the abstract methods are implemented here, while others\n    are still required to be implemented by derived classes.\n    \"\"\"\n\n    service_type: ClassVar[ServiceTypeT]\n    \"\"\"The type of service this class implements. This is set by the ServiceFactory.register decorator.\"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        self.service_config = service_config\n        self.user_config = user_config\n        self.service_id = service_id or f\"{self.service_type}_{uuid.uuid4().hex[:8]}\"\n        super().__init__(\n            service_id=self.service_id,\n            id=self.service_id,\n            service_config=self.service_config,\n            user_config=self.user_config,\n            **kwargs,\n        )\n        self.debug(\n            lambda: f\"__init__ {self.service_type} service (id: {self.service_id})\"\n        )\n        self._set_process_title()\n\n    def _set_process_title(self) -&gt; None:\n        try:\n            import setproctitle\n\n            setproctitle.setproctitle(f\"aiperf {self.service_id}\")\n        except Exception:\n            # setproctitle is not available on all platforms, so we ignore the error\n            self.debug(\"Failed to set process title, ignoring\")\n\n    def _service_error(self, message: str) -&gt; ServiceError:\n        return ServiceError(\n            message=message,\n            service_type=self.service_type,\n            service_id=self.service_id,\n        )\n\n    @on_command(CommandType.SHUTDOWN)\n    async def _on_shutdown_command(self, message: CommandMessage) -&gt; None:\n        self.debug(f\"Received shutdown command from {message.service_id}\")\n        # Send an acknowledged response back to the sender, because we won't be able to send it after we stop.\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n\n        try:\n            await self.stop()\n        except Exception as e:\n            self.exception(\n                f\"Failed to stop service {self} ({self.service_id}) after receiving shutdown command: {e}. Killing.\"\n            )\n            await self._kill()\n\n    async def stop(self) -&gt; None:\n        \"\"\"This overrides the base class stop method to handle the case where the service is already stopping.\n        In this case, we need to kill the process to be safe.\"\"\"\n        if self.stop_requested:\n            self.error(f\"Attempted to stop {self} in state {self.state}. Killing.\")\n            await self._kill()\n            return\n        await super().stop()\n\n    async def _kill(self) -&gt; None:\n        \"\"\"Kill the lifecycle. This is used when the lifecycle is requested to stop, but is already in a stopping state.\n        This is a last resort to ensure that the lifecycle is stopped.\n        \"\"\"\n        await self._set_state(LifecycleState.FAILED)\n        self.error(lambda: f\"Killing {self}\")\n        self.stop_requested = True\n        self.stopped_event.set()\n        # TODO: This is a hack to ensure that the process is killed.\n        #       We should find a better way to do this.\n        os.kill(os.getpid(), signal.SIGKILL)\n        raise asyncio.CancelledError(f\"Killed {self}\")\n</code></pre>"},{"location":"api/#aiperf.common.base_service.BaseService.service_type","title":"<code>service_type</code>  <code>class-attribute</code>","text":"<p>The type of service this class implements. This is set by the ServiceFactory.register decorator.</p>"},{"location":"api/#aiperf.common.base_service.BaseService.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>This overrides the base class stop method to handle the case where the service is already stopping. In this case, we need to kill the process to be safe.</p> Source code in <code>aiperf/common/base_service.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"This overrides the base class stop method to handle the case where the service is already stopping.\n    In this case, we need to kill the process to be safe.\"\"\"\n    if self.stop_requested:\n        self.error(f\"Attempted to stop {self} in state {self.state}. Killing.\")\n        await self._kill()\n        return\n    await super().stop()\n</code></pre>"},{"location":"api/#aiperfcommonbootstrap","title":"aiperf.common.bootstrap","text":""},{"location":"api/#aiperf.common.bootstrap.bootstrap_and_run_service","title":"<code>bootstrap_and_run_service(service_class, service_config=None, user_config=None, service_id=None, log_queue=None, **kwargs)</code>","text":"<p>Bootstrap the service and run it.</p> <p>This function will load the service configuration, create an instance of the service, and run it.</p> <p>Parameters:</p> Name Type Description Default <code>service_class</code> <code>type[ServiceProtocol]</code> <p>The python class of the service to run. This should be a subclass of BaseService. This should be a type and not an instance.</p> required <code>service_config</code> <code>ServiceConfig | None</code> <p>The service configuration to use. If not provided, the service configuration will be loaded from the environment variables.</p> <code>None</code> <code>user_config</code> <code>UserConfig | None</code> <p>The user configuration to use. If not provided, the user configuration will be loaded from the environment variables.</p> <code>None</code> <code>log_queue</code> <code>Queue | None</code> <p>Optional multiprocessing queue for child process logging. If provided, the child process logging will be set up.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the service constructor.</p> <code>{}</code> Source code in <code>aiperf/common/bootstrap.py</code> <pre><code>def bootstrap_and_run_service(\n    service_class: type[ServiceProtocol],\n    service_config: ServiceConfig | None = None,\n    user_config: UserConfig | None = None,\n    service_id: str | None = None,\n    log_queue: \"multiprocessing.Queue | None\" = None,\n    **kwargs,\n):\n    \"\"\"Bootstrap the service and run it.\n\n    This function will load the service configuration,\n    create an instance of the service, and run it.\n\n    Args:\n        service_class: The python class of the service to run. This should be a subclass of\n            BaseService. This should be a type and not an instance.\n        service_config: The service configuration to use. If not provided, the service\n            configuration will be loaded from the environment variables.\n        user_config: The user configuration to use. If not provided, the user configuration\n            will be loaded from the environment variables.\n        log_queue: Optional multiprocessing queue for child process logging. If provided,\n            the child process logging will be set up.\n        kwargs: Additional keyword arguments to pass to the service constructor.\n    \"\"\"\n\n    # Load the service configuration\n    if service_config is None:\n        from aiperf.common.config import load_service_config\n\n        service_config = load_service_config()\n\n    # Load the user configuration\n    if user_config is None:\n        from aiperf.common.config import load_user_config\n\n        # TODO: Add support for loading user config from a file/environment variables\n        user_config = load_user_config()\n\n    async def _run_service():\n        if service_config.developer.enable_yappi:\n            _start_yappi_profiling()\n\n        from aiperf.module_loader import ensure_modules_loaded\n\n        ensure_modules_loaded()\n\n        service = service_class(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            **kwargs,\n        )\n\n        from aiperf.common.logging import setup_child_process_logging\n\n        setup_child_process_logging(\n            log_queue, service.service_id, service_config, user_config\n        )\n\n        if user_config.input.random_seed is not None:\n            random.seed(user_config.input.random_seed)\n            # Try and set the numpy random seed\n            # https://numpy.org/doc/stable/reference/random/index.html#random-quick-start\n            with contextlib.suppress(ImportError):\n                import numpy as np\n\n                np.random.seed(user_config.input.random_seed)\n\n        try:\n            await service.initialize()\n            await service.start()\n            await service.stopped_event.wait()\n        except Exception as e:\n            service.exception(f\"Unhandled exception in service: {e}\")\n\n        if service_config.developer.enable_yappi:\n            _stop_yappi_profiling(service.service_id, user_config)\n\n    with contextlib.suppress(asyncio.CancelledError):\n        if not service_config.developer.disable_uvloop:\n            import uvloop\n\n            uvloop.run(_run_service())\n        else:\n            asyncio.run(_run_service())\n</code></pre>"},{"location":"api/#aiperfcommonconfigaudio_config","title":"aiperf.common.config.audio_config","text":""},{"location":"api/#aiperf.common.config.audio_config.AudioConfig","title":"<code>AudioConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining audio related settings.</p> Source code in <code>aiperf/common/config/audio_config.py</code> <pre><code>class AudioConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining audio related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.AUDIO_INPUT\n\n    batch_size: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The batch size of audio requests AIPerf should send.\\n\"\n            \"This is currently supported with the OpenAI `chat` endpoint type\",\n        ),\n        CLIParameter(\n            name=(\n                \"--audio-batch-size\",\n                \"--batch-size-audio\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.BATCH_SIZE\n\n    length: AudioLengthConfig = AudioLengthConfig()\n\n    format: Annotated[\n        AudioFormat,\n        Field(\n            description=\"The format of the audio files (wav or mp3).\",\n        ),\n        CLIParameter(\n            name=(\n                \"--audio-format\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.FORMAT\n\n    depths: Annotated[\n        list[int],\n        Field(\n            min_length=1,\n            description=\"A list of audio bit depths to randomly select from in bits.\",\n        ),\n        BeforeValidator(parse_str_or_list_of_positive_values),\n        CLIParameter(\n            name=(\n                \"--audio-depths\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.DEPTHS\n\n    sample_rates: Annotated[\n        list[float],\n        Field(\n            min_length=1,\n            description=\"A list of audio sample rates to randomly select from in kHz.\\n\"\n            \"Common sample rates are 16, 44.1, 48, 96, etc.\",\n        ),\n        BeforeValidator(parse_str_or_list_of_positive_values),\n        CLIParameter(\n            name=(\n                \"--audio-sample-rates\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.SAMPLE_RATES\n\n    num_channels: Annotated[\n        int,\n        Field(\n            ge=1,\n            le=2,\n            description=\"The number of audio channels to use for the audio data generation.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--audio-num-channels\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.NUM_CHANNELS\n</code></pre>"},{"location":"api/#aiperf.common.config.audio_config.AudioLengthConfig","title":"<code>AudioLengthConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining audio length related settings.</p> Source code in <code>aiperf/common/config/audio_config.py</code> <pre><code>class AudioLengthConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining audio length related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.AUDIO_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean length of the audio in seconds.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--audio-length-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.LENGTH_MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the length of the audio in seconds.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--audio-length-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = AudioDefaults.LENGTH_STDDEV\n</code></pre>"},{"location":"api/#aiperfcommonconfigbase_config","title":"aiperf.common.config.base_config","text":""},{"location":"api/#aiperf.common.config.base_config.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base configuration class for all configurations.</p> Source code in <code>aiperf/common/config/base_config.py</code> <pre><code>class BaseConfig(BaseModel):\n    \"\"\"\n    Base configuration class for all configurations.\n    \"\"\"\n\n    def serialize_to_yaml(self, verbose: bool = False, indent: int = 4) -&gt; str:\n        \"\"\"\n        Serialize a Pydantic model to a YAML string.\n\n        Args:\n            verbose: Whether to include verbose comments in the YAML output.\n            indent: The per-level indentation to use.\n        \"\"\"\n        # Dump model to dict with context (flags propagate recursively)\n        context = {\n            \"verbose\": verbose,\n        }\n\n        data = self.model_dump(context=context)\n\n        # Attach comments recursively\n        commented_data = self._attach_comments(\n            data=data,\n            model=self,\n            context=context,\n            indent=indent,\n        )\n\n        # Dump to YAML\n        yaml = YAML(pure=True)\n        yaml.indent(mapping=indent, sequence=indent, offset=indent)\n\n        stream = io.StringIO()\n        yaml.dump(commented_data, stream)\n        return stream.getvalue()\n\n    @staticmethod\n    def _attach_comments(\n        data: Any,\n        model: BaseModel,\n        context: dict,\n        indent: int,\n        indent_level: int = 0,\n    ) -&gt; Any:\n        \"\"\"\n        Recursively convert dicts to ruamel.yaml CommentedMap and attach comments from\n        Pydantic field descriptions, or based on context (e.g., verbose flag).\n\n        Args:\n            data: The raw data to convert to a CommentedMap.\n            model: The Pydantic model that contains the field descriptions.\n            context: The Pydantic serializer context which contains the serializer flags.\n            indent: The per-level indentation to use for the comments.\n            indent_level: The current level of indentation. The actual indentation is\n                `indent * indent_level`.\n\n        Returns:\n            The data with comments attached.\n        \"\"\"\n        if isinstance(data, dict):\n            # Create a CommentedMap to store the commented data. This is a special type of\n            # dict provided by the ruamel.yaml library that preserves the order of the keys and\n            # allows for comments to be attached to the keys.\n            commented_map = CommentedMap()\n\n            for field_name, value in data.items():\n                field = model.__class__.model_fields.get(field_name)\n\n                if not BaseConfig._should_add_field_to_template(field):\n                    continue\n\n                if BaseConfig._is_a_nested_config(field, value):\n                    # Recursively process nested models\n                    commented_map[field_name] = BaseConfig._attach_comments(\n                        value,\n                        getattr(model, field_name),\n                        context=context,\n                        indent=indent,\n                        indent_level=indent_level + 1,\n                    )\n\n                    commented_map.yaml_set_comment_before_after_key(\n                        field_name,\n                        before=\"\\n\",\n                        indent=indent * (indent_level + 1),\n                    )\n                else:\n                    # Attach the value to the commented map\n                    commented_map[field_name] = BaseConfig._preprocess_value(value)\n\n                # Attach comment if verbose and description exists\n                if context.get(\"verbose\") and field and field.description:\n                    # Set the comment before the key, with the specified indentation\n                    commented_map.yaml_set_comment_before_after_key(\n                        field_name,\n                        before=\"\\n\" + field.description,\n                        indent=indent * indent_level,\n                    )\n\n            return commented_map\n\n    @staticmethod\n    def _should_add_field_to_template(field: Any) -&gt; bool:\n        # Check if the field should be added to the template based on json_schema_extra\n        # and the add_to_template flag.\n        # If add_to_template is False, we skip adding the field to the template.\n        # If add_to_template is True or not present, we include the field in the template.\n        if field and field.json_schema_extra:\n            return field.json_schema_extra.get(ADD_TO_TEMPLATE, True)\n        else:\n            return True\n\n    @staticmethod\n    def _is_a_nested_config(field: Any, value: Any) -&gt; bool:\n        return (\n            isinstance(value, dict)\n            and field\n            and issubclass(field.annotation, BaseModel)\n        )\n\n    @staticmethod\n    def _preprocess_value(value: Any) -&gt; Any:\n        \"\"\"\n        Preprocess the value before serialization.\n        \"\"\"\n\n        if isinstance(value, Enum):\n            return str(value.value).lower()\n        elif isinstance(value, Path):\n            return str(value)\n        else:\n            return value\n</code></pre>"},{"location":"api/#aiperf.common.config.base_config.BaseConfig.serialize_to_yaml","title":"<code>serialize_to_yaml(verbose=False, indent=4)</code>","text":"<p>Serialize a Pydantic model to a YAML string.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to include verbose comments in the YAML output.</p> <code>False</code> <code>indent</code> <code>int</code> <p>The per-level indentation to use.</p> <code>4</code> Source code in <code>aiperf/common/config/base_config.py</code> <pre><code>def serialize_to_yaml(self, verbose: bool = False, indent: int = 4) -&gt; str:\n    \"\"\"\n    Serialize a Pydantic model to a YAML string.\n\n    Args:\n        verbose: Whether to include verbose comments in the YAML output.\n        indent: The per-level indentation to use.\n    \"\"\"\n    # Dump model to dict with context (flags propagate recursively)\n    context = {\n        \"verbose\": verbose,\n    }\n\n    data = self.model_dump(context=context)\n\n    # Attach comments recursively\n    commented_data = self._attach_comments(\n        data=data,\n        model=self,\n        context=context,\n        indent=indent,\n    )\n\n    # Dump to YAML\n    yaml = YAML(pure=True)\n    yaml.indent(mapping=indent, sequence=indent, offset=indent)\n\n    stream = io.StringIO()\n    yaml.dump(commented_data, stream)\n    return stream.getvalue()\n</code></pre>"},{"location":"api/#aiperfcommonconfigcli_parameter","title":"aiperf.common.config.cli_parameter","text":""},{"location":"api/#aiperf.common.config.cli_parameter.CLIParameter","title":"<code>CLIParameter</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Configuration for a CLI parameter.</p> <p>This is a subclass of the cyclopts.Parameter class that includes the default configuration AIPerf uses for all of its CLI parameters. This is used to ensure that the CLI parameters are consistent across all of the AIPerf config.</p> Source code in <code>aiperf/common/config/cli_parameter.py</code> <pre><code>class CLIParameter(Parameter):\n    \"\"\"Configuration for a CLI parameter.\n\n    This is a subclass of the cyclopts.Parameter class that includes the default configuration AIPerf uses\n    for all of its CLI parameters. This is used to ensure that the CLI parameters are consistent across all\n    of the AIPerf config.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, show_env_var=False, negative=False, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.config.cli_parameter.DeveloperOnlyCLI","title":"<code>DeveloperOnlyCLI</code>","text":"<p>               Bases: <code>CLIParameter</code></p> <p>Configuration for a CLI parameter that is only available to developers.</p> <p>This is a subclass of the CLIParameter class that is used to set a CLI parameter to only be available to developers.</p> Source code in <code>aiperf/common/config/cli_parameter.py</code> <pre><code>class DeveloperOnlyCLI(CLIParameter):\n    \"\"\"Configuration for a CLI parameter that is only available to developers.\n\n    This is a subclass of the CLIParameter class that is used to set a CLI parameter to only be available to developers.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, parse=AIPERF_DEV_MODE, group=Groups.DEVELOPER, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.config.cli_parameter.DisableCLI","title":"<code>DisableCLI</code>","text":"<p>               Bases: <code>CLIParameter</code></p> <p>Configuration for a CLI parameter that is disabled.</p> <p>This is a subclass of the CLIParameter class that is used to set a CLI parameter to disabled.</p> Source code in <code>aiperf/common/config/cli_parameter.py</code> <pre><code>class DisableCLI(CLIParameter):\n    \"\"\"Configuration for a CLI parameter that is disabled.\n\n    This is a subclass of the CLIParameter class that is used to set a CLI parameter to disabled.\n    \"\"\"\n\n    def __init__(self, reason: str, *args, **kwargs):\n        super().__init__(*args, parse=False, **kwargs)\n</code></pre>"},{"location":"api/#aiperfcommonconfigconfig_defaults","title":"aiperf.common.config.config_defaults","text":""},{"location":"api/#aiperfcommonconfigconfig_validators","title":"aiperf.common.config.config_validators","text":""},{"location":"api/#aiperf.common.config.config_validators.coerce_value","title":"<code>coerce_value(value)</code>","text":"<p>Coerce the value to the correct type.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def coerce_value(value: Any) -&gt; Any:\n    \"\"\"Coerce the value to the correct type.\"\"\"\n    if not isinstance(value, str):\n        return value\n    if value.lower() in (\"true\", \"false\"):\n        return value.lower() == \"true\"\n    if value.lower() in (\"none\", \"null\"):\n        return None\n    if value.isdigit() and (not value.startswith(\"0\") or value == \"0\"):\n        return int(value)\n    if (\n        value.startswith(\"-\")\n        and value[1:].isdigit()\n        and (not value.startswith(\"-0\") or value == \"-0\")\n    ):\n        return int(value)\n    if value.count(\".\") == 1 and (\n        value.replace(\".\", \"\").isdigit()\n        or (value.startswith(\"-\") and value[1:].replace(\".\", \"\").isdigit())\n    ):\n        return float(value)\n    return value\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.custom_enum_converter","title":"<code>custom_enum_converter(type_, value)</code>","text":"<p>This is a custom converter for cyclopts that allows us to use our custom enum types</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def custom_enum_converter(type_: Any, value: Sequence[Token]) -&gt; Any:\n    \"\"\"This is a custom converter for cyclopts that allows us to use our custom enum types\"\"\"\n    if len(value) != 1:\n        raise ValueError(f\"Expected 1 value, but got {len(value)}\")\n    return type_(value[0].value)\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_file","title":"<code>parse_file(value)</code>","text":"<p>Parses the given string value and returns a Path object if the value represents a valid file or directory. Returns None if the input value is empty. Args:     value (str): The string value to parse. Returns:     Optional[Path]: A Path object if the value is valid, or None if the value is empty. Raises:     ValueError: If the value is not a valid file or directory.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_file(value: str | None) -&gt; Path | None:\n    \"\"\"\n    Parses the given string value and returns a Path object if the value represents\n    a valid file or directory. Returns None if the input value is empty.\n    Args:\n        value (str): The string value to parse.\n    Returns:\n        Optional[Path]: A Path object if the value is valid, or None if the value is empty.\n    Raises:\n        ValueError: If the value is not a valid file or directory.\n    \"\"\"\n\n    if not value:\n        return None\n    elif not isinstance(value, str):\n        raise ValueError(f\"Expected a string, but got {type(value).__name__}\")\n    else:\n        path = Path(value)\n        if path.is_file() or path.is_dir():\n            return path\n        else:\n            raise ValueError(f\"'{value}' is not a valid file or directory\")\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_service_types","title":"<code>parse_service_types(input)</code>","text":"<p>Parses the input to ensure it is a set of service types. Will replace hyphens with underscores for user convenience.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_service_types(input: Any | None) -&gt; set[ServiceType] | None:\n    \"\"\"Parses the input to ensure it is a set of service types.\n    Will replace hyphens with underscores for user convenience.\"\"\"\n    if input is None:\n        return None\n\n    return {\n        ServiceType(service_type.replace(\"-\", \"_\"))\n        for service_type in parse_str_or_csv_list(input)\n    }\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_csv_list","title":"<code>parse_str_or_csv_list(input)</code>","text":"<p>Parses the input to ensure it is either a string or a list. If the input is a string, it splits the string by commas and trims any whitespace around each element, returning the result as a list. If the input is already a list, it will split each item by commas and trim any whitespace around each element, returning the combined result as a list. If the input is neither a string nor a list, a ValueError is raised.</p> <p>[1, 2, 3] -&gt; [1, 2, 3] \"1,2,3\" -&gt; [\"1\", \"2\", \"3\"][\"1,2,3\", \"4,5,6\"] -&gt; [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"][\"1,2,3\", 4, 5] -&gt; [\"1\", \"2\", \"3\", 4, 5]</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_csv_list(input: Any) -&gt; list[Any]:\n    \"\"\"\n    Parses the input to ensure it is either a string or a list. If the input is a string,\n    it splits the string by commas and trims any whitespace around each element, returning\n    the result as a list. If the input is already a list, it will split each item by commas\n    and trim any whitespace around each element, returning the combined result as a list.\n    If the input is neither a string nor a list, a ValueError is raised.\n\n    [1, 2, 3] -&gt; [1, 2, 3]\n    \"1,2,3\" -&gt; [\"1\", \"2\", \"3\"]\n    [\"1,2,3\", \"4,5,6\"] -&gt; [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n    [\"1,2,3\", 4, 5] -&gt; [\"1\", \"2\", \"3\", 4, 5]\n    \"\"\"\n    if isinstance(input, str):\n        output = [item.strip() for item in input.split(\",\")]\n    elif isinstance(input, list):\n        output = []\n        for item in input:\n            if isinstance(item, str):\n                output.extend([token.strip() for token in item.split(\",\")])\n            else:\n                output.append(item)\n    else:\n        raise ValueError(f\"User Config: {input} - must be a string or list\")\n\n    return output\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_dict_as_tuple_list","title":"<code>parse_str_or_dict_as_tuple_list(input)</code>","text":"<p>Parses the input to ensure it is a list of tuples. (key, value) pairs.</p> <ul> <li>If the input is a string:<ul> <li>If the string starts with a '{', it is parsed as a JSON string.</li> <li>Otherwise, it splits the string by commas and then for each item, it splits the item by colons into key and value, trims any whitespace, and coerces the value to the correct type.</li> </ul> </li> <li>If the input is a dictionary, it is converted to a list of tuples by key and value pairs.</li> <li>If the input is a list, it recursively calls this function on each item, and aggregates the results.</li> <li>Otherwise, a ValueError is raised.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input to be parsed. Expected to be a string, list, or dictionary.</p> required <p>Returns:     list[tuple[str, Any]]: A list of tuples derived from the input. Raises:     ValueError: If the input is neither a string, list, nor dictionary, or if the parsing fails.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_dict_as_tuple_list(input: Any | None) -&gt; list[tuple[str, Any]] | None:\n    \"\"\"\n    Parses the input to ensure it is a list of tuples. (key, value) pairs.\n\n    - If the input is a string:\n        - If the string starts with a '{', it is parsed as a JSON string.\n        - Otherwise, it splits the string by commas and then for each item, it splits the item by colons\n        into key and value, trims any whitespace, and coerces the value to the correct type.\n    - If the input is a dictionary, it is converted to a list of tuples by key and value pairs.\n    - If the input is a list, it recursively calls this function on each item, and aggregates the results.\n    - Otherwise, a ValueError is raised.\n\n    Args:\n        input (Any): The input to be parsed. Expected to be a string, list, or dictionary.\n    Returns:\n        list[tuple[str, Any]]: A list of tuples derived from the input.\n    Raises:\n        ValueError: If the input is neither a string, list, nor dictionary, or if the parsing fails.\n    \"\"\"\n    if input is None:\n        return None\n\n    if isinstance(input, list | tuple | set):\n        output = []\n        for item in input:\n            res = parse_str_or_dict_as_tuple_list(item)\n            if res is not None:\n                output.extend(res)\n        return output\n\n    if isinstance(input, dict):\n        return [(key, coerce_value(value)) for key, value in input.items()]\n\n    if isinstance(input, str):\n        if input.startswith(\"{\"):\n            try:\n                return [(key, value) for key, value in load_json_str(input).items()]\n            except orjson.JSONDecodeError as e:\n                raise ValueError(\n                    f\"User Config: {input} - must be a valid JSON string\"\n                ) from e\n        else:\n            return [\n                (key.strip(), coerce_value(value.strip()))\n                for item in input.split(\",\")\n                for key, value in [item.split(\":\")]\n            ]\n\n    raise ValueError(f\"User Config: {input} - must be a valid string, list, or dict\")\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_list","title":"<code>parse_str_or_list(input)</code>","text":"<p>Parses the input to ensure it is either a string or a list. If the input is a string, it splits the string by commas and trims any whitespace around each element, returning the result as a list. If the input is already a list, it is returned as-is. If the input is neither a string nor a list, a ValueError is raised. Args:     input (Any): The input to be parsed. Expected to be a string or a list. Returns:     list: A list of strings derived from the input. Raises:     ValueError: If the input is neither a string nor a list.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_list(input: Any) -&gt; list[Any]:\n    \"\"\"\n    Parses the input to ensure it is either a string or a list. If the input is a string,\n    it splits the string by commas and trims any whitespace around each element, returning\n    the result as a list. If the input is already a list, it is returned as-is. If the input\n    is neither a string nor a list, a ValueError is raised.\n    Args:\n        input (Any): The input to be parsed. Expected to be a string or a list.\n    Returns:\n        list: A list of strings derived from the input.\n    Raises:\n        ValueError: If the input is neither a string nor a list.\n    \"\"\"\n    if isinstance(input, str):\n        output = [item.strip() for item in input.split(\",\")]\n    elif isinstance(input, list):\n        # TODO: When using cyclopts, the values are already lists, so we have to split them by commas.\n        output = []\n        for item in input:\n            if isinstance(item, str):\n                output.extend([token.strip() for token in item.split(\",\")])\n            else:\n                output.append(item)\n    else:\n        raise ValueError(f\"User Config: {input} - must be a string or list\")\n\n    return output\n</code></pre>"},{"location":"api/#aiperf.common.config.config_validators.parse_str_or_list_of_positive_values","title":"<code>parse_str_or_list_of_positive_values(input)</code>","text":"<p>Parses the input to ensure it is a list of positive integers or floats. This function first converts the input into a list using <code>parse_str_or_list</code>. It then validates that each value in the list is either an integer or a float and that all values are strictly greater than zero. If any value fails this validation, a <code>ValueError</code> is raised. Args:     input (Any): The input to be parsed. It can be a string or a list. Returns:     List[Any]: A list of positive integers or floats. Raises:     ValueError: If any value in the parsed list is not a positive integer or float.</p> Source code in <code>aiperf/common/config/config_validators.py</code> <pre><code>def parse_str_or_list_of_positive_values(input: Any) -&gt; list[Any]:\n    \"\"\"\n    Parses the input to ensure it is a list of positive integers or floats.\n    This function first converts the input into a list using `parse_str_or_list`.\n    It then validates that each value in the list is either an integer or a float\n    and that all values are strictly greater than zero. If any value fails this\n    validation, a `ValueError` is raised.\n    Args:\n        input (Any): The input to be parsed. It can be a string or a list.\n    Returns:\n        List[Any]: A list of positive integers or floats.\n    Raises:\n        ValueError: If any value in the parsed list is not a positive integer or float.\n    \"\"\"\n\n    output = parse_str_or_list(input)\n\n    try:\n        output = [\n            float(x) if \".\" in str(x) or \"e\" in str(x).lower() else int(x)\n            for x in output\n        ]\n    except ValueError as e:\n        raise ValueError(f\"User Config: {output} - all values must be numeric\") from e\n\n    if not all(isinstance(x, (int | float)) and x &gt; 0 for x in output):\n        raise ValueError(f\"User Config: {output} - all values must be positive numbers\")\n\n    return output\n</code></pre>"},{"location":"api/#aiperfcommonconfigconversation_config","title":"aiperf.common.config.conversation_config","text":""},{"location":"api/#aiperf.common.config.conversation_config.ConversationConfig","title":"<code>ConversationConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining conversations related settings.</p> Source code in <code>aiperf/common/config/conversation_config.py</code> <pre><code>class ConversationConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining conversations related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.CONVERSATION_INPUT\n\n    num: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The total number of unique conversations to generate.\\n\"\n            \"Each conversation represents a single request session between client and server.\\n\"\n            \"Supported on synthetic mode and the custom random_pool dataset. The number of conversations \\n\"\n            \"will be used to determine the number of entries in both the custom random_pool and synthetic \\n\"\n            \"datasets and will be reused until benchmarking is complete.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--conversation-num\",\n                \"--num-conversations\",\n                \"--num-sessions\",  # GenAI-Perf\n                \"--num-dataset-entries\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ConversationDefaults.NUM\n\n    turn: TurnConfig = TurnConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.conversation_config.TurnConfig","title":"<code>TurnConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining turn related settings in a conversation.</p> Source code in <code>aiperf/common/config/conversation_config.py</code> <pre><code>class TurnConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining turn related settings in a conversation.\n    \"\"\"\n\n    _CLI_GROUP = Groups.CONVERSATION_INPUT\n\n    mean: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The mean number of turns within a conversation.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--conversation-turn-mean\",\n                \"--session-turns-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDefaults.MEAN\n\n    stddev: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the number of turns within a conversation.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--conversation-turn-stddev\",\n                \"--session-turns-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDefaults.STDDEV\n\n    delay: TurnDelayConfig = TurnDelayConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.conversation_config.TurnDelayConfig","title":"<code>TurnDelayConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining turn delay related settings.</p> Source code in <code>aiperf/common/config/conversation_config.py</code> <pre><code>class TurnDelayConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining turn delay related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.CONVERSATION_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean delay between turns within a conversation in milliseconds.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--conversation-turn-delay-mean\",\n                \"--session-turn-delay-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDelayDefaults.MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of the delay between turns \\n\"\n            \"within a conversation in milliseconds.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--conversation-turn-delay-stddev\",\n                \"--session-turn-delay-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDelayDefaults.STDDEV\n\n    ratio: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"A ratio to scale multi-turn delays.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--conversation-turn-delay-ratio\",\n                \"--session-delay-ratio\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = TurnDelayDefaults.RATIO\n</code></pre>"},{"location":"api/#aiperfcommonconfigdev_config","title":"aiperf.common.config.dev_config","text":""},{"location":"api/#aiperf.common.config.dev_config.DeveloperConfig","title":"<code>DeveloperConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining developer related settings.</p> <p>NOTE: These settings are only available in developer mode.</p> Source code in <code>aiperf/common/config/dev_config.py</code> <pre><code>class DeveloperConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining developer related settings.\n\n    NOTE: These settings are only available in developer mode.\n    \"\"\"\n\n    enable_yappi: Annotated[\n        bool,\n        Field(\n            description=\"*[Developer use only]* Enable yappi profiling (Yet Another Python Profiler) to profile AIPerf's internal python code. \"\n            \"This can be used in the development of AIPerf in order to find performance bottlenecks across the various services. \"\n            \"The output '.prof' files can be viewed with snakeviz. Requires yappi and snakeviz to be installed. \"\n            \"Run 'pip install yappi snakeviz' to install them.\",\n        ),\n        DeveloperOnlyCLI(\n            name=(\"--enable-yappi-profiling\"),\n        ),\n    ] = DevDefaults.ENABLE_YAPPI\n\n    debug_services: Annotated[\n        set[ServiceType] | None,\n        Field(\n            description=\"*[Developer use only]* List of services to enable debug logging for. Can be a comma-separated list, a single service type, \"\n            \"or the cli flag can be used multiple times.\",\n        ),\n        DeveloperOnlyCLI(\n            name=(\"--debug-service\", \"--debug-services\"),\n        ),\n        BeforeValidator(parse_service_types),\n    ] = DevDefaults.DEBUG_SERVICES\n\n    trace_services: Annotated[\n        set[ServiceType] | None,\n        Field(\n            description=\"*[Developer use only]* List of services to enable trace logging for. Can be a comma-separated list, a single service type, \"\n            \"or the cli flag can be used multiple times.\",\n        ),\n        DeveloperOnlyCLI(\n            name=(\"--trace-service\", \"--trace-services\"),\n        ),\n        BeforeValidator(parse_service_types),\n    ] = DevDefaults.TRACE_SERVICES\n\n    show_internal_metrics: Annotated[\n        bool,\n        Field(\n            description=\"*[Developer use only]* Whether to show internal and hidden metrics in the output\",\n        ),\n        DeveloperOnlyCLI(\n            name=(\"--show-internal-metrics\"),\n        ),\n    ] = DevDefaults.SHOW_INTERNAL_METRICS\n\n    disable_uvloop: Annotated[\n        bool,\n        Field(\n            description=\"*[Developer use only]* Disable the use of uvloop, and use the default asyncio event loop instead.\",\n        ),\n        DeveloperOnlyCLI(\n            name=(\"--disable-uvloop\"),\n        ),\n    ] = DevDefaults.DISABLE_UVLOOP\n</code></pre>"},{"location":"api/#aiperf.common.config.dev_config.print_developer_mode_warning","title":"<code>print_developer_mode_warning()</code>","text":"<p>Print a warning message to the console if developer mode is enabled.</p> Source code in <code>aiperf/common/config/dev_config.py</code> <pre><code>def print_developer_mode_warning() -&gt; None:\n    \"\"\"Print a warning message to the console if developer mode is enabled.\"\"\"\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.text import Text\n\n    console = Console()\n    panel = Panel(\n        Text(\n            \"Developer Mode is active. This is a developer-only feature. Use at your own risk.\",\n            style=\"yellow\",\n        ),\n        title=\"AIPerf Developer Mode\",\n        border_style=\"bold yellow\",\n        title_align=\"left\",\n    )\n    console.print(panel)\n</code></pre>"},{"location":"api/#aiperfcommonconfigendpoint_config","title":"aiperf.common.config.endpoint_config","text":""},{"location":"api/#aiperf.common.config.endpoint_config.EndpointConfig","title":"<code>EndpointConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining endpoint related settings.</p> Source code in <code>aiperf/common/config/endpoint_config.py</code> <pre><code>class EndpointConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining endpoint related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.ENDPOINT\n\n    @model_validator(mode=\"after\")\n    def validate_streaming(self) -&gt; Self:\n        if not self.type.supports_streaming:\n            _logger.warning(\n                f\"Streaming is not supported for --endpoint-type {self.type}, setting streaming to False\"\n            )\n            self.streaming = False\n        return self\n\n    model_names: Annotated[\n        list[str],\n        Field(\n            ...,  # This must be set by the user\n            description=\"Model name(s) to be benchmarked. Can be a comma-separated list or a single model name.\",\n        ),\n        BeforeValidator(parse_str_or_list),\n        CLIParameter(\n            name=(\n                \"--model-names\",\n                \"--model\",  # GenAI-Perf\n                \"-m\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ]\n\n    model_selection_strategy: Annotated[\n        ModelSelectionStrategy,\n        Field(\n            description=\"When multiple models are specified, this is how a specific model should be assigned to a prompt.\\n\"\n            \"round_robin: nth prompt in the list gets assigned to n-mod len(models).\\n\"\n            \"random: assignment is uniformly random\",\n        ),\n        CLIParameter(\n            name=(\n                \"--model-selection-strategy\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndpointDefaults.MODEL_SELECTION_STRATEGY\n\n    custom_endpoint: Annotated[\n        str | None,\n        Field(\n            description=\"Set a custom endpoint that differs from the OpenAI defaults.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--custom-endpoint\",\n                \"--endpoint\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndpointDefaults.CUSTOM_ENDPOINT\n\n    type: Annotated[\n        EndpointType,\n        Field(\n            description=\"The endpoint type to send requests to on the server.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--endpoint-type\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n            converter=custom_enum_converter,\n        ),\n    ] = EndpointDefaults.TYPE\n\n    streaming: Annotated[\n        bool,\n        Field(\n            description=\"An option to enable the use of the streaming API.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--streaming\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndpointDefaults.STREAMING\n\n    url: Annotated[\n        str,\n        Field(\n            description=\"URL of the endpoint to target for benchmarking.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--url\",  # GenAI-Perf\n                \"-u\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = EndpointDefaults.URL\n\n    # NEW AIPerf Option\n    timeout_seconds: Annotated[\n        float,\n        Field(\n            description=\"The timeout in floating points seconds for each request to the endpoint.\",\n        ),\n        CLIParameter(\n            name=(\"--request-timeout-seconds\"),\n            group=_CLI_GROUP,\n        ),\n    ] = EndpointDefaults.TIMEOUT\n\n    # NEW AIPerf Option\n    api_key: Annotated[\n        str | None,\n        Field(\n            description=\"The API key to use for the endpoint. If provided, it will be sent with every request as \"\n            \"a header: `Authorization: Bearer &lt;api_key&gt;`.\",\n        ),\n        CLIParameter(\n            name=(\"--api-key\"),\n            group=_CLI_GROUP,\n        ),\n    ] = EndpointDefaults.API_KEY\n</code></pre>"},{"location":"api/#aiperfcommonconfiggroups","title":"aiperf.common.config.groups","text":""},{"location":"api/#aiperf.common.config.groups.Groups","title":"<code>Groups</code>","text":"<p>Groups for the CLI.</p> <p>NOTE: The order of these groups are the order they will be displayed in the help text.</p> Source code in <code>aiperf/common/config/groups.py</code> <pre><code>class Groups:\n    \"\"\"Groups for the CLI.\n\n    NOTE: The order of these groups are the order they will be displayed in the help text.\n    \"\"\"\n\n    ENDPOINT = Group.create_ordered(\"Endpoint\")\n    INPUT = Group.create_ordered(\"Input\")\n    OUTPUT = Group.create_ordered(\"Output\")\n    TOKENIZER = Group.create_ordered(\"Tokenizer\")\n    LOAD_GENERATOR = Group.create_ordered(\"Load Generator\")\n    CONVERSATION_INPUT = Group.create_ordered(\"Conversation Input\")\n    INPUT_SEQUENCE_LENGTH = Group.create_ordered(\"Input Sequence Length (ISL)\")\n    OUTPUT_SEQUENCE_LENGTH = Group.create_ordered(\"Output Sequence Length (OSL)\")\n    PROMPT = Group.create_ordered(\"Prompt\")\n    PREFIX_PROMPT = Group.create_ordered(\"Prefix Prompt\")\n    AUDIO_INPUT = Group.create_ordered(\"Audio Input\")\n    IMAGE_INPUT = Group.create_ordered(\"Image Input\")\n    SERVICE = Group.create_ordered(\"Service\")\n    UI = Group.create_ordered(\"UI\")\n    WORKERS = Group.create_ordered(\"Workers\")\n    DEVELOPER = Group.create_ordered(\"Developer\")\n    ZMQ_COMMUNICATION = Group.create_ordered(\"ZMQ Communication\")\n</code></pre>"},{"location":"api/#aiperfcommonconfigimage_config","title":"aiperf.common.config.image_config","text":""},{"location":"api/#aiperf.common.config.image_config.ImageConfig","title":"<code>ImageConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining image related settings.</p> Source code in <code>aiperf/common/config/image_config.py</code> <pre><code>class ImageConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining image related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.IMAGE_INPUT\n\n    width: ImageWidthConfig = ImageWidthConfig()\n    height: ImageHeightConfig = ImageHeightConfig()\n    batch_size: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The image batch size of the requests AIPerf should send.\\n\"\n            \"This is currently supported with the image retrieval endpoint type.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--image-batch-size\",\n                \"--batch-size-image\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.BATCH_SIZE\n\n    format: Annotated[\n        ImageFormat,\n        Field(\n            description=\"The compression format of the images.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--image-format\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.FORMAT\n</code></pre>"},{"location":"api/#aiperf.common.config.image_config.ImageHeightConfig","title":"<code>ImageHeightConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining image height related settings.</p> Source code in <code>aiperf/common/config/image_config.py</code> <pre><code>class ImageHeightConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining image height related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.IMAGE_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean height of images when generating synthetic image data.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--image-height-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.HEIGHT_MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of height of images when generating synthetic image data.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--image-height-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.HEIGHT_STDDEV\n</code></pre>"},{"location":"api/#aiperf.common.config.image_config.ImageWidthConfig","title":"<code>ImageWidthConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining image width related settings.</p> Source code in <code>aiperf/common/config/image_config.py</code> <pre><code>class ImageWidthConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining image width related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.IMAGE_INPUT\n\n    mean: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The mean width of images when generating synthetic image data.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--image-width-mean\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.WIDTH_MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of width of images when generating synthetic image data.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--image-width-stddev\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = ImageDefaults.WIDTH_STDDEV\n</code></pre>"},{"location":"api/#aiperfcommonconfiginput_config","title":"aiperf.common.config.input_config","text":""},{"location":"api/#aiperf.common.config.input_config.InputConfig","title":"<code>InputConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining input related settings.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>class InputConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining input related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.INPUT\n\n    @model_validator(mode=\"after\")\n    def validate_fixed_schedule(self) -&gt; Self:\n        \"\"\"Validate the fixed schedule configuration.\"\"\"\n        if self.fixed_schedule and self.file is None:\n            raise ValueError(\"Fixed schedule requires a file to be provided\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_fixed_schedule_start_offset(self) -&gt; Self:\n        \"\"\"Validate the fixed schedule start offset configuration.\"\"\"\n        if (\n            self.fixed_schedule_start_offset is not None\n            and self.fixed_schedule_auto_offset\n        ):\n            raise ValueError(\n                \"The --fixed-schedule-start-offset and --fixed-schedule-auto-offset options cannot be used together\"\n            )\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_fixed_schedule_start_and_end_offset(self) -&gt; Self:\n        \"\"\"Validate the fixed schedule start and end offset configuration.\"\"\"\n        if (\n            self.fixed_schedule_start_offset is not None\n            and self.fixed_schedule_end_offset is not None\n            and self.fixed_schedule_start_offset &gt; self.fixed_schedule_end_offset\n        ):\n            raise ValueError(\n                \"The --fixed-schedule-start-offset must be less than or equal to the --fixed-schedule-end-offset\"\n            )\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_dataset_type(self) -&gt; Self:\n        \"\"\"Validate the different dataset type configuration.\"\"\"\n        if self.public_dataset is not None and self.custom_dataset_type is not None:\n            raise ValueError(\n                \"The --public-dataset and --custom-dataset-type options cannot be set together\"\n            )\n        return self\n\n    extra: Annotated[\n        Any,\n        Field(\n            description=\"Provide additional inputs to include with every request.\\n\"\n            \"Inputs should be in an 'input_name:value' format.\\n\"\n            \"Alternatively, a string representing a json formatted dict can be provided.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--extra-inputs\",  # GenAI-Perf\n            ),\n            consume_multiple=True,\n            group=_CLI_GROUP,\n        ),\n        BeforeValidator(parse_str_or_dict_as_tuple_list),\n    ] = InputDefaults.EXTRA\n\n    headers: Annotated[\n        Any,\n        Field(\n            description=\"Adds a custom header to the requests.\\n\"\n            \"Headers must be specified as 'Header:Value' pairs.\\n\"\n            \"Alternatively, a string representing a json formatted dict can be provided.\",\n        ),\n        BeforeValidator(parse_str_or_dict_as_tuple_list),\n        CLIParameter(\n            name=(\n                \"--header\",  # GenAI-Perf\n                \"-H\",  # GenAI-Perf\n            ),\n            consume_multiple=True,\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.HEADERS\n\n    file: Annotated[\n        Any,\n        Field(\n            description=\"The file or directory path that contains the dataset to use for profiling.\\n\"\n            \"This parameter is used in conjunction with the `custom_dataset_type` parameter\\n\"\n            \"to support different types of user provided datasets.\",\n        ),\n        BeforeValidator(parse_file),\n        CLIParameter(\n            name=(\n                \"--input-file\",  # GenAI-Perf,\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FILE\n\n    fixed_schedule: Annotated[\n        bool,\n        Field(\n            description=\"Specifies to run a fixed schedule of requests. This is normally inferred from the --input-file parameter, but can be set manually here.\"\n        ),\n        CLIParameter(\n            name=(\n                \"--fixed-schedule\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FIXED_SCHEDULE\n\n    # NEW AIPerf Option\n    fixed_schedule_auto_offset: Annotated[\n        bool,\n        Field(\n            description=\"Specifies to automatically offset the timestamps in the fixed schedule, such that the first \"\n            \"timestamp is considered 0, and the rest are shifted accordingly. If disabled, the timestamps will be \"\n            \"assumed to be relative to 0.\"\n        ),\n        CLIParameter(\n            name=(\"--fixed-schedule-auto-offset\",),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FIXED_SCHEDULE_AUTO_OFFSET\n\n    # NEW AIPerf Option\n    fixed_schedule_start_offset: Annotated[\n        int | None,\n        Field(\n            ge=0,\n            description=\"Specifies the offset in milliseconds to start the fixed schedule at. By default, the schedule \"\n            \"starts at 0, but this option can be used to start at a reference point further in the schedule. This \"\n            \"option cannot be used in conjunction with the --fixed-schedule-auto-offset. The schedule will include \"\n            \"any requests at the start offset.\",\n        ),\n        CLIParameter(\n            name=(\"--fixed-schedule-start-offset\",),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FIXED_SCHEDULE_START_OFFSET\n\n    # NEW AIPerf Option\n    fixed_schedule_end_offset: Annotated[\n        int | None,\n        Field(\n            ge=0,\n            description=\"Specifies the offset in milliseconds to end the fixed schedule at. By default, the schedule \"\n            \"ends at the last timestamp in the trace dataset, but this option can be used to only run a subset of the trace. \"\n            \"The schedule will include any requests at the end offset.\",\n        ),\n        CLIParameter(\n            name=(\"--fixed-schedule-end-offset\",),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.FIXED_SCHEDULE_END_OFFSET\n\n    public_dataset: Annotated[\n        PublicDatasetType | None,\n        Field(description=\"The public dataset to use for the requests.\"),\n        CLIParameter(\n            name=(\"--public-dataset\"),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.PUBLIC_DATASET\n\n    # NEW AIPerf Option\n    custom_dataset_type: Annotated[\n        CustomDatasetType | None,\n        Field(\n            description=\"The type of custom dataset to use.\\n\"\n            \"This parameter is used in conjunction with the --input-file parameter.\",\n        ),\n        CLIParameter(\n            name=(\"--custom-dataset-type\"),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.CUSTOM_DATASET_TYPE\n\n    random_seed: Annotated[\n        int | None,\n        Field(\n            default=None,\n            description=\"The seed used to generate random values.\\n\"\n            \"Set to some value to make the synthetic data generation deterministic.\\n\"\n            \"It will use system default if not provided.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--random-seed\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputDefaults.RANDOM_SEED\n\n    audio: AudioConfig = AudioConfig()\n    image: ImageConfig = ImageConfig()\n    prompt: PromptConfig = PromptConfig()\n    conversation: ConversationConfig = ConversationConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.input_config.InputConfig.validate_dataset_type","title":"<code>validate_dataset_type()</code>","text":"<p>Validate the different dataset type configuration.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_dataset_type(self) -&gt; Self:\n    \"\"\"Validate the different dataset type configuration.\"\"\"\n    if self.public_dataset is not None and self.custom_dataset_type is not None:\n        raise ValueError(\n            \"The --public-dataset and --custom-dataset-type options cannot be set together\"\n        )\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.input_config.InputConfig.validate_fixed_schedule","title":"<code>validate_fixed_schedule()</code>","text":"<p>Validate the fixed schedule configuration.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_fixed_schedule(self) -&gt; Self:\n    \"\"\"Validate the fixed schedule configuration.\"\"\"\n    if self.fixed_schedule and self.file is None:\n        raise ValueError(\"Fixed schedule requires a file to be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.input_config.InputConfig.validate_fixed_schedule_start_and_end_offset","title":"<code>validate_fixed_schedule_start_and_end_offset()</code>","text":"<p>Validate the fixed schedule start and end offset configuration.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_fixed_schedule_start_and_end_offset(self) -&gt; Self:\n    \"\"\"Validate the fixed schedule start and end offset configuration.\"\"\"\n    if (\n        self.fixed_schedule_start_offset is not None\n        and self.fixed_schedule_end_offset is not None\n        and self.fixed_schedule_start_offset &gt; self.fixed_schedule_end_offset\n    ):\n        raise ValueError(\n            \"The --fixed-schedule-start-offset must be less than or equal to the --fixed-schedule-end-offset\"\n        )\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.input_config.InputConfig.validate_fixed_schedule_start_offset","title":"<code>validate_fixed_schedule_start_offset()</code>","text":"<p>Validate the fixed schedule start offset configuration.</p> Source code in <code>aiperf/common/config/input_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_fixed_schedule_start_offset(self) -&gt; Self:\n    \"\"\"Validate the fixed schedule start offset configuration.\"\"\"\n    if (\n        self.fixed_schedule_start_offset is not None\n        and self.fixed_schedule_auto_offset\n    ):\n        raise ValueError(\n            \"The --fixed-schedule-start-offset and --fixed-schedule-auto-offset options cannot be used together\"\n        )\n    return self\n</code></pre>"},{"location":"api/#aiperfcommonconfigloader","title":"aiperf.common.config.loader","text":""},{"location":"api/#aiperf.common.config.loader.load_service_config","title":"<code>load_service_config()</code>","text":"<p>Load the service configuration.</p> Source code in <code>aiperf/common/config/loader.py</code> <pre><code>def load_service_config() -&gt; ServiceConfig:\n    \"\"\"Load the service configuration.\"\"\"\n    # TODO: implement\n    return ServiceConfig()\n</code></pre>"},{"location":"api/#aiperf.common.config.loader.load_user_config","title":"<code>load_user_config()</code>","text":"<p>Load the user configuration.</p> Source code in <code>aiperf/common/config/loader.py</code> <pre><code>def load_user_config() -&gt; UserConfig:\n    \"\"\"Load the user configuration.\"\"\"\n    # TODO: implement\n    raise NotImplementedError(\"User configuration is not implemented\")\n</code></pre>"},{"location":"api/#aiperfcommonconfigloadgen_config","title":"aiperf.common.config.loadgen_config","text":""},{"location":"api/#aiperf.common.config.loadgen_config.LoadGeneratorConfig","title":"<code>LoadGeneratorConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining top-level load generator settings.</p> Source code in <code>aiperf/common/config/loadgen_config.py</code> <pre><code>class LoadGeneratorConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining top-level load generator settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.LOAD_GENERATOR\n\n    # NEW AIPerf Option\n    benchmark_duration: Annotated[\n        float | None,\n        Field(\n            ge=1,\n            description=\"The duration in seconds for benchmarking.\",\n        ),\n        CLIParameter(\n            name=(\"--benchmark-duration\",),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.BENCHMARK_DURATION\n\n    # TODO: Potentially add a validator to ensure that the concurrency is not greater than the request count\n    concurrency: Annotated[\n        int | None,\n        Field(\n            ge=1,\n            description=\"The concurrency value to benchmark.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--concurrency\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.CONCURRENCY\n\n    request_rate: Annotated[\n        float | None,\n        Field(\n            gt=0,\n            description=\"Sets the request rate for the load generated by AIPerf. Unit: requests/second\",\n        ),\n        CLIParameter(\n            name=(\n                \"--request-rate\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.REQUEST_RATE\n\n    # NEW AIPerf Option\n    request_rate_mode: Annotated[\n        RequestRateMode,\n        Field(\n            description=\"Sets the request rate mode for the load generated by AIPerf. Valid values: constant, poisson.\\n\"\n            \"constant: Generate requests at a fixed rate.\\n\"\n            \"poisson: Generate requests using a poisson distribution.\"\n        ),\n        CLIParameter(\n            name=(\"--request-rate-mode\"),\n            group=_CLI_GROUP,\n            show_choices=False,\n        ),\n    ] = LoadGeneratorDefaults.REQUEST_RATE_MODE\n\n    request_count: Annotated[\n        int,\n        Field(\n            ge=1,\n            description=\"The number of requests to use for measurement.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--request-count\",  # GenAI-Perf\n                \"--num-requests\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.REQUEST_COUNT\n\n    warmup_request_count: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The number of warmup requests to send before benchmarking.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--warmup-request-count\",  # GenAI-Perf\n                \"--num-warmup-requests\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = LoadGeneratorDefaults.WARMUP_REQUEST_COUNT\n</code></pre>"},{"location":"api/#aiperfcommonconfigoutput_config","title":"aiperf.common.config.output_config","text":""},{"location":"api/#aiperf.common.config.output_config.OutputConfig","title":"<code>OutputConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining output related settings.</p> Source code in <code>aiperf/common/config/output_config.py</code> <pre><code>class OutputConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining output related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.OUTPUT\n\n    artifact_directory: Annotated[\n        Path,\n        Field(\n            description=\"The directory to store all the (output) artifacts generated by AIPerf.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--output-artifact-dir\",\n                \"--artifact-dir\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = OutputDefaults.ARTIFACT_DIRECTORY\n</code></pre>"},{"location":"api/#aiperfcommonconfigprompt_config","title":"aiperf.common.config.prompt_config","text":""},{"location":"api/#aiperf.common.config.prompt_config.InputTokensConfig","title":"<code>InputTokensConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining input token related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class InputTokensConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining input token related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.INPUT_SEQUENCE_LENGTH\n\n    mean: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=\"The mean of number of tokens in the generated prompts when using synthetic data.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-input-tokens-mean\",\n                \"--synthetic-input-tokens-mean\",  # GenAI-Perf\n                \"--isl\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputTokensDefaults.MEAN\n\n    stddev: Annotated[\n        float,\n        Field(\n            ge=0,\n            description=\"The standard deviation of number of tokens in the generated prompts when using synthetic data.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-input-tokens-stddev\",\n                \"--synthetic-input-tokens-stddev\",  # GenAI-Perf\n                \"--isl-stddev\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputTokensDefaults.STDDEV\n\n    # NEW AIPerf Option\n    block_size: Annotated[\n        int,\n        Field(\n            default=512,\n            description=\"The block size of the prompt.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-input-tokens-block-size\",\n                \"--synthetic-input-tokens-block-size\",\n                \"--isl-block-size\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = InputTokensDefaults.BLOCK_SIZE\n</code></pre>"},{"location":"api/#aiperf.common.config.prompt_config.OutputTokensConfig","title":"<code>OutputTokensConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining output token related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class OutputTokensConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining output token related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.OUTPUT_SEQUENCE_LENGTH\n\n    mean: Annotated[\n        int | None,\n        Field(\n            default=None,\n            ge=0,\n            description=\"The mean number of tokens in each output.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-output-tokens-mean\",\n                \"--output-tokens-mean\",  # GenAI-Perf\n                \"--osl\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = None\n\n    stddev: Annotated[\n        float | None,\n        Field(\n            default=None,\n            ge=0,\n            description=\"The standard deviation of the number of tokens in each output.\",\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-output-tokens-stddev\",\n                \"--output-tokens-stddev\",  # GenAI-Perf\n                \"--osl-stddev\",\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = OutputTokensDefaults.STDDEV\n</code></pre>"},{"location":"api/#aiperf.common.config.prompt_config.PrefixPromptConfig","title":"<code>PrefixPromptConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining prefix prompt related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class PrefixPromptConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining prefix prompt related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.PREFIX_PROMPT\n\n    pool_size: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=(\n                \"The total size of the prefix prompt pool to select prefixes from.\\n\"\n                \"If this value is not zero, these are prompts that are prepended to input prompts.\\n\"\n                \"This is useful for benchmarking models that use a K-V cache.\"\n            ),\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-prefix-pool-size\",\n                \"--prefix-prompt-pool-size\",\n                \"--num-prefix-prompts\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = PrefixPromptDefaults.POOL_SIZE\n\n    length: Annotated[\n        int,\n        Field(\n            ge=0,\n            description=(\n                \"The number of tokens in each prefix prompt.\\n\"\n                'This is only used if \"num\" is greater than zero.\\n'\n                \"Note that due to the prefix and user prompts being concatenated,\\n\"\n                \"the number of tokens in the final prompt may be off by one.\"\n            ),\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-prefix-length\",\n                \"--prefix-prompt-length\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = PrefixPromptDefaults.LENGTH\n</code></pre>"},{"location":"api/#aiperf.common.config.prompt_config.PromptConfig","title":"<code>PromptConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining prompt related settings.</p> Source code in <code>aiperf/common/config/prompt_config.py</code> <pre><code>class PromptConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining prompt related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.PROMPT\n\n    batch_size: Annotated[\n        int,\n        Field(\n            description=\"The batch size of text requests AIPerf should send.\\n\"\n            \"This is currently supported with the embeddings and rankings endpoint types\",\n        ),\n        CLIParameter(\n            name=(\n                \"--prompt-batch-size\",\n                \"--batch-size-text\",  # GenAI-Perf\n                \"--batch-size\",  # GenAI-Perf\n                \"-b\",  # GenAI-Perf\n            ),\n            group=_CLI_GROUP,\n        ),\n    ] = PromptDefaults.BATCH_SIZE\n\n    input_tokens: InputTokensConfig = InputTokensConfig()\n    output_tokens: OutputTokensConfig = OutputTokensConfig()\n    prefix_prompt: PrefixPromptConfig = PrefixPromptConfig()\n</code></pre>"},{"location":"api/#aiperfcommonconfigservice_config","title":"aiperf.common.config.service_config","text":""},{"location":"api/#aiperf.common.config.service_config.ServiceConfig","title":"<code>ServiceConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Base configuration for all services. It will be provided to all services during their init function.</p> Source code in <code>aiperf/common/config/service_config.py</code> <pre><code>class ServiceConfig(BaseSettings):\n    \"\"\"Base configuration for all services. It will be provided to all services during their __init__ function.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"AIPERF_\",\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"allow\",\n    )\n\n    _CLI_GROUP = Groups.SERVICE\n    _comm_config: BaseZMQCommunicationConfig | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_log_level_from_verbose_flags(self) -&gt; Self:\n        \"\"\"Set log level based on verbose flags.\"\"\"\n        if self.extra_verbose:\n            self.log_level = AIPerfLogLevel.TRACE\n        elif self.verbose:\n            self.log_level = AIPerfLogLevel.DEBUG\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_comm_config(self) -&gt; Self:\n        \"\"\"Initialize the comm_config based on the zmq_tcp or zmq_ipc config.\"\"\"\n        _logger.debug(\n            f\"Validating comm_config: tcp: {self.zmq_tcp}, ipc: {self.zmq_ipc}\"\n        )\n        if self.zmq_tcp is not None and self.zmq_ipc is not None:\n            raise ValueError(\n                \"Cannot use both ZMQ TCP and ZMQ IPC configuration at the same time\"\n            )\n        elif self.zmq_tcp is not None:\n            _logger.info(\"Using ZMQ TCP configuration\")\n            self._comm_config = self.zmq_tcp\n        elif self.zmq_ipc is not None:\n            _logger.info(\"Using ZMQ IPC configuration\")\n            self._comm_config = self.zmq_ipc\n        else:\n            _logger.info(\"Using default ZMQ IPC configuration\")\n            self._comm_config = ZMQIPCConfig()\n        return self\n\n    service_run_type: Annotated[\n        ServiceRunType,\n        Field(\n            description=\"Type of service run (process, k8s)\",\n        ),\n        DisableCLI(reason=\"Only single support for now\"),\n    ] = ServiceDefaults.SERVICE_RUN_TYPE\n\n    zmq_tcp: Annotated[\n        ZMQTCPConfig | None,\n        Field(\n            description=\"ZMQ TCP configuration\",\n        ),\n    ] = None\n\n    zmq_ipc: Annotated[\n        ZMQIPCConfig | None,\n        Field(\n            description=\"ZMQ IPC configuration\",\n        ),\n    ] = None\n\n    workers: Annotated[\n        WorkersConfig,\n        Field(\n            description=\"Worker configuration\",\n        ),\n    ] = WorkersConfig()\n\n    log_level: Annotated[\n        AIPerfLogLevel,\n        Field(\n            description=\"Logging level\",\n        ),\n        CLIParameter(\n            name=(\"--log-level\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.LOG_LEVEL\n\n    verbose: Annotated[\n        bool,\n        Field(\n            description=\"Equivalent to --log-level DEBUG. Enables more verbose logging output, but lacks some raw message logging.\",\n            json_schema_extra={ADD_TO_TEMPLATE: False},\n        ),\n        CLIParameter(\n            name=(\"--verbose\", \"-v\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.VERBOSE\n\n    extra_verbose: Annotated[\n        bool,\n        Field(\n            description=\"Equivalent to --log-level TRACE. Enables the most verbose logging output possible.\",\n            json_schema_extra={ADD_TO_TEMPLATE: False},\n        ),\n        CLIParameter(\n            name=(\"--extra-verbose\", \"-vv\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.EXTRA_VERBOSE\n\n    record_processor_service_count: Annotated[\n        int | None,\n        Field(\n            ge=1,\n            description=\"Number of services to spawn for processing records. The higher the request rate, the more services \"\n            \"should be spawned in order to keep up with the incoming records. If not specified, the number of services will be \"\n            \"automatically determined based on the worker count.\",\n        ),\n        CLIParameter(\n            name=(\"--record-processor-service-count\", \"--record-processors\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.RECORD_PROCESSOR_SERVICE_COUNT\n\n    ui_type: Annotated[\n        AIPerfUIType,\n        Field(\n            description=\"Type of UI to use\",\n        ),\n        CLIParameter(\n            name=(\"--ui-type\", \"--ui\"),\n            group=_CLI_GROUP,\n        ),\n    ] = ServiceDefaults.UI_TYPE\n\n    developer: DeveloperConfig = DeveloperConfig()\n\n    @property\n    def comm_config(self) -&gt; BaseZMQCommunicationConfig:\n        \"\"\"Get the communication configuration.\"\"\"\n        if not self._comm_config:\n            raise ValueError(\n                \"Communication configuration is not set. Please provide a valid configuration.\"\n            )\n        return self._comm_config\n</code></pre>"},{"location":"api/#aiperf.common.config.service_config.ServiceConfig.comm_config","title":"<code>comm_config</code>  <code>property</code>","text":"<p>Get the communication configuration.</p>"},{"location":"api/#aiperf.common.config.service_config.ServiceConfig.validate_comm_config","title":"<code>validate_comm_config()</code>","text":"<p>Initialize the comm_config based on the zmq_tcp or zmq_ipc config.</p> Source code in <code>aiperf/common/config/service_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_comm_config(self) -&gt; Self:\n    \"\"\"Initialize the comm_config based on the zmq_tcp or zmq_ipc config.\"\"\"\n    _logger.debug(\n        f\"Validating comm_config: tcp: {self.zmq_tcp}, ipc: {self.zmq_ipc}\"\n    )\n    if self.zmq_tcp is not None and self.zmq_ipc is not None:\n        raise ValueError(\n            \"Cannot use both ZMQ TCP and ZMQ IPC configuration at the same time\"\n        )\n    elif self.zmq_tcp is not None:\n        _logger.info(\"Using ZMQ TCP configuration\")\n        self._comm_config = self.zmq_tcp\n    elif self.zmq_ipc is not None:\n        _logger.info(\"Using ZMQ IPC configuration\")\n        self._comm_config = self.zmq_ipc\n    else:\n        _logger.info(\"Using default ZMQ IPC configuration\")\n        self._comm_config = ZMQIPCConfig()\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.service_config.ServiceConfig.validate_log_level_from_verbose_flags","title":"<code>validate_log_level_from_verbose_flags()</code>","text":"<p>Set log level based on verbose flags.</p> Source code in <code>aiperf/common/config/service_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_log_level_from_verbose_flags(self) -&gt; Self:\n    \"\"\"Set log level based on verbose flags.\"\"\"\n    if self.extra_verbose:\n        self.log_level = AIPerfLogLevel.TRACE\n    elif self.verbose:\n        self.log_level = AIPerfLogLevel.DEBUG\n    return self\n</code></pre>"},{"location":"api/#aiperfcommonconfigtokenizer_config","title":"aiperf.common.config.tokenizer_config","text":""},{"location":"api/#aiperf.common.config.tokenizer_config.TokenizerConfig","title":"<code>TokenizerConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining tokenizer related settings.</p> Source code in <code>aiperf/common/config/tokenizer_config.py</code> <pre><code>class TokenizerConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining tokenizer related settings.\n    \"\"\"\n\n    _CLI_GROUP = Groups.TOKENIZER\n\n    name: Annotated[\n        str | None,\n        Field(\n            description=(\n                \"The HuggingFace tokenizer to use to interpret token metrics \"\n                \"from prompts and responses.\\nThe value can be the \"\n                \"name of a tokenizer or the filepath of the tokenizer.\\n\"\n                \"The default value is the model name.\"\n            ),\n        ),\n        CLIParameter(\n            name=(\"--tokenizer\"),\n            group=_CLI_GROUP,\n        ),\n    ] = TokenizerDefaults.NAME\n\n    revision: Annotated[\n        str,\n        Field(\n            description=(\n                \"The specific model version to use.\\n\"\n                \"It can be a branch name, tag name, or commit ID.\"\n            ),\n        ),\n        CLIParameter(\n            name=(\"--tokenizer-revision\"),\n            group=_CLI_GROUP,\n        ),\n    ] = TokenizerDefaults.REVISION\n\n    trust_remote_code: Annotated[\n        bool,\n        Field(\n            description=(\n                \"Allows custom tokenizer to be downloaded and executed.\\n\"\n                \"This carries security risks and should only be used for repositories you trust.\\n\"\n                \"This is only necessary for custom tokenizers stored in HuggingFace Hub.\"\n            ),\n        ),\n        CLIParameter(\n            name=(\"--tokenizer-trust-remote-code\"),\n            group=_CLI_GROUP,\n        ),\n    ] = TokenizerDefaults.TRUST_REMOTE_CODE\n</code></pre>"},{"location":"api/#aiperfcommonconfiguser_config","title":"aiperf.common.config.user_config","text":""},{"location":"api/#aiperf.common.config.user_config.UserConfig","title":"<code>UserConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>A configuration class for defining top-level user settings.</p> Source code in <code>aiperf/common/config/user_config.py</code> <pre><code>class UserConfig(BaseConfig):\n    \"\"\"\n    A configuration class for defining top-level user settings.\n    \"\"\"\n\n    _timing_mode: TimingMode = TimingMode.REQUEST_RATE\n\n    @model_validator(mode=\"after\")\n    def validate_cli_args(self) -&gt; Self:\n        \"\"\"Set the CLI command based on the command line arguments, if it has not already been set.\"\"\"\n        if not self.cli_command:\n            args = [coerce_value(x) for x in sys.argv[1:]]\n            args = [f'\"{x}\"' if _should_quote_arg(x) else str(x) for x in args]\n            self.cli_command = \" \".join([\"aiperf\", *args])\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_timing_mode(self) -&gt; Self:\n        \"\"\"Set the timing mode based on the user config. Will be called after all user config is set.\"\"\"\n        if self.input.fixed_schedule:\n            self._timing_mode = TimingMode.FIXED_SCHEDULE\n        elif self.loadgen.request_rate is not None:\n            # Request rate is checked first, as if user has provided request rate and concurrency,\n            # we will still use the request rate strategy.\n            self._timing_mode = TimingMode.REQUEST_RATE\n            if self.loadgen.request_rate_mode == RequestRateMode.CONCURRENCY_BURST:\n                raise ValueError(\n                    f\"Request rate mode cannot be {RequestRateMode.CONCURRENCY_BURST!r} when a request rate is specified.\"\n                )\n        else:\n            # Default to concurrency burst mode if no request rate or schedule is provided\n            if self.loadgen.concurrency is None:\n                # If user has not provided a concurrency value, set it to 1\n                self.loadgen.concurrency = 1\n            self._timing_mode = TimingMode.REQUEST_RATE\n            self.loadgen.request_rate_mode = RequestRateMode.CONCURRENCY_BURST\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_benchmark_mode(self) -&gt; Self:\n        \"\"\"Validate benchmarking is count-based or timing-based.\"\"\"\n        if (\n            \"benchmark_duration\" in self.loadgen.model_fields_set\n            and \"request_count\" in self.loadgen.model_fields_set\n        ):\n            raise ValueError(\n                \"Count-based and duration-based benchmarking cannot be used together. \"\n                \"Use either --request-count or --benchmark-duration.\"\n            )\n        return self\n\n    endpoint: Annotated[\n        EndpointConfig,\n        Field(\n            description=\"Endpoint configuration\",\n        ),\n    ]\n\n    input: Annotated[\n        InputConfig,\n        Field(\n            description=\"Input configuration\",\n        ),\n    ] = InputConfig()\n\n    output: Annotated[\n        OutputConfig,\n        Field(\n            description=\"Output configuration\",\n        ),\n    ] = OutputConfig()\n\n    tokenizer: Annotated[\n        TokenizerConfig,\n        Field(\n            description=\"Tokenizer configuration\",\n        ),\n    ] = TokenizerConfig()\n\n    loadgen: Annotated[\n        LoadGeneratorConfig,\n        Field(\n            description=\"Load Generator configuration\",\n        ),\n    ] = LoadGeneratorConfig()\n\n    cli_command: Annotated[\n        str | None,\n        Field(\n            default=None,\n            description=\"The CLI command for the user config.\",\n        ),\n        DisableCLI(reason=\"This is automatically set by the CLI\"),\n    ] = None\n\n    @model_validator(mode=\"after\")\n    def _compute_config(self) -&gt; Self:\n        \"\"\"Compute additional configuration.\n\n        This method is automatically called after the model is validated to compute additional configuration.\n        \"\"\"\n\n        if \"artifact_directory\" not in self.output.model_fields_set:\n            self.output.artifact_directory = self._compute_artifact_directory()\n\n        return self\n\n    def _compute_artifact_directory(self) -&gt; Path:\n        \"\"\"Compute the artifact directory based on the user selected options.\"\"\"\n        names: list[str] = [\n            self._get_artifact_model_name(),\n            self._get_artifact_service_kind(),\n            self._get_artifact_stimulus(),\n        ]\n        return self.output.artifact_directory / \"-\".join(names)\n\n    def _get_artifact_model_name(self) -&gt; str:\n        \"\"\"Get the artifact model name based on the user selected options.\"\"\"\n        model_name: str = self.endpoint.model_names[0]\n        if len(self.endpoint.model_names) &gt; 1:\n            model_name = f\"{model_name}_multi\"\n\n        # Preprocess Huggingface model names that include '/' in their model name.\n        if \"/\" in model_name:\n            filtered_name = \"_\".join(model_name.split(\"/\"))\n            from aiperf.common.logging import AIPerfLogger\n\n            _logger = AIPerfLogger(__name__)\n            _logger.info(\n                f\"Model name '{model_name}' cannot be used to create artifact \"\n                f\"directory. Instead, '{filtered_name}' will be used.\"\n            )\n            model_name = filtered_name\n        return model_name\n\n    def _get_artifact_service_kind(self) -&gt; str:\n        \"\"\"Get the service kind name based on the endpoint config.\"\"\"\n        if self.endpoint.type.service_kind == EndpointServiceKind.OPENAI:\n            return f\"{self.endpoint.type.service_kind}-{self.endpoint.type}\"\n        else:\n            raise ValueError(\n                f\"Unknown service kind '{self.endpoint.type.service_kind}'.\"\n            )\n\n    def _get_artifact_stimulus(self) -&gt; str:\n        \"\"\"Get the stimulus name based on the timing mode.\"\"\"\n        match self._timing_mode:\n            case TimingMode.REQUEST_RATE:\n                stimulus = []\n                if self.loadgen.concurrency is not None:\n                    stimulus.append(f\"concurrency{self.loadgen.concurrency}\")\n                if self.loadgen.request_rate is not None:\n                    stimulus.append(f\"request_rate{self.loadgen.request_rate}\")\n                return \"-\".join(stimulus)\n            case TimingMode.FIXED_SCHEDULE:\n                return \"fixed_schedule\"\n            case _:\n                raise ValueError(f\"Unknown timing mode '{self._timing_mode}'.\")\n\n    @property\n    def timing_mode(self) -&gt; TimingMode:\n        \"\"\"Get the timing mode based on the user config.\"\"\"\n        return self._timing_mode\n</code></pre>"},{"location":"api/#aiperf.common.config.user_config.UserConfig.timing_mode","title":"<code>timing_mode</code>  <code>property</code>","text":"<p>Get the timing mode based on the user config.</p>"},{"location":"api/#aiperf.common.config.user_config.UserConfig.validate_benchmark_mode","title":"<code>validate_benchmark_mode()</code>","text":"<p>Validate benchmarking is count-based or timing-based.</p> Source code in <code>aiperf/common/config/user_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_benchmark_mode(self) -&gt; Self:\n    \"\"\"Validate benchmarking is count-based or timing-based.\"\"\"\n    if (\n        \"benchmark_duration\" in self.loadgen.model_fields_set\n        and \"request_count\" in self.loadgen.model_fields_set\n    ):\n        raise ValueError(\n            \"Count-based and duration-based benchmarking cannot be used together. \"\n            \"Use either --request-count or --benchmark-duration.\"\n        )\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.user_config.UserConfig.validate_cli_args","title":"<code>validate_cli_args()</code>","text":"<p>Set the CLI command based on the command line arguments, if it has not already been set.</p> Source code in <code>aiperf/common/config/user_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_cli_args(self) -&gt; Self:\n    \"\"\"Set the CLI command based on the command line arguments, if it has not already been set.\"\"\"\n    if not self.cli_command:\n        args = [coerce_value(x) for x in sys.argv[1:]]\n        args = [f'\"{x}\"' if _should_quote_arg(x) else str(x) for x in args]\n        self.cli_command = \" \".join([\"aiperf\", *args])\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.user_config.UserConfig.validate_timing_mode","title":"<code>validate_timing_mode()</code>","text":"<p>Set the timing mode based on the user config. Will be called after all user config is set.</p> Source code in <code>aiperf/common/config/user_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_timing_mode(self) -&gt; Self:\n    \"\"\"Set the timing mode based on the user config. Will be called after all user config is set.\"\"\"\n    if self.input.fixed_schedule:\n        self._timing_mode = TimingMode.FIXED_SCHEDULE\n    elif self.loadgen.request_rate is not None:\n        # Request rate is checked first, as if user has provided request rate and concurrency,\n        # we will still use the request rate strategy.\n        self._timing_mode = TimingMode.REQUEST_RATE\n        if self.loadgen.request_rate_mode == RequestRateMode.CONCURRENCY_BURST:\n            raise ValueError(\n                f\"Request rate mode cannot be {RequestRateMode.CONCURRENCY_BURST!r} when a request rate is specified.\"\n            )\n    else:\n        # Default to concurrency burst mode if no request rate or schedule is provided\n        if self.loadgen.concurrency is None:\n            # If user has not provided a concurrency value, set it to 1\n            self.loadgen.concurrency = 1\n        self._timing_mode = TimingMode.REQUEST_RATE\n        self.loadgen.request_rate_mode = RequestRateMode.CONCURRENCY_BURST\n    return self\n</code></pre>"},{"location":"api/#aiperfcommonconfigworker_config","title":"aiperf.common.config.worker_config","text":""},{"location":"api/#aiperf.common.config.worker_config.WorkersConfig","title":"<code>WorkersConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Worker configuration.</p> Source code in <code>aiperf/common/config/worker_config.py</code> <pre><code>class WorkersConfig(BaseConfig):\n    \"\"\"Worker configuration.\"\"\"\n\n    _CLI_GROUP = Groups.WORKERS\n\n    min: Annotated[\n        int | None,\n        Field(\n            description=\"Minimum number of workers to maintain\",\n        ),\n        DisableCLI(reason=\"Not currently supported\"),\n    ] = WorkersDefaults.MIN\n\n    max: Annotated[\n        int | None,\n        Field(\n            description=\"Maximum number of workers to create. If not specified, the number of\"\n            \" workers will be determined by the smaller of (concurrency + 1) and (num CPUs - 1).\",\n        ),\n        CLIParameter(\n            name=(\"--workers-max\", \"--max-workers\"),\n            group=_CLI_GROUP,\n        ),\n    ] = WorkersDefaults.MAX\n</code></pre>"},{"location":"api/#aiperfcommonconfigzmq_config","title":"aiperf.common.config.zmq_config","text":""},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig","title":"<code>BaseZMQCommunicationConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Configuration for ZMQ communication.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class BaseZMQCommunicationConfig(BaseModel, ABC):\n    \"\"\"Configuration for ZMQ communication.\"\"\"\n\n    comm_backend: ClassVar[CommunicationBackend]\n\n    # Proxy config options to be overridden by subclasses\n    event_bus_proxy_config: ClassVar[BaseZMQProxyConfig]\n    dataset_manager_proxy_config: ClassVar[BaseZMQProxyConfig]\n    raw_inference_proxy_config: ClassVar[BaseZMQProxyConfig]\n\n    @property\n    @abstractmethod\n    def records_push_pull_address(self) -&gt; str:\n        \"\"\"Get the inference push/pull address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def credit_drop_address(self) -&gt; str:\n        \"\"\"Get the credit drop address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def credit_return_address(self) -&gt; str:\n        \"\"\"Get the credit return address based on protocol configuration.\"\"\"\n\n    def get_address(self, address_type: CommAddress) -&gt; str:\n        \"\"\"Get the actual address based on the address type.\"\"\"\n        address_map = {\n            CommAddress.EVENT_BUS_PROXY_FRONTEND: self.event_bus_proxy_config.frontend_address,\n            CommAddress.EVENT_BUS_PROXY_BACKEND: self.event_bus_proxy_config.backend_address,\n            CommAddress.DATASET_MANAGER_PROXY_FRONTEND: self.dataset_manager_proxy_config.frontend_address,\n            CommAddress.DATASET_MANAGER_PROXY_BACKEND: self.dataset_manager_proxy_config.backend_address,\n            CommAddress.CREDIT_DROP: self.credit_drop_address,\n            CommAddress.CREDIT_RETURN: self.credit_return_address,\n            CommAddress.RECORDS: self.records_push_pull_address,\n            CommAddress.RAW_INFERENCE_PROXY_FRONTEND: self.raw_inference_proxy_config.frontend_address,\n            CommAddress.RAW_INFERENCE_PROXY_BACKEND: self.raw_inference_proxy_config.backend_address,\n        }\n\n        if address_type not in address_map:\n            raise ValueError(f\"Invalid address type: {address_type}\")\n\n        return address_map[address_type]\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.credit_drop_address","title":"<code>credit_drop_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the credit drop address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.credit_return_address","title":"<code>credit_return_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the credit return address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.records_push_pull_address","title":"<code>records_push_pull_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the inference push/pull address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQCommunicationConfig.get_address","title":"<code>get_address(address_type)</code>","text":"<p>Get the actual address based on the address type.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>def get_address(self, address_type: CommAddress) -&gt; str:\n    \"\"\"Get the actual address based on the address type.\"\"\"\n    address_map = {\n        CommAddress.EVENT_BUS_PROXY_FRONTEND: self.event_bus_proxy_config.frontend_address,\n        CommAddress.EVENT_BUS_PROXY_BACKEND: self.event_bus_proxy_config.backend_address,\n        CommAddress.DATASET_MANAGER_PROXY_FRONTEND: self.dataset_manager_proxy_config.frontend_address,\n        CommAddress.DATASET_MANAGER_PROXY_BACKEND: self.dataset_manager_proxy_config.backend_address,\n        CommAddress.CREDIT_DROP: self.credit_drop_address,\n        CommAddress.CREDIT_RETURN: self.credit_return_address,\n        CommAddress.RECORDS: self.records_push_pull_address,\n        CommAddress.RAW_INFERENCE_PROXY_FRONTEND: self.raw_inference_proxy_config.frontend_address,\n        CommAddress.RAW_INFERENCE_PROXY_BACKEND: self.raw_inference_proxy_config.backend_address,\n    }\n\n    if address_type not in address_map:\n        raise ValueError(f\"Invalid address type: {address_type}\")\n\n    return address_map[address_type]\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig","title":"<code>BaseZMQProxyConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Configuration Protocol for ZMQ Proxy.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class BaseZMQProxyConfig(BaseModel, ABC):\n    \"\"\"Configuration Protocol for ZMQ Proxy.\"\"\"\n\n    @property\n    @abstractmethod\n    def frontend_address(self) -&gt; str:\n        \"\"\"Get the frontend address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def backend_address(self) -&gt; str:\n        \"\"\"Get the backend address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def control_address(self) -&gt; str | None:\n        \"\"\"Get the control address based on protocol configuration.\"\"\"\n\n    @property\n    @abstractmethod\n    def capture_address(self) -&gt; str | None:\n        \"\"\"Get the capture address based on protocol configuration.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.backend_address","title":"<code>backend_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the backend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.capture_address","title":"<code>capture_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the capture address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.control_address","title":"<code>control_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the control address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.BaseZMQProxyConfig.frontend_address","title":"<code>frontend_address</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the frontend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig","title":"<code>ZMQIPCConfig</code>","text":"<p>               Bases: <code>BaseZMQCommunicationConfig</code></p> <p>Configuration for IPC transport.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQIPCConfig(BaseZMQCommunicationConfig):\n    \"\"\"Configuration for IPC transport.\"\"\"\n\n    _CLI_GROUP = Groups.ZMQ_COMMUNICATION\n    comm_backend: ClassVar[CommunicationBackend] = CommunicationBackend.ZMQ_IPC\n\n    @model_validator(mode=\"after\")\n    def validate_path(self) -&gt; Self:\n        \"\"\"Validate provided path or create a temporary path for IPC sockets.\"\"\"\n        if self.path is None:\n            self.path = Path(tempfile.mkdtemp()) / \"aiperf\"\n        if not self.path.exists():\n            self.path.mkdir(parents=True, exist_ok=True)\n        for proxy_config in [\n            self.dataset_manager_proxy_config,\n            self.event_bus_proxy_config,\n            self.raw_inference_proxy_config,\n        ]:\n            if proxy_config.path is None:\n                proxy_config.path = self.path\n        return self\n\n    path: Annotated[\n        Path | None,\n        Field(\n            description=\"Path for IPC sockets\",\n        ),\n        CLIParameter(\n            name=(\"--zmq-ipc-path\"),\n            group=_CLI_GROUP,\n        ),\n    ] = None\n\n    dataset_manager_proxy_config: ZMQIPCProxyConfig = Field(  # type: ignore\n        default=ZMQIPCProxyConfig(name=\"dataset_manager_proxy\"),\n        description=\"Configuration for the ZMQ Dealer Router Proxy. If provided, the proxy will be created and started.\",\n    )\n    event_bus_proxy_config: ZMQIPCProxyConfig = Field(  # type: ignore\n        default=ZMQIPCProxyConfig(name=\"event_bus_proxy\"),\n        description=\"Configuration for the ZMQ XPUB/XSUB Proxy. If provided, the proxy will be created and started.\",\n    )\n    raw_inference_proxy_config: ZMQIPCProxyConfig = Field(  # type: ignore\n        default=ZMQIPCProxyConfig(name=\"raw_inference_proxy\"),\n        description=\"Configuration for the ZMQ Push/Pull Proxy. If provided, the proxy will be created and started.\",\n    )\n\n    @property\n    def records_push_pull_address(self) -&gt; str:\n        \"\"\"Get the records push/pull address based on protocol configuration.\"\"\"\n        if not self.path:\n            raise ValueError(\"Path is required for IPC transport\")\n        return f\"ipc://{self.path / 'records_push_pull.ipc'}\"\n\n    @property\n    def credit_drop_address(self) -&gt; str:\n        \"\"\"Get the credit drop address based on protocol configuration.\"\"\"\n        if not self.path:\n            raise ValueError(\"Path is required for IPC transport\")\n        return f\"ipc://{self.path / 'credit_drop.ipc'}\"\n\n    @property\n    def credit_return_address(self) -&gt; str:\n        \"\"\"Get the credit return address based on protocol configuration.\"\"\"\n        if not self.path:\n            raise ValueError(\"Path is required for IPC transport\")\n        return f\"ipc://{self.path / 'credit_return.ipc'}\"\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.credit_drop_address","title":"<code>credit_drop_address</code>  <code>property</code>","text":"<p>Get the credit drop address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.credit_return_address","title":"<code>credit_return_address</code>  <code>property</code>","text":"<p>Get the credit return address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.records_push_pull_address","title":"<code>records_push_pull_address</code>  <code>property</code>","text":"<p>Get the records push/pull address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCConfig.validate_path","title":"<code>validate_path()</code>","text":"<p>Validate provided path or create a temporary path for IPC sockets.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_path(self) -&gt; Self:\n    \"\"\"Validate provided path or create a temporary path for IPC sockets.\"\"\"\n    if self.path is None:\n        self.path = Path(tempfile.mkdtemp()) / \"aiperf\"\n    if not self.path.exists():\n        self.path.mkdir(parents=True, exist_ok=True)\n    for proxy_config in [\n        self.dataset_manager_proxy_config,\n        self.event_bus_proxy_config,\n        self.raw_inference_proxy_config,\n    ]:\n        if proxy_config.path is None:\n            proxy_config.path = self.path\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig","title":"<code>ZMQIPCProxyConfig</code>","text":"<p>               Bases: <code>BaseZMQProxyConfig</code></p> <p>Configuration for IPC proxy.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQIPCProxyConfig(BaseZMQProxyConfig):\n    \"\"\"Configuration for IPC proxy.\"\"\"\n\n    path: Path | None = Field(default=None, description=\"Path for IPC sockets\")\n    name: str = Field(default=\"proxy\", description=\"Name for IPC sockets\")\n    enable_control: bool = Field(default=False, description=\"Enable control socket\")\n    enable_capture: bool = Field(default=False, description=\"Enable capture socket\")\n\n    @property\n    def frontend_address(self) -&gt; str:\n        \"\"\"Get the frontend address based on protocol configuration.\"\"\"\n        if self.path is None:\n            raise ValueError(\"Path is required for IPC transport\")\n        return f\"ipc://{self.path / self.name}_frontend.ipc\"\n\n    @property\n    def backend_address(self) -&gt; str:\n        \"\"\"Get the backend address based on protocol configuration.\"\"\"\n        if self.path is None:\n            raise ValueError(\"Path is required for IPC transport\")\n        return f\"ipc://{self.path / self.name}_backend.ipc\"\n\n    @property\n    def control_address(self) -&gt; str | None:\n        \"\"\"Get the control address based on protocol configuration.\"\"\"\n        if self.path is None:\n            raise ValueError(\"Path is required for IPC transport\")\n        return (\n            f\"ipc://{self.path / self.name}_control.ipc\"\n            if self.enable_control\n            else None\n        )\n\n    @property\n    def capture_address(self) -&gt; str | None:\n        \"\"\"Get the capture address based on protocol configuration.\"\"\"\n        if self.path is None:\n            raise ValueError(\"Path is required for IPC transport\")\n        return (\n            f\"ipc://{self.path / self.name}_capture.ipc\"\n            if self.enable_capture\n            else None\n        )\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.backend_address","title":"<code>backend_address</code>  <code>property</code>","text":"<p>Get the backend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.capture_address","title":"<code>capture_address</code>  <code>property</code>","text":"<p>Get the capture address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.control_address","title":"<code>control_address</code>  <code>property</code>","text":"<p>Get the control address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQIPCProxyConfig.frontend_address","title":"<code>frontend_address</code>  <code>property</code>","text":"<p>Get the frontend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig","title":"<code>ZMQTCPConfig</code>","text":"<p>               Bases: <code>BaseZMQCommunicationConfig</code></p> <p>Configuration for TCP transport.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQTCPConfig(BaseZMQCommunicationConfig):\n    \"\"\"Configuration for TCP transport.\"\"\"\n\n    _CLI_GROUP = Groups.ZMQ_COMMUNICATION\n    comm_backend: ClassVar[CommunicationBackend] = CommunicationBackend.ZMQ_TCP\n\n    @model_validator(mode=\"after\")\n    def validate_host(self) -&gt; Self:\n        \"\"\"Fill in the host address for the proxy configs if not provided.\"\"\"\n        for proxy_config in [\n            self.dataset_manager_proxy_config,\n            self.event_bus_proxy_config,\n            self.raw_inference_proxy_config,\n        ]:\n            if proxy_config.host is None:\n                proxy_config.host = self.host\n        return self\n\n    host: Annotated[\n        str,\n        Field(\n            description=\"Host address for TCP connections\",\n        ),\n        CLIParameter(\n            name=(\"--zmq-host\"),\n            group=_CLI_GROUP,\n        ),\n    ] = \"127.0.0.1\"\n    records_push_pull_port: int = Field(\n        default=5557, description=\"Port for inference push/pull messages\"\n    )\n    credit_drop_port: int = Field(\n        default=5562, description=\"Port for credit drop operations\"\n    )\n    credit_return_port: int = Field(\n        default=5563, description=\"Port for credit return operations\"\n    )\n    dataset_manager_proxy_config: ZMQTCPProxyConfig = Field(  # type: ignore\n        default=ZMQTCPProxyConfig(\n            frontend_port=5661,\n            backend_port=5662,\n        ),\n        description=\"Configuration for the ZMQ Proxy. If provided, the proxy will be created and started.\",\n    )\n    event_bus_proxy_config: ZMQTCPProxyConfig = Field(  # type: ignore\n        default=ZMQTCPProxyConfig(\n            frontend_port=5663,\n            backend_port=5664,\n        ),\n        description=\"Configuration for the ZMQ Proxy. If provided, the proxy will be created and started.\",\n    )\n    raw_inference_proxy_config: ZMQTCPProxyConfig = Field(  # type: ignore\n        default=ZMQTCPProxyConfig(\n            frontend_port=5665,\n            backend_port=5666,\n        ),\n        description=\"Configuration for the ZMQ Proxy. If provided, the proxy will be created and started.\",\n    )\n\n    @property\n    def records_push_pull_address(self) -&gt; str:\n        \"\"\"Get the records push/pull address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.records_push_pull_port}\"\n\n    @property\n    def credit_drop_address(self) -&gt; str:\n        \"\"\"Get the credit drop address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.credit_drop_port}\"\n\n    @property\n    def credit_return_address(self) -&gt; str:\n        \"\"\"Get the credit return address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host}:{self.credit_return_port}\"\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.credit_drop_address","title":"<code>credit_drop_address</code>  <code>property</code>","text":"<p>Get the credit drop address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.credit_return_address","title":"<code>credit_return_address</code>  <code>property</code>","text":"<p>Get the credit return address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.records_push_pull_address","title":"<code>records_push_pull_address</code>  <code>property</code>","text":"<p>Get the records push/pull address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPConfig.validate_host","title":"<code>validate_host()</code>","text":"<p>Fill in the host address for the proxy configs if not provided.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_host(self) -&gt; Self:\n    \"\"\"Fill in the host address for the proxy configs if not provided.\"\"\"\n    for proxy_config in [\n        self.dataset_manager_proxy_config,\n        self.event_bus_proxy_config,\n        self.raw_inference_proxy_config,\n    ]:\n        if proxy_config.host is None:\n            proxy_config.host = self.host\n    return self\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig","title":"<code>ZMQTCPProxyConfig</code>","text":"<p>               Bases: <code>BaseZMQProxyConfig</code></p> <p>Configuration for TCP proxy.</p> Source code in <code>aiperf/common/config/zmq_config.py</code> <pre><code>class ZMQTCPProxyConfig(BaseZMQProxyConfig):\n    \"\"\"Configuration for TCP proxy.\"\"\"\n\n    host: str | None = Field(\n        default=None,\n        description=\"Host address for TCP connections\",\n    )\n    frontend_port: int = Field(\n        default=15555, description=\"Port for frontend address for proxy\"\n    )\n    backend_port: int = Field(\n        default=15556, description=\"Port for backend address for proxy\"\n    )\n    control_port: int | None = Field(\n        default=None, description=\"Port for control address for proxy\"\n    )\n    capture_port: int | None = Field(\n        default=None, description=\"Port for capture address for proxy\"\n    )\n\n    @property\n    def host_address(self) -&gt; str:\n        \"\"\"Get the host address based on protocol configuration.\"\"\"\n        return self.host or \"127.0.0.1\"\n\n    @property\n    def frontend_address(self) -&gt; str:\n        \"\"\"Get the frontend address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host_address}:{self.frontend_port}\"\n\n    @property\n    def backend_address(self) -&gt; str:\n        \"\"\"Get the backend address based on protocol configuration.\"\"\"\n        return f\"tcp://{self.host_address}:{self.backend_port}\"\n\n    @property\n    def control_address(self) -&gt; str | None:\n        \"\"\"Get the control address based on protocol configuration.\"\"\"\n        return (\n            f\"tcp://{self.host_address}:{self.control_port}\"\n            if self.control_port\n            else None\n        )\n\n    @property\n    def capture_address(self) -&gt; str | None:\n        \"\"\"Get the capture address based on protocol configuration.\"\"\"\n        return (\n            f\"tcp://{self.host_address}:{self.capture_port}\"\n            if self.capture_port\n            else None\n        )\n</code></pre>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.backend_address","title":"<code>backend_address</code>  <code>property</code>","text":"<p>Get the backend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.capture_address","title":"<code>capture_address</code>  <code>property</code>","text":"<p>Get the capture address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.control_address","title":"<code>control_address</code>  <code>property</code>","text":"<p>Get the control address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.frontend_address","title":"<code>frontend_address</code>  <code>property</code>","text":"<p>Get the frontend address based on protocol configuration.</p>"},{"location":"api/#aiperf.common.config.zmq_config.ZMQTCPProxyConfig.host_address","title":"<code>host_address</code>  <code>property</code>","text":"<p>Get the host address based on protocol configuration.</p>"},{"location":"api/#aiperfcommonconstants","title":"aiperf.common.constants","text":""},{"location":"api/#aiperf.common.constants.DEFAULT_COMMAND_RESPONSE_TIMEOUT","title":"<code>DEFAULT_COMMAND_RESPONSE_TIMEOUT = 30.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for command responses in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_COMMS_REQUEST_TIMEOUT","title":"<code>DEFAULT_COMMS_REQUEST_TIMEOUT = 90.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for requests from req_clients to rep_clients in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_CONNECTION_PROBE_INTERVAL","title":"<code>DEFAULT_CONNECTION_PROBE_INTERVAL = 0.1</code>  <code>module-attribute</code>","text":"<p>Default interval for connection probes in seconds until a response is received.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_CONNECTION_PROBE_TIMEOUT","title":"<code>DEFAULT_CONNECTION_PROBE_TIMEOUT = 30.0</code>  <code>module-attribute</code>","text":"<p>Maximum amount of time to wait for connection probe response.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_CREDIT_PROGRESS_REPORT_INTERVAL","title":"<code>DEFAULT_CREDIT_PROGRESS_REPORT_INTERVAL = 2.0</code>  <code>module-attribute</code>","text":"<p>Default interval in seconds between credit progress report messages.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_HEARTBEAT_INTERVAL","title":"<code>DEFAULT_HEARTBEAT_INTERVAL = 5.0</code>  <code>module-attribute</code>","text":"<p>Default interval between heartbeat messages in seconds for component services.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_MAX_REGISTRATION_ATTEMPTS","title":"<code>DEFAULT_MAX_REGISTRATION_ATTEMPTS = 10</code>  <code>module-attribute</code>","text":"<p>Default maximum number of registration attempts for component services before giving up.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_PROFILE_CANCEL_TIMEOUT","title":"<code>DEFAULT_PROFILE_CANCEL_TIMEOUT = 10.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for cancelling a profile run in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_PROFILE_CONFIGURE_TIMEOUT","title":"<code>DEFAULT_PROFILE_CONFIGURE_TIMEOUT = 300.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for profile configure command in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_PROFILE_START_TIMEOUT","title":"<code>DEFAULT_PROFILE_START_TIMEOUT = 60.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for profile start command in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_PULL_CLIENT_MAX_CONCURRENCY","title":"<code>DEFAULT_PULL_CLIENT_MAX_CONCURRENCY = 100000</code>  <code>module-attribute</code>","text":"<p>Default maximum concurrency for pull clients.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_REALTIME_METRICS_INTERVAL","title":"<code>DEFAULT_REALTIME_METRICS_INTERVAL = 5.0</code>  <code>module-attribute</code>","text":"<p>Default interval in seconds between real-time metrics messages.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_RECORDS_PROGRESS_REPORT_INTERVAL","title":"<code>DEFAULT_RECORDS_PROGRESS_REPORT_INTERVAL = 2.0</code>  <code>module-attribute</code>","text":"<p>Default interval in seconds between records progress report messages.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_REGISTRATION_INTERVAL","title":"<code>DEFAULT_REGISTRATION_INTERVAL = 1.0</code>  <code>module-attribute</code>","text":"<p>Default interval between registration attempts in seconds for component services.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_SERVICE_REGISTRATION_TIMEOUT","title":"<code>DEFAULT_SERVICE_REGISTRATION_TIMEOUT = 30.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for service registration in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_SERVICE_START_TIMEOUT","title":"<code>DEFAULT_SERVICE_START_TIMEOUT = 30.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for service start in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_SHUTDOWN_ACK_TIMEOUT","title":"<code>DEFAULT_SHUTDOWN_ACK_TIMEOUT = 5.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for waiting for a shutdown command response in seconds.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_UI_MIN_UPDATE_PERCENT","title":"<code>DEFAULT_UI_MIN_UPDATE_PERCENT = 1.0</code>  <code>module-attribute</code>","text":"<p>Default minimum percentage difference from the last update to trigger a UI update (for non-dashboard UIs).</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_CHECK_INTERVAL","title":"<code>DEFAULT_WORKER_CHECK_INTERVAL = 1.0</code>  <code>module-attribute</code>","text":"<p>Default interval between worker checks in seconds for the WorkerManager.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_ERROR_RECOVERY_TIME","title":"<code>DEFAULT_WORKER_ERROR_RECOVERY_TIME = 3.0</code>  <code>module-attribute</code>","text":"<p>Default time in seconds from the last time a worker had an error before it is considered healthy again.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_HEALTH_CHECK_INTERVAL","title":"<code>DEFAULT_WORKER_HEALTH_CHECK_INTERVAL = 2.0</code>  <code>module-attribute</code>","text":"<p>Default interval in seconds between worker health check messages.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_HIGH_LOAD_CPU_USAGE","title":"<code>DEFAULT_WORKER_HIGH_LOAD_CPU_USAGE = 75.0</code>  <code>module-attribute</code>","text":"<p>Default CPU usage threshold for a worker to be considered high load.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_HIGH_LOAD_RECOVERY_TIME","title":"<code>DEFAULT_WORKER_HIGH_LOAD_RECOVERY_TIME = 5.0</code>  <code>module-attribute</code>","text":"<p>Default time in seconds from the last time a worker was in high load before it is considered healthy again.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_STALE_TIME","title":"<code>DEFAULT_WORKER_STALE_TIME = 10.0</code>  <code>module-attribute</code>","text":"<p>Default time in seconds from the last time a worker reported any status before it is considered stale.</p>"},{"location":"api/#aiperf.common.constants.DEFAULT_WORKER_STATUS_SUMMARY_INTERVAL","title":"<code>DEFAULT_WORKER_STATUS_SUMMARY_INTERVAL = 0.5</code>  <code>module-attribute</code>","text":"<p>Default interval in seconds between worker status summary messages.</p>"},{"location":"api/#aiperf.common.constants.GRACEFUL_SHUTDOWN_TIMEOUT","title":"<code>GRACEFUL_SHUTDOWN_TIMEOUT = 5.0</code>  <code>module-attribute</code>","text":"<p>Default timeout for shutting down services in seconds.</p>"},{"location":"api/#aiperf.common.constants.TASK_CANCEL_TIMEOUT_SHORT","title":"<code>TASK_CANCEL_TIMEOUT_SHORT = 2.0</code>  <code>module-attribute</code>","text":"<p>Maximum time to wait for simple tasks to complete when cancelling them.</p>"},{"location":"api/#aiperfcommondecorators","title":"aiperf.common.decorators","text":"<p>Decorators for AIPerf components. Note that these are not the same as hooks. Hooks are used to specify that a function should be called at a specific time, while decorators are used to specify that a class or function should be treated a specific way.</p> <p>see also: :mod:<code>aiperf.common.hooks</code> for hook decorators.</p>"},{"location":"api/#aiperf.common.decorators.DecoratorAttrs","title":"<code>DecoratorAttrs</code>","text":"<p>Constant attribute names for decorators.</p> <p>When you decorate a class with a decorator, the decorator type and parameters are set as attributes on the class.</p> Source code in <code>aiperf/common/decorators.py</code> <pre><code>class DecoratorAttrs:\n    \"\"\"Constant attribute names for decorators.\n\n    When you decorate a class with a decorator, the decorator type and parameters are\n    set as attributes on the class.\n    \"\"\"\n\n    IMPLEMENTS_PROTOCOL = \"__implements_protocol__\"\n</code></pre>"},{"location":"api/#aiperf.common.decorators.implements_protocol","title":"<code>implements_protocol(protocol)</code>","text":"<p>Decorator to specify that the class implements the given protocol.</p> <p>Example:</p> <pre><code>@implements_protocol(ServiceProtocol)\nclass BaseService:\n    pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>BaseService.__implements_protocol__ = ServiceProtocol\n</code></pre> Source code in <code>aiperf/common/decorators.py</code> <pre><code>def implements_protocol(protocol: type[ProtocolT]) -&gt; Callable:\n    \"\"\"Decorator to specify that the class implements the given protocol.\n\n    Example:\n    ```python\n    @implements_protocol(ServiceProtocol)\n    class BaseService:\n        pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    BaseService.__implements_protocol__ = ServiceProtocol\n    ```\n    \"\"\"\n\n    def decorator(cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n        if TYPE_CHECKING:\n            if not hasattr(protocol, \"_is_runtime_protocol\"):\n                warn(\n                    f\"Protocol {protocol.__name__} is not a runtime protocol. \"\n                    \"Please use the @runtime_checkable decorator to mark it as a runtime protocol.\",\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n                raise TypeError(\n                    f\"Protocol {protocol.__name__} is not a runtime protocol. \"\n                    \"Please use the @runtime_checkable decorator to mark it as a runtime protocol.\"\n                )\n            if not issubclass(cls, protocol):\n                warn(\n                    f\"Class {cls.__name__} does not implement the {protocol.__name__} protocol.\",\n                    category=UserWarning,\n                    stacklevel=2,\n                )\n                raise TypeError(\n                    f\"Class {cls.__name__} does not implement the {protocol.__name__} protocol.\"\n                )\n        setattr(cls, DecoratorAttrs.IMPLEMENTS_PROTOCOL, protocol)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperfcommonenumsbase_enums","title":"aiperf.common.enums.base_enums","text":""},{"location":"api/#aiperf.common.enums.base_enums.BasePydanticBackedStrEnum","title":"<code>BasePydanticBackedStrEnum</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Custom enumeration class that extends <code>CaseInsensitiveStrEnum</code> and is backed by a <code>BasePydanticEnumInfo</code> that contains the <code>tag</code>, and any other information that is needed to represent the enum member.</p> Source code in <code>aiperf/common/enums/base_enums.py</code> <pre><code>class BasePydanticBackedStrEnum(CaseInsensitiveStrEnum):\n    \"\"\"\n    Custom enumeration class that extends `CaseInsensitiveStrEnum`\n    and is backed by a `BasePydanticEnumInfo` that contains the `tag`, and any other information that is needed\n    to represent the enum member.\n    \"\"\"\n\n    # Override the __new__ method to store the `BasePydanticEnumInfo` subclass model as an attribute. This is a python feature that\n    # allows us to modify the behavior of the enum class's constructor. We use this to ensure the the enums still look like\n    # a regular string enum, but also have the additional information stored as an attribute.\n    def __new__(cls, info: BasePydanticEnumInfo) -&gt; Self:\n        # Create a new string object based on this class and the tag value.\n        obj = str.__new__(cls, info.tag)\n        # Ensure string value is set for comparison. This is how enums work internally.\n        obj._value_ = info.tag\n        # Store the Pydantic model as an attribute.\n        obj._info: BasePydanticEnumInfo = info  # type: ignore\n        return obj\n\n    @cached_property\n    def info(self) -&gt; BasePydanticEnumInfo:\n        \"\"\"Get the enum info for the enum member.\"\"\"\n        # This is the Pydantic model that was stored as an attribute in the __new__ method.\n        return self._info  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.common.enums.base_enums.BasePydanticBackedStrEnum.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the enum info for the enum member.</p>"},{"location":"api/#aiperf.common.enums.base_enums.BasePydanticEnumInfo","title":"<code>BasePydanticEnumInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all enum info classes that extend <code>BasePydanticBackedStrEnum</code>. By default, it provides a <code>tag</code> for the enum member, which is used for lookup and string comparison, and the subclass can provide additional information as needed.</p> Source code in <code>aiperf/common/enums/base_enums.py</code> <pre><code>class BasePydanticEnumInfo(BaseModel):\n    \"\"\"Base class for all enum info classes that extend `BasePydanticBackedStrEnum`. By default, it\n    provides a `tag` for the enum member, which is used for lookup and string comparison,\n    and the subclass can provide additional information as needed.\"\"\"\n\n    tag: str = Field(\n        ...,\n        min_length=1,\n        description=\"The string value of the enum member used for lookup, serialization, and string insensitive comparison.\",\n    )\n\n    def __str__(self) -&gt; str:\n        return self.tag\n</code></pre>"},{"location":"api/#aiperf.common.enums.base_enums.CaseInsensitiveStrEnum","title":"<code>CaseInsensitiveStrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>CaseInsensitiveStrEnum is a custom enumeration class that extends <code>str</code> and <code>Enum</code> to provide case-insensitive lookup functionality for its members.</p> Source code in <code>aiperf/common/enums/base_enums.py</code> <pre><code>class CaseInsensitiveStrEnum(str, Enum):\n    \"\"\"\n    CaseInsensitiveStrEnum is a custom enumeration class that extends `str` and `Enum` to provide case-insensitive\n    lookup functionality for its members.\n    \"\"\"\n\n    def __str__(self) -&gt; str:\n        return self.value\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}.{self.name}\"\n\n    def __eq__(self, other: object) -&gt; bool:\n        if isinstance(other, str):\n            return self.value.lower() == other.lower()\n        if isinstance(other, Enum):\n            return self.value.lower() == other.value.lower()\n        return super().__eq__(other)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.value.lower())\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Handles cases where a value is not directly found in the enumeration.\n\n        This method is called when an attempt is made to access an enumeration\n        member using a value that does not directly match any of the defined\n        members. It provides custom logic to handle such cases.\n\n        Returns:\n            The matching enumeration member if a case-insensitive match is found\n            for string values; otherwise, returns None.\n        \"\"\"\n        if isinstance(value, str):\n            for member in cls:\n                if member.value.lower() == value.lower():\n                    return member\n        return None\n</code></pre>"},{"location":"api/#aiperfcommonenumscommand_enums","title":"aiperf.common.enums.command_enums","text":""},{"location":"api/#aiperfcommonenumscommunication_enums","title":"aiperf.common.enums.communication_enums","text":""},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress","title":"<code>CommAddress</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Enum for specifying the address type for communication clients. This is used to lookup the address in the communication config.</p> Source code in <code>aiperf/common/enums/communication_enums.py</code> <pre><code>class CommAddress(CaseInsensitiveStrEnum):\n    \"\"\"Enum for specifying the address type for communication clients.\n    This is used to lookup the address in the communication config.\"\"\"\n\n    EVENT_BUS_PROXY_FRONTEND = \"event_bus_proxy_frontend\"\n    \"\"\"Frontend address for services to publish messages to.\"\"\"\n\n    EVENT_BUS_PROXY_BACKEND = \"event_bus_proxy_backend\"\n    \"\"\"Backend address for services to subscribe to messages.\"\"\"\n\n    CREDIT_DROP = \"credit_drop\"\n    \"\"\"Address to send CreditDrop messages from the TimingManager to the Worker.\"\"\"\n\n    CREDIT_RETURN = \"credit_return\"\n    \"\"\"Address to send CreditReturn messages from the Worker to the TimingManager.\"\"\"\n\n    RECORDS = \"records\"\n    \"\"\"Address to send parsed records from InferenceParser to RecordManager.\"\"\"\n\n    DATASET_MANAGER_PROXY_FRONTEND = \"dataset_manager_proxy_frontend\"\n    \"\"\"Frontend address for sending requests to the DatasetManager.\"\"\"\n\n    DATASET_MANAGER_PROXY_BACKEND = \"dataset_manager_proxy_backend\"\n    \"\"\"Backend address for the DatasetManager to receive requests from clients.\"\"\"\n\n    RAW_INFERENCE_PROXY_FRONTEND = \"raw_inference_proxy_frontend\"\n    \"\"\"Frontend address for sending raw inference messages to the InferenceParser from Workers.\"\"\"\n\n    RAW_INFERENCE_PROXY_BACKEND = \"raw_inference_proxy_backend\"\n    \"\"\"Backend address for the InferenceParser to receive raw inference messages from Workers.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.CREDIT_DROP","title":"<code>CREDIT_DROP = 'credit_drop'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address to send CreditDrop messages from the TimingManager to the Worker.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.CREDIT_RETURN","title":"<code>CREDIT_RETURN = 'credit_return'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address to send CreditReturn messages from the Worker to the TimingManager.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.DATASET_MANAGER_PROXY_BACKEND","title":"<code>DATASET_MANAGER_PROXY_BACKEND = 'dataset_manager_proxy_backend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend address for the DatasetManager to receive requests from clients.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.DATASET_MANAGER_PROXY_FRONTEND","title":"<code>DATASET_MANAGER_PROXY_FRONTEND = 'dataset_manager_proxy_frontend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frontend address for sending requests to the DatasetManager.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.EVENT_BUS_PROXY_BACKEND","title":"<code>EVENT_BUS_PROXY_BACKEND = 'event_bus_proxy_backend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend address for services to subscribe to messages.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.EVENT_BUS_PROXY_FRONTEND","title":"<code>EVENT_BUS_PROXY_FRONTEND = 'event_bus_proxy_frontend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frontend address for services to publish messages to.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.RAW_INFERENCE_PROXY_BACKEND","title":"<code>RAW_INFERENCE_PROXY_BACKEND = 'raw_inference_proxy_backend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Backend address for the InferenceParser to receive raw inference messages from Workers.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.RAW_INFERENCE_PROXY_FRONTEND","title":"<code>RAW_INFERENCE_PROXY_FRONTEND = 'raw_inference_proxy_frontend'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frontend address for sending raw inference messages to the InferenceParser from Workers.</p>"},{"location":"api/#aiperf.common.enums.communication_enums.CommAddress.RECORDS","title":"<code>RECORDS = 'records'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address to send parsed records from InferenceParser to RecordManager.</p>"},{"location":"api/#aiperfcommonenumsdata_exporter_enums","title":"aiperf.common.enums.data_exporter_enums","text":""},{"location":"api/#aiperfcommonenumsdataset_enums","title":"aiperf.common.enums.dataset_enums","text":""},{"location":"api/#aiperfcommonenumsendpoints_enums","title":"aiperf.common.enums.endpoints_enums","text":""},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointServiceKind","title":"<code>EndpointServiceKind</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Endpoint service kind.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>class EndpointServiceKind(CaseInsensitiveStrEnum):\n    \"\"\"Endpoint service kind.\"\"\"\n\n    OPENAI = \"openai\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType","title":"<code>EndpointType</code>","text":"<p>               Bases: <code>BasePydanticBackedStrEnum</code></p> <p>Endpoint types supported by AIPerf.</p> <p>These are the full definitions of the endpoints that are supported by AIPerf. Each enum value contains additional metadata about the endpoint, such as whether it supports streaming, produces tokens, and the default endpoint path. This is stored as an attribute on the enum value, and can be accessed via the <code>info</code> property. The enum values can still be used as strings for user input and comparison (via the <code>tag</code> field).</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>class EndpointType(BasePydanticBackedStrEnum):\n    \"\"\"Endpoint types supported by AIPerf.\n\n    These are the full definitions of the endpoints that are supported by AIPerf.\n    Each enum value contains additional metadata about the endpoint, such as whether it supports streaming,\n    produces tokens, and the default endpoint path. This is stored as an attribute on the enum value, and can be accessed\n    via the `info` property. The enum values can still be used as strings for user input and comparison (via the `tag` field).\n    \"\"\"\n\n    OPENAI_CHAT_COMPLETIONS = EndpointTypeInfo(\n        tag=\"chat\",\n        service_kind=EndpointServiceKind.OPENAI,\n        supports_streaming=True,\n        produces_tokens=True,\n        supports_audio=True,\n        supports_images=True,\n        endpoint_path=\"/v1/chat/completions\",\n        metrics_title=\"LLM Metrics\",\n    )\n    OPENAI_COMPLETIONS = EndpointTypeInfo(\n        tag=\"completions\",\n        service_kind=EndpointServiceKind.OPENAI,\n        supports_streaming=True,\n        produces_tokens=True,\n        endpoint_path=\"/v1/completions\",\n        metrics_title=\"LLM Metrics\",\n    )\n    OPENAI_EMBEDDINGS = EndpointTypeInfo(\n        tag=\"embeddings\",\n        service_kind=EndpointServiceKind.OPENAI,\n        supports_streaming=False,\n        produces_tokens=False,\n        endpoint_path=\"/v1/embeddings\",\n        metrics_title=\"Embeddings Metrics\",\n    )\n    RANKINGS = EndpointTypeInfo(\n        tag=\"rankings\",\n        service_kind=EndpointServiceKind.OPENAI,\n        supports_streaming=False,\n        produces_tokens=False,\n        endpoint_path=\"/v1/ranking\",\n        metrics_title=\"Rankings Metrics\",\n    )\n    OPENAI_RESPONSES = EndpointTypeInfo(\n        tag=\"responses\",\n        service_kind=EndpointServiceKind.OPENAI,\n        supports_streaming=True,\n        produces_tokens=True,\n        supports_audio=False,  # Not yet supported by OpenAI\n        supports_images=True,\n        endpoint_path=\"/v1/responses\",\n        metrics_title=\"LLM Metrics\",\n    )\n\n    @cached_property\n    def info(self) -&gt; EndpointTypeInfo:\n        \"\"\"Get the endpoint info for the endpoint type.\"\"\"\n        return self._info  # type: ignore\n\n    @property\n    def service_kind(self) -&gt; EndpointServiceKind:\n        \"\"\"Get the service kind for the endpoint type.\"\"\"\n        return self.info.service_kind\n\n    @property\n    def supports_streaming(self) -&gt; bool:\n        \"\"\"Return True if the endpoint supports streaming. This is used for validation of user input.\"\"\"\n        return self.info.supports_streaming\n\n    @property\n    def produces_tokens(self) -&gt; bool:\n        \"\"\"Return True if the endpoint produces tokens. This is used to determine what metrics are applicable to the endpoint.\"\"\"\n        return self.info.produces_tokens\n\n    @property\n    def endpoint_path(self) -&gt; str | None:\n        \"\"\"Get the default endpoint path for the endpoint type. If None, the endpoint does not have a specific path.\"\"\"\n        return self.info.endpoint_path\n\n    @property\n    def supports_audio(self) -&gt; bool:\n        \"\"\"Return True if the endpoint supports audio input.\n        This is used to determine what metrics are applicable to the endpoint, as well as what inputs can be used.\"\"\"\n        return self.info.supports_audio\n\n    @property\n    def supports_images(self) -&gt; bool:\n        \"\"\"Return True if the endpoint supports image input.\n        This is used to determine what metrics are applicable to the endpoint, as well as what inputs can be used.\"\"\"\n        return self.info.supports_images\n\n    @property\n    def metrics_title(self) -&gt; str:\n        \"\"\"Get the metrics table title string for the endpoint type. If None, the default title is used.\"\"\"\n        return self.info.metrics_title or \"Metrics\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.endpoint_path","title":"<code>endpoint_path</code>  <code>property</code>","text":"<p>Get the default endpoint path for the endpoint type. If None, the endpoint does not have a specific path.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the endpoint info for the endpoint type.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.metrics_title","title":"<code>metrics_title</code>  <code>property</code>","text":"<p>Get the metrics table title string for the endpoint type. If None, the default title is used.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.produces_tokens","title":"<code>produces_tokens</code>  <code>property</code>","text":"<p>Return True if the endpoint produces tokens. This is used to determine what metrics are applicable to the endpoint.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.service_kind","title":"<code>service_kind</code>  <code>property</code>","text":"<p>Get the service kind for the endpoint type.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.supports_audio","title":"<code>supports_audio</code>  <code>property</code>","text":"<p>Return True if the endpoint supports audio input. This is used to determine what metrics are applicable to the endpoint, as well as what inputs can be used.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.supports_images","title":"<code>supports_images</code>  <code>property</code>","text":"<p>Return True if the endpoint supports image input. This is used to determine what metrics are applicable to the endpoint, as well as what inputs can be used.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointType.supports_streaming","title":"<code>supports_streaming</code>  <code>property</code>","text":"<p>Return True if the endpoint supports streaming. This is used for validation of user input.</p>"},{"location":"api/#aiperf.common.enums.endpoints_enums.EndpointTypeInfo","title":"<code>EndpointTypeInfo</code>","text":"<p>               Bases: <code>BasePydanticEnumInfo</code></p> <p>Pydantic model for endpoint-specific metadata. This model is used to store additional info on each EndpointType enum value.</p> <p>For documentation on the fields, see the :class:<code>EndpointType</code> @property functions.</p> Source code in <code>aiperf/common/enums/endpoints_enums.py</code> <pre><code>class EndpointTypeInfo(BasePydanticEnumInfo):\n    \"\"\"Pydantic model for endpoint-specific metadata. This model is used to store additional info on each EndpointType enum value.\n\n    For documentation on the fields, see the :class:`EndpointType` @property functions.\n    \"\"\"\n\n    service_kind: EndpointServiceKind = Field(...)\n    supports_streaming: bool = Field(...)\n    produces_tokens: bool = Field(...)\n    supports_audio: bool = Field(default=False)\n    supports_images: bool = Field(default=False)\n    endpoint_path: str | None = Field(default=None)\n    metrics_title: str | None = Field(default=None)\n</code></pre>"},{"location":"api/#aiperfcommonenumslogging_enums","title":"aiperf.common.enums.logging_enums","text":""},{"location":"api/#aiperfcommonenumsmedia_enums","title":"aiperf.common.enums.media_enums","text":""},{"location":"api/#aiperf.common.enums.media_enums.MediaType","title":"<code>MediaType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The various types of media (e.g. text, image, audio).</p> Source code in <code>aiperf/common/enums/media_enums.py</code> <pre><code>class MediaType(CaseInsensitiveStrEnum):\n    \"\"\"The various types of media (e.g. text, image, audio).\"\"\"\n\n    TEXT = \"text\"\n    IMAGE = \"image\"\n    AUDIO = \"audio\"\n</code></pre>"},{"location":"api/#aiperfcommonenumsmessage_enums","title":"aiperf.common.enums.message_enums","text":""},{"location":"api/#aiperf.common.enums.message_enums.MessageType","title":"<code>MessageType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The various types of messages that can be sent between services.</p> <p>The message type is used to determine what Pydantic model the message maps to, based on the message_type field in the message model. For detailed explanations of each message type, go to its definition in :mod:<code>aiperf.common.messages</code>.</p> Source code in <code>aiperf/common/enums/message_enums.py</code> <pre><code>class MessageType(CaseInsensitiveStrEnum):\n    \"\"\"The various types of messages that can be sent between services.\n\n    The message type is used to determine what Pydantic model the message maps to,\n    based on the message_type field in the message model. For detailed explanations\n    of each message type, go to its definition in :mod:`aiperf.common.messages`.\n    \"\"\"\n\n    ALL_RECORDS_RECEIVED = \"all_records_received\"\n    COMMAND = \"command\"\n    COMMAND_RESPONSE = \"command_response\"\n    CONNECTION_PROBE = \"connection_probe\"\n    CONVERSATION_REQUEST = \"conversation_request\"\n    CONVERSATION_RESPONSE = \"conversation_response\"\n    CONVERSATION_TURN_REQUEST = \"conversation_turn_request\"\n    CONVERSATION_TURN_RESPONSE = \"conversation_turn_response\"\n    CREDITS_COMPLETE = \"credits_complete\"\n    CREDIT_DROP = \"credit_drop\"\n    CREDIT_PHASE_COMPLETE = \"credit_phase_complete\"\n    CREDIT_PHASE_PROGRESS = \"credit_phase_progress\"\n    CREDIT_PHASE_SENDING_COMPLETE = \"credit_phase_sending_complete\"\n    CREDIT_PHASE_START = \"credit_phase_start\"\n    CREDIT_RETURN = \"credit_return\"\n    DATASET_CONFIGURED_NOTIFICATION = \"dataset_configured_notification\"\n    DATASET_TIMING_REQUEST = \"dataset_timing_request\"\n    DATASET_TIMING_RESPONSE = \"dataset_timing_response\"\n    ERROR = \"error\"\n    HEARTBEAT = \"heartbeat\"\n    INFERENCE_RESULTS = \"inference_results\"\n    METRIC_RECORDS = \"metric_records\"\n    PARSED_INFERENCE_RESULTS = \"parsed_inference_results\"\n    PROCESSING_STATS = \"processing_stats\"\n    PROCESS_RECORDS_RESULT = \"process_records_result\"\n    PROFILE_PROGRESS = \"profile_progress\"\n    PROFILE_RESULTS = \"profile_results\"\n    REALTIME_METRICS = \"realtime_metrics\"\n    REGISTRATION = \"registration\"\n    SERVICE_ERROR = \"service_error\"\n    STATUS = \"status\"\n    WORKER_HEALTH = \"worker_health\"\n    WORKER_STATUS_SUMMARY = \"worker_status_summary\"\n</code></pre>"},{"location":"api/#aiperfcommonenumsmetric_enums","title":"aiperf.common.enums.metric_enums","text":""},{"location":"api/#aiperf.common.enums.metric_enums.BaseMetricUnit","title":"<code>BaseMetricUnit</code>","text":"<p>               Bases: <code>BasePydanticBackedStrEnum</code></p> <p>Base class for all metric units.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class BaseMetricUnit(BasePydanticBackedStrEnum):\n    \"\"\"Base class for all metric units.\"\"\"\n\n    @cached_property\n    def info(self) -&gt; BaseMetricUnitInfo:\n        \"\"\"Get the info for the metric unit.\"\"\"\n        return self._info  # type: ignore\n\n    def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n        \"\"\"Convert a value from this unit to another unit. This is a passthrough to the info class.\"\"\"\n        return self.info.convert_to(other_unit, value)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.BaseMetricUnit.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the info for the metric unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.BaseMetricUnit.convert_to","title":"<code>convert_to(other_unit, value)</code>","text":"<p>Convert a value from this unit to another unit. This is a passthrough to the info class.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n    \"\"\"Convert a value from this unit to another unit. This is a passthrough to the info class.\"\"\"\n    return self.info.convert_to(other_unit, value)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.BaseMetricUnitInfo","title":"<code>BaseMetricUnitInfo</code>","text":"<p>               Bases: <code>BasePydanticEnumInfo</code></p> <p>Base class for all metric units. Provides a base implementation for converting between units which can be overridden by subclasses to support more complex conversions.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class BaseMetricUnitInfo(BasePydanticEnumInfo):\n    \"\"\"Base class for all metric units. Provides a base implementation for converting between units which\n    can be overridden by subclasses to support more complex conversions.\n    \"\"\"\n\n    def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n        \"\"\"Convert a value from this unit to another unit.\"\"\"\n        # If the other unit is the same as this unit, return the value. This allows for chaining conversions,\n        # as well as if a type does not have a conversion method, we do not want to raise an error if the conversion is a no-op.\n        if other_unit == self:\n            return value\n\n        # Otherwise, we cannot convert between the two units.\n        raise MetricUnitError(\n            f\"Cannot convert from '{self}' to '{other_unit}'.\",\n        )\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.BaseMetricUnitInfo.convert_to","title":"<code>convert_to(other_unit, value)</code>","text":"<p>Convert a value from this unit to another unit.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n    \"\"\"Convert a value from this unit to another unit.\"\"\"\n    # If the other unit is the same as this unit, return the value. This allows for chaining conversions,\n    # as well as if a type does not have a conversion method, we do not want to raise an error if the conversion is a no-op.\n    if other_unit == self:\n        return value\n\n    # Otherwise, we cannot convert between the two units.\n    raise MetricUnitError(\n        f\"Cannot convert from '{self}' to '{other_unit}'.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.GenericMetricUnit","title":"<code>GenericMetricUnit</code>","text":"<p>               Bases: <code>BaseMetricUnit</code></p> <p>Defines generic units for metrics. These dont have any extra information other than the tag, which is used for display purposes.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class GenericMetricUnit(BaseMetricUnit):\n    \"\"\"Defines generic units for metrics. These dont have any extra information other than the tag, which is used for display purposes.\"\"\"\n\n    COUNT = _unit(\"count\")\n    REQUESTS = _unit(\"requests\")\n    TOKENS = _unit(\"tokens\")\n    RATIO = _unit(\"ratio\")\n    USER = _unit(\"user\")\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricDateTimeUnit","title":"<code>MetricDateTimeUnit</code>","text":"<p>               Bases: <code>BaseMetricUnit</code></p> <p>Defines the various date time units that can be used for metrics.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricDateTimeUnit(BaseMetricUnit):\n    \"\"\"Defines the various date time units that can be used for metrics.\"\"\"\n\n    DATE_TIME = _unit(\"datetime\")\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags","title":"<code>MetricFlags</code>","text":"<p>               Bases: <code>Flag</code></p> <p>Defines the possible flags for metrics that are used to determine how they are processed or grouped. These flags are intended to be an easy way to group metrics, or turn on/off certain features.</p> <p>Note that the flags are a bitmask, so they can be combined using the bitwise OR operator (<code>|</code>). For example, to create a flag that is both <code>STREAMING_ONLY</code> and <code>HIDDEN</code>, you can do:</p> <pre><code>MetricFlags.STREAMING_ONLY | MetricFlags.HIDDEN\n</code></pre> <p>To check if a metric has a flag, you can use the <code>has_flags</code> method. For example, to check if a metric has both the <code>STREAMING_ONLY</code> and <code>HIDDEN</code> flags, you can do:</p> <pre><code>metric.has_flags(MetricFlags.STREAMING_ONLY | MetricFlags.HIDDEN)\n</code></pre> <p>To check if a metric does not have a flag(s), you can use the <code>missing_flags</code> method. For example, to check if a metric does not have either the <code>STREAMING_ONLY</code> or <code>HIDDEN</code> flags, you can do:</p> <pre><code>metric.missing_flags(MetricFlags.STREAMING_ONLY | MetricFlags.HIDDEN)\n</code></pre> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricFlags(Flag):\n    \"\"\"Defines the possible flags for metrics that are used to determine how they are processed or grouped.\n    These flags are intended to be an easy way to group metrics, or turn on/off certain features.\n\n    Note that the flags are a bitmask, so they can be combined using the bitwise OR operator (`|`).\n    For example, to create a flag that is both `STREAMING_ONLY` and `HIDDEN`, you can do:\n    ```python\n    MetricFlags.STREAMING_ONLY | MetricFlags.HIDDEN\n    ```\n\n    To check if a metric has a flag, you can use the `has_flags` method.\n    For example, to check if a metric has both the `STREAMING_ONLY` and `HIDDEN` flags, you can do:\n    ```python\n    metric.has_flags(MetricFlags.STREAMING_ONLY | MetricFlags.HIDDEN)\n    ```\n\n    To check if a metric does not have a flag(s), you can use the `missing_flags` method.\n    For example, to check if a metric does not have either the `STREAMING_ONLY` or `HIDDEN` flags, you can do:\n    ```python\n    metric.missing_flags(MetricFlags.STREAMING_ONLY | MetricFlags.HIDDEN)\n    ```\n    \"\"\"\n\n    # NOTE: The flags are a bitmask, so they must be powers of 2 (or a combination thereof).\n\n    NONE = 0\n    \"\"\"No flags.\"\"\"\n\n    STREAMING_ONLY = 1 &lt;&lt; 0\n    \"\"\"Metrics that are only applicable to streamed responses.\"\"\"\n\n    ERROR_ONLY = 1 &lt;&lt; 1\n    \"\"\"Metrics that are only applicable to error records. By default, metrics are only computed if the record is valid.\n    If this flag is set, the metric will only be computed if the record is invalid.\"\"\"\n\n    PRODUCES_TOKENS_ONLY = 1 &lt;&lt; 2\n    \"\"\"Metrics that are only applicable when profiling an endpoint that produces tokens.\"\"\"\n\n    HIDDEN = 1 &lt;&lt; 3\n    \"\"\"Metrics that should not be displayed in the UI.\"\"\"\n\n    LARGER_IS_BETTER = 1 &lt;&lt; 4\n    \"\"\"Metrics that are better when the value is larger. By default, it is assumed that metrics are\n    better when the value is smaller.\"\"\"\n\n    INTERNAL = (1 &lt;&lt; 5) | HIDDEN\n    \"\"\"Metrics that are internal to the system and not applicable to the user. This inherently means that the metric\n    is HIDDEN as well.\"\"\"\n\n    SUPPORTS_AUDIO_ONLY = 1 &lt;&lt; 6\n    \"\"\"Metrics that are only applicable to audio-based endpoints.\"\"\"\n\n    SUPPORTS_IMAGE_ONLY = 1 &lt;&lt; 7\n    \"\"\"Metrics that are only applicable to image-based endpoints.\"\"\"\n\n    SUPPORTS_REASONING = 1 &lt;&lt; 8\n    \"\"\"Metrics that are only applicable to reasoning-based models and endpoints.\"\"\"\n\n    EXPERIMENTAL = (1 &lt;&lt; 9) | HIDDEN\n    \"\"\"Metrics that are experimental and are not yet ready for production use, and may be subject to change.\n    This inherently means that the metric is HIDDEN as well.\"\"\"\n\n    STREAMING_TOKENS_ONLY = STREAMING_ONLY | PRODUCES_TOKENS_ONLY\n    \"\"\"Metrics that are only applicable to streamed responses and token-based endpoints.\n    This is a convenience flag that is the combination of the `STREAMING_ONLY` and `PRODUCES_TOKENS_ONLY` flags.\"\"\"\n\n    def has_flags(self, flags: \"MetricFlags\") -&gt; bool:\n        \"\"\"Return True if the metric has ALL of the given flag(s) (regardless of other flags).\"\"\"\n        # Bitwise AND will return the input flags only if all of the given flags are present.\n        return (flags &amp; self) == flags\n\n    def has_any_flags(self, flags: \"MetricFlags\") -&gt; bool:\n        \"\"\"Return True if the metric has ANY of the given flag(s) (regardless of other flags).\"\"\"\n        return (flags &amp; self) != MetricFlags.NONE\n\n    def missing_flags(self, flags: \"MetricFlags\") -&gt; bool:\n        \"\"\"Return True if the metric does not have ANY of the given flag(s) (regardless of other flags). It will\n        return False if the metric has ANY of the given flags. If the input flags are NONE, it will return True.\"\"\"\n        if flags == MetricFlags.NONE:\n            return True  # If there are no flags to check, return True\n\n        # Bitwise AND will return 0 (MetricFlags.NONE) if there are no common flags.\n        # If there are some missing, but some found, the result will not be 0.\n        return (self &amp; flags) == MetricFlags.NONE\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.ERROR_ONLY","title":"<code>ERROR_ONLY = 1 &lt;&lt; 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable to error records. By default, metrics are only computed if the record is valid. If this flag is set, the metric will only be computed if the record is invalid.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.EXPERIMENTAL","title":"<code>EXPERIMENTAL = 1 &lt;&lt; 9 | HIDDEN</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are experimental and are not yet ready for production use, and may be subject to change. This inherently means that the metric is HIDDEN as well.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.HIDDEN","title":"<code>HIDDEN = 1 &lt;&lt; 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that should not be displayed in the UI.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.INTERNAL","title":"<code>INTERNAL = 1 &lt;&lt; 5 | HIDDEN</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are internal to the system and not applicable to the user. This inherently means that the metric is HIDDEN as well.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.LARGER_IS_BETTER","title":"<code>LARGER_IS_BETTER = 1 &lt;&lt; 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are better when the value is larger. By default, it is assumed that metrics are better when the value is smaller.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.NONE","title":"<code>NONE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>No flags.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.PRODUCES_TOKENS_ONLY","title":"<code>PRODUCES_TOKENS_ONLY = 1 &lt;&lt; 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable when profiling an endpoint that produces tokens.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.STREAMING_ONLY","title":"<code>STREAMING_ONLY = 1 &lt;&lt; 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable to streamed responses.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.STREAMING_TOKENS_ONLY","title":"<code>STREAMING_TOKENS_ONLY = STREAMING_ONLY | PRODUCES_TOKENS_ONLY</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable to streamed responses and token-based endpoints. This is a convenience flag that is the combination of the <code>STREAMING_ONLY</code> and <code>PRODUCES_TOKENS_ONLY</code> flags.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.SUPPORTS_AUDIO_ONLY","title":"<code>SUPPORTS_AUDIO_ONLY = 1 &lt;&lt; 6</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable to audio-based endpoints.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.SUPPORTS_IMAGE_ONLY","title":"<code>SUPPORTS_IMAGE_ONLY = 1 &lt;&lt; 7</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable to image-based endpoints.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.SUPPORTS_REASONING","title":"<code>SUPPORTS_REASONING = 1 &lt;&lt; 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are only applicable to reasoning-based models and endpoints.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.has_any_flags","title":"<code>has_any_flags(flags)</code>","text":"<p>Return True if the metric has ANY of the given flag(s) (regardless of other flags).</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def has_any_flags(self, flags: \"MetricFlags\") -&gt; bool:\n    \"\"\"Return True if the metric has ANY of the given flag(s) (regardless of other flags).\"\"\"\n    return (flags &amp; self) != MetricFlags.NONE\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.has_flags","title":"<code>has_flags(flags)</code>","text":"<p>Return True if the metric has ALL of the given flag(s) (regardless of other flags).</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def has_flags(self, flags: \"MetricFlags\") -&gt; bool:\n    \"\"\"Return True if the metric has ALL of the given flag(s) (regardless of other flags).\"\"\"\n    # Bitwise AND will return the input flags only if all of the given flags are present.\n    return (flags &amp; self) == flags\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricFlags.missing_flags","title":"<code>missing_flags(flags)</code>","text":"<p>Return True if the metric does not have ANY of the given flag(s) (regardless of other flags). It will return False if the metric has ANY of the given flags. If the input flags are NONE, it will return True.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def missing_flags(self, flags: \"MetricFlags\") -&gt; bool:\n    \"\"\"Return True if the metric does not have ANY of the given flag(s) (regardless of other flags). It will\n    return False if the metric has ANY of the given flags. If the input flags are NONE, it will return True.\"\"\"\n    if flags == MetricFlags.NONE:\n        return True  # If there are no flags to check, return True\n\n    # Bitwise AND will return 0 (MetricFlags.NONE) if there are no common flags.\n    # If there are some missing, but some found, the result will not be 0.\n    return (self &amp; flags) == MetricFlags.NONE\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnit","title":"<code>MetricOverTimeUnit</code>","text":"<p>               Bases: <code>BaseMetricUnit</code></p> <p>Defines the units for metrics that are a generic unit over a specific time unit.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricOverTimeUnit(BaseMetricUnit):\n    \"\"\"Defines the units for metrics that are a generic unit over a specific time unit.\"\"\"\n\n    REQUESTS_PER_SECOND = MetricOverTimeUnitInfo(\n        primary_unit=GenericMetricUnit.REQUESTS,\n        time_unit=MetricTimeUnit.SECONDS,\n    )\n    TOKENS_PER_SECOND = MetricOverTimeUnitInfo(\n        primary_unit=GenericMetricUnit.TOKENS,\n        time_unit=MetricTimeUnit.SECONDS,\n    )\n    TOKENS_PER_SECOND_PER_USER = MetricOverTimeUnitInfo(\n        primary_unit=GenericMetricUnit.TOKENS,\n        time_unit=MetricTimeUnit.SECONDS,\n        third_unit=GenericMetricUnit.USER,\n    )\n\n    @cached_property\n    def info(self) -&gt; MetricOverTimeUnitInfo:\n        \"\"\"Get the info for the metric over time unit.\"\"\"\n        return self._info  # type: ignore\n\n    @cached_property\n    def primary_unit(self) -&gt; \"MetricUnitT\":\n        \"\"\"Get the primary unit.\"\"\"\n        return self.info.primary_unit\n\n    @cached_property\n    def time_unit(self) -&gt; MetricTimeUnit | MetricTimeUnitInfo:\n        \"\"\"Get the time unit.\"\"\"\n        return self.info.time_unit\n\n    @cached_property\n    def third_unit(self) -&gt; \"MetricUnitT | None\":\n        \"\"\"Get the third unit (if applicable).\"\"\"\n        return self.info.third_unit\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnit.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the info for the metric over time unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnit.primary_unit","title":"<code>primary_unit</code>  <code>cached</code> <code>property</code>","text":"<p>Get the primary unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnit.third_unit","title":"<code>third_unit</code>  <code>cached</code> <code>property</code>","text":"<p>Get the third unit (if applicable).</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnit.time_unit","title":"<code>time_unit</code>  <code>cached</code> <code>property</code>","text":"<p>Get the time unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnitInfo","title":"<code>MetricOverTimeUnitInfo</code>","text":"<p>               Bases: <code>BaseMetricUnitInfo</code></p> <p>Information about a metric over time unit.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricOverTimeUnitInfo(BaseMetricUnitInfo):\n    \"\"\"Information about a metric over time unit.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def _set_tag(self: Self) -&gt; Self:\n        \"\"\"Set the tag based on the existing units. ie. requests/sec, tokens/sec, etc.\"\"\"\n        self.tag = f\"{self.primary_unit}/{self.time_unit}\"\n        if self.third_unit:\n            # If there is a third unit, add it to the tag. ie. tokens/sec/user\n            self.tag += f\"/{self.third_unit}\"\n        return self\n\n    tag: str = Field(\n        default=\"\",\n        description=\"The tag for the metric over time unit. This will be set automatically by the model_validator.\",\n    )\n    primary_unit: \"MetricUnitT\"\n    time_unit: MetricTimeUnit | MetricTimeUnitInfo\n    third_unit: \"MetricUnitT | None\" = None\n\n    def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n        \"\"\"Convert a value from this unit to another unit.\"\"\"\n        # If the other unit is the same as this unit, return the value.\n        if other_unit == self:\n            return value\n\n        if isinstance(other_unit, MetricOverTimeUnit | MetricOverTimeUnitInfo):\n            # Chain convert each unit to the other unit.\n            value = self.primary_unit.convert_to(other_unit.primary_unit, value)\n            value = self.time_unit.convert_to(other_unit.time_unit, value)\n            if self.third_unit and other_unit.third_unit:\n                value = self.third_unit.convert_to(other_unit.third_unit, value)\n            return value\n\n        # If the other unit is a time unit, convert our time unit to the other unit.\n        # TODO: Should we even allow this?\n        if isinstance(other_unit, MetricTimeUnit | MetricTimeUnitInfo):\n            return self.time_unit.convert_to(other_unit, value)\n\n        # Otherwise, convert the primary unit to the other unit.\n        return self.primary_unit.convert_to(other_unit, value)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricOverTimeUnitInfo.convert_to","title":"<code>convert_to(other_unit, value)</code>","text":"<p>Convert a value from this unit to another unit.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n    \"\"\"Convert a value from this unit to another unit.\"\"\"\n    # If the other unit is the same as this unit, return the value.\n    if other_unit == self:\n        return value\n\n    if isinstance(other_unit, MetricOverTimeUnit | MetricOverTimeUnitInfo):\n        # Chain convert each unit to the other unit.\n        value = self.primary_unit.convert_to(other_unit.primary_unit, value)\n        value = self.time_unit.convert_to(other_unit.time_unit, value)\n        if self.third_unit and other_unit.third_unit:\n            value = self.third_unit.convert_to(other_unit.third_unit, value)\n        return value\n\n    # If the other unit is a time unit, convert our time unit to the other unit.\n    # TODO: Should we even allow this?\n    if isinstance(other_unit, MetricTimeUnit | MetricTimeUnitInfo):\n        return self.time_unit.convert_to(other_unit, value)\n\n    # Otherwise, convert the primary unit to the other unit.\n    return self.primary_unit.convert_to(other_unit, value)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricSizeUnit","title":"<code>MetricSizeUnit</code>","text":"<p>               Bases: <code>BaseMetricUnit</code></p> <p>Defines the size types for metrics.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricSizeUnit(BaseMetricUnit):\n    \"\"\"Defines the size types for metrics.\"\"\"\n\n    BYTES = MetricSizeUnitInfo(\n        tag=\"B\",\n        long_name=\"bytes\",\n        num_bytes=1,\n    )\n    KILOBYTES = MetricSizeUnitInfo(\n        tag=\"KB\",\n        long_name=\"kilobytes\",\n        num_bytes=1024,\n    )\n    MEGABYTES = MetricSizeUnitInfo(\n        tag=\"MB\",\n        long_name=\"megabytes\",\n        num_bytes=1024 * 1024,\n    )\n    GIGABYTES = MetricSizeUnitInfo(\n        tag=\"GB\",\n        long_name=\"gigabytes\",\n        num_bytes=1024 * 1024 * 1024,\n    )\n    TERABYTES = MetricSizeUnitInfo(\n        tag=\"TB\",\n        long_name=\"terabytes\",\n        num_bytes=1024 * 1024 * 1024 * 1024,\n    )\n\n    @cached_property\n    def info(self) -&gt; MetricSizeUnitInfo:\n        \"\"\"Get the info for the metric size unit.\"\"\"\n        return self._info  # type: ignore\n\n    @cached_property\n    def num_bytes(self) -&gt; int:\n        \"\"\"The number of bytes in the metric size unit.\"\"\"\n        return self.info.num_bytes\n\n    @cached_property\n    def long_name(self) -&gt; str:\n        \"\"\"The long name of the metric size unit.\"\"\"\n        return self.info.long_name\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricSizeUnit.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the info for the metric size unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricSizeUnit.long_name","title":"<code>long_name</code>  <code>cached</code> <code>property</code>","text":"<p>The long name of the metric size unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricSizeUnit.num_bytes","title":"<code>num_bytes</code>  <code>cached</code> <code>property</code>","text":"<p>The number of bytes in the metric size unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricSizeUnitInfo","title":"<code>MetricSizeUnitInfo</code>","text":"<p>               Bases: <code>BaseMetricUnitInfo</code></p> <p>Information about a size unit for metrics.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricSizeUnitInfo(BaseMetricUnitInfo):\n    \"\"\"Information about a size unit for metrics.\"\"\"\n\n    long_name: str\n    num_bytes: int\n\n    def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n        \"\"\"Convert a value from this unit to another unit.\"\"\"\n        if not isinstance(other_unit, MetricSizeUnit | MetricSizeUnitInfo):\n            return super().convert_to(other_unit, value)\n\n        return value * (self.num_bytes / other_unit.num_bytes)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricSizeUnitInfo.convert_to","title":"<code>convert_to(other_unit, value)</code>","text":"<p>Convert a value from this unit to another unit.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n    \"\"\"Convert a value from this unit to another unit.\"\"\"\n    if not isinstance(other_unit, MetricSizeUnit | MetricSizeUnitInfo):\n        return super().convert_to(other_unit, value)\n\n    return value * (self.num_bytes / other_unit.num_bytes)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeUnit","title":"<code>MetricTimeUnit</code>","text":"<p>               Bases: <code>BaseMetricUnit</code></p> <p>Defines the various time units that can be used for metrics, as well as the conversion factor to convert to other units.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricTimeUnit(BaseMetricUnit):\n    \"\"\"Defines the various time units that can be used for metrics, as well as the conversion factor to convert to other units.\"\"\"\n\n    NANOSECONDS = MetricTimeUnitInfo(\n        tag=\"ns\",\n        long_name=\"nanoseconds\",\n        per_second=1_000_000_000,\n    )\n    MICROSECONDS = MetricTimeUnitInfo(\n        tag=\"us\",\n        long_name=\"microseconds\",\n        per_second=1_000_000,\n    )\n    MILLISECONDS = MetricTimeUnitInfo(\n        tag=\"ms\",\n        long_name=\"milliseconds\",\n        per_second=1_000,\n    )\n    SECONDS = MetricTimeUnitInfo(\n        tag=\"sec\",\n        long_name=\"seconds\",\n        per_second=1,\n    )\n\n    @cached_property\n    def info(self) -&gt; MetricTimeUnitInfo:\n        \"\"\"Get the info for the metric time unit.\"\"\"\n        return self._info  # type: ignore\n\n    @cached_property\n    def per_second(self) -&gt; int:\n        \"\"\"How many of these units there are in one second. Used as a common conversion factor to convert to other units.\"\"\"\n        return self.info.per_second\n\n    @cached_property\n    def long_name(self) -&gt; str:\n        \"\"\"The long name of the metric time unit.\"\"\"\n        return self.info.long_name\n\n    def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n        \"\"\"Convert a value from this unit to another unit.\"\"\"\n        if not isinstance(\n            other_unit, MetricTimeUnit | MetricTimeUnitInfo | MetricDateTimeUnit\n        ):\n            return super().convert_to(other_unit, value)\n\n        if isinstance(other_unit, MetricDateTimeUnit):\n            return datetime.fromtimestamp(\n                self.convert_to(MetricTimeUnit.SECONDS, value)\n            )\n\n        return value * (other_unit.per_second / self.per_second)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeUnit.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the info for the metric time unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeUnit.long_name","title":"<code>long_name</code>  <code>cached</code> <code>property</code>","text":"<p>The long name of the metric time unit.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeUnit.per_second","title":"<code>per_second</code>  <code>cached</code> <code>property</code>","text":"<p>How many of these units there are in one second. Used as a common conversion factor to convert to other units.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeUnit.convert_to","title":"<code>convert_to(other_unit, value)</code>","text":"<p>Convert a value from this unit to another unit.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>def convert_to(self, other_unit: \"MetricUnitT\", value: int | float) -&gt; float:\n    \"\"\"Convert a value from this unit to another unit.\"\"\"\n    if not isinstance(\n        other_unit, MetricTimeUnit | MetricTimeUnitInfo | MetricDateTimeUnit\n    ):\n        return super().convert_to(other_unit, value)\n\n    if isinstance(other_unit, MetricDateTimeUnit):\n        return datetime.fromtimestamp(\n            self.convert_to(MetricTimeUnit.SECONDS, value)\n        )\n\n    return value * (other_unit.per_second / self.per_second)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricTimeUnitInfo","title":"<code>MetricTimeUnitInfo</code>","text":"<p>               Bases: <code>BaseMetricUnitInfo</code></p> <p>Information about a time unit for metrics.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricTimeUnitInfo(BaseMetricUnitInfo):\n    \"\"\"Information about a time unit for metrics.\"\"\"\n\n    long_name: str\n    per_second: int\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricType","title":"<code>MetricType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Defines the possible types of metrics.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricType(CaseInsensitiveStrEnum):\n    \"\"\"Defines the possible types of metrics.\"\"\"\n\n    RECORD = \"record\"\n    \"\"\"Metrics that provide a distinct value for each request. Every request that comes in will produce a new value that is not affected by any other requests.\n    These metrics can be tracked over time and compared to each other.\n    Examples: request latency, ISL, ITL, OSL, etc.\"\"\"\n\n    SUM_AGGREGATE = \"sum_aggregate\"\n    \"\"\"Metrics that are assigned as the sum aggregator for a specific metric.\n    Examples: total request count, benchmark duration, etc.\"\"\"\n\n    AGGREGATE = \"aggregate\"\n    \"\"\"Metrics that keep track of one or more values over time, that are updated for each request, such as total counts, min/max values, etc.\n    These metrics may or may not change each request, and are affected by other requests.\n    Examples: min/max request latency, total request count, benchmark duration, etc.\"\"\"\n\n    DERIVED = \"derived\"\n    \"\"\"Metrics that are purely derived from other metrics as a summary, and do not require per-request values.\n    Examples: request throughput, output token throughput, etc.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricType.AGGREGATE","title":"<code>AGGREGATE = 'aggregate'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that keep track of one or more values over time, that are updated for each request, such as total counts, min/max values, etc. These metrics may or may not change each request, and are affected by other requests. Examples: min/max request latency, total request count, benchmark duration, etc.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricType.DERIVED","title":"<code>DERIVED = 'derived'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are purely derived from other metrics as a summary, and do not require per-request values. Examples: request throughput, output token throughput, etc.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricType.RECORD","title":"<code>RECORD = 'record'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that provide a distinct value for each request. Every request that comes in will produce a new value that is not affected by any other requests. These metrics can be tracked over time and compared to each other. Examples: request latency, ISL, ITL, OSL, etc.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricType.SUM_AGGREGATE","title":"<code>SUM_AGGREGATE = 'sum_aggregate'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that are assigned as the sum aggregator for a specific metric. Examples: total request count, benchmark duration, etc.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueType","title":"<code>MetricValueType</code>","text":"<p>               Bases: <code>BasePydanticBackedStrEnum</code></p> <p>Defines the possible types of values for metrics.</p> <p>NOTE: The string representation is important here, as it is used to automatically determine the type based on the python generic type definition.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricValueType(BasePydanticBackedStrEnum):\n    \"\"\"Defines the possible types of values for metrics.\n\n    NOTE: The string representation is important here, as it is used to automatically determine the type\n    based on the python generic type definition.\n    \"\"\"\n\n    FLOAT = MetricValueTypeInfo(\n        tag=\"float\",\n        default_factory=float,\n        converter=float,\n        dtype=float,\n    )\n    INT = MetricValueTypeInfo(\n        tag=\"int\",\n        default_factory=int,\n        converter=int,\n        dtype=int,\n    )\n\n    @cached_property\n    def info(self) -&gt; MetricValueTypeInfo:\n        \"\"\"Get the info for the metric value type.\"\"\"\n        return self._info  # type: ignore\n\n    @cached_property\n    def default_factory(self) -&gt; Callable[[], MetricValueTypeT]:\n        \"\"\"Get the default value generator for the metric value type.\"\"\"\n        return self.info.default_factory\n\n    @cached_property\n    def converter(self) -&gt; Callable[[Any], MetricValueTypeT]:\n        \"\"\"Get the converter for the metric value type.\"\"\"\n        return self.info.converter\n\n    @cached_property\n    def dtype(self) -&gt; Any:\n        \"\"\"Get the dtype for the metric value type (for pandas/numpy).\"\"\"\n        return self.info.dtype\n\n    @classmethod\n    def from_python_type(cls, type: type[MetricValueTypeT]) -&gt; \"MetricValueType\":\n        \"\"\"Get the MetricValueType for a given type.\"\"\"\n        # If the type is a simple type like float or int, we have to use __name__.\n        # This is because using str() on float or int will return &lt;class 'float'&gt; or &lt;class 'int'&gt;, etc.\n        type_name = type.__name__\n        if type_name == \"list\":\n            # However, if the type is a list, we have to use str() to get the list type as well, e.g. list[int]\n            type_name = str(type)\n        elif type_name == \"MetricValueTypeVarT\":\n            type_name = \"float\"  # Default to float if the user did not specify a type.\n        return MetricValueType(type_name)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueType.converter","title":"<code>converter</code>  <code>cached</code> <code>property</code>","text":"<p>Get the converter for the metric value type.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueType.default_factory","title":"<code>default_factory</code>  <code>cached</code> <code>property</code>","text":"<p>Get the default value generator for the metric value type.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueType.dtype","title":"<code>dtype</code>  <code>cached</code> <code>property</code>","text":"<p>Get the dtype for the metric value type (for pandas/numpy).</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueType.info","title":"<code>info</code>  <code>cached</code> <code>property</code>","text":"<p>Get the info for the metric value type.</p>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueType.from_python_type","title":"<code>from_python_type(type)</code>  <code>classmethod</code>","text":"<p>Get the MetricValueType for a given type.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>@classmethod\ndef from_python_type(cls, type: type[MetricValueTypeT]) -&gt; \"MetricValueType\":\n    \"\"\"Get the MetricValueType for a given type.\"\"\"\n    # If the type is a simple type like float or int, we have to use __name__.\n    # This is because using str() on float or int will return &lt;class 'float'&gt; or &lt;class 'int'&gt;, etc.\n    type_name = type.__name__\n    if type_name == \"list\":\n        # However, if the type is a list, we have to use str() to get the list type as well, e.g. list[int]\n        type_name = str(type)\n    elif type_name == \"MetricValueTypeVarT\":\n        type_name = \"float\"  # Default to float if the user did not specify a type.\n    return MetricValueType(type_name)\n</code></pre>"},{"location":"api/#aiperf.common.enums.metric_enums.MetricValueTypeInfo","title":"<code>MetricValueTypeInfo</code>","text":"<p>               Bases: <code>BasePydanticEnumInfo</code></p> <p>Information about a metric value type.</p> Source code in <code>aiperf/common/enums/metric_enums.py</code> <pre><code>class MetricValueTypeInfo(BasePydanticEnumInfo):\n    \"\"\"Information about a metric value type.\"\"\"\n\n    default_factory: Callable[[], MetricValueTypeT]\n    converter: Callable[[Any], MetricValueTypeT]\n    dtype: Any\n</code></pre>"},{"location":"api/#aiperfcommonenumsmodel_enums","title":"aiperf.common.enums.model_enums","text":""},{"location":"api/#aiperf.common.enums.model_enums.ModelSelectionStrategy","title":"<code>ModelSelectionStrategy</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Strategy for selecting the model to use for the request.</p> Source code in <code>aiperf/common/enums/model_enums.py</code> <pre><code>class ModelSelectionStrategy(CaseInsensitiveStrEnum):\n    \"\"\"Strategy for selecting the model to use for the request.\"\"\"\n\n    ROUND_ROBIN = \"round_robin\"\n    RANDOM = \"random\"\n</code></pre>"},{"location":"api/#aiperfcommonenumsopenai_enums","title":"aiperf.common.enums.openai_enums","text":""},{"location":"api/#aiperf.common.enums.openai_enums.OpenAIObjectType","title":"<code>OpenAIObjectType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>OpenAI object type definitions. See: https://platform.openai.com/docs/api-reference</p> Source code in <code>aiperf/common/enums/openai_enums.py</code> <pre><code>class OpenAIObjectType(CaseInsensitiveStrEnum):\n    \"\"\"OpenAI object type definitions.\n    See: https://platform.openai.com/docs/api-reference\n    \"\"\"\n\n    CHAT_COMPLETION = \"chat.completion\"\n    CHAT_COMPLETION_CHUNK = \"chat.completion.chunk\"\n    COMPLETION = \"completion\"\n    EMBEDDING = \"embedding\"\n    LIST = \"list\"\n    RANKINGS = \"rankings\"\n    RESPONSE = \"response\"\n    TEXT_COMPLETION = \"text_completion\"\n</code></pre>"},{"location":"api/#aiperfcommonenumspost_processor_enums","title":"aiperf.common.enums.post_processor_enums","text":""},{"location":"api/#aiperf.common.enums.post_processor_enums.RecordProcessorType","title":"<code>RecordProcessorType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Type of streaming record processor.</p> Source code in <code>aiperf/common/enums/post_processor_enums.py</code> <pre><code>class RecordProcessorType(CaseInsensitiveStrEnum):\n    \"\"\"Type of streaming record processor.\"\"\"\n\n    METRIC_RECORD = \"metric_record\"\n    \"\"\"Streamer that streams records and computes metrics from MetricType.RECORD and MetricType.AGGREGATE.\n    This is the first stage of the metrics processing pipeline, and is done is a distributed manner across multiple service instances.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.post_processor_enums.RecordProcessorType.METRIC_RECORD","title":"<code>METRIC_RECORD = 'metric_record'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Streamer that streams records and computes metrics from MetricType.RECORD and MetricType.AGGREGATE. This is the first stage of the metrics processing pipeline, and is done is a distributed manner across multiple service instances.</p>"},{"location":"api/#aiperf.common.enums.post_processor_enums.ResultsProcessorType","title":"<code>ResultsProcessorType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Type of streaming results processor.</p> Source code in <code>aiperf/common/enums/post_processor_enums.py</code> <pre><code>class ResultsProcessorType(CaseInsensitiveStrEnum):\n    \"\"\"Type of streaming results processor.\"\"\"\n\n    METRIC_RESULTS = \"metric_results\"\n    \"\"\"Processor that processes the metric results from METRIC_RECORD and computes metrics from MetricType.DERIVED. as well as aggregates the results.\n    This is the last stage of the metrics processing pipeline, and is done from the RecordsManager after all the service instances have completed their processing.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.post_processor_enums.ResultsProcessorType.METRIC_RESULTS","title":"<code>METRIC_RESULTS = 'metric_results'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Processor that processes the metric results from METRIC_RECORD and computes metrics from MetricType.DERIVED. as well as aggregates the results. This is the last stage of the metrics processing pipeline, and is done from the RecordsManager after all the service instances have completed their processing.</p>"},{"location":"api/#aiperfcommonenumsservice_enums","title":"aiperf.common.enums.service_enums","text":""},{"location":"api/#aiperf.common.enums.service_enums.LifecycleState","title":"<code>LifecycleState</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>This is the various states a lifecycle can be in.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class LifecycleState(CaseInsensitiveStrEnum):\n    \"\"\"This is the various states a lifecycle can be in.\"\"\"\n\n    CREATED = \"created\"\n    INITIALIZING = \"initializing\"\n    INITIALIZED = \"initialized\"\n    STARTING = \"starting\"\n    RUNNING = \"running\"\n    STOPPING = \"stopping\"\n    STOPPED = \"stopped\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus","title":"<code>ServiceRegistrationStatus</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Defines the various states a service can be in during registration with the SystemController.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class ServiceRegistrationStatus(CaseInsensitiveStrEnum):\n    \"\"\"Defines the various states a service can be in during registration with\n    the SystemController.\"\"\"\n\n    UNREGISTERED = \"unregistered\"\n    \"\"\"The service is not registered with the SystemController. This is the\n    initial state.\"\"\"\n\n    WAITING = \"waiting\"\n    \"\"\"The service is waiting for the SystemController to register it.\n    This is a temporary state that should be followed by REGISTERED, TIMEOUT, or ERROR.\"\"\"\n\n    REGISTERED = \"registered\"\n    \"\"\"The service is registered with the SystemController.\"\"\"\n\n    TIMEOUT = \"timeout\"\n    \"\"\"The service registration timed out.\"\"\"\n\n    ERROR = \"error\"\n    \"\"\"The service registration failed.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.ERROR","title":"<code>ERROR = 'error'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service registration failed.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.REGISTERED","title":"<code>REGISTERED = 'registered'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service is registered with the SystemController.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.TIMEOUT","title":"<code>TIMEOUT = 'timeout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service registration timed out.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.UNREGISTERED","title":"<code>UNREGISTERED = 'unregistered'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service is not registered with the SystemController. This is the initial state.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRegistrationStatus.WAITING","title":"<code>WAITING = 'waiting'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The service is waiting for the SystemController to register it. This is a temporary state that should be followed by REGISTERED, TIMEOUT, or ERROR.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRunType","title":"<code>ServiceRunType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The different ways the SystemController should run the component services.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class ServiceRunType(CaseInsensitiveStrEnum):\n    \"\"\"The different ways the SystemController should run the component services.\"\"\"\n\n    MULTIPROCESSING = \"process\"\n    \"\"\"Run each service as a separate process.\n    This is the default way for single-node deployments.\"\"\"\n\n    KUBERNETES = \"k8s\"\n    \"\"\"Run each service as a separate Kubernetes pod.\n    This is the default way for multi-node deployments.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRunType.KUBERNETES","title":"<code>KUBERNETES = 'k8s'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Run each service as a separate Kubernetes pod. This is the default way for multi-node deployments.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceRunType.MULTIPROCESSING","title":"<code>MULTIPROCESSING = 'process'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Run each service as a separate process. This is the default way for single-node deployments.</p>"},{"location":"api/#aiperf.common.enums.service_enums.ServiceType","title":"<code>ServiceType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Types of services in the AIPerf system.</p> <p>This is used to identify the service type when registering with the SystemController. It can also be used for tracking purposes if multiple instances of the same service type are running.</p> Source code in <code>aiperf/common/enums/service_enums.py</code> <pre><code>class ServiceType(CaseInsensitiveStrEnum):\n    \"\"\"Types of services in the AIPerf system.\n\n    This is used to identify the service type when registering with the\n    SystemController. It can also be used for tracking purposes if multiple\n    instances of the same service type are running.\n    \"\"\"\n\n    SYSTEM_CONTROLLER = \"system_controller\"\n    DATASET_MANAGER = \"dataset_manager\"\n    TIMING_MANAGER = \"timing_manager\"\n    RECORD_PROCESSOR = \"record_processor\"\n    RECORDS_MANAGER = \"records_manager\"\n    WORKER_MANAGER = \"worker_manager\"\n    WORKER = \"worker\"\n\n    # For testing purposes only\n    TEST = \"test_service\"\n</code></pre>"},{"location":"api/#aiperfcommonenumssse_enums","title":"aiperf.common.enums.sse_enums","text":""},{"location":"api/#aiperf.common.enums.sse_enums.SSEFieldType","title":"<code>SSEFieldType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>Field types in an SSE message.</p> Source code in <code>aiperf/common/enums/sse_enums.py</code> <pre><code>class SSEFieldType(CaseInsensitiveStrEnum):\n    \"\"\"Field types in an SSE message.\"\"\"\n\n    DATA = \"data\"\n    EVENT = \"event\"\n    ID = \"id\"\n    RETRY = \"retry\"\n    COMMENT = \"comment\"\n</code></pre>"},{"location":"api/#aiperfcommonenumssystem_enums","title":"aiperf.common.enums.system_enums","text":""},{"location":"api/#aiperf.common.enums.system_enums.SystemState","title":"<code>SystemState</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>State of the system as a whole.</p> <p>This is used to track the state of the system as a whole, and is used to determine what actions to take when a signal is received.</p> Source code in <code>aiperf/common/enums/system_enums.py</code> <pre><code>class SystemState(CaseInsensitiveStrEnum):\n    \"\"\"State of the system as a whole.\n\n    This is used to track the state of the system as a whole, and is used to\n    determine what actions to take when a signal is received.\n    \"\"\"\n\n    INITIALIZING = \"initializing\"\n    \"\"\"The system is initializing. This is the initial state.\"\"\"\n\n    CONFIGURING = \"configuring\"\n    \"\"\"The system is configuring services.\"\"\"\n\n    READY = \"ready\"\n    \"\"\"The system is ready to start profiling. This is a temporary state that should be\n    followed by PROFILING.\"\"\"\n\n    PROFILING = \"profiling\"\n    \"\"\"The system is running a profiling run.\"\"\"\n\n    PROCESSING = \"processing\"\n    \"\"\"The system is processing results.\"\"\"\n\n    STOPPING = \"stopping\"\n    \"\"\"The system is stopping.\"\"\"\n\n    SHUTDOWN = \"shutdown\"\n    \"\"\"The system is shutting down. This is the final state.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.CONFIGURING","title":"<code>CONFIGURING = 'configuring'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is configuring services.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.INITIALIZING","title":"<code>INITIALIZING = 'initializing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is initializing. This is the initial state.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.PROCESSING","title":"<code>PROCESSING = 'processing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is processing results.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.PROFILING","title":"<code>PROFILING = 'profiling'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is running a profiling run.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.READY","title":"<code>READY = 'ready'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is ready to start profiling. This is a temporary state that should be followed by PROFILING.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.SHUTDOWN","title":"<code>SHUTDOWN = 'shutdown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is shutting down. This is the final state.</p>"},{"location":"api/#aiperf.common.enums.system_enums.SystemState.STOPPING","title":"<code>STOPPING = 'stopping'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The system is stopping.</p>"},{"location":"api/#aiperfcommonenumstiming_enums","title":"aiperf.common.enums.timing_enums","text":""},{"location":"api/#aiperf.common.enums.timing_enums.CreditPhase","title":"<code>CreditPhase</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The type of credit phase. This is used to identify which phase of the benchmark the credit is being used in, for tracking and reporting purposes.</p> Source code in <code>aiperf/common/enums/timing_enums.py</code> <pre><code>class CreditPhase(CaseInsensitiveStrEnum):\n    \"\"\"The type of credit phase. This is used to identify which phase of the\n    benchmark the credit is being used in, for tracking and reporting purposes.\"\"\"\n\n    WARMUP = \"warmup\"\n    \"\"\"The credit phase while the warmup is active. This is used to warm up the model and\n    ensure that the model is ready to be profiled.\"\"\"\n\n    PROFILING = \"profiling\"\n    \"\"\"The credit phase while profiling is active. This is the primary phase of the\n    benchmark, and what is used to calculate the final results.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.timing_enums.CreditPhase.PROFILING","title":"<code>PROFILING = 'profiling'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The credit phase while profiling is active. This is the primary phase of the benchmark, and what is used to calculate the final results.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.CreditPhase.WARMUP","title":"<code>WARMUP = 'warmup'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The credit phase while the warmup is active. This is used to warm up the model and ensure that the model is ready to be profiled.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode","title":"<code>RequestRateMode</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The different ways the RequestRateStrategy should generate requests.</p> Source code in <code>aiperf/common/enums/timing_enums.py</code> <pre><code>class RequestRateMode(CaseInsensitiveStrEnum):\n    \"\"\"The different ways the RequestRateStrategy should generate requests.\"\"\"\n\n    CONSTANT = \"constant\"\n    \"\"\"Generate requests at a constant rate.\"\"\"\n\n    POISSON = \"poisson\"\n    \"\"\"Generate requests using a poisson process.\"\"\"\n\n    CONCURRENCY_BURST = \"concurrency_burst\"\n    \"\"\"Generate requests as soon as possible, up to a max concurrency limit. Only allowed when a request rate is not specified.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode.CONCURRENCY_BURST","title":"<code>CONCURRENCY_BURST = 'concurrency_burst'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate requests as soon as possible, up to a max concurrency limit. Only allowed when a request rate is not specified.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode.CONSTANT","title":"<code>CONSTANT = 'constant'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate requests at a constant rate.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.RequestRateMode.POISSON","title":"<code>POISSON = 'poisson'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate requests using a poisson process.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode","title":"<code>TimingMode</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The different ways the TimingManager should generate requests.</p> Source code in <code>aiperf/common/enums/timing_enums.py</code> <pre><code>class TimingMode(CaseInsensitiveStrEnum):\n    \"\"\"The different ways the TimingManager should generate requests.\"\"\"\n\n    FIXED_SCHEDULE = \"fixed_schedule\"\n    \"\"\"A mode where the TimingManager will send requests according to a fixed schedule.\"\"\"\n\n    REQUEST_RATE = \"request_rate\"\n    \"\"\"A mode where the TimingManager will send requests using a request rate generator based on various modes.\n    Optionally, a max concurrency limit can be specified as well.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode.FIXED_SCHEDULE","title":"<code>FIXED_SCHEDULE = 'fixed_schedule'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A mode where the TimingManager will send requests according to a fixed schedule.</p>"},{"location":"api/#aiperf.common.enums.timing_enums.TimingMode.REQUEST_RATE","title":"<code>REQUEST_RATE = 'request_rate'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A mode where the TimingManager will send requests using a request rate generator based on various modes. Optionally, a max concurrency limit can be specified as well.</p>"},{"location":"api/#aiperfcommonenumsui_enums","title":"aiperf.common.enums.ui_enums","text":""},{"location":"api/#aiperf.common.enums.ui_enums.AIPerfUIType","title":"<code>AIPerfUIType</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The type of UI to use.</p> Source code in <code>aiperf/common/enums/ui_enums.py</code> <pre><code>class AIPerfUIType(CaseInsensitiveStrEnum):\n    \"\"\"The type of UI to use.\"\"\"\n\n    DASHBOARD = \"dashboard\"\n    SIMPLE = \"simple\"\n    NONE = \"none\"\n</code></pre>"},{"location":"api/#aiperfcommonenumsworker_enums","title":"aiperf.common.enums.worker_enums","text":""},{"location":"api/#aiperf.common.enums.worker_enums.WorkerStatus","title":"<code>WorkerStatus</code>","text":"<p>               Bases: <code>CaseInsensitiveStrEnum</code></p> <p>The current status of a worker service.</p> <p>NOTE: The order of the statuses is important for the UI.</p> Source code in <code>aiperf/common/enums/worker_enums.py</code> <pre><code>class WorkerStatus(CaseInsensitiveStrEnum):\n    \"\"\"The current status of a worker service.\n\n    NOTE: The order of the statuses is important for the UI.\n    \"\"\"\n\n    HEALTHY = \"healthy\"\n    HIGH_LOAD = \"high_load\"\n    ERROR = \"error\"\n    IDLE = \"idle\"\n    STALE = \"stale\"\n</code></pre>"},{"location":"api/#aiperfcommonexceptions","title":"aiperf.common.exceptions","text":""},{"location":"api/#aiperf.common.exceptions.AIPerfError","title":"<code>AIPerfError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions raised by AIPerf.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class AIPerfError(Exception):\n    \"\"\"Base class for all exceptions raised by AIPerf.\"\"\"\n\n    def raw_str(self) -&gt; str:\n        \"\"\"Return the raw string representation of the exception.\"\"\"\n        return super().__str__()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the exception with the class name.\"\"\"\n        return f\"{self.__class__.__name__}: {super().__str__()}\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.AIPerfError.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the exception with the class name.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the exception with the class name.\"\"\"\n    return f\"{self.__class__.__name__}: {super().__str__()}\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.AIPerfError.raw_str","title":"<code>raw_str()</code>","text":"<p>Return the raw string representation of the exception.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>def raw_str(self) -&gt; str:\n    \"\"\"Return the raw string representation of the exception.\"\"\"\n    return super().__str__()\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.AIPerfMultiError","title":"<code>AIPerfMultiError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when running multiple tasks and one or more fail.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class AIPerfMultiError(AIPerfError):\n    \"\"\"Exception raised when running multiple tasks and one or more fail.\"\"\"\n\n    def __init__(self, message: str, exceptions: list[Exception]) -&gt; None:\n        err_strings = [\n            e.raw_str() if isinstance(e, AIPerfError) else str(e) for e in exceptions\n        ]\n        super().__init__(f\"{message}: {','.join(err_strings)}\")\n        self.exceptions = exceptions\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.CommunicationError","title":"<code>CommunicationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic communication error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class CommunicationError(AIPerfError):\n    \"\"\"Generic communication error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something fails to configure, or there is a configuration error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ConfigurationError(AIPerfError):\n    \"\"\"Exception raised when something fails to configure, or there is a configuration error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.DatasetError","title":"<code>DatasetError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic dataset error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class DatasetError(AIPerfError):\n    \"\"\"Generic dataset error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.DatasetGeneratorError","title":"<code>DatasetGeneratorError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic dataset generator error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class DatasetGeneratorError(AIPerfError):\n    \"\"\"Generic dataset generator error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.DatasetLoaderError","title":"<code>DatasetLoaderError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic dataset loader error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class DatasetLoaderError(AIPerfError):\n    \"\"\"Generic dataset loader error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.FactoryCreationError","title":"<code>FactoryCreationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a factory encounters an error while creating a class.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class FactoryCreationError(AIPerfError):\n    \"\"\"Exception raised when a factory encounters an error while creating a class.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InferenceClientError","title":"<code>InferenceClientError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a inference client encounters an error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InferenceClientError(AIPerfError):\n    \"\"\"Exception raised when a inference client encounters an error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InitializationError","title":"<code>InitializationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something fails to initialize.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InitializationError(AIPerfError):\n    \"\"\"Exception raised when something fails to initialize.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InvalidOperationError","title":"<code>InvalidOperationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when an operation is invalid.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InvalidOperationError(AIPerfError):\n    \"\"\"Exception raised when an operation is invalid.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InvalidPayloadError","title":"<code>InvalidPayloadError</code>","text":"<p>               Bases: <code>InferenceClientError</code></p> <p>Exception raised when a inference client receives an invalid payload.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InvalidPayloadError(InferenceClientError):\n    \"\"\"Exception raised when a inference client receives an invalid payload.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.InvalidStateError","title":"<code>InvalidStateError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something is in an invalid state.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class InvalidStateError(AIPerfError):\n    \"\"\"Exception raised when something is in an invalid state.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.MetricTypeError","title":"<code>MetricTypeError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a metric type encounters an error while creating a class.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class MetricTypeError(AIPerfError):\n    \"\"\"Exception raised when a metric type encounters an error while creating a class.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.MetricUnitError","title":"<code>MetricUnitError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when trying to convert a metric to or from a unit that is does not support it.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class MetricUnitError(AIPerfError):\n    \"\"\"Exception raised when trying to convert a metric to or from a unit that is does not support it.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.NoMetricValue","title":"<code>NoMetricValue</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Raised when a metric value is not available.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class NoMetricValue(AIPerfError):\n    \"\"\"Raised when a metric value is not available.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something is not found or not available.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class NotFoundError(AIPerfError):\n    \"\"\"Exception raised when something is not found or not available.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.NotInitializedError","title":"<code>NotInitializedError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something that should be initialized is not.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class NotInitializedError(AIPerfError):\n    \"\"\"Exception raised when something that should be initialized is not.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ProxyError","title":"<code>ProxyError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a proxy encounters an error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ProxyError(AIPerfError):\n    \"\"\"Exception raised when a proxy encounters an error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ServiceError","title":"<code>ServiceError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Generic service error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ServiceError(AIPerfError):\n    \"\"\"Generic service error.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        service_type: \"ServiceTypeT\",\n        service_id: str,\n    ) -&gt; None:\n        super().__init__(\n            f\"{message} for service of type {service_type} with id {service_id}\"\n        )\n        self.service_type = service_type\n        self.service_id = service_id\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ShutdownError","title":"<code>ShutdownError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a service encounters an error while shutting down.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ShutdownError(AIPerfError):\n    \"\"\"Exception raised when a service encounters an error while shutting down.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.TokenizerError","title":"<code>TokenizerError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a tokenizer encounters an error.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class TokenizerError(AIPerfError):\n    \"\"\"Exception raised when a tokenizer encounters an error.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.UnsupportedHookError","title":"<code>UnsupportedHookError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when a hook is defined on a class that does not have any base classes that provide that hook type.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class UnsupportedHookError(AIPerfError):\n    \"\"\"Exception raised when a hook is defined on a class that does not have any base classes that provide that hook type.\"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>AIPerfError</code></p> <p>Exception raised when something fails validation.</p> Source code in <code>aiperf/common/exceptions.py</code> <pre><code>class ValidationError(AIPerfError):\n    \"\"\"Exception raised when something fails validation.\"\"\"\n</code></pre>"},{"location":"api/#aiperfcommonfactories","title":"aiperf.common.factories","text":""},{"location":"api/#aiperf.common.factories.AIPerfFactory","title":"<code>AIPerfFactory</code>","text":"<p>               Bases: <code>Generic[ClassEnumT, ClassProtocolT]</code></p> <p>Defines a custom factory for AIPerf components.</p> <p>This class is used to create a factory for a given class type and protocol.</p> <p>Example:</p> <pre><code>    # Define a new enum for the expected implementation types\n    # This is optional, but recommended for type safety.\n    class DatasetLoaderType(CaseInsensitiveStrEnum):\n        FILE = \"file\"\n        S3 = \"s3\"\n\n    # Define a new class protocol.\n    class DatasetLoaderProtocol(Protocol):\n        def load(self) -&gt; Dataset:\n            pass\n\n    # Create a new factory for a given class type and protocol.\n    class DatasetFactory(FactoryMixin[DatasetLoaderType, DatasetLoaderProtocol]):\n        pass\n\n    # Register a new class type mapping to its corresponding class. It should implement the class protocol.\n    @DatasetFactory.register(DatasetLoaderType.FILE)\n    class FileDatasetLoader:\n        def __init__(self, filename: str):\n            self.filename = filename\n\n        def load(self) -&gt; Dataset:\n            return Dataset.from_file(self.filename)\n\n    DatasetConfig = {\n        \"type\": DatasetLoaderType.FILE,\n        \"filename\": \"data.csv\"\n    }\n\n    # Create a new instance of the class.\n    if DatasetConfig[\"type\"] == DatasetLoaderType.FILE:\n        dataset_instance = DatasetFactory.create_instance(DatasetLoaderType.FILE, filename=DatasetConfig[\"filename\"])\n    else:\n        raise ValueError(f\"Unsupported dataset loader type: {DatasetConfig['type']}\")\n\n    dataset_instance.load()\n</code></pre> Source code in <code>aiperf/common/factories.py</code> <pre><code>class AIPerfFactory(Generic[ClassEnumT, ClassProtocolT]):\n    \"\"\"Defines a custom factory for AIPerf components.\n\n    This class is used to create a factory for a given class type and protocol.\n\n    Example:\n    ```python\n        # Define a new enum for the expected implementation types\n        # This is optional, but recommended for type safety.\n        class DatasetLoaderType(CaseInsensitiveStrEnum):\n            FILE = \"file\"\n            S3 = \"s3\"\n\n        # Define a new class protocol.\n        class DatasetLoaderProtocol(Protocol):\n            def load(self) -&gt; Dataset:\n                pass\n\n        # Create a new factory for a given class type and protocol.\n        class DatasetFactory(FactoryMixin[DatasetLoaderType, DatasetLoaderProtocol]):\n            pass\n\n        # Register a new class type mapping to its corresponding class. It should implement the class protocol.\n        @DatasetFactory.register(DatasetLoaderType.FILE)\n        class FileDatasetLoader:\n            def __init__(self, filename: str):\n                self.filename = filename\n\n            def load(self) -&gt; Dataset:\n                return Dataset.from_file(self.filename)\n\n        DatasetConfig = {\n            \"type\": DatasetLoaderType.FILE,\n            \"filename\": \"data.csv\"\n        }\n\n        # Create a new instance of the class.\n        if DatasetConfig[\"type\"] == DatasetLoaderType.FILE:\n            dataset_instance = DatasetFactory.create_instance(DatasetLoaderType.FILE, filename=DatasetConfig[\"filename\"])\n        else:\n            raise ValueError(f\"Unsupported dataset loader type: {DatasetConfig['type']}\")\n\n        dataset_instance.load()\n    ```\n    \"\"\"\n\n    _logger: AIPerfLogger\n    _registry: dict[ClassEnumT | str, type[ClassProtocolT]]\n    _override_priorities: dict[ClassEnumT | str, int]\n\n    def __init_subclass__(cls) -&gt; None:\n        cls._registry = {}\n        cls._override_priorities = {}\n        cls._logger = AIPerfLogger(cls.__name__)\n        super().__init_subclass__()\n\n    @classmethod\n    def register_all(\n        cls, *class_types: ClassEnumT | str, override_priority: int = 0\n    ) -&gt; Callable:\n        \"\"\"Register multiple class types mapping to a single corresponding class.\n        This is useful if a single class implements multiple types. Currently only supports\n        registering as a single override priority for all types.\"\"\"\n\n        def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n            for class_type in class_types:\n                cls.register(class_type, override_priority)(class_cls)\n            return class_cls\n\n        return decorator\n\n    @classmethod\n    def register(\n        cls, class_type: ClassEnumT | str, override_priority: int = 0\n    ) -&gt; Callable:\n        \"\"\"Register a new class type mapping to its corresponding class.\n\n        Args:\n            class_type: The type of class to register\n            override_priority: The priority of the override. The higher the priority,\n                the more precedence the override has when multiple classes are registered\n                for the same class type. Built-in classes have a priority of 0.\n\n        Returns:\n            Decorator for the class that implements the class protocol\n        \"\"\"\n\n        def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n            existing_priority = cls._override_priorities.get(class_type, -1)\n            if class_type in cls._registry and existing_priority &gt;= override_priority:\n                cls._logger.warning(\n                    f\"{class_type!r} class {cls._registry[class_type].__name__} already registered with same or higher priority \"\n                    f\"({existing_priority}). The new registration of class {class_cls.__name__} with priority \"\n                    f\"{override_priority} will be ignored.\",\n                )\n                return class_cls\n\n            if class_type not in cls._registry:\n                cls._logger.debug(\n                    lambda: f\"{class_type!r} class {class_cls.__name__} registered with priority {override_priority}.\",\n                )\n            else:\n                cls._logger.warning(\n                    f\"{class_type!r} class {class_cls.__name__} with priority {override_priority} overrides \"\n                    f\"already registered class {cls._registry[class_type].__name__} with lower priority ({existing_priority}).\",\n                )\n            cls._registry[class_type] = class_cls\n            cls._override_priorities[class_type] = override_priority\n            return class_cls\n\n        return decorator\n\n    @classmethod\n    def create_instance(\n        cls,\n        class_type: ClassEnumT | str,\n        **kwargs: Any,\n    ) -&gt; ClassProtocolT:\n        \"\"\"Create a new class instance.\n\n        Args:\n            class_type: The type of class to create\n            **kwargs: Additional arguments for the class\n\n        Returns:\n            The created class instance\n\n        Raises:\n            FactoryCreationError: If the class type is not registered or there is an error creating the instance\n        \"\"\"\n        if class_type not in cls._registry:\n            raise FactoryCreationError(\n                f\"No implementation registered for {class_type!r} in {cls.__name__}.\"\n            )\n        try:\n            return cls._registry[class_type](**kwargs)\n        except Exception as e:\n            raise FactoryCreationError(\n                f\"Error creating {class_type!r} instance for {cls.__name__}: {e}\"\n            ) from e\n\n    @classmethod\n    def get_class_from_type(cls, class_type: ClassEnumT | str) -&gt; type[ClassProtocolT]:\n        \"\"\"Get the class from a class type.\n\n        Args:\n            class_type: The class type to get the class from\n\n        Returns:\n            The class for the given class type\n\n        Raises:\n            TypeError: If the class type is not registered\n        \"\"\"\n        if class_type not in cls._registry:\n            raise TypeError(\n                f\"No class found for {class_type!r}. Please register the class first.\"\n            )\n        return cls._registry[class_type]\n\n    @classmethod\n    def get_all_classes(cls) -&gt; list[type[ClassProtocolT]]:\n        \"\"\"Get all registered classes.\n\n        Returns:\n            A list of all registered class types implementing the expected protocol\n        \"\"\"\n        return list(cls._registry.values())\n\n    @classmethod\n    def get_all_class_types(cls) -&gt; list[ClassEnumT | str]:\n        \"\"\"Get all registered class types.\"\"\"\n        return list(cls._registry.keys())\n\n    @classmethod\n    def get_all_classes_and_types(\n        cls,\n    ) -&gt; list[tuple[type[ClassProtocolT], ClassEnumT | str]]:\n        \"\"\"Get all registered classes and their corresponding class types.\"\"\"\n        return [(cls, class_type) for class_type, cls in cls._registry.items()]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.create_instance","title":"<code>create_instance(class_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new class instance.</p> <p>Parameters:</p> Name Type Description Default <code>class_type</code> <code>ClassEnumT | str</code> <p>The type of class to create</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the class</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClassProtocolT</code> <p>The created class instance</p> <p>Raises:</p> Type Description <code>FactoryCreationError</code> <p>If the class type is not registered or there is an error creating the instance</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef create_instance(\n    cls,\n    class_type: ClassEnumT | str,\n    **kwargs: Any,\n) -&gt; ClassProtocolT:\n    \"\"\"Create a new class instance.\n\n    Args:\n        class_type: The type of class to create\n        **kwargs: Additional arguments for the class\n\n    Returns:\n        The created class instance\n\n    Raises:\n        FactoryCreationError: If the class type is not registered or there is an error creating the instance\n    \"\"\"\n    if class_type not in cls._registry:\n        raise FactoryCreationError(\n            f\"No implementation registered for {class_type!r} in {cls.__name__}.\"\n        )\n    try:\n        return cls._registry[class_type](**kwargs)\n    except Exception as e:\n        raise FactoryCreationError(\n            f\"Error creating {class_type!r} instance for {cls.__name__}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_all_class_types","title":"<code>get_all_class_types()</code>  <code>classmethod</code>","text":"<p>Get all registered class types.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_all_class_types(cls) -&gt; list[ClassEnumT | str]:\n    \"\"\"Get all registered class types.\"\"\"\n    return list(cls._registry.keys())\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_all_classes","title":"<code>get_all_classes()</code>  <code>classmethod</code>","text":"<p>Get all registered classes.</p> <p>Returns:</p> Type Description <code>list[type[ClassProtocolT]]</code> <p>A list of all registered class types implementing the expected protocol</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_all_classes(cls) -&gt; list[type[ClassProtocolT]]:\n    \"\"\"Get all registered classes.\n\n    Returns:\n        A list of all registered class types implementing the expected protocol\n    \"\"\"\n    return list(cls._registry.values())\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_all_classes_and_types","title":"<code>get_all_classes_and_types()</code>  <code>classmethod</code>","text":"<p>Get all registered classes and their corresponding class types.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_all_classes_and_types(\n    cls,\n) -&gt; list[tuple[type[ClassProtocolT], ClassEnumT | str]]:\n    \"\"\"Get all registered classes and their corresponding class types.\"\"\"\n    return [(cls, class_type) for class_type, cls in cls._registry.items()]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.get_class_from_type","title":"<code>get_class_from_type(class_type)</code>  <code>classmethod</code>","text":"<p>Get the class from a class type.</p> <p>Parameters:</p> Name Type Description Default <code>class_type</code> <code>ClassEnumT | str</code> <p>The class type to get the class from</p> required <p>Returns:</p> Type Description <code>type[ClassProtocolT]</code> <p>The class for the given class type</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the class type is not registered</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_class_from_type(cls, class_type: ClassEnumT | str) -&gt; type[ClassProtocolT]:\n    \"\"\"Get the class from a class type.\n\n    Args:\n        class_type: The class type to get the class from\n\n    Returns:\n        The class for the given class type\n\n    Raises:\n        TypeError: If the class type is not registered\n    \"\"\"\n    if class_type not in cls._registry:\n        raise TypeError(\n            f\"No class found for {class_type!r}. Please register the class first.\"\n        )\n    return cls._registry[class_type]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.register","title":"<code>register(class_type, override_priority=0)</code>  <code>classmethod</code>","text":"<p>Register a new class type mapping to its corresponding class.</p> <p>Parameters:</p> Name Type Description Default <code>class_type</code> <code>ClassEnumT | str</code> <p>The type of class to register</p> required <code>override_priority</code> <code>int</code> <p>The priority of the override. The higher the priority, the more precedence the override has when multiple classes are registered for the same class type. Built-in classes have a priority of 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Decorator for the class that implements the class protocol</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef register(\n    cls, class_type: ClassEnumT | str, override_priority: int = 0\n) -&gt; Callable:\n    \"\"\"Register a new class type mapping to its corresponding class.\n\n    Args:\n        class_type: The type of class to register\n        override_priority: The priority of the override. The higher the priority,\n            the more precedence the override has when multiple classes are registered\n            for the same class type. Built-in classes have a priority of 0.\n\n    Returns:\n        Decorator for the class that implements the class protocol\n    \"\"\"\n\n    def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n        existing_priority = cls._override_priorities.get(class_type, -1)\n        if class_type in cls._registry and existing_priority &gt;= override_priority:\n            cls._logger.warning(\n                f\"{class_type!r} class {cls._registry[class_type].__name__} already registered with same or higher priority \"\n                f\"({existing_priority}). The new registration of class {class_cls.__name__} with priority \"\n                f\"{override_priority} will be ignored.\",\n            )\n            return class_cls\n\n        if class_type not in cls._registry:\n            cls._logger.debug(\n                lambda: f\"{class_type!r} class {class_cls.__name__} registered with priority {override_priority}.\",\n            )\n        else:\n            cls._logger.warning(\n                f\"{class_type!r} class {class_cls.__name__} with priority {override_priority} overrides \"\n                f\"already registered class {cls._registry[class_type].__name__} with lower priority ({existing_priority}).\",\n            )\n        cls._registry[class_type] = class_cls\n        cls._override_priorities[class_type] = override_priority\n        return class_cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfFactory.register_all","title":"<code>register_all(*class_types, override_priority=0)</code>  <code>classmethod</code>","text":"<p>Register multiple class types mapping to a single corresponding class. This is useful if a single class implements multiple types. Currently only supports registering as a single override priority for all types.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef register_all(\n    cls, *class_types: ClassEnumT | str, override_priority: int = 0\n) -&gt; Callable:\n    \"\"\"Register multiple class types mapping to a single corresponding class.\n    This is useful if a single class implements multiple types. Currently only supports\n    registering as a single override priority for all types.\"\"\"\n\n    def decorator(class_cls: type[ClassProtocolT]) -&gt; type[ClassProtocolT]:\n        for class_type in class_types:\n            cls.register(class_type, override_priority)(class_cls)\n        return class_cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfSingletonFactory","title":"<code>AIPerfSingletonFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ClassEnumT, ClassProtocolT]</code></p> <p>Factory for registering and creating singleton instances of a given class type and protocol. This factory is useful for creating instances that are shared across the application, such as communication clients. Calling create_instance will create a new instance if it doesn't exist, otherwise it will return the existing instance. Calling get_instance will return the existing instance if it exists, otherwise it will raise an error. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class AIPerfSingletonFactory(AIPerfFactory[ClassEnumT, ClassProtocolT]):\n    \"\"\"Factory for registering and creating singleton instances of a given class type and protocol.\n    This factory is useful for creating instances that are shared across the application, such as communication clients.\n    Calling create_instance will create a new instance if it doesn't exist, otherwise it will return the existing instance.\n    Calling get_instance will return the existing instance if it exists, otherwise it will raise an error.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    _instances: dict[ClassEnumT | str, ClassProtocolT]\n    _instances_lock: Lock\n    _instances_pid: dict[ClassEnumT | str, int]\n\n    def __init_subclass__(cls) -&gt; None:\n        cls._instances = {}\n        cls._instances_lock = Lock()\n        cls._instances_pid = {}\n        super().__init_subclass__()\n\n    @classmethod\n    def set_instance(\n        cls, class_type: ClassEnumT | str, instance: ClassProtocolT\n    ) -&gt; None:\n        cls._instances[class_type] = instance\n\n    @classmethod\n    def get_or_create_instance(\n        cls, class_type: ClassEnumT | str, **kwargs: Any\n    ) -&gt; ClassProtocolT:\n        \"\"\"Syntactic sugar for create_instance, but with a more descriptive name for singleton factories.\"\"\"\n        return cls.create_instance(class_type, **kwargs)\n\n    @classmethod\n    def create_instance(\n        cls, class_type: ClassEnumT | str, **kwargs: Any\n    ) -&gt; ClassProtocolT:\n        \"\"\"Create a new instance of the given class type.\n        If the instance does not exist, or the process ID has changed, a new instance will be created.\n        \"\"\"\n        # TODO: Technically, this this should handle the case where kwargs are different,\n        #       but that would require a more complex implementation.\n        if (\n            class_type not in cls._instances\n            or os.getpid() != cls._instances_pid[class_type]\n        ):\n            cls._logger.debug(\n                lambda: f\"Creating new instance for {class_type!r} in {cls.__name__}.\"\n            )\n            with cls._instances_lock:\n                if (\n                    class_type not in cls._instances\n                    or os.getpid() != cls._instances_pid[class_type]\n                ):\n                    cls._instances[class_type] = super().create_instance(\n                        class_type, **kwargs\n                    )\n                    cls._instances_pid[class_type] = os.getpid()\n                    cls._logger.debug(\n                        lambda: f\"New instance for {class_type!r} in {cls.__name__} created.\"\n                    )\n        else:\n            cls._logger.debug(\n                lambda: f\"Instance for {class_type!r} in {cls.__name__} already exists. Returning existing instance.\"\n            )\n        return cls._instances[class_type]\n\n    @classmethod\n    def get_instance(cls, class_type: ClassEnumT | str) -&gt; ClassProtocolT:\n        if class_type not in cls._instances:\n            raise InvalidStateError(\n                f\"No instance found for {class_type!r} in {cls.__name__}. \"\n                f\"Ensure you call AIPerfSingletonFactory.create_instance({class_type!r}) first.\"\n            )\n        if os.getpid() != cls._instances_pid[class_type]:\n            raise InvalidStateError(\n                f\"Instance for {class_type!r} in {cls.__name__} is not valid for the current process. \"\n                f\"Ensure you call AIPerfSingletonFactory.create_instance({class_type!r}) first after forking.\"\n            )\n        return cls._instances[class_type]\n\n    @classmethod\n    def get_all_instances(cls) -&gt; dict[ClassEnumT | str, ClassProtocolT]:\n        return cls._instances\n\n    @classmethod\n    def has_instance(cls, class_type: ClassEnumT | str) -&gt; bool:\n        return class_type in cls._instances\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfSingletonFactory.create_instance","title":"<code>create_instance(class_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new instance of the given class type. If the instance does not exist, or the process ID has changed, a new instance will be created.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef create_instance(\n    cls, class_type: ClassEnumT | str, **kwargs: Any\n) -&gt; ClassProtocolT:\n    \"\"\"Create a new instance of the given class type.\n    If the instance does not exist, or the process ID has changed, a new instance will be created.\n    \"\"\"\n    # TODO: Technically, this this should handle the case where kwargs are different,\n    #       but that would require a more complex implementation.\n    if (\n        class_type not in cls._instances\n        or os.getpid() != cls._instances_pid[class_type]\n    ):\n        cls._logger.debug(\n            lambda: f\"Creating new instance for {class_type!r} in {cls.__name__}.\"\n        )\n        with cls._instances_lock:\n            if (\n                class_type not in cls._instances\n                or os.getpid() != cls._instances_pid[class_type]\n            ):\n                cls._instances[class_type] = super().create_instance(\n                    class_type, **kwargs\n                )\n                cls._instances_pid[class_type] = os.getpid()\n                cls._logger.debug(\n                    lambda: f\"New instance for {class_type!r} in {cls.__name__} created.\"\n                )\n    else:\n        cls._logger.debug(\n            lambda: f\"Instance for {class_type!r} in {cls.__name__} already exists. Returning existing instance.\"\n        )\n    return cls._instances[class_type]\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfSingletonFactory.get_or_create_instance","title":"<code>get_or_create_instance(class_type, **kwargs)</code>  <code>classmethod</code>","text":"<p>Syntactic sugar for create_instance, but with a more descriptive name for singleton factories.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>@classmethod\ndef get_or_create_instance(\n    cls, class_type: ClassEnumT | str, **kwargs: Any\n) -&gt; ClassProtocolT:\n    \"\"\"Syntactic sugar for create_instance, but with a more descriptive name for singleton factories.\"\"\"\n    return cls.create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.AIPerfUIFactory","title":"<code>AIPerfUIFactory</code>","text":"<p>               Bases: <code>AIPerfSingletonFactory[AIPerfUIType, 'AIPerfUIProtocol']</code></p> <p>Factory for registering and creating AIPerfUIProtocol instances based on the specified AIPerfUI type. see: :class:<code>aiperf.common.factories.AIPerfSingletonFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class AIPerfUIFactory(AIPerfSingletonFactory[AIPerfUIType, \"AIPerfUIProtocol\"]):\n    \"\"\"Factory for registering and creating AIPerfUIProtocol instances based on the specified AIPerfUI type.\n    see: :class:`aiperf.common.factories.AIPerfSingletonFactory` for more details.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.factories.CommunicationClientFactory","title":"<code>CommunicationClientFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[CommClientType, 'CommunicationClientProtocol']</code></p> <p>Factory for registering and creating CommunicationClientProtocol instances based on the specified communication client type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class CommunicationClientFactory(\n    AIPerfFactory[CommClientType, \"CommunicationClientProtocol\"]\n):\n    \"\"\"Factory for registering and creating CommunicationClientProtocol instances based on the specified communication client type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: CommClientType | str,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; \"CommunicationClientProtocol\":\n        return super().create_instance(\n            class_type, address=address, bind=bind, socket_ops=socket_ops, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.CommunicationFactory","title":"<code>CommunicationFactory</code>","text":"<p>               Bases: <code>AIPerfSingletonFactory[CommunicationBackend, 'CommunicationProtocol']</code></p> <p>Factory for registering and creating CommunicationProtocol instances based on the specified communication backend. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class CommunicationFactory(\n    AIPerfSingletonFactory[CommunicationBackend, \"CommunicationProtocol\"]\n):\n    \"\"\"Factory for registering and creating CommunicationProtocol instances based on the specified communication backend.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: CommunicationBackend | str,\n        config: \"BaseZMQCommunicationConfig\",\n        **kwargs,\n    ) -&gt; \"CommunicationProtocol\":\n        return super().create_instance(class_type, config=config, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.ComposerFactory","title":"<code>ComposerFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ComposerType, 'BaseDatasetComposer']</code></p> <p>Factory for registering and creating BaseDatasetComposer instances based on the specified composer type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ComposerFactory(AIPerfFactory[ComposerType, \"BaseDatasetComposer\"]):\n    \"\"\"Factory for registering and creating BaseDatasetComposer instances based on the specified composer type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ComposerType | str,\n        **kwargs,\n    ) -&gt; \"BaseDatasetComposer\":\n        return super().create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.ConsoleExporterFactory","title":"<code>ConsoleExporterFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ConsoleExporterType, 'ConsoleExporterProtocol']</code></p> <p>Factory for registering and creating ConsoleExporterProtocol instances based on the specified data exporter type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ConsoleExporterFactory(\n    AIPerfFactory[ConsoleExporterType, \"ConsoleExporterProtocol\"]\n):\n    \"\"\"Factory for registering and creating ConsoleExporterProtocol instances based on the specified data exporter type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ConsoleExporterType | str,\n        exporter_config: \"ExporterConfig\",\n        **kwargs,\n    ) -&gt; \"ConsoleExporterProtocol\":\n        return super().create_instance(\n            class_type, exporter_config=exporter_config, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.CustomDatasetFactory","title":"<code>CustomDatasetFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[CustomDatasetType, 'CustomDatasetLoaderProtocol']</code></p> <p>Factory for registering and creating CustomDatasetLoaderProtocol instances based on the specified custom dataset type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class CustomDatasetFactory(\n    AIPerfFactory[CustomDatasetType, \"CustomDatasetLoaderProtocol\"]\n):\n    \"\"\"Factory for registering and creating CustomDatasetLoaderProtocol instances based on the specified custom dataset type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: CustomDatasetType | str,\n        **kwargs,\n    ) -&gt; \"CustomDatasetLoaderProtocol\":\n        return super().create_instance(class_type, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.factories.DataExporterFactory","title":"<code>DataExporterFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[DataExporterType, 'DataExporterProtocol']</code></p> <p>Factory for registering and creating DataExporterProtocol instances based on the specified data exporter type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class DataExporterFactory(AIPerfFactory[DataExporterType, \"DataExporterProtocol\"]):\n    \"\"\"Factory for registering and creating DataExporterProtocol instances based on the specified data exporter type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: DataExporterType | str,\n        exporter_config: \"ExporterConfig\",\n        **kwargs,\n    ) -&gt; \"DataExporterProtocol\":\n        return super().create_instance(\n            class_type, exporter_config=exporter_config, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.InferenceClientFactory","title":"<code>InferenceClientFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[EndpointType, 'InferenceClientProtocol']</code></p> <p>Factory for registering and creating InferenceClientProtocol instances based on the specified endpoint type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class InferenceClientFactory(AIPerfFactory[EndpointType, \"InferenceClientProtocol\"]):\n    \"\"\"Factory for registering and creating InferenceClientProtocol instances based on the specified endpoint type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: EndpointType | str,\n        model_endpoint: \"ModelEndpointInfo\",\n        **kwargs,\n    ) -&gt; \"InferenceClientProtocol\":\n        return super().create_instance(\n            class_type, model_endpoint=model_endpoint, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.OpenAIObjectParserFactory","title":"<code>OpenAIObjectParserFactory</code>","text":"<p>               Bases: <code>AIPerfSingletonFactory[OpenAIObjectType, 'OpenAIObjectParserProtocol']</code></p> <p>Factory for registering and creating OpenAIObjectParserProtocol instances based on the specified object type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class OpenAIObjectParserFactory(\n    AIPerfSingletonFactory[OpenAIObjectType, \"OpenAIObjectParserProtocol\"]\n):\n    \"\"\"Factory for registering and creating OpenAIObjectParserProtocol instances based on the specified object type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.factories.RecordProcessorFactory","title":"<code>RecordProcessorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[RecordProcessorType, 'RecordProcessorProtocol']</code></p> <p>Factory for registering and creating RecordProcessorProtocol instances based on the specified record processor type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class RecordProcessorFactory(\n    AIPerfFactory[RecordProcessorType, \"RecordProcessorProtocol\"]\n):\n    \"\"\"Factory for registering and creating RecordProcessorProtocol instances based on the specified record processor type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: RecordProcessorType | str,\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        **kwargs,\n    ) -&gt; \"RecordProcessorProtocol\":\n        return super().create_instance(\n            class_type,\n            service_config=service_config,\n            user_config=user_config,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.RequestConverterFactory","title":"<code>RequestConverterFactory</code>","text":"<p>               Bases: <code>AIPerfSingletonFactory[EndpointType, 'RequestConverterProtocol']</code></p> <p>Factory for registering and creating RequestConverterProtocol instances based on the specified request payload type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class RequestConverterFactory(\n    AIPerfSingletonFactory[EndpointType, \"RequestConverterProtocol\"]\n):\n    \"\"\"Factory for registering and creating RequestConverterProtocol instances based on the specified request payload type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.factories.RequestRateGeneratorFactory","title":"<code>RequestRateGeneratorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[RequestRateMode, 'RequestRateGeneratorProtocol']</code></p> <p>Factory for registering and creating RequestRateGeneratorProtocol instances based on the specified RequestRateMode. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class RequestRateGeneratorFactory(\n    AIPerfFactory[RequestRateMode, \"RequestRateGeneratorProtocol\"]\n):\n    \"\"\"Factory for registering and creating RequestRateGeneratorProtocol instances based on the specified RequestRateMode.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        config: \"TimingManagerConfig\",\n    ) -&gt; \"RequestRateGeneratorProtocol\":\n        return super().create_instance(config.request_rate_mode, config=config)\n</code></pre>"},{"location":"api/#aiperf.common.factories.ResponseExtractorFactory","title":"<code>ResponseExtractorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[EndpointType, 'ResponseExtractorProtocol']</code></p> <p>Factory for registering and creating ResponseExtractorProtocol instances based on the specified response extractor type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ResponseExtractorFactory(\n    AIPerfFactory[EndpointType, \"ResponseExtractorProtocol\"]\n):\n    \"\"\"Factory for registering and creating ResponseExtractorProtocol instances based on the specified response extractor type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: EndpointType | str,\n        model_endpoint: \"ModelEndpointInfo\",\n        **kwargs,\n    ) -&gt; \"ResponseExtractorProtocol\":\n        return super().create_instance(\n            class_type, model_endpoint=model_endpoint, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.ResultsProcessorFactory","title":"<code>ResultsProcessorFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ResultsProcessorType, 'ResultsProcessorProtocol']</code></p> <p>Factory for registering and creating ResultsProcessorProtocol instances based on the specified results processor type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ResultsProcessorFactory(\n    AIPerfFactory[ResultsProcessorType, \"ResultsProcessorProtocol\"]\n):\n    \"\"\"Factory for registering and creating ResultsProcessorProtocol instances based on the specified results processor type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ResultsProcessorType | str,\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        **kwargs,\n    ) -&gt; \"ResultsProcessorProtocol\":\n        return super().create_instance(\n            class_type,\n            service_config=service_config,\n            user_config=user_config,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.ServiceFactory","title":"<code>ServiceFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ServiceType, 'ServiceProtocol']</code></p> <p>Factory for registering and creating ServiceProtocol instances based on the specified service type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ServiceFactory(AIPerfFactory[ServiceType, \"ServiceProtocol\"]):\n    \"\"\"Factory for registering and creating ServiceProtocol instances based on the specified service type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def register_all(\n        cls, *class_types: ServiceTypeT, override_priority: int = 0\n    ) -&gt; Callable[..., Any]:\n        raise InvalidOperationError(\n            \"ServiceFactory.register_all is not supported. A single service can only be registered with a single type.\"\n        )\n\n    @classmethod\n    def register(\n        cls, class_type: ServiceTypeT, override_priority: int = 0\n    ) -&gt; Callable[..., Any]:\n        # Override the register method to set the service_type on the class\n        original_decorator = super().register(class_type, override_priority)\n\n        def decorator(class_cls: type[ServiceProtocolT]) -&gt; type[ServiceProtocolT]:\n            class_cls.service_type = class_type\n            original_decorator(class_cls)\n            return class_cls\n\n        return decorator\n</code></pre>"},{"location":"api/#aiperf.common.factories.ServiceManagerFactory","title":"<code>ServiceManagerFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ServiceRunType, 'ServiceManagerProtocol']</code></p> <p>Factory for registering and creating ServiceManagerProtocol instances based on the specified service run type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ServiceManagerFactory(AIPerfFactory[ServiceRunType, \"ServiceManagerProtocol\"]):\n    \"\"\"Factory for registering and creating ServiceManagerProtocol instances based on the specified service run type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ServiceRunType | str,\n        required_services: dict[ServiceTypeT, int],\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        **kwargs,\n    ) -&gt; \"ServiceManagerProtocol\":\n        return super().create_instance(\n            class_type,\n            required_services=required_services,\n            service_config=service_config,\n            user_config=user_config,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.factories.ZMQProxyFactory","title":"<code>ZMQProxyFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[ZMQProxyType, 'BaseZMQProxy']</code></p> <p>Factory for registering and creating BaseZMQProxy instances based on the specified ZMQ proxy type. see: :class:<code>aiperf.common.factories.AIPerfFactory</code> for more details.</p> Source code in <code>aiperf/common/factories.py</code> <pre><code>class ZMQProxyFactory(AIPerfFactory[ZMQProxyType, \"BaseZMQProxy\"]):\n    \"\"\"Factory for registering and creating BaseZMQProxy instances based on the specified ZMQ proxy type.\n    see: :class:`aiperf.common.factories.AIPerfFactory` for more details.\n    \"\"\"\n\n    @classmethod\n    def create_instance(  # type: ignore[override]\n        cls,\n        class_type: ZMQProxyType | str,\n        zmq_proxy_config: \"BaseZMQProxyConfig\",\n        **kwargs,\n    ) -&gt; \"BaseZMQProxy\":\n        return super().create_instance(\n            class_type, zmq_proxy_config=zmq_proxy_config, **kwargs\n        )\n</code></pre>"},{"location":"api/#aiperfcommonhooks","title":"aiperf.common.hooks","text":"<p>This module provides an extensive set of hook definitions for AIPerf. It is designed to be used in conjunction with the :class:<code>HooksMixin</code> for classes to provide support for hooks. It provides a simple interface for registering hooks.</p> <p>Classes should inherit from the :class:<code>HooksMixin</code>, and specify the provided hook types by decorating the class with the :func:<code>provides_hooks</code> decorator.</p> <p>The hook functions are registered by decorating functions with the various hook decorators such as :func:<code>on_init</code>, :func:<code>on_start</code>, :func:<code>on_stop</code>, etc.</p> <p>More than one hook can be registered for a given hook type, and classes that inherit from classes with existing hooks will inherit the hooks from the base classes as well.</p> <p>The hooks are run by calling the :meth:<code>HooksMixin.run_hooks</code> method or retrieved via the :meth:<code>HooksMixin.get_hooks</code> method on the class.</p>"},{"location":"api/#aiperf.common.hooks.HookType","title":"<code>HookType = AIPerfHook | str</code>  <code>module-attribute</code>","text":"<p>Type alias for valid hook types. This is a union of the AIPerfHook enum and any user-defined custom strings.</p>"},{"location":"api/#aiperf.common.hooks.Hook","title":"<code>Hook</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[HookParamsT]</code></p> <p>A hook is a function that is decorated with a hook type and optional parameters. The HookParamsT is the type of the parameters. You can either have a static value, or a callable that returns the parameters.</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>class Hook(BaseModel, Generic[HookParamsT]):\n    \"\"\"A hook is a function that is decorated with a hook type and optional parameters.\n    The HookParamsT is the type of the parameters. You can either have a static value,\n    or a callable that returns the parameters.\n    \"\"\"\n\n    func: Callable\n    params: HookParamsT | Callable[[SelfT], HookParamsT] | None = None  # type: ignore\n\n    @property\n    def hook_type(self) -&gt; HookType:\n        return getattr(self.func, HookAttrs.HOOK_TYPE)\n\n    @property\n    def func_name(self) -&gt; str:\n        return self.func.__name__\n\n    @property\n    def qualified_name(self) -&gt; str:\n        return f\"{self.func.__qualname__}\"\n\n    def resolve_params(self, self_obj: SelfT) -&gt; HookParamsT | None:\n        \"\"\"Resolve the parameters for the hook. If the parameters are a callable, it will be called\n        with the self_obj as the argument, otherwise the parameters are returned as is.\"\"\"\n        if self.params is None:\n            return None\n        # With variable length parameters, you get a tuple with 1 item in it, so we need to check for that.\n        if (\n            isinstance(self.params, Iterable)\n            and len(self.params) == 1\n            and callable(self.params[0])\n        ):  # type: ignore\n            return self.params[0](self_obj)  # type: ignore\n        if callable(self.params):\n            return self.params(self_obj)\n        return self.params  # type: ignore\n\n    async def __call__(self, **kwargs) -&gt; None:\n        if asyncio.iscoroutinefunction(self.func):\n            await self.func(**kwargs)\n        else:\n            await asyncio.to_thread(self.func, **kwargs)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.hook_type} \ud83e\udc52 {self.qualified_name}\"\n</code></pre>"},{"location":"api/#aiperf.common.hooks.Hook.resolve_params","title":"<code>resolve_params(self_obj)</code>","text":"<p>Resolve the parameters for the hook. If the parameters are a callable, it will be called with the self_obj as the argument, otherwise the parameters are returned as is.</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def resolve_params(self, self_obj: SelfT) -&gt; HookParamsT | None:\n    \"\"\"Resolve the parameters for the hook. If the parameters are a callable, it will be called\n    with the self_obj as the argument, otherwise the parameters are returned as is.\"\"\"\n    if self.params is None:\n        return None\n    # With variable length parameters, you get a tuple with 1 item in it, so we need to check for that.\n    if (\n        isinstance(self.params, Iterable)\n        and len(self.params) == 1\n        and callable(self.params[0])\n    ):  # type: ignore\n        return self.params[0](self_obj)  # type: ignore\n    if callable(self.params):\n        return self.params(self_obj)\n    return self.params  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.common.hooks.HookAttrs","title":"<code>HookAttrs</code>","text":"<p>Constant attribute names for hooks.</p> <p>When you decorate a function with a hook decorator, the hook type and parameters are set as attributes on the function or class.</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>class HookAttrs:\n    \"\"\"Constant attribute names for hooks.\n\n    When you decorate a function with a hook decorator, the hook type and parameters are\n    set as attributes on the function or class.\n    \"\"\"\n\n    HOOK_TYPE = \"__aiperf_hook_type__\"\n    HOOK_PARAMS = \"__aiperf_hook_params__\"\n    PROVIDES_HOOKS = \"__provides_hooks__\"\n</code></pre>"},{"location":"api/#aiperf.common.hooks.background_task","title":"<code>background_task(interval=None, immediate=True, stop_on_error=False)</code>","text":"<p>Decorator to mark a method as a background task with automatic management.</p> <p>Tasks are automatically started when the service starts and stopped when the service stops. The decorated method will be run periodically in the background when the service is running.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float | Callable[[SelfT], float] | None</code> <p>Time between task executions in seconds. If None, the task will run once. Can be a callable that returns the interval, and will be called with 'self' as the argument.</p> <code>None</code> <code>immediate</code> <code>bool</code> <p>If True, run the task immediately on start, otherwise wait for the interval first.</p> <code>True</code> <code>stop_on_error</code> <code>bool</code> <p>If True, stop the task on any exception, otherwise log and continue.</p> <code>False</code> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @background_task(interval=1.0)\n    def _background_task(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._background_task.__aiperf_hook_type__ = AIPerfHook.BACKGROUND_TASK\nMyPlugin._background_task.__aiperf_hook_params__ = BackgroundTaskParams(\n    interval=1.0, immediate=True, stop_on_error=False\n)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def background_task(\n    interval: float | Callable[[SelfT], float] | None = None,\n    immediate: bool = True,\n    stop_on_error: bool = False,\n) -&gt; Callable:\n    \"\"\"\n    Decorator to mark a method as a background task with automatic management.\n\n    Tasks are automatically started when the service starts and stopped when the service stops.\n    The decorated method will be run periodically in the background when the service is running.\n\n    Args:\n        interval: Time between task executions in seconds. If None, the task will run once.\n            Can be a callable that returns the interval, and will be called with 'self' as the argument.\n        immediate: If True, run the task immediately on start, otherwise wait for the interval first.\n        stop_on_error: If True, stop the task on any exception, otherwise log and continue.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @background_task(interval=1.0)\n        def _background_task(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._background_task.__aiperf_hook_type__ = AIPerfHook.BACKGROUND_TASK\n    MyPlugin._background_task.__aiperf_hook_params__ = BackgroundTaskParams(\n        interval=1.0, immediate=True, stop_on_error=False\n    )\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(\n        AIPerfHook.BACKGROUND_TASK,\n        BackgroundTaskParams(\n            interval=interval, immediate=immediate, stop_on_error=stop_on_error\n        ),\n    )\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_command","title":"<code>on_command(*command_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a CommandMessage with the given command type(s) is received from the message bus. See :func:<code>aiperf.common.hooks._hook_decorator_for_message_types</code>.</p> <p>Example:</p> <pre><code>class MyService(BaseComponentService):\n    @on_command(CommandType.PROFILE_START)\n    def _on_profile_start(self, message: ProfileStartCommand) -&gt; CommandResponse:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyService._on_profile_start.__aiperf_hook_type__ = AIPerfHook.ON_COMMAND\nMyService._on_profile_start.__aiperf_hook_params__ = (CommandType.PROFILE_START,)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_command(\n    *command_types: CommandTypeT | Callable[[SelfT], Iterable[CommandTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a CommandMessage with the given\n    command type(s) is received from the message bus.\n    See :func:`aiperf.common.hooks._hook_decorator_for_message_types`.\n\n    Example:\n    ```python\n    class MyService(BaseComponentService):\n        @on_command(CommandType.PROFILE_START)\n        def _on_profile_start(self, message: ProfileStartCommand) -&gt; CommandResponse:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._on_profile_start.__aiperf_hook_type__ = AIPerfHook.ON_COMMAND\n    MyService._on_profile_start.__aiperf_hook_params__ = (CommandType.PROFILE_START,)\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_COMMAND, command_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_init","title":"<code>on_init(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during initialization. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_init\n    def _init_plugin(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._init_plugin.__aiperf_hook_type__ = AIPerfHook.ON_INIT\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_init(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called during initialization.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_init\n        def _init_plugin(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._init_plugin.__aiperf_hook_type__ = AIPerfHook.ON_INIT\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_INIT, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_message","title":"<code>on_message(*message_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when messages of the given type(s) (or topics) are received from the message bus. See :func:<code>aiperf.common.hooks._hook_decorator_with_params</code>.</p> <p>Example:</p> <pre><code>class MyService(MessageBusClientMixin):\n    @on_message(MessageType.STATUS)\n    def _on_status_message(self, message: StatusMessage) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyService._on_status_message.__aiperf_hook_type__ = AIPerfHook.ON_MESSAGE\nMyService._on_status_message.__aiperf_hook_params__ = (MessageType.STATUS,)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_message(\n    *message_types: MessageTypeT | Callable[[SelfT], Iterable[MessageTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when messages of the\n    given type(s) (or topics) are received from the message bus.\n    See :func:`aiperf.common.hooks._hook_decorator_with_params`.\n\n    Example:\n    ```python\n    class MyService(MessageBusClientMixin):\n        @on_message(MessageType.STATUS)\n        def _on_status_message(self, message: StatusMessage) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._on_status_message.__aiperf_hook_type__ = AIPerfHook.ON_MESSAGE\n    MyService._on_status_message.__aiperf_hook_params__ = (MessageType.STATUS,)\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_MESSAGE, message_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_profiling_progress","title":"<code>on_profiling_progress(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a profiling progress update is received. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(ProgressTrackerMixin):\n    @on_profiling_progress\n    def _on_profiling_progress(self, profiling_stats: RequestsStats) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_profiling_progress.__aiperf_hook_type__ = AIPerfHook.ON_PROFILING_PROGRESS\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_profiling_progress(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a profiling progress update is received.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(ProgressTrackerMixin):\n        @on_profiling_progress\n        def _on_profiling_progress(self, profiling_stats: RequestsStats) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_profiling_progress.__aiperf_hook_type__ = AIPerfHook.ON_PROFILING_PROGRESS\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_PROFILING_PROGRESS, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_pull_message","title":"<code>on_pull_message(*message_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called a pull client receives a message of the given type(s). See :func:<code>aiperf.common.hooks._hook_decorator_for_message_types</code>.</p> <p>Example:</p> <pre><code>class MyService(PullClientMixin, BaseComponentService):\n    @on_pull_message(MessageType.CREDIT_DROP)\n    def _on_credit_drop_pull(self, message: CreditDropMessage) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting: ```python MyService._on_pull_message.aiperf_hook_type = AIPerfHook.ON_PULL_MESSAGE MyService._on_pull_message.aiperf_hook_params = (MessageType.CREDIT_DROP,)</p> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_pull_message(\n    *message_types: MessageTypeT | Callable[[SelfT], Iterable[MessageTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called a pull client\n    receives a message of the given type(s).\n    See :func:`aiperf.common.hooks._hook_decorator_for_message_types`.\n\n    Example:\n    ```python\n    class MyService(PullClientMixin, BaseComponentService):\n        @on_pull_message(MessageType.CREDIT_DROP)\n        def _on_credit_drop_pull(self, message: CreditDropMessage) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._on_pull_message.__aiperf_hook_type__ = AIPerfHook.ON_PULL_MESSAGE\n    MyService._on_pull_message.__aiperf_hook_params__ = (MessageType.CREDIT_DROP,)\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_PULL_MESSAGE, message_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_realtime_metrics","title":"<code>on_realtime_metrics(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when real-time metrics are received. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(RealtimeMetricsMixin):\n    @on_realtime_metrics\n    def _on_realtime_metrics(self, metrics: list[MetricResult]) -&gt; None:\n        pass\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_realtime_metrics(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when real-time metrics are received.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(RealtimeMetricsMixin):\n        @on_realtime_metrics\n        def _on_realtime_metrics(self, metrics: list[MetricResult]) -&gt; None:\n            pass\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_REALTIME_METRICS, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_records_progress","title":"<code>on_records_progress(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a records progress update is received. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(ProgressTrackerMixin):\n    @on_records_progress\n    def _on_records_progress(self, progress: RecordsStats) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_records_progress.__aiperf_hook_type__ = AIPerfHook.ON_RECORDS_PROGRESS\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_records_progress(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a records progress update is received.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(ProgressTrackerMixin):\n        @on_records_progress\n        def _on_records_progress(self, progress: RecordsStats) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_records_progress.__aiperf_hook_type__ = AIPerfHook.ON_RECORDS_PROGRESS\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_RECORDS_PROGRESS, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_request","title":"<code>on_request(*message_types)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when requests of the given type(s) are received from a ReplyClient. See :func:<code>aiperf.common.hooks._hook_decorator_for_message_types</code>.</p> <p>Example:</p> <pre><code>class MyService(RequestClientMixin, BaseComponentService):\n    @on_request(MessageType.CONVERSATION_REQUEST)\n    async def _handle_conversation_request(\n        self, message: ConversationRequestMessage\n    ) -&gt; ConversationResponseMessage:\n        return ConversationResponseMessage(\n            ...\n        )\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyService._handle_conversation_request.__aiperf_hook_type__ = AIPerfHook.ON_REQUEST\nMyService._handle_conversation_request.__aiperf_hook_params__ = (MessageType.CONVERSATION_REQUEST,)\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_request(\n    *message_types: MessageTypeT | Callable[[SelfT], Iterable[MessageTypeT]],\n) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when requests of the\n    given type(s) are received from a ReplyClient.\n    See :func:`aiperf.common.hooks._hook_decorator_for_message_types`.\n\n    Example:\n    ```python\n    class MyService(RequestClientMixin, BaseComponentService):\n        @on_request(MessageType.CONVERSATION_REQUEST)\n        async def _handle_conversation_request(\n            self, message: ConversationRequestMessage\n        ) -&gt; ConversationResponseMessage:\n            return ConversationResponseMessage(\n                ...\n            )\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyService._handle_conversation_request.__aiperf_hook_type__ = AIPerfHook.ON_REQUEST\n    MyService._handle_conversation_request.__aiperf_hook_params__ = (MessageType.CONVERSATION_REQUEST,)\n    ```\n    \"\"\"\n    return _hook_decorator_with_params(AIPerfHook.ON_REQUEST, message_types)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_start","title":"<code>on_start(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during start. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_start\n    def _start_plugin(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._start_plugin.__aiperf_hook_type__ = AIPerfHook.ON_START\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_start(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called during start.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_start\n        def _start_plugin(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._start_plugin.__aiperf_hook_type__ = AIPerfHook.ON_START\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_START, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_state_change","title":"<code>on_state_change(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during the service state change. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_state_change\n    def _on_state_change(self, old_state: LifecycleState, new_state: LifecycleState) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_state_change.__aiperf_hook_type__ = AIPerfHook.ON_STATE_CHANGE\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_state_change(\n    func: Callable[[\"HooksMixinT\", LifecycleState, LifecycleState], Awaitable],\n) -&gt; Callable[[\"HooksMixinT\", LifecycleState, LifecycleState], Awaitable]:\n    \"\"\"Decorator to specify that the function is a hook that should be called during the service state change.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_state_change\n        def _on_state_change(self, old_state: LifecycleState, new_state: LifecycleState) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_state_change.__aiperf_hook_type__ = AIPerfHook.ON_STATE_CHANGE\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_STATE_CHANGE, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_stop","title":"<code>on_stop(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called during stop. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(AIPerfLifecycleMixin):\n    @on_stop\n    def _stop_plugin(self) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._stop_plugin.__aiperf_hook_type__ = AIPerfHook.ON_STOP\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_stop(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called during stop.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(AIPerfLifecycleMixin):\n        @on_stop\n        def _stop_plugin(self) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._stop_plugin.__aiperf_hook_type__ = AIPerfHook.ON_STOP\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_STOP, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_warmup_progress","title":"<code>on_warmup_progress(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a warmup progress update is received. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(ProgressTrackerMixin):\n    @on_warmup_progress\n    def _on_warmup_progress(self, warmup_stats: RequestsStats) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_warmup_progress.__aiperf_hook_type__ = AIPerfHook.ON_WARMUP_PROGRESS\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_warmup_progress(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a warmup progress update is received.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(ProgressTrackerMixin):\n        @on_warmup_progress\n        def _on_warmup_progress(self, warmup_stats: RequestsStats) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_warmup_progress.__aiperf_hook_type__ = AIPerfHook.ON_WARMUP_PROGRESS\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_WARMUP_PROGRESS, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_worker_status_summary","title":"<code>on_worker_status_summary(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a worker status summary is received from the WorkerManager. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(WorkerTrackerMixin):\n    @on_worker_status_summary\n    def _on_worker_status_summary(self, worker_statuses: dict[str, WorkerStatus]) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_worker_status_summary.__aiperf_hook_type__ = AIPerfHook.ON_WORKER_STATUS_SUMMARY\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_worker_status_summary(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a worker status summary is received\n    from the WorkerManager.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(WorkerTrackerMixin):\n        @on_worker_status_summary\n        def _on_worker_status_summary(self, worker_statuses: dict[str, WorkerStatus]) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_worker_status_summary.__aiperf_hook_type__ = AIPerfHook.ON_WORKER_STATUS_SUMMARY\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_WORKER_STATUS_SUMMARY, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.on_worker_update","title":"<code>on_worker_update(func)</code>","text":"<p>Decorator to specify that the function is a hook that should be called when a worker update is received. See :func:<code>aiperf.common.hooks._hook_decorator</code>.</p> <p>Example:</p> <pre><code>class MyPlugin(WorkerTrackerMixin):\n    @on_worker_update\n    def _on_worker_update(self, worker_id: str, worker_stats: WorkerStats) -&gt; None:\n        pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MyPlugin._on_worker_update.__aiperf_hook_type__ = AIPerfHook.ON_WORKER_UPDATE\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def on_worker_update(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to specify that the function is a hook that should be called when a worker update is received.\n    See :func:`aiperf.common.hooks._hook_decorator`.\n\n    Example:\n    ```python\n    class MyPlugin(WorkerTrackerMixin):\n        @on_worker_update\n        def _on_worker_update(self, worker_id: str, worker_stats: WorkerStats) -&gt; None:\n            pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MyPlugin._on_worker_update.__aiperf_hook_type__ = AIPerfHook.ON_WORKER_UPDATE\n    ```\n    \"\"\"\n    return _hook_decorator(AIPerfHook.ON_WORKER_UPDATE, func)\n</code></pre>"},{"location":"api/#aiperf.common.hooks.provides_hooks","title":"<code>provides_hooks(*hook_types)</code>","text":"<p>Decorator to specify that the class provides a hook of the given type to all of its subclasses.</p> <p>Example:</p> <pre><code>@provides_hooks(AIPerfHook.ON_MESSAGE)\nclass MessageBusClientMixin(CommunicationMixin):\n    pass\n</code></pre> <p>The above is the equivalent to setting:</p> <pre><code>MessageBusClientMixin.__provides_hooks__ = {AIPerfHook.ON_MESSAGE}\n</code></pre> Source code in <code>aiperf/common/hooks.py</code> <pre><code>def provides_hooks(\n    *hook_types: HookType,\n) -&gt; Callable[[type[HooksMixinT]], type[HooksMixinT]]:\n    \"\"\"Decorator to specify that the class provides a hook of the given type to all of its subclasses.\n\n    Example:\n    ```python\n    @provides_hooks(AIPerfHook.ON_MESSAGE)\n    class MessageBusClientMixin(CommunicationMixin):\n        pass\n    ```\n\n    The above is the equivalent to setting:\n    ```python\n    MessageBusClientMixin.__provides_hooks__ = {AIPerfHook.ON_MESSAGE}\n    ```\n    \"\"\"\n\n    def decorator(cls: type[HooksMixinT]) -&gt; type[HooksMixinT]:\n        setattr(cls, HookAttrs.PROVIDES_HOOKS, set(hook_types))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperfcommonlogging","title":"aiperf.common.logging","text":""},{"location":"api/#aiperf.common.logging.MultiProcessLogHandler","title":"<code>MultiProcessLogHandler</code>","text":"<p>               Bases: <code>RichHandler</code></p> <p>Custom logging handler that forwards log records to a multiprocessing queue.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>class MultiProcessLogHandler(RichHandler):\n    \"\"\"Custom logging handler that forwards log records to a multiprocessing queue.\"\"\"\n\n    def __init__(\n        self, log_queue: multiprocessing.Queue, service_id: str | None = None\n    ) -&gt; None:\n        super().__init__()\n        self.log_queue = log_queue\n        self.service_id = service_id\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"Emit a log record to the queue.\"\"\"\n        try:\n            # Create a serializable log data structure\n            log_data = {\n                \"name\": record.name,\n                \"levelname\": record.levelname,\n                \"levelno\": record.levelno,\n                \"msg\": record.getMessage(),\n                \"created\": record.created,\n                \"process_name\": multiprocessing.current_process().name,\n                \"process_id\": multiprocessing.current_process().pid,\n                \"service_id\": self.service_id,\n            }\n            self.log_queue.put_nowait(log_data)\n        except queue.Full:\n            # Drop logs if queue is full to prevent blocking. Do not log to prevent recursion.\n            pass\n        except Exception:\n            # Do not log to prevent recursion\n            pass\n</code></pre>"},{"location":"api/#aiperf.common.logging.MultiProcessLogHandler.emit","title":"<code>emit(record)</code>","text":"<p>Emit a log record to the queue.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>def emit(self, record: logging.LogRecord) -&gt; None:\n    \"\"\"Emit a log record to the queue.\"\"\"\n    try:\n        # Create a serializable log data structure\n        log_data = {\n            \"name\": record.name,\n            \"levelname\": record.levelname,\n            \"levelno\": record.levelno,\n            \"msg\": record.getMessage(),\n            \"created\": record.created,\n            \"process_name\": multiprocessing.current_process().name,\n            \"process_id\": multiprocessing.current_process().pid,\n            \"service_id\": self.service_id,\n        }\n        self.log_queue.put_nowait(log_data)\n    except queue.Full:\n        # Drop logs if queue is full to prevent blocking. Do not log to prevent recursion.\n        pass\n    except Exception:\n        # Do not log to prevent recursion\n        pass\n</code></pre>"},{"location":"api/#aiperf.common.logging.create_file_handler","title":"<code>create_file_handler(log_folder, level)</code>","text":"<p>Configure a file handler for logging.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>def create_file_handler(\n    log_folder: Path,\n    level: str | int,\n) -&gt; logging.FileHandler:\n    \"\"\"Configure a file handler for logging.\"\"\"\n\n    log_folder.mkdir(parents=True, exist_ok=True)\n    log_file_path = log_folder / \"aiperf.log\"\n\n    file_handler = logging.FileHandler(log_file_path, encoding=\"utf-8\")\n    file_handler.setLevel(level)\n    file_handler.setFormatter(\n        logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n    )\n    return file_handler\n</code></pre>"},{"location":"api/#aiperf.common.logging.get_global_log_queue","title":"<code>get_global_log_queue()</code>  <code>cached</code>","text":"<p>Get the global log queue. Will create a new queue if it doesn't exist.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_global_log_queue() -&gt; multiprocessing.Queue:\n    \"\"\"Get the global log queue. Will create a new queue if it doesn't exist.\"\"\"\n    return multiprocessing.Queue(maxsize=LOG_QUEUE_MAXSIZE)\n</code></pre>"},{"location":"api/#aiperf.common.logging.setup_child_process_logging","title":"<code>setup_child_process_logging(log_queue=None, service_id=None, service_config=None, user_config=None)</code>","text":"<p>Set up logging for a child process to send logs to the main process.</p> <p>This should be called early in child process initialization.</p> <p>Parameters:</p> Name Type Description Default <code>log_queue</code> <code>Queue | None</code> <p>The multiprocessing queue to send logs to. If None, tries to get the global queue.</p> <code>None</code> <code>service_id</code> <code>str | None</code> <p>The ID of the service to log under. If None, logs will be under the process name.</p> <code>None</code> <code>service_config</code> <code>ServiceConfig | None</code> <p>The service configuration used to determine the log level.</p> <code>None</code> <code>user_config</code> <code>UserConfig | None</code> <p>The user configuration used to determine the log folder.</p> <code>None</code> Source code in <code>aiperf/common/logging.py</code> <pre><code>def setup_child_process_logging(\n    log_queue: \"multiprocessing.Queue | None\" = None,\n    service_id: str | None = None,\n    service_config: ServiceConfig | None = None,\n    user_config: UserConfig | None = None,\n) -&gt; None:\n    \"\"\"Set up logging for a child process to send logs to the main process.\n\n    This should be called early in child process initialization.\n\n    Args:\n        log_queue: The multiprocessing queue to send logs to. If None, tries to get the global queue.\n        service_id: The ID of the service to log under. If None, logs will be under the process name.\n        service_config: The service configuration used to determine the log level.\n        user_config: The user configuration used to determine the log folder.\n    \"\"\"\n    root_logger = logging.getLogger()\n    level = ServiceDefaults.LOG_LEVEL.upper()\n    if service_config:\n        level = service_config.log_level.upper()\n\n        if service_id:\n            # If the service is in the trace or debug services, set the level to trace or debug\n            if service_config.developer.trace_services and _is_service_in_types(\n                service_id, service_config.developer.trace_services\n            ):\n                level = _TRACE\n            elif service_config.developer.debug_services and _is_service_in_types(\n                service_id, service_config.developer.debug_services\n            ):\n                level = _DEBUG\n\n    # Set the root logger level to ensure logs are passed to handlers\n    root_logger.setLevel(level)\n\n    # Remove all existing handlers to avoid duplicate logs\n    for existing_handler in root_logger.handlers[:]:\n        root_logger.removeHandler(existing_handler)\n\n    if (\n        log_queue is not None\n        and service_config\n        and service_config.ui_type == AIPerfUIType.DASHBOARD\n    ):\n        # For dashboard UI, we want to log to the queue, so it can be displayed in the UI\n        # log viewer, instead of the console directly.\n        queue_handler = MultiProcessLogHandler(log_queue, service_id)\n        queue_handler.setLevel(level)\n        root_logger.addHandler(queue_handler)\n    else:\n        # For all other cases, set up rich logging to the console\n        rich_handler = RichHandler(\n            rich_tracebacks=True,\n            show_path=True,\n            console=Console(),\n            show_time=True,\n            show_level=True,\n            tracebacks_show_locals=False,\n            log_time_format=\"%H:%M:%S.%f\",\n            omit_repeated_times=False,\n        )\n        rich_handler.setLevel(level)\n        root_logger.addHandler(rich_handler)\n\n    if user_config and user_config.output.artifact_directory:\n        file_handler = create_file_handler(\n            user_config.output.artifact_directory / \"logs\", level\n        )\n        root_logger.addHandler(file_handler)\n</code></pre>"},{"location":"api/#aiperf.common.logging.setup_rich_logging","title":"<code>setup_rich_logging(user_config, service_config)</code>","text":"<p>Set up rich logging with appropriate configuration.</p> Source code in <code>aiperf/common/logging.py</code> <pre><code>def setup_rich_logging(user_config: UserConfig, service_config: ServiceConfig) -&gt; None:\n    \"\"\"Set up rich logging with appropriate configuration.\"\"\"\n    # Set logging level for the root logger (affects all loggers)\n    level = service_config.log_level.upper()\n    logging.root.setLevel(level)\n\n    rich_handler = RichHandler(\n        rich_tracebacks=True,\n        show_path=True,\n        console=Console(),\n        show_time=True,\n        show_level=True,\n        tracebacks_show_locals=False,\n        log_time_format=\"%H:%M:%S.%f\",\n        omit_repeated_times=False,\n    )\n    logging.root.addHandler(rich_handler)\n\n    # Enable file logging for services\n    # TODO: Use config to determine if file logging is enabled and the folder path.\n    log_folder = user_config.output.artifact_directory / \"logs\"\n    log_folder.mkdir(parents=True, exist_ok=True)\n    file_handler = logging.FileHandler(log_folder / \"aiperf.log\")\n    file_handler.setLevel(level)\n    file_handler.formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    logging.root.addHandler(file_handler)\n\n    _logger.debug(lambda: f\"Logging initialized with level: {level}\")\n</code></pre>"},{"location":"api/#aiperfcommonmessagesbase_messages","title":"aiperf.common.messages.base_messages","text":""},{"location":"api/#aiperf.common.messages.base_messages.ErrorMessage","title":"<code>ErrorMessage</code>","text":"<p>               Bases: <code>Message</code></p> <p>Message containing error data.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>class ErrorMessage(Message):\n    \"\"\"Message containing error data.\"\"\"\n\n    message_type: MessageTypeT = MessageType.ERROR\n\n    error: ErrorDetails = Field(..., description=\"Error information\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base message class for optimized message handling. Based on the AIPerfBaseModel class, so it supports @exclude_if_none decorator. see :class:<code>AIPerfBaseModel</code> for more details.</p> <p>This class provides a base for all messages, including common fields like message_type, request_ns, and request_id. It also supports optional field exclusion based on the @exclude_if_none decorator.</p> <p>Each message model should inherit from this class, set the message_type field, and define its own additional fields.</p> <p>Example:</p> <pre><code>@exclude_if_none(\"some_field\")\nclass ExampleMessage(Message):\n    some_field: int | None = Field(default=None)\n    other_field: int = Field(default=1)\n</code></pre> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>@exclude_if_none(\"request_ns\", \"request_id\")\nclass Message(AIPerfBaseModel):\n    \"\"\"Base message class for optimized message handling. Based on the AIPerfBaseModel class,\n    so it supports @exclude_if_none decorator. see :class:`AIPerfBaseModel` for more details.\n\n    This class provides a base for all messages, including common fields like message_type,\n    request_ns, and request_id. It also supports optional field exclusion based on the\n    @exclude_if_none decorator.\n\n    Each message model should inherit from this class, set the message_type field,\n    and define its own additional fields.\n\n    Example:\n    ```python\n    @exclude_if_none(\"some_field\")\n    class ExampleMessage(Message):\n        some_field: int | None = Field(default=None)\n        other_field: int = Field(default=1)\n    ```\n    \"\"\"\n\n    _message_type_lookup: ClassVar[dict[MessageTypeT, type[\"Message\"]]] = {}\n    \"\"\"Lookup table for message types to their corresponding message classes. This is used to automatically\n    deserialize messages from JSON strings to their corresponding class type.\"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if hasattr(cls, \"message_type\") and cls.message_type is not None:\n            # Store concrete message classes in the lookup table\n            cls._message_type_lookup[cls.message_type] = cls\n            _logger.trace(f\"Added {cls.message_type} to message type lookup\")\n\n    message_type: MessageTypeT = Field(\n        ...,\n        description=\"The type of the message. Must be set in the subclass.\",\n    )\n\n    request_ns: int | None = Field(\n        default=None,\n        description=\"Timestamp of the request\",\n    )\n\n    request_id: str | None = Field(\n        default=None,\n        description=\"ID of the request\",\n    )\n\n    # TODO: Does this allow you to use model_validate_json and have it forward it to from_json? Need to test.\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.from_json\n\n    @classmethod\n    def from_json(cls, json_str: str | bytes | bytearray) -&gt; \"Message\":\n        \"\"\"Deserialize a message from a JSON string, attempting to auto-detect the message type.\n        NOTE: If you already know the message type, use the more performant :meth:`from_json_with_type` instead.\"\"\"\n        data = json.loads(json_str)\n        message_type = data.get(\"message_type\")\n        if not message_type:\n            raise ValueError(f\"Missing message_type: {json_str}\")\n\n        # Use cached message type lookup\n        message_class = cls._message_type_lookup[message_type]\n        if not message_class:\n            raise ValueError(f\"Unknown message type: {message_type}\")\n\n        return message_class.model_validate(data)\n\n    @classmethod\n    def from_json_with_type(\n        cls, message_type: MessageTypeT, json_str: str | bytes | bytearray\n    ) -&gt; \"Message\":\n        \"\"\"Deserialize a message from a JSON string with a specific message type.\n        NOTE: This is more performant than :meth:`from_json` because it does not need to\n        convert the JSON string to a dictionary first.\"\"\"\n        # Use cached message type lookup\n        message_class = cls._message_type_lookup[message_type]\n        if not message_class:\n            raise ValueError(f\"Unknown message type: {message_type}\")\n        return message_class.model_validate_json(json_str)\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a message from a JSON string, attempting to auto-detect the message type. NOTE: If you already know the message type, use the more performant :meth:<code>from_json_with_type</code> instead.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str | bytes | bytearray) -&gt; \"Message\":\n    \"\"\"Deserialize a message from a JSON string, attempting to auto-detect the message type.\n    NOTE: If you already know the message type, use the more performant :meth:`from_json_with_type` instead.\"\"\"\n    data = json.loads(json_str)\n    message_type = data.get(\"message_type\")\n    if not message_type:\n        raise ValueError(f\"Missing message_type: {json_str}\")\n\n    # Use cached message type lookup\n    message_class = cls._message_type_lookup[message_type]\n    if not message_class:\n        raise ValueError(f\"Unknown message type: {message_type}\")\n\n    return message_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.Message.from_json_with_type","title":"<code>from_json_with_type(message_type, json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a message from a JSON string with a specific message type. NOTE: This is more performant than :meth:<code>from_json</code> because it does not need to convert the JSON string to a dictionary first.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>@classmethod\ndef from_json_with_type(\n    cls, message_type: MessageTypeT, json_str: str | bytes | bytearray\n) -&gt; \"Message\":\n    \"\"\"Deserialize a message from a JSON string with a specific message type.\n    NOTE: This is more performant than :meth:`from_json` because it does not need to\n    convert the JSON string to a dictionary first.\"\"\"\n    # Use cached message type lookup\n    message_class = cls._message_type_lookup[message_type]\n    if not message_class:\n        raise ValueError(f\"Unknown message type: {message_type}\")\n    return message_class.model_validate_json(json_str)\n</code></pre>"},{"location":"api/#aiperf.common.messages.base_messages.RequiresRequestNSMixin","title":"<code>RequiresRequestNSMixin</code>","text":"<p>               Bases: <code>Message</code></p> <p>Mixin for messages that require a request_ns field.</p> Source code in <code>aiperf/common/messages/base_messages.py</code> <pre><code>class RequiresRequestNSMixin(Message):\n    \"\"\"Mixin for messages that require a request_ns field.\"\"\"\n\n    request_ns: int = Field(  # type: ignore[assignment]\n        default_factory=time.time_ns,\n        description=\"Timestamp of the request in nanoseconds\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagescommand_messages","title":"aiperf.common.messages.command_messages","text":""},{"location":"api/#aiperf.common.messages.command_messages.CommandMessage","title":"<code>CommandMessage</code>","text":"<p>               Bases: <code>TargetedServiceMessage</code></p> <p>Message containing command data. This message is sent by the system controller to a service to command it to do something.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class CommandMessage(TargetedServiceMessage):\n    \"\"\"Message containing command data.\n    This message is sent by the system controller to a service to command it to do something.\n    \"\"\"\n\n    _command_type_lookup: ClassVar[dict[CommandTypeT, type[\"CommandMessage\"]]] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if hasattr(cls, \"command\"):\n            cls._command_type_lookup[cls.command] = cls\n\n    message_type: MessageTypeT = MessageType.COMMAND\n\n    command: CommandTypeT = Field(\n        ...,\n        description=\"Command to execute\",\n    )\n    command_id: str = Field(\n        default_factory=lambda: str(uuid.uuid4()),\n        description=\"Unique identifier for this command. If not provided, a random UUID will be generated.\",\n    )\n\n    @classmethod\n    def from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandMessage\":\n        \"\"\"Deserialize a command message from a JSON string, attempting to auto-detect the command type.\"\"\"\n        data = json.loads(json_str)\n        command_type = data.get(\"command\")\n        if not command_type:\n            raise ValueError(f\"Missing command: {json_str}\")\n\n        # Use cached command type lookup\n        command_class = cls._command_type_lookup[command_type]\n        if not command_class:\n            _logger.debug(\n                lambda: f\"No command class found for command type: {command_type}\"\n            )\n            # fallback to regular command class\n            command_class = cls\n\n        return command_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandMessage.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a command message from a JSON string, attempting to auto-detect the command type.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandMessage\":\n    \"\"\"Deserialize a command message from a JSON string, attempting to auto-detect the command type.\"\"\"\n    data = json.loads(json_str)\n    command_type = data.get(\"command\")\n    if not command_type:\n        raise ValueError(f\"Missing command: {json_str}\")\n\n    # Use cached command type lookup\n    command_class = cls._command_type_lookup[command_type]\n    if not command_class:\n        _logger.debug(\n            lambda: f\"No command class found for command type: {command_type}\"\n        )\n        # fallback to regular command class\n        command_class = cls\n\n    return command_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandResponse","title":"<code>CommandResponse</code>","text":"<p>               Bases: <code>TargetedServiceMessage</code></p> <p>Message containing a command response.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class CommandResponse(TargetedServiceMessage):\n    \"\"\"Message containing a command response.\"\"\"\n\n    # Specialized lookup for command response messages by status\n    _command_status_lookup: ClassVar[\n        dict[CommandResponseStatus, type[\"CommandResponse\"]]\n    ] = {}\n    # Specialized lookup for command response messages by command type, for success messages\n    _command_success_type_lookup: ClassVar[\n        dict[CommandTypeT, type[\"CommandResponse\"]]\n    ] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if (\n            hasattr(cls, \"status\")\n            and cls.status is not None\n            and cls.status not in cls._command_status_lookup\n        ):\n            cls._command_status_lookup[cls.status] = cls\n        elif (\n            cls.__pydantic_fields__.get(\"status\") is not None\n            and cls.__pydantic_fields__.get(\"status\").default\n            == CommandResponseStatus.SUCCESS\n        ):\n            # Cache the specialized lookup by command type for success messages\n            cls._command_success_type_lookup[cls.command] = cls\n\n    message_type: MessageTypeT = MessageType.COMMAND_RESPONSE\n\n    command: CommandTypeT = Field(\n        ...,\n        description=\"Command type that is being responded to\",\n    )\n    command_id: str = Field(\n        ..., description=\"The ID of the command that is being responded to\"\n    )\n    status: CommandResponseStatus = Field(..., description=\"The status of the command\")\n\n    @classmethod\n    def from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandResponse\":\n        \"\"\"Deserialize a command response message from a JSON string, attempting to auto-detect the command response type.\"\"\"\n        data = json.loads(json_str)\n        status = data.get(\"status\")\n        if not status:\n            raise ValueError(f\"Missing command response status: {json_str}\")\n        command = data.get(\"command\")\n        if not command:\n            raise ValueError(f\"Missing command in command response: {json_str}\")\n\n        if status not in cls._command_status_lookup:\n            raise ValueError(\n                f\"Unknown command response status: {status}. Valid statuses are: {list(cls._command_status_lookup.keys())}\"\n            )\n\n        # Use cached command response type lookup by status\n        command_response_class = cls._command_status_lookup[status]\n\n        if (\n            status == CommandResponseStatus.SUCCESS\n            and command in cls._command_success_type_lookup\n        ):\n            # For success messages, use the specialized lookup by command type if it exists\n            command_response_class = cls._command_success_type_lookup[command]\n\n        return command_response_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandResponse.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize a command response message from a JSON string, attempting to auto-detect the command response type.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str | bytes | bytearray) -&gt; \"CommandResponse\":\n    \"\"\"Deserialize a command response message from a JSON string, attempting to auto-detect the command response type.\"\"\"\n    data = json.loads(json_str)\n    status = data.get(\"status\")\n    if not status:\n        raise ValueError(f\"Missing command response status: {json_str}\")\n    command = data.get(\"command\")\n    if not command:\n        raise ValueError(f\"Missing command in command response: {json_str}\")\n\n    if status not in cls._command_status_lookup:\n        raise ValueError(\n            f\"Unknown command response status: {status}. Valid statuses are: {list(cls._command_status_lookup.keys())}\"\n        )\n\n    # Use cached command response type lookup by status\n    command_response_class = cls._command_status_lookup[status]\n\n    if (\n        status == CommandResponseStatus.SUCCESS\n        and command in cls._command_success_type_lookup\n    ):\n        # For success messages, use the specialized lookup by command type if it exists\n        command_response_class = cls._command_success_type_lookup[command]\n\n    return command_response_class.model_validate(data)\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.CommandSuccessResponse","title":"<code>CommandSuccessResponse</code>","text":"<p>               Bases: <code>CommandResponse</code></p> <p>Generic command response message when a command succeeds. It should be subclassed for specific command types.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class CommandSuccessResponse(CommandResponse):\n    \"\"\"Generic command response message when a command succeeds. It should be\n    subclassed for specific command types.\"\"\"\n\n    status: CommandResponseStatus = CommandResponseStatus.SUCCESS\n    data: Any | None = Field(\n        default=None,\n        description=\"The data of the command response\",\n    )\n\n    @classmethod\n    def from_command_message(\n        cls, command_message: CommandMessage, service_id: str, data: Any | None = None\n    ) -&gt; Self:\n        return cls(\n            service_id=service_id,\n            target_service_id=command_message.service_id,\n            command=command_message.command,\n            command_id=command_message.command_id,\n            data=data,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ConnectionProbeMessage","title":"<code>ConnectionProbeMessage</code>","text":"<p>               Bases: <code>TargetedServiceMessage</code></p> <p>Message containing a connection probe from a service. This is used to probe the connection to the service.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ConnectionProbeMessage(TargetedServiceMessage):\n    \"\"\"Message containing a connection probe from a service. This is used to probe the connection to the service.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONNECTION_PROBE\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProcessRecordsCommand","title":"<code>ProcessRecordsCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Data to send with the process records command.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProcessRecordsCommand(CommandMessage):\n    \"\"\"Data to send with the process records command.\"\"\"\n\n    command: CommandTypeT = CommandType.PROCESS_RECORDS\n\n    cancelled: bool = Field(\n        default=False,\n        description=\"Whether the profile run was cancelled\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProcessRecordsResponse","title":"<code>ProcessRecordsResponse</code>","text":"<p>               Bases: <code>CommandSuccessResponse</code></p> <p>Response to the process records command.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProcessRecordsResponse(CommandSuccessResponse):\n    \"\"\"Response to the process records command.\"\"\"\n\n    command: CommandTypeT = CommandType.PROCESS_RECORDS\n\n    data: ProcessRecordsResult | None = Field(  # type: ignore[assignment]\n        default=None,\n        description=\"The result of the process records command\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProfileCancelCommand","title":"<code>ProfileCancelCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request services to cancel profiling.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProfileCancelCommand(CommandMessage):\n    \"\"\"Command message sent to request services to cancel profiling.\"\"\"\n\n    command: CommandTypeT = CommandType.PROFILE_CANCEL\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProfileConfigureCommand","title":"<code>ProfileConfigureCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Data to send with the profile configure command.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProfileConfigureCommand(CommandMessage):\n    \"\"\"Data to send with the profile configure command.\"\"\"\n\n    command: CommandTypeT = CommandType.PROFILE_CONFIGURE\n\n    # TODO: Define this type\n    config: Any = Field(..., description=\"Configuration for the profile\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ProfileStartCommand","title":"<code>ProfileStartCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request services to start profiling.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ProfileStartCommand(CommandMessage):\n    \"\"\"Command message sent to request services to start profiling.\"\"\"\n\n    command: CommandTypeT = CommandType.PROFILE_START\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.RegisterServiceCommand","title":"<code>RegisterServiceCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent from a service to the system controller to register itself.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class RegisterServiceCommand(CommandMessage):\n    \"\"\"Command message sent from a service to the system controller to register itself.\"\"\"\n\n    command: CommandTypeT = CommandType.REGISTER_SERVICE\n\n    service_id: str = Field(..., description=\"The ID of the service to register\")\n    service_type: ServiceTypeT = Field(\n        ..., description=\"The type of the service to register\"\n    )\n    state: LifecycleState = Field(..., description=\"The current state of the service\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.ShutdownCommand","title":"<code>ShutdownCommand</code>","text":"<p>               Bases: <code>CommandMessage</code></p> <p>Command message sent to request a service to shutdown.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>class ShutdownCommand(CommandMessage):\n    \"\"\"Command message sent to request a service to shutdown.\"\"\"\n\n    command: CommandTypeT = CommandType.SHUTDOWN\n</code></pre>"},{"location":"api/#aiperf.common.messages.command_messages.TargetedServiceMessage","title":"<code>TargetedServiceMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message that can be targeted to a specific service by id or type. If both <code>target_service_type</code> and <code>target_service_id</code> are None, the message is sent to all services that are subscribed to the message type.</p> Source code in <code>aiperf/common/messages/command_messages.py</code> <pre><code>@exclude_if_none(\"target_service_id\", \"target_service_type\")\nclass TargetedServiceMessage(BaseServiceMessage):\n    \"\"\"Message that can be targeted to a specific service by id or type.\n    If both `target_service_type` and `target_service_id` are None, the message is\n    sent to all services that are subscribed to the message type.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_target_service(self) -&gt; Self:\n        if self.target_service_id is not None and self.target_service_type is not None:\n            raise ValueError(\n                \"Either target_service_id or target_service_type can be provided, but not both\"\n            )\n        return self\n\n    target_service_id: str | None = Field(\n        default=None,\n        description=\"ID of the target service to send the message to. \"\n        \"If both `target_service_type` and `target_service_id` are None, the message is \"\n        \"sent to all services that are subscribed to the message type.\",\n    )\n    target_service_type: ServiceTypeT | None = Field(\n        default=None,\n        description=\"Type of the service to send the message to. \"\n        \"If both `target_service_type` and `target_service_id` are None, the message is \"\n        \"sent to all services that are subscribed to the message type.\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagescredit_messages","title":"aiperf.common.messages.credit_messages","text":""},{"location":"api/#aiperf.common.messages.credit_messages.CreditDropMessage","title":"<code>CreditDropMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message indicating that a credit has been dropped. This message is sent by the timing manager to workers to indicate that credit(s) have been dropped.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditDropMessage(BaseServiceMessage):\n    \"\"\"Message indicating that a credit has been dropped.\n    This message is sent by the timing manager to workers to indicate that credit(s)\n    have been dropped.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_DROP\n\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    conversation_id: str | None = Field(\n        default=None, description=\"The ID of the conversation, if applicable.\"\n    )\n    credit_drop_ns: int | None = Field(\n        default=None,\n        description=\"Timestamp of the credit drop, if applicable. None means send ASAP.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseCompleteMessage","title":"<code>CreditPhaseCompleteMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for credit phase complete. Sent by the TimingManager to report that a credit phase has completed.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseCompleteMessage(BaseServiceMessage):\n    \"\"\"Message for credit phase complete. Sent by the TimingManager to report that a credit phase has completed.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_COMPLETE\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    completed: int = Field(\n        ...,\n        ge=0,\n        description=\"The number of completed credits (returned from the workers). This is the final count of completed credits.\",\n    )\n    end_ns: int = Field(\n        ...,\n        ge=1,\n        description=\"The time in which the last credit was returned from the workers in nanoseconds\",\n    )\n    timeout_triggered: bool = Field(\n        default=False,\n        description=\"Whether this phase completed because a timeout was triggered\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseProgressMessage","title":"<code>CreditPhaseProgressMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Sent by the TimingManager to report the progress of a credit phase.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseProgressMessage(BaseServiceMessage):\n    \"\"\"Sent by the TimingManager to report the progress of a credit phase.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_PROGRESS\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    sent: int = Field(\n        ...,\n        ge=0,\n        description=\"The number of sent credits\",\n    )\n    completed: int = Field(\n        ...,\n        ge=0,\n        description=\"The number of completed credits (returned from the workers)\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseSendingCompleteMessage","title":"<code>CreditPhaseSendingCompleteMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for credit phase sending complete. Sent by the TimingManager to report that a credit phase has completed sending.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseSendingCompleteMessage(BaseServiceMessage):\n    \"\"\"Message for credit phase sending complete. Sent by the TimingManager to report that a credit phase has completed sending.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_SENDING_COMPLETE\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    sent_end_ns: int = Field(\n        ...,\n        ge=1,\n        description=\"The time of the last sent credit in nanoseconds.\",\n    )\n    sent: int = Field(\n        ...,\n        ge=0,\n        description=\"The final number of sent credits.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditPhaseStartMessage","title":"<code>CreditPhaseStartMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for credit phase start. Sent by the TimingManager to report that a credit phase has started.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditPhaseStartMessage(BaseServiceMessage):\n    \"\"\"Message for credit phase start. Sent by the TimingManager to report that a credit phase has started.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_PHASE_START\n    phase: CreditPhase = Field(..., description=\"The type of credit phase\")\n    start_ns: int = Field(\n        ge=1,\n        description=\"The start time of the credit phase in nanoseconds.\",\n    )\n    total_expected_requests: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The total number of expected requests. If None, the phase is not request count based.\",\n    )\n    expected_duration_sec: float | None = Field(\n        default=None,\n        ge=1,\n        description=\"The expected duration of the credit phase in seconds. If None, the phase is not time based.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditReturnMessage","title":"<code>CreditReturnMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message indicating that a credit has been returned. This message is sent by a worker to the timing manager to indicate that work has been completed.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditReturnMessage(BaseServiceMessage):\n    \"\"\"Message indicating that a credit has been returned.\n    This message is sent by a worker to the timing manager to indicate that work has\n    been completed.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDIT_RETURN\n\n    phase: CreditPhase = Field(\n        ...,\n        description=\"The Credit Phase of the credit drop. This is so the TimingManager can track the progress of the credit phase.\",\n    )\n    delayed_ns: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The number of nanoseconds the credit drop was delayed by, or None if the credit was sent on time. \"\n        \"NOTE: This is only applicable if the original credit_drop_ns was not None.\",\n    )\n\n    @property\n    def delayed(self) -&gt; bool:\n        return self.delayed_ns is not None\n</code></pre>"},{"location":"api/#aiperf.common.messages.credit_messages.CreditsCompleteMessage","title":"<code>CreditsCompleteMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Credits complete message sent by the TimingManager to the System controller to signify all Credit Phases have been completed.</p> Source code in <code>aiperf/common/messages/credit_messages.py</code> <pre><code>class CreditsCompleteMessage(BaseServiceMessage):\n    \"\"\"Credits complete message sent by the TimingManager to the System controller to signify all Credit Phases\n    have been completed.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CREDITS_COMPLETE\n</code></pre>"},{"location":"api/#aiperfcommonmessagesdataset_messages","title":"aiperf.common.messages.dataset_messages","text":""},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationRequestMessage","title":"<code>ConversationRequestMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message to request a full conversation by ID.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationRequestMessage(BaseServiceMessage):\n    \"\"\"Message to request a full conversation by ID.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_REQUEST\n\n    conversation_id: str | None = Field(\n        default=None, description=\"The session ID of the conversation\"\n    )\n    credit_phase: CreditPhase | None = Field(\n        default=None,\n        description=\"The type of credit phase (either warmup or profiling). If not provided, the timing manager will use the default credit phase.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationResponseMessage","title":"<code>ConversationResponseMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message containing a full conversation.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationResponseMessage(BaseServiceMessage):\n    \"\"\"Message containing a full conversation.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_RESPONSE\n    conversation: Conversation = Field(..., description=\"The conversation data\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationTurnRequestMessage","title":"<code>ConversationTurnRequestMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message to request a single turn from a conversation.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationTurnRequestMessage(BaseServiceMessage):\n    \"\"\"Message to request a single turn from a conversation.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_TURN_REQUEST\n\n    conversation_id: str = Field(\n        ...,\n        description=\"The ID of the conversation.\",\n    )\n    turn_index: int = Field(\n        ...,\n        ge=0,\n        description=\"The index of the turn in the conversation.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.ConversationTurnResponseMessage","title":"<code>ConversationTurnResponseMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message containing a single turn from a conversation.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class ConversationTurnResponseMessage(BaseServiceMessage):\n    \"\"\"Message containing a single turn from a conversation.\"\"\"\n\n    message_type: MessageTypeT = MessageType.CONVERSATION_TURN_RESPONSE\n\n    turn: Turn = Field(..., description=\"The turn data\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.DatasetConfiguredNotification","title":"<code>DatasetConfiguredNotification</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Notification sent to notify other services that the dataset has been configured.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class DatasetConfiguredNotification(BaseServiceMessage):\n    \"\"\"Notification sent to notify other services that the dataset has been configured.\"\"\"\n\n    message_type: MessageTypeT = MessageType.DATASET_CONFIGURED_NOTIFICATION\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.DatasetTimingRequest","title":"<code>DatasetTimingRequest</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a dataset timing request.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class DatasetTimingRequest(BaseServiceMessage):\n    \"\"\"Message for a dataset timing request.\"\"\"\n\n    message_type: MessageTypeT = MessageType.DATASET_TIMING_REQUEST\n</code></pre>"},{"location":"api/#aiperf.common.messages.dataset_messages.DatasetTimingResponse","title":"<code>DatasetTimingResponse</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a dataset timing response.</p> Source code in <code>aiperf/common/messages/dataset_messages.py</code> <pre><code>class DatasetTimingResponse(BaseServiceMessage):\n    \"\"\"Message for a dataset timing response.\"\"\"\n\n    message_type: MessageTypeT = MessageType.DATASET_TIMING_RESPONSE\n\n    timing_data: list[tuple[int, str]] = Field(\n        ...,\n        description=\"The timing data of the dataset. Tuple of (timestamp, conversation_id)\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagesinference_messages","title":"aiperf.common.messages.inference_messages","text":""},{"location":"api/#aiperf.common.messages.inference_messages.InferenceResultsMessage","title":"<code>InferenceResultsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a inference results.</p> Source code in <code>aiperf/common/messages/inference_messages.py</code> <pre><code>class InferenceResultsMessage(BaseServiceMessage):\n    \"\"\"Message for a inference results.\"\"\"\n\n    message_type: MessageTypeT = MessageType.INFERENCE_RESULTS\n\n    record: SerializeAsAny[RequestRecord] = Field(\n        ..., description=\"The inference results record\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.inference_messages.MetricRecordsMessage","title":"<code>MetricRecordsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message from the result parser to the records manager to notify it of the metric records for a single request.</p> Source code in <code>aiperf/common/messages/inference_messages.py</code> <pre><code>class MetricRecordsMessage(BaseServiceMessage):\n    \"\"\"Message from the result parser to the records manager to notify it\n    of the metric records for a single request.\"\"\"\n\n    message_type: MessageTypeT = MessageType.METRIC_RECORDS\n\n    worker_id: str = Field(\n        ..., description=\"The ID of the worker that processed the request.\"\n    )\n    credit_phase: CreditPhase = Field(\n        ..., description=\"The credit phase of the request.\"\n    )\n    results: list[dict[MetricTagT, MetricValueTypeT]] = Field(\n        ..., description=\"The record processor results\"\n    )\n    error: ErrorDetails | None = Field(\n        default=None, description=\"The error details if the request failed.\"\n    )\n\n    @property\n    def valid(self) -&gt; bool:\n        \"\"\"Whether the request was valid.\"\"\"\n        return self.error is None\n</code></pre>"},{"location":"api/#aiperf.common.messages.inference_messages.MetricRecordsMessage.valid","title":"<code>valid</code>  <code>property</code>","text":"<p>Whether the request was valid.</p>"},{"location":"api/#aiperf.common.messages.inference_messages.ParsedInferenceResultsMessage","title":"<code>ParsedInferenceResultsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a parsed inference results.</p> Source code in <code>aiperf/common/messages/inference_messages.py</code> <pre><code>class ParsedInferenceResultsMessage(BaseServiceMessage):\n    \"\"\"Message for a parsed inference results.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PARSED_INFERENCE_RESULTS\n\n    worker_id: str = Field(\n        ..., description=\"The ID of the worker that processed the request.\"\n    )\n    record: SerializeAsAny[ParsedResponseRecord] = Field(\n        ..., description=\"The post process results record\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.inference_messages.RealtimeMetricsMessage","title":"<code>RealtimeMetricsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message from the records manager to show real-time metrics for the profile run.</p> Source code in <code>aiperf/common/messages/inference_messages.py</code> <pre><code>class RealtimeMetricsMessage(BaseServiceMessage):\n    \"\"\"Message from the records manager to show real-time metrics for the profile run.\"\"\"\n\n    message_type: MessageTypeT = MessageType.REALTIME_METRICS\n\n    metrics: list[MetricResult] = Field(\n        ..., description=\"The current real-time metrics.\"\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagesprogress_messages","title":"aiperf.common.messages.progress_messages","text":""},{"location":"api/#aiperf.common.messages.progress_messages.AllRecordsReceivedMessage","title":"<code>AllRecordsReceivedMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code>, <code>RequiresRequestNSMixin</code></p> <p>This is sent by the RecordsManager to signal that all parsed records have been received, and the final processing stats are available.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class AllRecordsReceivedMessage(BaseServiceMessage, RequiresRequestNSMixin):\n    \"\"\"This is sent by the RecordsManager to signal that all parsed records have been received, and the final processing stats are available.\"\"\"\n\n    message_type: MessageTypeT = MessageType.ALL_RECORDS_RECEIVED\n    final_processing_stats: ProcessingStats = Field(\n        ..., description=\"The final processing stats for the profile run\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProcessRecordsResultMessage","title":"<code>ProcessRecordsResultMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for process records result.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProcessRecordsResultMessage(BaseServiceMessage):\n    \"\"\"Message for process records result.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROCESS_RECORDS_RESULT\n\n    results: ProcessRecordsResult = Field(..., description=\"The process records result\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProcessingStatsMessage","title":"<code>ProcessingStatsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for processing stats. Sent by the records manager to the system controller to report the stats of the profile run.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProcessingStatsMessage(BaseServiceMessage):\n    \"\"\"Message for processing stats. Sent by the records manager to the system controller to report the stats of the profile run.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROCESSING_STATS\n\n    error_count: int = Field(default=0, description=\"The number of errors encountered\")\n    completed: int = Field(\n        default=0, description=\"The number of requests processed by the records manager\"\n    )\n    worker_completed: dict[str, int] = Field(\n        default_factory=dict,\n        description=\"Per-worker request completion counts, keyed by worker service_id\",\n    )\n    worker_errors: dict[str, int] = Field(\n        default_factory=dict,\n        description=\"Per-worker error counts, keyed by worker service_id\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProfileProgressMessage","title":"<code>ProfileProgressMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for profile progress. Sent by the timing manager to the system controller to report the progress of the profile run.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProfileProgressMessage(BaseServiceMessage):\n    \"\"\"Message for profile progress. Sent by the timing manager to the system controller to report the progress of the profile run.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROFILE_PROGRESS\n\n    profile_id: str | None = Field(\n        default=None, description=\"The ID of the current profile\"\n    )\n    start_ns: int = Field(\n        ..., description=\"The start time of the profile run in nanoseconds\"\n    )\n    end_ns: int | None = Field(\n        default=None, description=\"The end time of the profile run in nanoseconds\"\n    )\n    total: int = Field(\n        ..., description=\"The total number of inference requests to be made (if known)\"\n    )\n    completed: int = Field(\n        ..., description=\"The number of inference requests completed\"\n    )\n    warmup: bool = Field(\n        default=False,\n        description=\"Whether this is the warmup phase of the profile run\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.ProfileResultsMessage","title":"<code>ProfileResultsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for profile results.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class ProfileResultsMessage(BaseServiceMessage):\n    \"\"\"Message for profile results.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROFILE_RESULTS\n\n    profile_results: ProfileResults = Field(..., description=\"The profile results\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.progress_messages.RecordsProcessingStatsMessage","title":"<code>RecordsProcessingStatsMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for processing stats. Sent by the RecordsManager to report the stats of the profile run. This contains the stats for a single credit phase only.</p> Source code in <code>aiperf/common/messages/progress_messages.py</code> <pre><code>class RecordsProcessingStatsMessage(BaseServiceMessage):\n    \"\"\"Message for processing stats. Sent by the RecordsManager to report the stats of the profile run.\n    This contains the stats for a single credit phase only.\"\"\"\n\n    message_type: MessageTypeT = MessageType.PROCESSING_STATS\n\n    processing_stats: ProcessingStats = Field(\n        ..., description=\"The stats for the credit phase\"\n    )\n    worker_stats: dict[str, ProcessingStats] = Field(\n        default_factory=dict,\n        description=\"The stats for each worker how many requests were processed and how many errors were \"\n        \"encountered, keyed by worker service_id\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmessagesservice_messages","title":"aiperf.common.messages.service_messages","text":""},{"location":"api/#aiperf.common.messages.service_messages.BaseServiceErrorMessage","title":"<code>BaseServiceErrorMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Base message containing error data.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class BaseServiceErrorMessage(BaseServiceMessage):\n    \"\"\"Base message containing error data.\"\"\"\n\n    message_type: MessageTypeT = MessageType.SERVICE_ERROR\n\n    error: ErrorDetails = Field(..., description=\"Error information\")\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.BaseServiceMessage","title":"<code>BaseServiceMessage</code>","text":"<p>               Bases: <code>Message</code></p> <p>Base message that is sent from a service. Requires a service_id field to specify the service that sent the message.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class BaseServiceMessage(Message):\n    \"\"\"Base message that is sent from a service. Requires a service_id field to specify\n    the service that sent the message.\"\"\"\n\n    service_id: str = Field(\n        ...,\n        description=\"ID of the service sending the message\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.BaseStatusMessage","title":"<code>BaseStatusMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Base message containing status data. This message is sent by a service to the system controller to report its status.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class BaseStatusMessage(BaseServiceMessage):\n    \"\"\"Base message containing status data.\n    This message is sent by a service to the system controller to report its status.\n    \"\"\"\n\n    # override request_ns to be auto-filled if not provided\n    request_ns: int | None = Field(\n        default=time.time_ns(),\n        description=\"Timestamp of the request\",\n    )\n    state: LifecycleState = Field(\n        ...,\n        description=\"Current state of the service\",\n    )\n    service_type: ServiceTypeT = Field(\n        ...,\n        description=\"Type of service\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.HeartbeatMessage","title":"<code>HeartbeatMessage</code>","text":"<p>               Bases: <code>BaseStatusMessage</code></p> <p>Message containing heartbeat data. This message is sent by a service to the system controller to indicate that it is still running.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class HeartbeatMessage(BaseStatusMessage):\n    \"\"\"Message containing heartbeat data.\n    This message is sent by a service to the system controller to indicate that it is\n    still running.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.HEARTBEAT\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.RegistrationMessage","title":"<code>RegistrationMessage</code>","text":"<p>               Bases: <code>BaseStatusMessage</code></p> <p>Message containing registration data. This message is sent by a service to the system controller to register itself.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class RegistrationMessage(BaseStatusMessage):\n    \"\"\"Message containing registration data.\n    This message is sent by a service to the system controller to register itself.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.REGISTRATION\n</code></pre>"},{"location":"api/#aiperf.common.messages.service_messages.StatusMessage","title":"<code>StatusMessage</code>","text":"<p>               Bases: <code>BaseStatusMessage</code></p> <p>Message containing status data. This message is sent by a service to the system controller to report its status.</p> Source code in <code>aiperf/common/messages/service_messages.py</code> <pre><code>class StatusMessage(BaseStatusMessage):\n    \"\"\"Message containing status data.\n    This message is sent by a service to the system controller to report its status.\n    \"\"\"\n\n    message_type: MessageTypeT = MessageType.STATUS\n</code></pre>"},{"location":"api/#aiperfcommonmessagesworker_messages","title":"aiperf.common.messages.worker_messages","text":""},{"location":"api/#aiperf.common.messages.worker_messages.WorkerHealthMessage","title":"<code>WorkerHealthMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a worker health check.</p> Source code in <code>aiperf/common/messages/worker_messages.py</code> <pre><code>class WorkerHealthMessage(BaseServiceMessage):\n    \"\"\"Message for a worker health check.\"\"\"\n\n    message_type: MessageTypeT = MessageType.WORKER_HEALTH\n\n    health: ProcessHealth = Field(..., description=\"The health of the worker process\")\n\n    # Worker specific fields\n    task_stats: WorkerTaskStats = Field(\n        ...,\n        description=\"Stats for the tasks that have been sent to the worker\",\n    )\n\n    @property\n    def error_rate(self) -&gt; float:\n        \"\"\"The error rate of the worker.\"\"\"\n        if self.task_stats.total == 0:\n            return 0\n        return self.task_stats.failed / self.task_stats.total\n</code></pre>"},{"location":"api/#aiperf.common.messages.worker_messages.WorkerHealthMessage.error_rate","title":"<code>error_rate</code>  <code>property</code>","text":"<p>The error rate of the worker.</p>"},{"location":"api/#aiperf.common.messages.worker_messages.WorkerStatusSummaryMessage","title":"<code>WorkerStatusSummaryMessage</code>","text":"<p>               Bases: <code>BaseServiceMessage</code></p> <p>Message for a worker status summary.</p> Source code in <code>aiperf/common/messages/worker_messages.py</code> <pre><code>class WorkerStatusSummaryMessage(BaseServiceMessage):\n    \"\"\"Message for a worker status summary.\"\"\"\n\n    message_type: MessageTypeT = MessageType.WORKER_STATUS_SUMMARY\n\n    worker_statuses: dict[str, WorkerStatus] = Field(\n        ...,\n        description=\"A mapping of worker IDs to their status\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsaiperf_lifecycle_mixin","title":"aiperf.common.mixins.aiperf_lifecycle_mixin","text":""},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin","title":"<code>AIPerfLifecycleMixin</code>","text":"<p>               Bases: <code>TaskManagerMixin</code>, <code>HooksMixin</code></p> <p>This mixin provides a lifecycle state machine, and is the basis for most components in the AIPerf framework. It provides a set of hooks that are run at each state transition, and the ability to define background tasks that are automatically ran on @on_start, and canceled via @on_stop.</p> <p>It exposes to the outside world <code>initialize</code>, <code>start</code>, and <code>stop</code> methods, as well as getting the current state of the lifecycle. These simple methods promote a simple interface for users to interact with.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>@provides_hooks(\n    AIPerfHook.ON_INIT,\n    AIPerfHook.ON_START,\n    AIPerfHook.ON_STOP,\n    AIPerfHook.ON_STATE_CHANGE,\n    AIPerfHook.BACKGROUND_TASK,\n)\n@implements_protocol(AIPerfLifecycleProtocol)\nclass AIPerfLifecycleMixin(TaskManagerMixin, HooksMixin):\n    \"\"\"This mixin provides a lifecycle state machine, and is the basis for most components in the AIPerf framework.\n    It provides a set of hooks that are run at each state transition, and the ability to define background tasks\n    that are automatically ran on @on_start, and canceled via @on_stop.\n\n    It exposes to the outside world `initialize`, `start`, and `stop` methods, as well as getting the\n    current state of the lifecycle. These simple methods promote a simple interface for users to interact with.\n    \"\"\"\n\n    def __init__(self, id: str | None = None, **kwargs) -&gt; None:\n        \"\"\"\n        Args:\n            id: The id of the lifecycle. If not provided, a random uuid will be generated.\n        \"\"\"\n        self.id = id or f\"{self.__class__.__name__}_{uuid.uuid4().hex[:8]}\"\n        self._state = LifecycleState.CREATED\n        self.initialized_event = asyncio.Event()\n        self.started_event = asyncio.Event()\n        self._stop_requested_event = asyncio.Event()\n        self.stopped_event = asyncio.Event()  # set on stop or failure\n        self._children: list[AIPerfLifecycleProtocol] = []\n        if \"logger_name\" not in kwargs:\n            kwargs[\"logger_name\"] = self.id\n        super().__init__(**kwargs)\n\n    @property\n    def state(self) -&gt; LifecycleState:\n        return self._state\n\n    # NOTE: This was moved to not be a property setter, as we want it to be async so we can\n    # run the hooks and await them. Otherwise there is issues with creating a task when the\n    # lifecycle is trying to stop.\n    async def _set_state(self, state: LifecycleState) -&gt; None:\n        if state == self._state:\n            return\n        old_state = self._state\n        self._state = state\n        if self.is_debug_enabled:\n            self.debug(f\"State changed from {old_state!r} to {state!r} for {self}\")\n        await self.run_hooks(\n            AIPerfHook.ON_STATE_CHANGE, old_state=old_state, new_state=state\n        )\n\n    @property\n    def was_initialized(self) -&gt; bool:\n        return self.initialized_event.is_set()\n\n    @property\n    def was_started(self) -&gt; bool:\n        return self.started_event.is_set()\n\n    @property\n    def was_stopped(self) -&gt; bool:\n        return self.stopped_event.is_set()\n\n    @property\n    def is_running(self) -&gt; bool:\n        \"\"\"Whether the lifecycle's current state is LifecycleState.RUNNING.\"\"\"\n        return self.state == LifecycleState.RUNNING\n\n    @property\n    def stop_requested(self) -&gt; bool:\n        \"\"\"Whether the lifecycle has been requested to stop.\"\"\"\n        return self._stop_requested_event.is_set()\n\n    @stop_requested.setter\n    def stop_requested(self, value: bool) -&gt; None:\n        if value:\n            self._stop_requested_event.set()\n        else:\n            self._stop_requested_event.clear()\n\n    async def _execute_state_transition(\n        self,\n        transient_state: LifecycleState,\n        final_state: LifecycleState,\n        hook_type: AIPerfHook,\n        event: asyncio.Event,\n        reverse: bool = False,\n    ) -&gt; None:\n        \"\"\"This method wraps the functionality of changing the state of the lifecycle, and running the hooks.\n        It is used to ensure that the state change and hook running are atomic, and that the state change is\n        only made after the hooks have completed. It also takes in an event that is set when the state change is complete.\n        This is useful for external code waiting for the state change to complete before continuing.\n\n        If reverse is True, the hooks are run in reverse order. This is useful for stopping the lifecycle in the reverse order of starting it.\n        \"\"\"\n        await self._set_state(transient_state)\n        self.debug(lambda: f\"{transient_state.title()} {self}\")\n        try:\n            await self.run_hooks(hook_type, reverse=reverse)\n            await self._set_state(final_state)\n            self.debug(lambda: f\"{self} is now {final_state.title()}\")\n            event.set()\n        except Exception as e:\n            await self._fail(e)\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the lifecycle and run the @on_init hooks.\n\n        NOTE: It is generally discouraged from overriding this method.\n        Instead, use the @on_init hook to handle your own initialization logic.\n        \"\"\"\n        if self.state in (\n            LifecycleState.INITIALIZING,\n            LifecycleState.INITIALIZED,\n            LifecycleState.STARTING,\n            LifecycleState.RUNNING,\n        ):\n            self.debug(\n                lambda: f\"Ignoring initialize request for {self} in state {self.state}\"\n            )\n            return\n\n        if self.state != LifecycleState.CREATED:\n            raise InvalidStateError(\n                f\"Cannot initialize from state {self.state} for {self}\"\n            )\n\n        await self._execute_state_transition(\n            LifecycleState.INITIALIZING,\n            LifecycleState.INITIALIZED,\n            AIPerfHook.ON_INIT,\n            self.initialized_event,\n        )\n\n    async def start(self) -&gt; None:\n        \"\"\"Start the lifecycle and run the @on_start hooks.\n\n        NOTE: It is generally discouraged from overriding this method.\n        Instead, use the @on_start hook to handle your own starting logic.\n        \"\"\"\n        if self.state in (\n            LifecycleState.STARTING,\n            LifecycleState.RUNNING,\n        ):\n            self.debug(\n                lambda: f\"Ignoring start request for {self} in state {self.state}\"\n            )\n            return\n\n        if self.state != LifecycleState.INITIALIZED:\n            raise InvalidStateError(f\"Cannot start from state {self.state} for {self}\")\n\n        await self._execute_state_transition(\n            LifecycleState.STARTING,\n            LifecycleState.RUNNING,\n            AIPerfHook.ON_START,\n            self.started_event,\n        )\n\n    async def initialize_and_start(self) -&gt; None:\n        \"\"\"Initialize and start the lifecycle. This is a convenience method that calls `initialize` and `start` in sequence.\"\"\"\n        await self.initialize()\n        await self.start()\n\n    async def stop(self) -&gt; None:\n        \"\"\"Stop the lifecycle and run the @on_stop hooks.\n\n        NOTE: It is generally discouraged from overriding this method.\n        Instead, use the @on_stop hook to handle your own stopping logic.\n        \"\"\"\n        if self.stop_requested:\n            self.debug(\n                lambda: f\"Ignoring stop request for {self} in state {self.state}\"\n            )\n            return\n\n        self.stop_requested = True\n        await self._execute_state_transition(\n            LifecycleState.STOPPING,\n            LifecycleState.STOPPED,\n            AIPerfHook.ON_STOP,\n            self.stopped_event,\n            reverse=True,  # run the stop hooks in reverse order\n        )\n\n    @on_start\n    async def _start_background_tasks(self) -&gt; None:\n        \"\"\"Start all tasks that are decorated with the @background_task decorator.\"\"\"\n        for hook in self.get_hooks(AIPerfHook.BACKGROUND_TASK):\n            if not isinstance(hook.params, BackgroundTaskParams):\n                raise AttributeError(\n                    f\"Invalid hook parameters for {hook}: {hook.params}. Expected BackgroundTaskParams.\"\n                )\n            self.start_background_task(\n                hook.func,\n                interval=hook.params.interval,\n                immediate=hook.params.immediate,\n                stop_on_error=hook.params.stop_on_error,\n                stop_event=self._stop_requested_event,\n            )\n\n    @on_stop\n    async def _stop_all_tasks(self) -&gt; None:\n        \"\"\"Stop all tasks that are decorated with the @background_task decorator,\n        and any custom ones that were ran using `self.execute_async()`.\n        \"\"\"\n        await self.cancel_all_tasks()\n\n    async def _fail(self, e: Exception) -&gt; None:\n        \"\"\"Set the state to FAILED and raise an asyncio.CancelledError.\n        This is used when the transition from one state to another fails.\n        \"\"\"\n        await self._set_state(LifecycleState.FAILED)\n        self.exception(f\"Failed for {self}: {e}\")\n        self.stop_requested = True\n        self.stopped_event.set()\n        raise asyncio.CancelledError(f\"Failed for {self}: {e}\") from e\n\n    def attach_child_lifecycle(self, child: AIPerfLifecycleProtocol) -&gt; None:\n        \"\"\"Attach a child lifecycle to manage. This child will now have its lifecycle managed and\n        controlled by this lifecycle. Common use cases are having a Service be a parent lifecycle,\n        and having supporting components such as streaming post processors, progress reporters, etc. be children.\n\n        Children will be called in the order they were attached for initialize and start,\n        and in reverse order for stop.\n        \"\"\"\n        if self.state != LifecycleState.CREATED:\n            raise InvalidStateError(\n                f\"Cannot attach child {child} to {self} in state {self.state}. \"\n                \"Please attach children before initializing or starting the lifecycle.\"\n            )\n        self._children.append(child)\n\n    @on_init\n    async def _initialize_children(self) -&gt; None:\n        \"\"\"Initialize all children. This is done via the @on_init hook to ensure that the children\n        initialize along with the parent hooks, and not after the parent hooks, which would cause\n        a race condition.\n        \"\"\"\n        for child in self._children:\n            await child.initialize()\n\n    @on_start\n    async def _start_children(self) -&gt; None:\n        \"\"\"Start all children. This is done via the @on_start hook to ensure that the children\n        start along with the parent hooks, and not after the parent hooks, which would cause\n        a race condition.\n        \"\"\"\n        for child in self._children:\n            await child.start()\n\n    @on_stop\n    async def _stop_children(self) -&gt; None:\n        \"\"\"Stop all children. This is done via the @on_stop hook to ensure that the children\n        are stopped along with the parent hooks, and not after the parent hooks, which would cause\n        a race condition.\n        \"\"\"\n        for child in reversed(self._children):\n            await child.stop()\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__} (id={self.id})\"\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__qualname__} {self.id} (state={self.state})&gt;\"\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.is_running","title":"<code>is_running</code>  <code>property</code>","text":"<p>Whether the lifecycle's current state is LifecycleState.RUNNING.</p>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.stop_requested","title":"<code>stop_requested</code>  <code>property</code> <code>writable</code>","text":"<p>Whether the lifecycle has been requested to stop.</p>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.__init__","title":"<code>__init__(id=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | None</code> <p>The id of the lifecycle. If not provided, a random uuid will be generated.</p> <code>None</code> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>def __init__(self, id: str | None = None, **kwargs) -&gt; None:\n    \"\"\"\n    Args:\n        id: The id of the lifecycle. If not provided, a random uuid will be generated.\n    \"\"\"\n    self.id = id or f\"{self.__class__.__name__}_{uuid.uuid4().hex[:8]}\"\n    self._state = LifecycleState.CREATED\n    self.initialized_event = asyncio.Event()\n    self.started_event = asyncio.Event()\n    self._stop_requested_event = asyncio.Event()\n    self.stopped_event = asyncio.Event()  # set on stop or failure\n    self._children: list[AIPerfLifecycleProtocol] = []\n    if \"logger_name\" not in kwargs:\n        kwargs[\"logger_name\"] = self.id\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.attach_child_lifecycle","title":"<code>attach_child_lifecycle(child)</code>","text":"<p>Attach a child lifecycle to manage. This child will now have its lifecycle managed and controlled by this lifecycle. Common use cases are having a Service be a parent lifecycle, and having supporting components such as streaming post processors, progress reporters, etc. be children.</p> <p>Children will be called in the order they were attached for initialize and start, and in reverse order for stop.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>def attach_child_lifecycle(self, child: AIPerfLifecycleProtocol) -&gt; None:\n    \"\"\"Attach a child lifecycle to manage. This child will now have its lifecycle managed and\n    controlled by this lifecycle. Common use cases are having a Service be a parent lifecycle,\n    and having supporting components such as streaming post processors, progress reporters, etc. be children.\n\n    Children will be called in the order they were attached for initialize and start,\n    and in reverse order for stop.\n    \"\"\"\n    if self.state != LifecycleState.CREATED:\n        raise InvalidStateError(\n            f\"Cannot attach child {child} to {self} in state {self.state}. \"\n            \"Please attach children before initializing or starting the lifecycle.\"\n        )\n    self._children.append(child)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the lifecycle and run the @on_init hooks.</p> <p>NOTE: It is generally discouraged from overriding this method. Instead, use the @on_init hook to handle your own initialization logic.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the lifecycle and run the @on_init hooks.\n\n    NOTE: It is generally discouraged from overriding this method.\n    Instead, use the @on_init hook to handle your own initialization logic.\n    \"\"\"\n    if self.state in (\n        LifecycleState.INITIALIZING,\n        LifecycleState.INITIALIZED,\n        LifecycleState.STARTING,\n        LifecycleState.RUNNING,\n    ):\n        self.debug(\n            lambda: f\"Ignoring initialize request for {self} in state {self.state}\"\n        )\n        return\n\n    if self.state != LifecycleState.CREATED:\n        raise InvalidStateError(\n            f\"Cannot initialize from state {self.state} for {self}\"\n        )\n\n    await self._execute_state_transition(\n        LifecycleState.INITIALIZING,\n        LifecycleState.INITIALIZED,\n        AIPerfHook.ON_INIT,\n        self.initialized_event,\n    )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.initialize_and_start","title":"<code>initialize_and_start()</code>  <code>async</code>","text":"<p>Initialize and start the lifecycle. This is a convenience method that calls <code>initialize</code> and <code>start</code> in sequence.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def initialize_and_start(self) -&gt; None:\n    \"\"\"Initialize and start the lifecycle. This is a convenience method that calls `initialize` and `start` in sequence.\"\"\"\n    await self.initialize()\n    await self.start()\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the lifecycle and run the @on_start hooks.</p> <p>NOTE: It is generally discouraged from overriding this method. Instead, use the @on_start hook to handle your own starting logic.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the lifecycle and run the @on_start hooks.\n\n    NOTE: It is generally discouraged from overriding this method.\n    Instead, use the @on_start hook to handle your own starting logic.\n    \"\"\"\n    if self.state in (\n        LifecycleState.STARTING,\n        LifecycleState.RUNNING,\n    ):\n        self.debug(\n            lambda: f\"Ignoring start request for {self} in state {self.state}\"\n        )\n        return\n\n    if self.state != LifecycleState.INITIALIZED:\n        raise InvalidStateError(f\"Cannot start from state {self.state} for {self}\")\n\n    await self._execute_state_transition(\n        LifecycleState.STARTING,\n        LifecycleState.RUNNING,\n        AIPerfHook.ON_START,\n        self.started_event,\n    )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the lifecycle and run the @on_stop hooks.</p> <p>NOTE: It is generally discouraged from overriding this method. Instead, use the @on_stop hook to handle your own stopping logic.</p> Source code in <code>aiperf/common/mixins/aiperf_lifecycle_mixin.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the lifecycle and run the @on_stop hooks.\n\n    NOTE: It is generally discouraged from overriding this method.\n    Instead, use the @on_stop hook to handle your own stopping logic.\n    \"\"\"\n    if self.stop_requested:\n        self.debug(\n            lambda: f\"Ignoring stop request for {self} in state {self.state}\"\n        )\n        return\n\n    self.stop_requested = True\n    await self._execute_state_transition(\n        LifecycleState.STOPPING,\n        LifecycleState.STOPPED,\n        AIPerfHook.ON_STOP,\n        self.stopped_event,\n        reverse=True,  # run the stop hooks in reverse order\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsaiperf_logger_mixin","title":"aiperf.common.mixins.aiperf_logger_mixin","text":""},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin","title":"<code>AIPerfLoggerMixin</code>","text":"<p>               Bases: <code>BaseMixin</code></p> <p>Mixin to provide lazy evaluated logging for f-strings.</p> <p>This mixin provides a logger with lazy evaluation support for f-strings, and direct log functions for all standard and custom logging levels.</p> <p>see :class:<code>AIPerfLogger</code> for more details.</p> Usage <p>class MyClass(AIPerfLoggerMixin):     def init(self):         super().init()         self.trace(lambda: f\"Processing {item} of {count} ({item / count * 100}% complete)\")         self.info(\"Simple string message\")         self.debug(lambda i=i: f\"Binding loop variable: {i}\")         self.warning(\"Warning message: %s\", \"legacy support\")         self.success(\"Benchmark completed successfully\")         self.notice(\"Warmup has completed\")         self.exception(f\"Direct f-string usage: {e}\")</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>@implements_protocol(AIPerfLoggerProtocol)\nclass AIPerfLoggerMixin(BaseMixin):\n    \"\"\"Mixin to provide lazy evaluated logging for f-strings.\n\n    This mixin provides a logger with lazy evaluation support for f-strings,\n    and direct log functions for all standard and custom logging levels.\n\n    see :class:`AIPerfLogger` for more details.\n\n    Usage:\n        class MyClass(AIPerfLoggerMixin):\n            def __init__(self):\n                super().__init__()\n                self.trace(lambda: f\"Processing {item} of {count} ({item / count * 100}% complete)\")\n                self.info(\"Simple string message\")\n                self.debug(lambda i=i: f\"Binding loop variable: {i}\")\n                self.warning(\"Warning message: %s\", \"legacy support\")\n                self.success(\"Benchmark completed successfully\")\n                self.notice(\"Warmup has completed\")\n                self.exception(f\"Direct f-string usage: {e}\")\n    \"\"\"\n\n    def __init__(self, logger_name: str | None = None, **kwargs) -&gt; None:\n        self.logger = AIPerfLogger(logger_name or self.__class__.__name__)\n        self._log = self.logger._log\n        self.is_enabled_for = self.logger._logger.isEnabledFor\n        # Directly set the trace_or_debug method to the logger's trace_or_debug method to avoid\n        # the overhead of the extra call stack.\n        self.trace_or_debug = self.logger.trace_or_debug\n        super().__init__(**kwargs)\n\n    @property\n    def is_debug_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_DEBUG)\n\n    @property\n    def is_trace_enabled(self) -&gt; bool:\n        return self.is_enabled_for(_TRACE)\n\n    def log(\n        self, level: int, message: str | Callable[..., str], *args, **kwargs\n    ) -&gt; None:\n        \"\"\"Log a message at a specified level with lazy evaluation.\"\"\"\n        if self.is_enabled_for(level):\n            self._log(level, message, *args, **kwargs)\n\n    def trace(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a trace message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_TRACE):\n            self._log(_TRACE, message, *args, **kwargs)\n\n    def debug(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a debug message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_DEBUG):\n            self._log(_DEBUG, message, *args, **kwargs)\n\n    def info(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an info message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_INFO):\n            self._log(_INFO, message, *args, **kwargs)\n\n    def notice(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a notice message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_NOTICE):\n            self._log(_NOTICE, message, *args, **kwargs)\n\n    def warning(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a warning message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_WARNING):\n            self._log(_WARNING, message, *args, **kwargs)\n\n    def success(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a success message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_SUCCESS):\n            self._log(_SUCCESS, message, *args, **kwargs)\n\n    def error(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an error message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, message, *args, **kwargs)\n\n    def exception(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log an exception message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_ERROR):\n            self._log(_ERROR, message, *args, exc_info=True, **kwargs)\n\n    def critical(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n        \"\"\"Log a critical message with lazy evaluation.\"\"\"\n        if self.is_enabled_for(_CRITICAL):\n            self._log(_CRITICAL, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.critical","title":"<code>critical(message, *args, **kwargs)</code>","text":"<p>Log a critical message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def critical(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a critical message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_CRITICAL):\n        self._log(_CRITICAL, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.debug","title":"<code>debug(message, *args, **kwargs)</code>","text":"<p>Log a debug message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def debug(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a debug message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_DEBUG):\n        self._log(_DEBUG, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.error","title":"<code>error(message, *args, **kwargs)</code>","text":"<p>Log an error message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def error(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an error message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.exception","title":"<code>exception(message, *args, **kwargs)</code>","text":"<p>Log an exception message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def exception(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an exception message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_ERROR):\n        self._log(_ERROR, message, *args, exc_info=True, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.info","title":"<code>info(message, *args, **kwargs)</code>","text":"<p>Log an info message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def info(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log an info message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_INFO):\n        self._log(_INFO, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.log","title":"<code>log(level, message, *args, **kwargs)</code>","text":"<p>Log a message at a specified level with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def log(\n    self, level: int, message: str | Callable[..., str], *args, **kwargs\n) -&gt; None:\n    \"\"\"Log a message at a specified level with lazy evaluation.\"\"\"\n    if self.is_enabled_for(level):\n        self._log(level, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.notice","title":"<code>notice(message, *args, **kwargs)</code>","text":"<p>Log a notice message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def notice(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a notice message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_NOTICE):\n        self._log(_NOTICE, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.success","title":"<code>success(message, *args, **kwargs)</code>","text":"<p>Log a success message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def success(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a success message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_SUCCESS):\n        self._log(_SUCCESS, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.trace","title":"<code>trace(message, *args, **kwargs)</code>","text":"<p>Log a trace message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def trace(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a trace message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_TRACE):\n        self._log(_TRACE, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.aiperf_logger_mixin.AIPerfLoggerMixin.warning","title":"<code>warning(message, *args, **kwargs)</code>","text":"<p>Log a warning message with lazy evaluation.</p> Source code in <code>aiperf/common/mixins/aiperf_logger_mixin.py</code> <pre><code>def warning(self, message: str | Callable[..., str], *args, **kwargs) -&gt; None:\n    \"\"\"Log a warning message with lazy evaluation.\"\"\"\n    if self.is_enabled_for(_WARNING):\n        self._log(_WARNING, message, *args, **kwargs)\n</code></pre>"},{"location":"api/#aiperfcommonmixinsbase_mixin","title":"aiperf.common.mixins.base_mixin","text":""},{"location":"api/#aiperf.common.mixins.base_mixin.BaseMixin","title":"<code>BaseMixin</code>","text":"<p>Base mixin class.</p> <p>This Mixin creates a contract that Mixins should always pass **kwargs to super().init, regardless of whether they extend another mixin or not.</p> <p>This will ensure that the BaseMixin is the last mixin to have its init method called, which means that all other mixins will have a proper chain of init methods with the correct arguments and no accidental broken inheritance.</p> Source code in <code>aiperf/common/mixins/base_mixin.py</code> <pre><code>class BaseMixin:\n    \"\"\"Base mixin class.\n\n    This Mixin creates a contract that Mixins should always pass **kwargs to\n    super().__init__, regardless of whether they extend another mixin or not.\n\n    This will ensure that the BaseMixin is the last mixin to have its __init__\n    method called, which means that all other mixins will have a proper\n    chain of __init__ methods with the correct arguments and no accidental\n    broken inheritance.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        # object.__init__ does not take any arguments\n        super().__init__()\n</code></pre>"},{"location":"api/#aiperfcommonmixinscommand_handler_mixin","title":"aiperf.common.mixins.command_handler_mixin","text":""},{"location":"api/#aiperf.common.mixins.command_handler_mixin.CommandHandlerMixin","title":"<code>CommandHandlerMixin</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code>, <code>ABC</code></p> <p>Mixin to provide command handling functionality to a service.</p> <p>This mixin is used by the BaseService class, and is not intended to be used directly.</p> Source code in <code>aiperf/common/mixins/command_handler_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_COMMAND)\nclass CommandHandlerMixin(MessageBusClientMixin, ABC):\n    \"\"\"Mixin to provide command handling functionality to a service.\n\n    This mixin is used by the BaseService class, and is not intended to be used directly.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str,\n        **kwargs,\n    ) -&gt; None:\n        self.service_config = service_config\n        self.user_config = user_config\n        self.service_id = service_id\n\n        # Keep track of command IDs that have been processed.\n        # This is used to avoid processing duplicate command messages.\n        self._processed_command_ids: set[str] = set()\n\n        # Keep track of futures for single response commands.\n        # This is used to wait for the response from a single service.\n        self._single_response_futures: dict[str, asyncio.Future[CommandResponse]] = {}\n\n        # Keep track of futures for multi response commands.\n        # This is used to wait for the responses from multiple services.\n        self._multi_response_futures: dict[\n            str, dict[str, asyncio.Future[CommandResponse]]\n        ] = {}\n\n        super().__init__(\n            service_config=self.service_config,\n            user_config=self.user_config,\n            **kwargs,\n        )\n\n    @on_message(\n        lambda self: {\n            # Subscribe to all broadcast command messages.\n            MessageType.COMMAND,\n            # Subscribe to all command messages for this specific service type.\n            f\"{MessageType.COMMAND}.{self.service_type}\",\n            # Subscribe to all command messages for this specific service ID.\n            f\"{MessageType.COMMAND}.{self.service_id}\",\n        }\n    )\n    async def _process_command_message(self, message: CommandMessage) -&gt; None:\n        \"\"\"\n        Process a command message received from the controller or another service, and forward it to the appropriate handler.\n        Wait for the handler to complete and publish the response, or handle the error and publish the failure response.\n        \"\"\"\n        self.debug(lambda: f\"Received command message: {message}\")\n        if message.command_id in self._processed_command_ids:\n            self.debug(\n                lambda: f\"Received duplicate command message: {message}. Ignoring.\"\n            )\n            # If we receive a duplicate command message, we just send an acknowledged response.\n            await self._publish_command_acknowledged_response(message)\n            return\n\n        self._processed_command_ids.add(message.command_id)\n\n        if message.service_id == self.service_id:\n            # In the case of a broadcast command, you will receive a command message from yourself.\n            # We ignore these messages.\n            self.debug(\n                lambda: f\"Received broadcast command message from self: {message}. Ignoring.\"\n            )\n            return\n\n        # Go through the hooks and find the first one that matches the command type.\n        # Currently, we only support a single handler per command type, so we break out of the loop after the first one.\n        # The reason for this is because we are sending the result of the handler function back to the original service that sent the command.\n        # If there were multiple handlers, we would need to handle multiple responses, partial errors, etc.\n        # TODO: Do we want/need to add support for multiple handlers per command type?\n        for hook in self.get_hooks(AIPerfHook.ON_COMMAND):\n            if isinstance(hook.params, Iterable) and message.command in hook.params:\n                await self._execute_command_hook(message, hook)\n                # Only one handler per command type, so return after the first handler.\n                return\n\n        # If we reach here, no handler was found for the command, so we publish an unhandled response.\n        await self._publish_command_unhandled_response(message)\n\n    async def _execute_command_hook(self, message: CommandMessage, hook: Hook) -&gt; None:\n        \"\"\"Execute a command hook.\n        This is the internal function that wraps calling a hook function and publishing the response\n        based on the return value of the hook function.\n        \"\"\"\n        try:\n            result = await hook.func(message)\n            if result is None:\n                # If there is no data to send back, just send an acknowledged response.\n                await self._publish_command_acknowledged_response(message)\n                return\n            await self._publish_command_success_response(message, result)\n        except Exception as e:\n            self.exception(\n                f\"Failed to handle command {message.command} with hook {hook}: {e}\"\n            )\n            await self._publish_command_error_response(\n                message, ErrorDetails.from_exception(e)\n            )\n\n    async def _publish_command_acknowledged_response(\n        self, message: CommandMessage\n    ) -&gt; None:\n        \"\"\"Publish a command acknowledged response to a command message.\"\"\"\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n\n    async def _publish_command_success_response(\n        self, message: CommandMessage, result: Any\n    ) -&gt; None:\n        \"\"\"Publish a command success response to a command message.\"\"\"\n        await self.publish(\n            CommandSuccessResponse.from_command_message(\n                message, self.service_id, result\n            )\n        )\n\n    async def _publish_command_error_response(\n        self, message: CommandMessage, error: ErrorDetails\n    ) -&gt; None:\n        \"\"\"Publish a command error response to a command message.\"\"\"\n        await self.publish(\n            CommandErrorResponse.from_command_message(message, self.service_id, error)\n        )\n\n    async def _publish_command_unhandled_response(\n        self, message: CommandMessage\n    ) -&gt; None:\n        \"\"\"Publish a command unhandled response to a command message.\"\"\"\n        await self.publish(\n            CommandUnhandledResponse.from_command_message(message, self.service_id)\n        )\n\n    async def send_command_and_wait_for_response(\n        self, message: CommandMessage, timeout: float = DEFAULT_COMMAND_RESPONSE_TIMEOUT\n    ) -&gt; CommandResponse | ErrorDetails:\n        \"\"\"Send a single command message to a single service and wait for the response.\n        This is useful communicating directly with a single service.\n        \"\"\"\n        # Create a future that we can asynchronously wait for the response.\n        future = asyncio.Future[CommandResponse]()\n        self._single_response_futures[message.command_id] = future\n        await self.publish(message)\n        try:\n            # Wait for the response future to be set by the command response message handler.\n            return await asyncio.wait_for(future, timeout=timeout)\n        except asyncio.TimeoutError as e:\n            return ErrorDetails.from_exception(e)\n        finally:\n            future.cancel()\n            del self._single_response_futures[message.command_id]\n\n    async def send_command_and_wait_for_all_responses(\n        self,\n        command: CommandMessage,\n        service_ids: list[str],\n        timeout: float = DEFAULT_COMMAND_RESPONSE_TIMEOUT,\n    ) -&gt; list[CommandResponse | ErrorDetails]:\n        \"\"\"Broadcast a command message to multiple services and wait for the responses from all of the specified services.\n        This is useful for the system controller to send one command but wait for all services to respond.\n        \"\"\"\n        # Create a future to track the response for each service ID.\n        self._multi_response_futures[command.command_id] = {\n            service_id: asyncio.Future[CommandResponse]() for service_id in service_ids\n        }\n        # Send the command to all services based on the target service ID and target service type.\n        await self.publish(command)\n        try:\n            # Wait for all the responses to come in.\n            return await asyncio.wait_for(\n                asyncio.gather(\n                    *[\n                        future\n                        for future in self._multi_response_futures[\n                            command.command_id\n                        ].values()\n                    ]\n                ),\n                timeout=timeout,\n            )\n        except asyncio.TimeoutError as e:\n            return [ErrorDetails.from_exception(e) for _ in range(len(service_ids))]\n        finally:\n            # Clean up the response futures.\n            for future in self._multi_response_futures[command.command_id].values():\n                future.cancel()\n            del self._multi_response_futures[command.command_id]\n\n    @on_message(\n        lambda self: {\n            # NOTE: Command responses are only ever sent to the original service that sent the command,\n            #       so we only need to subscribe to the service ID specific topic.\n            f\"{MessageType.COMMAND_RESPONSE}.{self.service_id}\",\n        }\n    )\n    async def _process_command_response_message(self, message: CommandResponse) -&gt; None:\n        \"\"\"\n        Process a command response message received from a service. This function is called whenever\n        a command response is received, and we use it to set the result of the future for the command ID.\n        This will alert the the task that is waiting for the response to continue.\n        \"\"\"\n        self.debug(lambda: f\"Received command response message: {message}\")\n\n        # If the command ID is in the single response futures, we set the result of the future.\n        # This will alert the the task that is waiting for the response to continue.\n        if message.command_id in self._single_response_futures:\n            self._single_response_futures[message.command_id].set_result(message)\n            return\n\n        # If the command ID is in the multi response futures, we set the result of the future for the service ID of the sender.\n        # This will alert the the task that is waiting for the response to continue.\n        if message.command_id in self._multi_response_futures:\n            if message.service_id in self._multi_response_futures[message.command_id]:\n                self._multi_response_futures[message.command_id][\n                    message.service_id\n                ].set_result(message)\n            else:\n                self.warning(\n                    f\"Received command response for service we were not expecting: {message.service_id}. Ignoring.\"\n                )\n            return\n\n        # If we reach here, we received a command response that we were not tracking. It is\n        # safe to ignore.\n        self.debug(\n            lambda: f\"Received command response for untracked command: {message}. Ignoring.\"\n        )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.command_handler_mixin.CommandHandlerMixin.send_command_and_wait_for_all_responses","title":"<code>send_command_and_wait_for_all_responses(command, service_ids, timeout=DEFAULT_COMMAND_RESPONSE_TIMEOUT)</code>  <code>async</code>","text":"<p>Broadcast a command message to multiple services and wait for the responses from all of the specified services. This is useful for the system controller to send one command but wait for all services to respond.</p> Source code in <code>aiperf/common/mixins/command_handler_mixin.py</code> <pre><code>async def send_command_and_wait_for_all_responses(\n    self,\n    command: CommandMessage,\n    service_ids: list[str],\n    timeout: float = DEFAULT_COMMAND_RESPONSE_TIMEOUT,\n) -&gt; list[CommandResponse | ErrorDetails]:\n    \"\"\"Broadcast a command message to multiple services and wait for the responses from all of the specified services.\n    This is useful for the system controller to send one command but wait for all services to respond.\n    \"\"\"\n    # Create a future to track the response for each service ID.\n    self._multi_response_futures[command.command_id] = {\n        service_id: asyncio.Future[CommandResponse]() for service_id in service_ids\n    }\n    # Send the command to all services based on the target service ID and target service type.\n    await self.publish(command)\n    try:\n        # Wait for all the responses to come in.\n        return await asyncio.wait_for(\n            asyncio.gather(\n                *[\n                    future\n                    for future in self._multi_response_futures[\n                        command.command_id\n                    ].values()\n                ]\n            ),\n            timeout=timeout,\n        )\n    except asyncio.TimeoutError as e:\n        return [ErrorDetails.from_exception(e) for _ in range(len(service_ids))]\n    finally:\n        # Clean up the response futures.\n        for future in self._multi_response_futures[command.command_id].values():\n            future.cancel()\n        del self._multi_response_futures[command.command_id]\n</code></pre>"},{"location":"api/#aiperf.common.mixins.command_handler_mixin.CommandHandlerMixin.send_command_and_wait_for_response","title":"<code>send_command_and_wait_for_response(message, timeout=DEFAULT_COMMAND_RESPONSE_TIMEOUT)</code>  <code>async</code>","text":"<p>Send a single command message to a single service and wait for the response. This is useful communicating directly with a single service.</p> Source code in <code>aiperf/common/mixins/command_handler_mixin.py</code> <pre><code>async def send_command_and_wait_for_response(\n    self, message: CommandMessage, timeout: float = DEFAULT_COMMAND_RESPONSE_TIMEOUT\n) -&gt; CommandResponse | ErrorDetails:\n    \"\"\"Send a single command message to a single service and wait for the response.\n    This is useful communicating directly with a single service.\n    \"\"\"\n    # Create a future that we can asynchronously wait for the response.\n    future = asyncio.Future[CommandResponse]()\n    self._single_response_futures[message.command_id] = future\n    await self.publish(message)\n    try:\n        # Wait for the response future to be set by the command response message handler.\n        return await asyncio.wait_for(future, timeout=timeout)\n    except asyncio.TimeoutError as e:\n        return ErrorDetails.from_exception(e)\n    finally:\n        future.cancel()\n        del self._single_response_futures[message.command_id]\n</code></pre>"},{"location":"api/#aiperfcommonmixinscommunication_mixin","title":"aiperf.common.mixins.communication_mixin","text":""},{"location":"api/#aiperf.common.mixins.communication_mixin.CommunicationMixin","title":"<code>CommunicationMixin</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>Mixin to provide access to a CommunicationProtocol instance. This mixin should be inherited by any mixin that needs access to the communication layer to create Communication clients.</p> Source code in <code>aiperf/common/mixins/communication_mixin.py</code> <pre><code>class CommunicationMixin(AIPerfLifecycleMixin, ABC):\n    \"\"\"Mixin to provide access to a CommunicationProtocol instance. This mixin should be inherited\n    by any mixin that needs access to the communication layer to create Communication clients.\n    \"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        self.service_config = service_config\n        self.comms: CommunicationProtocol = CommunicationFactory.get_or_create_instance(\n            self.service_config.comm_config.comm_backend,\n            config=self.service_config.comm_config,\n        )\n        self.attach_child_lifecycle(self.comms)\n</code></pre>"},{"location":"api/#aiperfcommonmixinshooks_mixin","title":"aiperf.common.mixins.hooks_mixin","text":""},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin","title":"<code>HooksMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Mixin for a class to be able to provide hooks to its subclasses, and to be able to run them. A \"hook\" is a function that is decorated with a hook type (AIPerfHook), and optional parameters.</p> <p>In order to provide hooks, a class MUST use the <code>@provides_hooks</code> decorator to declare the hook types it provides. Only list hook types that you call <code>get_hooks</code> or <code>run_hooks</code> on, to get or run the functions that are decorated with those hook types.</p> <p>Provided hooks are recursively inherited by subclasses, so if a base class provides a hook, all subclasses will also provide that hook (without having to explicitly declare it, or call <code>get_hooks</code> or <code>run_hooks</code>). In fact, you typically should not get or run hooks from the base class, as this may lead to calling hooks twice.</p> <p>Hooks are registered in the order they are defined within the same class from top to bottom, and each class's hooks are inspected starting with hooks defined in the lowest level of base classes, moving up to the highest subclass.</p> <p>IMPORTANT: - Hook decorated methods from one class can be named the same as methods in their base classes, and BOTH will be registered. Meaning if class A and class B both have a method named <code>_initialize</code>, which is decorated with <code>@on_init</code>, and class B inherits from class A, then both <code>_initialize</code> methods will be registered as hooks, and will be run in the order A._initialize, then B._initialize. This is done without requiring the user to call <code>super()._initialize</code> in the subclass, as the base class hook will be run automatically. However, the caveat is that it is not possible to disable the hook from the base class without extra work, and if the user does accidentally call <code>super()._initialize</code> in the subclass, the base class hook may be called twice.</p> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>@implements_protocol(HooksProtocol)\nclass HooksMixin(AIPerfLoggerMixin):\n    \"\"\"Mixin for a class to be able to provide hooks to its subclasses, and to be able to run them. A \"hook\" is a function\n    that is decorated with a hook type (AIPerfHook), and optional parameters.\n\n    In order to provide hooks, a class MUST use the `@provides_hooks` decorator to declare the hook types it provides.\n    Only list hook types that you call `get_hooks` or `run_hooks` on, to get or run the functions that are decorated\n    with those hook types.\n\n    Provided hooks are recursively inherited by subclasses, so if a base class provides a hook,\n    all subclasses will also provide that hook (without having to explicitly declare it, or call `get_hooks` or `run_hooks`).\n    In fact, you typically should not get or run hooks from the base class, as this may lead to calling hooks twice.\n\n    Hooks are registered in the order they are defined within the same class from top to bottom, and each class's hooks\n    are inspected starting with hooks defined in the lowest level of base classes, moving up to the highest subclass.\n\n    IMPORTANT:\n    - Hook decorated methods from one class can be named the same as methods in their base classes, and BOTH will be registered.\n    Meaning if class A and class B both have a method named `_initialize`, which is decorated with `@on_init`, and class B inherits from class A,\n    then both `_initialize` methods will be registered as hooks, and will be run in the order A._initialize, then B._initialize.\n    This is done without requiring the user to call `super()._initialize` in the subclass, as the base class hook will be run automatically.\n    However, the caveat is that it is not possible to disable the hook from the base class without extra work, and if the user does accidentally\n    call `super()._initialize` in the subclass, the base class hook may be called twice.\n    \"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self._provided_hook_types: set[HookType] = set()\n\n        self._hooks: dict[HookType, list[Hook]] = {}\n        # Go through the MRO in reverse order to ensure that the hooks are\n        # registered in the correct order (base classes first, then subclasses).\n        for cls in reversed(self.__class__.__mro__):\n            if hasattr(cls, HookAttrs.PROVIDES_HOOKS):\n                # As we find base classes that provide hooks, we add them to the\n                # set of provided hook types, which is used for validation.\n                self._provided_hook_types.update(getattr(cls, HookAttrs.PROVIDES_HOOKS))\n\n            # Go through the class's methods to find the hooks.\n            for method in cls.__dict__.values():\n                if not callable(method):\n                    continue\n\n                # If the method has the AIPERF_HOOK_TYPE attribute, it is a hook.\n                if hasattr(method, HookAttrs.HOOK_TYPE):\n                    method_hook_type = getattr(method, HookAttrs.HOOK_TYPE)\n                    self._check_hook_type_is_provided(method_hook_type)\n\n                    # Bind the method to the instance (\"self\"), extract the hook parameters,\n                    # and add it to the hooks dictionary.\n                    bound_method = method.__get__(self)\n                    self._hooks.setdefault(method_hook_type, []).append(\n                        Hook(\n                            func=bound_method,\n                            params=getattr(method, HookAttrs.HOOK_PARAMS, None),\n                        ),\n                    )\n\n        self.debug(\n            lambda: f\"Provided hook types: {self._provided_hook_types} for {self.__class__.__name__}\"\n        )\n\n    def _check_hook_type_is_provided(self, hook_type: HookType) -&gt; None:\n        \"\"\"Check if the hook type is provided by any base class of the class.\n\n        Args:\n            hook_type: The hook type to check.\n\n        Raises:\n            UnsupportedHookError: If the hook type is not provided by any base class of the class.\n        \"\"\"\n        # If the hook type is not provided by any base class, it is an error.\n        # This is to ensure that the hook is only registered if it is provided by a base class.\n        # This is to avoid the case where a developer accidentally uses a hook that is not provided by a base class.\n        if hook_type not in self._provided_hook_types:\n            raise UnsupportedHookError(\n                f\"Hook {hook_type} is not provided by any base class of {self.__class__.__name__}. \"\n                f\"(Provided Hooks: {[f'{hook_type}' for hook_type in self._provided_hook_types]})\"\n            )\n\n    def attach_hook(\n        self,\n        hook_type: HookType,\n        func: Callable,\n        params: HookParamsT | Callable[[SelfT], HookParamsT] | None = None,\n    ) -&gt; None:\n        \"\"\"Attach a hook to this class. This is useful for attaching hooks to a class directly,\n        without having to inherit from this class, or use a decorator.\n\n        Args:\n            hook_type: The hook type to attach the hook to.\n            func: The function to attach the hook to.\n            params: The parameters to attach to the hook.\n        \"\"\"\n        if not callable(func):\n            raise ValueError(f\"Invalid hook function: {func}\")\n\n        self._check_hook_type_is_provided(hook_type)\n        self._hooks.setdefault(hook_type, []).append(Hook(func=func, params=params))\n\n    def get_hooks(self, *hook_types: HookType, reverse: bool = False) -&gt; list[Hook]:\n        \"\"\"Get the hooks that are defined by the class for the given hook type(s), optionally reversed.\n        This will return a list of Hook objects that can be inspected for their type and parameters,\n        and optionally called.\"\"\"\n        hooks = [\n            hook\n            for hook_type, hooks in self._hooks.items()\n            if not hook_types or hook_type in hook_types\n            for hook in hooks\n        ]\n        if reverse:\n            hooks.reverse()\n        return hooks\n\n    def for_each_hook_param(\n        self,\n        *hook_types: HookType,\n        self_obj: Any,\n        param_type: AnyT,\n        lambda_func: Callable[[Hook, AnyT], None],\n        reverse: bool = False,\n    ) -&gt; None:\n        \"\"\"Iterate over the hooks for the given hook type(s), optionally reversed.\n        If a lambda_func is provided, it will be called for each parameter of the hook,\n        and the hook and parameter will be passed as arguments.\n\n        Args:\n            hook_types: The hook types to iterate over.\n            self_obj: The object to pass to the lambda_func.\n            param_type: The type of the parameter to pass to the lambda_func (for validation).\n            lambda_func: The function to call for each hook.\n            reverse: Whether to iterate over the hooks in reverse order.\n        \"\"\"\n        for hook in self.get_hooks(*hook_types, reverse=reverse):\n            # in case the hook params are a callable, we need to resolve them to get the actual params\n            params = hook.resolve_params(self_obj)\n            if not isinstance(params, Iterable):\n                raise ValueError(\n                    f\"Invalid hook params: {params}. Expected Iterable but got {type(params)}\"\n                )\n            for param in params:\n                self.trace(\n                    lambda param=param,\n                    type=param_type: f\"param: {param}, param_type: {type}\"\n                )\n                if not isinstance(param, param_type):\n                    raise ValueError(\n                        f\"Invalid hook param: {param}. Expected {param_type} but got {type(param)}\"\n                    )\n                # Call the lambda_func for each parameter of each hook.\n                lambda_func(hook, param)\n\n    async def run_hooks(\n        self, *hook_types: HookType, reverse: bool = False, **kwargs\n    ) -&gt; None:\n        \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n        Hooks are run in the order they are defined by the class, starting with hooks defined in the lowest level\n        of base classes, moving up to the top level class. If more than one hook type is provided, the hooks\n        from each level of classes will be run in the order of the hook types provided.\n\n        If reverse is True, the hooks will be run in reverse order. This is useful for stop/cleanup hooks, where you\n        want to start with the children and ending with the parent.\n\n        The kwargs are passed through as keyword arguments to each hook.\n        \"\"\"\n        exceptions: list[Exception] = []\n        for hook in self.get_hooks(*hook_types, reverse=reverse):\n            self.debug(lambda hook=hook: f\"Running hook: {hook!r}\")\n            try:\n                await hook(**kwargs)\n            except Exception as e:\n                exceptions.append(e)\n                self.exception(\n                    f\"Error running {hook!r} hook for {self.__class__.__name__}: {e}\"\n                )\n        if exceptions:\n            raise AIPerfMultiError(\n                f\"Errors running {hook_types} hooks for {self.__class__.__name__}\",\n                exceptions,\n            )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.attach_hook","title":"<code>attach_hook(hook_type, func, params=None)</code>","text":"<p>Attach a hook to this class. This is useful for attaching hooks to a class directly, without having to inherit from this class, or use a decorator.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HookType</code> <p>The hook type to attach the hook to.</p> required <code>func</code> <code>Callable</code> <p>The function to attach the hook to.</p> required <code>params</code> <code>HookParamsT | Callable[[SelfT], HookParamsT] | None</code> <p>The parameters to attach to the hook.</p> <code>None</code> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>def attach_hook(\n    self,\n    hook_type: HookType,\n    func: Callable,\n    params: HookParamsT | Callable[[SelfT], HookParamsT] | None = None,\n) -&gt; None:\n    \"\"\"Attach a hook to this class. This is useful for attaching hooks to a class directly,\n    without having to inherit from this class, or use a decorator.\n\n    Args:\n        hook_type: The hook type to attach the hook to.\n        func: The function to attach the hook to.\n        params: The parameters to attach to the hook.\n    \"\"\"\n    if not callable(func):\n        raise ValueError(f\"Invalid hook function: {func}\")\n\n    self._check_hook_type_is_provided(hook_type)\n    self._hooks.setdefault(hook_type, []).append(Hook(func=func, params=params))\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.for_each_hook_param","title":"<code>for_each_hook_param(*hook_types, self_obj, param_type, lambda_func, reverse=False)</code>","text":"<p>Iterate over the hooks for the given hook type(s), optionally reversed. If a lambda_func is provided, it will be called for each parameter of the hook, and the hook and parameter will be passed as arguments.</p> <p>Parameters:</p> Name Type Description Default <code>hook_types</code> <code>HookType</code> <p>The hook types to iterate over.</p> <code>()</code> <code>self_obj</code> <code>Any</code> <p>The object to pass to the lambda_func.</p> required <code>param_type</code> <code>AnyT</code> <p>The type of the parameter to pass to the lambda_func (for validation).</p> required <code>lambda_func</code> <code>Callable[[Hook, AnyT], None]</code> <p>The function to call for each hook.</p> required <code>reverse</code> <code>bool</code> <p>Whether to iterate over the hooks in reverse order.</p> <code>False</code> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>def for_each_hook_param(\n    self,\n    *hook_types: HookType,\n    self_obj: Any,\n    param_type: AnyT,\n    lambda_func: Callable[[Hook, AnyT], None],\n    reverse: bool = False,\n) -&gt; None:\n    \"\"\"Iterate over the hooks for the given hook type(s), optionally reversed.\n    If a lambda_func is provided, it will be called for each parameter of the hook,\n    and the hook and parameter will be passed as arguments.\n\n    Args:\n        hook_types: The hook types to iterate over.\n        self_obj: The object to pass to the lambda_func.\n        param_type: The type of the parameter to pass to the lambda_func (for validation).\n        lambda_func: The function to call for each hook.\n        reverse: Whether to iterate over the hooks in reverse order.\n    \"\"\"\n    for hook in self.get_hooks(*hook_types, reverse=reverse):\n        # in case the hook params are a callable, we need to resolve them to get the actual params\n        params = hook.resolve_params(self_obj)\n        if not isinstance(params, Iterable):\n            raise ValueError(\n                f\"Invalid hook params: {params}. Expected Iterable but got {type(params)}\"\n            )\n        for param in params:\n            self.trace(\n                lambda param=param,\n                type=param_type: f\"param: {param}, param_type: {type}\"\n            )\n            if not isinstance(param, param_type):\n                raise ValueError(\n                    f\"Invalid hook param: {param}. Expected {param_type} but got {type(param)}\"\n                )\n            # Call the lambda_func for each parameter of each hook.\n            lambda_func(hook, param)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.get_hooks","title":"<code>get_hooks(*hook_types, reverse=False)</code>","text":"<p>Get the hooks that are defined by the class for the given hook type(s), optionally reversed. This will return a list of Hook objects that can be inspected for their type and parameters, and optionally called.</p> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>def get_hooks(self, *hook_types: HookType, reverse: bool = False) -&gt; list[Hook]:\n    \"\"\"Get the hooks that are defined by the class for the given hook type(s), optionally reversed.\n    This will return a list of Hook objects that can be inspected for their type and parameters,\n    and optionally called.\"\"\"\n    hooks = [\n        hook\n        for hook_type, hooks in self._hooks.items()\n        if not hook_types or hook_type in hook_types\n        for hook in hooks\n    ]\n    if reverse:\n        hooks.reverse()\n    return hooks\n</code></pre>"},{"location":"api/#aiperf.common.mixins.hooks_mixin.HooksMixin.run_hooks","title":"<code>run_hooks(*hook_types, reverse=False, **kwargs)</code>  <code>async</code>","text":"<p>Run the hooks for the given hook type, waiting for each hook to complete before running the next one. Hooks are run in the order they are defined by the class, starting with hooks defined in the lowest level of base classes, moving up to the top level class. If more than one hook type is provided, the hooks from each level of classes will be run in the order of the hook types provided.</p> <p>If reverse is True, the hooks will be run in reverse order. This is useful for stop/cleanup hooks, where you want to start with the children and ending with the parent.</p> <p>The kwargs are passed through as keyword arguments to each hook.</p> Source code in <code>aiperf/common/mixins/hooks_mixin.py</code> <pre><code>async def run_hooks(\n    self, *hook_types: HookType, reverse: bool = False, **kwargs\n) -&gt; None:\n    \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n    Hooks are run in the order they are defined by the class, starting with hooks defined in the lowest level\n    of base classes, moving up to the top level class. If more than one hook type is provided, the hooks\n    from each level of classes will be run in the order of the hook types provided.\n\n    If reverse is True, the hooks will be run in reverse order. This is useful for stop/cleanup hooks, where you\n    want to start with the children and ending with the parent.\n\n    The kwargs are passed through as keyword arguments to each hook.\n    \"\"\"\n    exceptions: list[Exception] = []\n    for hook in self.get_hooks(*hook_types, reverse=reverse):\n        self.debug(lambda hook=hook: f\"Running hook: {hook!r}\")\n        try:\n            await hook(**kwargs)\n        except Exception as e:\n            exceptions.append(e)\n            self.exception(\n                f\"Error running {hook!r} hook for {self.__class__.__name__}: {e}\"\n            )\n    if exceptions:\n        raise AIPerfMultiError(\n            f\"Errors running {hook_types} hooks for {self.__class__.__name__}\",\n            exceptions,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsmessage_bus_mixin","title":"aiperf.common.mixins.message_bus_mixin","text":""},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin","title":"<code>MessageBusClientMixin</code>","text":"<p>               Bases: <code>CommunicationMixin</code>, <code>ABC</code></p> <p>Mixin to provide message bus clients (pub and sub)for AIPerf components, as well as a hook to handle messages: @on_message.</p> Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_MESSAGE)\n@implements_protocol(MessageBusClientProtocol)\nclass MessageBusClientMixin(CommunicationMixin, ABC):\n    \"\"\"Mixin to provide message bus clients (pub and sub)for AIPerf components, as well as\n    a hook to handle messages: @on_message.\"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        # NOTE: The communication base class will automatically manage the pub/sub clients' lifecycle.\n        self.sub_client = self.comms.create_sub_client(\n            CommAddress.EVENT_BUS_PROXY_BACKEND\n        )\n        self.pub_client = self.comms.create_pub_client(\n            CommAddress.EVENT_BUS_PROXY_FRONTEND\n        )\n        self._connection_probe_event = asyncio.Event()\n\n    @on_init\n    async def _setup_on_message_hooks(self) -&gt; None:\n        \"\"\"Send subscription requests for all @on_message hook decorators.\"\"\"\n        subscription_map: MessageCallbackMapT = {}\n\n        def _add_to_subscription_map(hook: Hook, message_type: MessageTypeT) -&gt; None:\n            \"\"\"\n            This function is called for every message_type parameter of every @on_message hook.\n            We use this to build a map of message types to callbacks, which is then used to call\n            subscribe_all for efficiency\n            \"\"\"\n            self.debug(\n                lambda: f\"Adding subscription for message type: '{message_type}' for hook: {hook}\"\n            )\n            subscription_map.setdefault(message_type, []).append(hook.func)\n\n        # For each @on_message hook, add each message type to the subscription map.\n        self.for_each_hook_param(\n            AIPerfHook.ON_MESSAGE,\n            self_obj=self,\n            param_type=MessageTypeT,\n            lambda_func=_add_to_subscription_map,\n        )\n        self.debug(lambda: f\"Subscribing to {len(subscription_map)} topics\")\n        await self.sub_client.subscribe_all(subscription_map)\n\n        # Subscribe to the connection probe last, to ensure the other subscriptions have been\n        # subscribed to before the connection probe is received.\n        await self.sub_client.subscribe(\n            # NOTE: It is important to use `self.id` here, as not all message bus clients are services\n            f\"{MessageType.CONNECTION_PROBE}.{self.id}\",\n            self._process_connection_probe_message,\n        )\n\n    @on_start\n    async def _wait_for_successful_probe(self) -&gt; None:\n        \"\"\"Send connection probe messages until a successful probe response is received.\"\"\"\n        self.debug(lambda: f\"Waiting for connection probe message for {self.id}\")\n\n        async def _probe_loop() -&gt; None:\n            while not self.stop_requested:\n                try:\n                    await asyncio.wait_for(\n                        self._probe_and_wait_for_response(),\n                        timeout=DEFAULT_CONNECTION_PROBE_INTERVAL,\n                    )\n                    break\n                except asyncio.TimeoutError:\n                    self.debug(\n                        \"Timeout waiting for connection probe message, sending another probe\"\n                    )\n                    await yield_to_event_loop()\n\n        await asyncio.wait_for(_probe_loop(), timeout=DEFAULT_CONNECTION_PROBE_TIMEOUT)\n\n    async def _process_connection_probe_message(\n        self, message: ConnectionProbeMessage\n    ) -&gt; None:\n        \"\"\"Process a connection probe message.\"\"\"\n        self.debug(lambda: f\"Received connection probe message: {message}\")\n        self._connection_probe_event.set()\n\n    async def _probe_and_wait_for_response(self) -&gt; None:\n        \"\"\"Wait for a connection probe message.\"\"\"\n        await self.publish(\n            ConnectionProbeMessage(service_id=self.id, target_service_id=self.id)\n        )\n        await self._connection_probe_event.wait()\n\n    async def subscribe(\n        self,\n        message_type: MessageTypeT,\n        callback: Callable[[Message], Coroutine[Any, Any, None]],\n    ) -&gt; None:\n        \"\"\"Subscribe to a specific message type. The callback will be called when\n        a message is received for the given message type.\"\"\"\n        await self.sub_client.subscribe(message_type, callback)\n\n    async def subscribe_all(\n        self,\n        message_callback_map: MessageCallbackMapT,\n    ) -&gt; None:\n        \"\"\"Subscribe to all message types in the map. The callback(s) will be called when\n        a message is received for the given message type.\n\n        Args:\n            message_callback_map: A map of message types to callbacks. The callbacks can be a single callback or a list of callbacks.\n        \"\"\"\n        await self.sub_client.subscribe_all(message_callback_map)\n\n    async def publish(self, message: Message) -&gt; None:\n        \"\"\"Publish a message. The message will be routed automatically based on the message type.\"\"\"\n        await self.pub_client.publish(message)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin.publish","title":"<code>publish(message)</code>  <code>async</code>","text":"<p>Publish a message. The message will be routed automatically based on the message type.</p> Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>async def publish(self, message: Message) -&gt; None:\n    \"\"\"Publish a message. The message will be routed automatically based on the message type.\"\"\"\n    await self.pub_client.publish(message)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin.subscribe","title":"<code>subscribe(message_type, callback)</code>  <code>async</code>","text":"<p>Subscribe to a specific message type. The callback will be called when a message is received for the given message type.</p> Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>async def subscribe(\n    self,\n    message_type: MessageTypeT,\n    callback: Callable[[Message], Coroutine[Any, Any, None]],\n) -&gt; None:\n    \"\"\"Subscribe to a specific message type. The callback will be called when\n    a message is received for the given message type.\"\"\"\n    await self.sub_client.subscribe(message_type, callback)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.message_bus_mixin.MessageBusClientMixin.subscribe_all","title":"<code>subscribe_all(message_callback_map)</code>  <code>async</code>","text":"<p>Subscribe to all message types in the map. The callback(s) will be called when a message is received for the given message type.</p> <p>Parameters:</p> Name Type Description Default <code>message_callback_map</code> <code>MessageCallbackMapT</code> <p>A map of message types to callbacks. The callbacks can be a single callback or a list of callbacks.</p> required Source code in <code>aiperf/common/mixins/message_bus_mixin.py</code> <pre><code>async def subscribe_all(\n    self,\n    message_callback_map: MessageCallbackMapT,\n) -&gt; None:\n    \"\"\"Subscribe to all message types in the map. The callback(s) will be called when\n    a message is received for the given message type.\n\n    Args:\n        message_callback_map: A map of message types to callbacks. The callbacks can be a single callback or a list of callbacks.\n    \"\"\"\n    await self.sub_client.subscribe_all(message_callback_map)\n</code></pre>"},{"location":"api/#aiperfcommonmixinsprocess_health_mixin","title":"aiperf.common.mixins.process_health_mixin","text":""},{"location":"api/#aiperf.common.mixins.process_health_mixin.ProcessHealthMixin","title":"<code>ProcessHealthMixin</code>","text":"<p>               Bases: <code>BaseMixin</code></p> <p>Mixin to provide process health information.</p> Source code in <code>aiperf/common/mixins/process_health_mixin.py</code> <pre><code>class ProcessHealthMixin(BaseMixin):\n    \"\"\"Mixin to provide process health information.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Initialize process-specific CPU monitoring\n        self._process: psutil.Process = psutil.Process()\n        self._process.cpu_percent()  # throw away the first result (will be 0)\n        self._create_time: float = self._process.create_time()\n\n        self._process_health: ProcessHealth | None = None\n        self._previous: ProcessHealth | None = None\n\n    def get_process_health(self) -&gt; ProcessHealth:\n        \"\"\"Get the process health information for the current process.\"\"\"\n\n        # Get process-specific CPU and memory usage\n        raw_cpu_times = self._process.cpu_times()\n        cpu_times = CPUTimes(\n            user=raw_cpu_times[0],\n            system=raw_cpu_times[1],\n            iowait=raw_cpu_times[4] if len(raw_cpu_times) &gt; 4 else 0.0,  # type: ignore\n        )\n\n        self._previous = self._process_health\n\n        self._process_health = ProcessHealth(\n            pid=self._process.pid,\n            create_time=self._create_time,\n            uptime=time.time() - self._create_time,\n            cpu_usage=self._process.cpu_percent(),\n            memory_usage=self._process.memory_info().rss,\n            io_counters=self._process.io_counters() if hasattr(self._process, \"io_counters\") else None,\n            cpu_times=cpu_times,\n            num_ctx_switches=CtxSwitches(*self._process.num_ctx_switches()),\n            num_threads=self._process.num_threads(),\n        )  # fmt: skip\n        return self._process_health\n</code></pre>"},{"location":"api/#aiperf.common.mixins.process_health_mixin.ProcessHealthMixin.get_process_health","title":"<code>get_process_health()</code>","text":"<p>Get the process health information for the current process.</p> Source code in <code>aiperf/common/mixins/process_health_mixin.py</code> <pre><code>def get_process_health(self) -&gt; ProcessHealth:\n    \"\"\"Get the process health information for the current process.\"\"\"\n\n    # Get process-specific CPU and memory usage\n    raw_cpu_times = self._process.cpu_times()\n    cpu_times = CPUTimes(\n        user=raw_cpu_times[0],\n        system=raw_cpu_times[1],\n        iowait=raw_cpu_times[4] if len(raw_cpu_times) &gt; 4 else 0.0,  # type: ignore\n    )\n\n    self._previous = self._process_health\n\n    self._process_health = ProcessHealth(\n        pid=self._process.pid,\n        create_time=self._create_time,\n        uptime=time.time() - self._create_time,\n        cpu_usage=self._process.cpu_percent(),\n        memory_usage=self._process.memory_info().rss,\n        io_counters=self._process.io_counters() if hasattr(self._process, \"io_counters\") else None,\n        cpu_times=cpu_times,\n        num_ctx_switches=CtxSwitches(*self._process.num_ctx_switches()),\n        num_threads=self._process.num_threads(),\n    )  # fmt: skip\n    return self._process_health\n</code></pre>"},{"location":"api/#aiperfcommonmixinsprogress_tracker_mixin","title":"aiperf.common.mixins.progress_tracker_mixin","text":""},{"location":"api/#aiperf.common.mixins.progress_tracker_mixin.ProgressTrackerMixin","title":"<code>ProgressTrackerMixin</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code></p> <p>A progress tracker that tracks the progress of the entire benchmark suite.</p> Source code in <code>aiperf/common/mixins/progress_tracker_mixin.py</code> <pre><code>@provides_hooks(\n    AIPerfHook.ON_RECORDS_PROGRESS,\n    AIPerfHook.ON_PROFILING_PROGRESS,\n    AIPerfHook.ON_WARMUP_PROGRESS,\n)\nclass ProgressTrackerMixin(MessageBusClientMixin):\n    \"\"\"A progress tracker that tracks the progress of the entire benchmark suite.\"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs):\n        super().__init__(service_config=service_config, **kwargs)\n        self._phase_progress_map: dict[CreditPhase, FullPhaseProgress] = {}\n        self._active_phase: CreditPhase | None = None\n        self._phase_progress_lock = asyncio.Lock()\n        self._workers_stats: dict[str, WorkerStats] = {}\n        self._workers_stats_lock = asyncio.Lock()\n\n    @on_message(MessageType.CREDIT_PHASE_START)\n    async def _on_credit_phase_start(self, message: CreditPhaseStartMessage):\n        \"\"\"Update the progress from a credit phase start message.\"\"\"\n        async with self._phase_progress_lock:\n            if message.phase in self._phase_progress_map:\n                self.warning(f\"Phase stats already started for {message.phase}\")\n                return\n            self._active_phase = message.phase\n            phase_progress = FullPhaseProgress(\n                requests=RequestsStats(\n                    type=message.phase,\n                    start_ns=message.start_ns,\n                    # Only one of the below would be set\n                    total_expected_requests=message.total_expected_requests,\n                    expected_duration_sec=message.expected_duration_sec,\n                ),\n                records=RecordsStats(\n                    # May potentially not be set if the phase is not request count based\n                    total_expected_requests=message.total_expected_requests,\n                    start_ns=message.start_ns,\n                ),\n            )\n            self._phase_progress_map[message.phase] = phase_progress\n            await self._update_requests_stats(\n                message.phase, phase_progress, message.start_ns\n            )\n            if message.phase == CreditPhase.PROFILING:\n                await self._update_records_stats(phase_progress, message.start_ns)\n\n    @on_message(MessageType.CREDIT_PHASE_PROGRESS)\n    async def _on_credit_phase_progress(self, message: CreditPhaseProgressMessage):\n        \"\"\"Update the progress from a credit phase progress message.\"\"\"\n        async with self.phase_progress_context(message.phase) as phase_progress:\n            phase_progress.requests.sent = message.sent\n            phase_progress.requests.completed = message.completed\n            await self._update_requests_stats(\n                message.phase, phase_progress, message.request_ns\n            )\n\n    @on_message(MessageType.CREDIT_PHASE_SENDING_COMPLETE)\n    async def _on_credit_phase_sending_complete(\n        self, message: CreditPhaseSendingCompleteMessage\n    ):\n        \"\"\"Update the progress from a credit phase sending complete message.\"\"\"\n        async with self.phase_progress_context(message.phase) as phase_progress:\n            phase_progress.requests.sent_end_ns = message.sent_end_ns\n            phase_progress.requests.sent = message.sent\n            await self._update_requests_stats(\n                message.phase, phase_progress, message.request_ns\n            )\n\n    @on_message(MessageType.CREDIT_PHASE_COMPLETE)\n    async def _on_credit_phase_complete(self, message: CreditPhaseCompleteMessage):\n        \"\"\"Update the progress from a credit phase complete message.\"\"\"\n        async with self.phase_progress_context(message.phase) as phase_progress:\n            phase_progress.requests.end_ns = message.end_ns\n            # Just in case we did not get a progress report for the last credit (timing issues due to network)\n            phase_progress.requests.completed = phase_progress.requests.sent\n            await self._update_requests_stats(\n                message.phase, phase_progress, message.request_ns\n            )\n\n    @on_message(MessageType.PROCESSING_STATS)\n    async def _on_phase_processing_stats(self, message: RecordsProcessingStatsMessage):\n        \"\"\"Update the progress from a phase processing stats message.\"\"\"\n        async with self.phase_progress_context(CreditPhase.PROFILING) as phase_progress:\n            phase_progress.records.processed = message.processing_stats.processed\n            phase_progress.records.errors = message.processing_stats.errors\n            phase_progress.records.last_update_ns = time.time_ns()\n\n            for worker_id, processing_stats in message.worker_stats.items():\n                async with self._workers_stats_lock:\n                    if worker_id not in self._workers_stats:\n                        self._workers_stats[worker_id] = WorkerStats(\n                            worker_id=worker_id\n                        )\n                    self._workers_stats[worker_id].processing_stats = processing_stats\n\n            await self._update_records_stats(phase_progress, message.request_ns)\n\n    @on_message(MessageType.PROFILE_RESULTS)\n    async def _on_profile_results(self, message: ProfileResultsMessage):\n        \"\"\"Update the progress from a profile results message.\"\"\"\n        self.profile_results = message\n\n    async def _update_requests_stats(\n        self,\n        phase: CreditPhase,\n        phase_progress: FullPhaseProgress,\n        request_ns: int | None,\n    ):\n        \"\"\"Update the requests stats based on the TimingManager stats.\"\"\"\n        if self.is_debug_enabled:\n            self.debug(\n                f\"Updating requests stats for phase '{phase.title()}': sent: {phase_progress.requests.sent}, \"\n                f\"completed: {phase_progress.requests.completed}, total_expected: {phase_progress.requests.total_expected_requests}\"\n            )\n        self._update_computed_stats(request_ns, phase_progress.requests)\n\n        if phase == CreditPhase.WARMUP:\n            await self.run_hooks(\n                AIPerfHook.ON_WARMUP_PROGRESS,\n                warmup_stats=phase_progress.requests,\n            )\n        elif phase == CreditPhase.PROFILING:\n            await self.run_hooks(\n                AIPerfHook.ON_PROFILING_PROGRESS,\n                profiling_stats=phase_progress.requests,\n            )\n        else:\n            raise ValueError(f\"Invalid phase: {phase}\")\n\n    async def _update_records_stats(\n        self, phase_progress: FullPhaseProgress, request_ns: int | None\n    ):\n        \"\"\"Update the records stats based on the RecordsManager stats.\"\"\"\n        if self.is_debug_enabled:\n            self.debug(\n                f\"Updating records stats for phase '{phase_progress.phase.title()}': \"\n                f\"processed: {phase_progress.records.processed}, errors: {phase_progress.records.errors}\"\n            )\n\n        self._update_computed_stats(request_ns, phase_progress.records)\n        await self.run_hooks(\n            AIPerfHook.ON_RECORDS_PROGRESS, records_stats=phase_progress.records\n        )\n\n    def _update_computed_stats(\n        self,\n        request_ns: int | None,\n        stats: StatsProtocol,\n    ):\n        \"\"\"Update the computed stats based on incoming data.\n\n        Args:\n            request_ns: The time of the last request (or time.time_ns() if not set)\n            stats: The stats to update\n            finished: The number of finished requests or records\n            progress_percent: The progress percent of the requests or records\n            start_ns: The start time of the requests\n        \"\"\"\n        dur_ns = (request_ns or time.time_ns()) - (stats.start_ns or time.time_ns())\n        stats.last_update_ns = request_ns or time.time_ns()\n        if dur_ns &lt;= 0:\n            stats.per_second = None\n            stats.eta = None\n            return\n\n        dur_sec = dur_ns / NANOS_PER_SECOND\n        stats.per_second = stats.finished / dur_sec\n        if stats.progress_percent:\n            # (% remaining) / (% per second)\n            stats.eta = (100 - stats.progress_percent) / (\n                stats.progress_percent / dur_sec\n            )\n        else:\n            stats.eta = None\n\n    @asynccontextmanager\n    async def phase_progress_context(\n        self, phase: CreditPhase\n    ) -&gt; AsyncGenerator[FullPhaseProgress, None]:\n        \"\"\"Context manager for safely accessing phase progress info with warning.\"\"\"\n        async with self._phase_progress_lock:\n            phase_progress = self._phase_progress_map.get(phase)\n            if phase_progress is None:\n                self.warning(\n                    f\"Phase '{phase.title()}' not found in current profile run\"\n                )\n                return\n            yield phase_progress\n</code></pre>"},{"location":"api/#aiperf.common.mixins.progress_tracker_mixin.ProgressTrackerMixin.phase_progress_context","title":"<code>phase_progress_context(phase)</code>  <code>async</code>","text":"<p>Context manager for safely accessing phase progress info with warning.</p> Source code in <code>aiperf/common/mixins/progress_tracker_mixin.py</code> <pre><code>@asynccontextmanager\nasync def phase_progress_context(\n    self, phase: CreditPhase\n) -&gt; AsyncGenerator[FullPhaseProgress, None]:\n    \"\"\"Context manager for safely accessing phase progress info with warning.\"\"\"\n    async with self._phase_progress_lock:\n        phase_progress = self._phase_progress_map.get(phase)\n        if phase_progress is None:\n            self.warning(\n                f\"Phase '{phase.title()}' not found in current profile run\"\n            )\n            return\n        yield phase_progress\n</code></pre>"},{"location":"api/#aiperfcommonmixinspull_client_mixin","title":"aiperf.common.mixins.pull_client_mixin","text":""},{"location":"api/#aiperf.common.mixins.pull_client_mixin.PullClientMixin","title":"<code>PullClientMixin</code>","text":"<p>               Bases: <code>CommunicationMixin</code>, <code>ABC</code></p> <p>Mixin to provide a pull client for AIPerf components using a PullClient for the specified CommAddress. Add the @on_pull_message decorator to specify a function that will be called when a pull is received.</p> <p>NOTE: This currently only supports a single pull client per service, as that is our current use case.</p> Source code in <code>aiperf/common/mixins/pull_client_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_PULL_MESSAGE)\nclass PullClientMixin(CommunicationMixin, ABC):\n    \"\"\"Mixin to provide a pull client for AIPerf components using a PullClient for the specified CommAddress.\n    Add the @on_pull_message decorator to specify a function that will be called when a pull is received.\n\n    NOTE: This currently only supports a single pull client per service, as that is our current use case.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        pull_client_address: CommAddress,\n        pull_client_bind: bool = False,\n        max_pull_concurrency: int | None = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        # NOTE: The communication base class will automatically manage the pull client's lifecycle.\n        self.pull_client = self.comms.create_pull_client(\n            pull_client_address,\n            bind=pull_client_bind,\n            max_pull_concurrency=max_pull_concurrency,\n        )\n\n    @on_init\n    async def _setup_pull_handler_hooks(self) -&gt; None:\n        \"\"\"Configure the pull client to register callbacks for all @on_pull_message hook decorators.\"\"\"\n\n        def _register_pull_callback(hook: Hook, message_type: MessageTypeT) -&gt; None:\n            self.debug(\n                lambda: f\"Registering pull callback for message type: {message_type} for hook: {hook}\"\n            )\n            self.pull_client.register_pull_callback(\n                message_type=message_type,\n                callback=hook.func,\n            )\n\n        # For each @on_pull_message hook, register a pull callback for each specified message type.\n        self.for_each_hook_param(\n            AIPerfHook.ON_PULL_MESSAGE,\n            self_obj=self,\n            param_type=MessageTypeT,\n            lambda_func=_register_pull_callback,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsrealtime_metrics_mixin","title":"aiperf.common.mixins.realtime_metrics_mixin","text":""},{"location":"api/#aiperf.common.mixins.realtime_metrics_mixin.RealtimeMetricsMixin","title":"<code>RealtimeMetricsMixin</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code></p> <p>A mixin that provides a hook for real-time metrics.</p> Source code in <code>aiperf/common/mixins/realtime_metrics_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_REALTIME_METRICS)\nclass RealtimeMetricsMixin(MessageBusClientMixin):\n    \"\"\"A mixin that provides a hook for real-time metrics.\"\"\"\n\n    def __init__(\n        self, service_config: ServiceConfig, controller: SystemController, **kwargs\n    ):\n        super().__init__(service_config=service_config, controller=controller, **kwargs)\n        self._controller = controller\n        self._metrics: list[MetricResult] = []\n        self._metrics_lock = asyncio.Lock()\n\n    @on_message(MessageType.REALTIME_METRICS)\n    async def _on_realtime_metrics(self, message: RealtimeMetricsMessage):\n        \"\"\"Update the metrics from a real-time metrics message.\"\"\"\n        async with self._metrics_lock:\n            self._metrics = message.metrics\n        await self.run_hooks(\n            AIPerfHook.ON_REALTIME_METRICS,\n            metrics=message.metrics,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinsreply_client_mixin","title":"aiperf.common.mixins.reply_client_mixin","text":""},{"location":"api/#aiperf.common.mixins.reply_client_mixin.ReplyClientMixin","title":"<code>ReplyClientMixin</code>","text":"<p>               Bases: <code>CommunicationMixin</code>, <code>ABC</code></p> <p>Mixin to provide a reply client for AIPerf components using a ReplyClient for the specified CommAddress. Add the @on_request decorator to specify a function that will be called when a request is received.</p> <p>NOTE: This currently only supports a single reply client per service, as that is our current use case.</p> Source code in <code>aiperf/common/mixins/reply_client_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_REQUEST)\nclass ReplyClientMixin(CommunicationMixin, ABC):\n    \"\"\"Mixin to provide a reply client for AIPerf components using a ReplyClient for the specified CommAddress.\n    Add the @on_request decorator to specify a function that will be called when a request is received.\n\n    NOTE: This currently only supports a single reply client per service, as that is our current use case.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        reply_client_address: CommAddress,\n        reply_client_bind: bool = False,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(service_config=service_config, **kwargs)\n        # NOTE: The communication base class will automatically manage the reply client's lifecycle.\n        self.reply_client = self.comms.create_reply_client(\n            reply_client_address, bind=reply_client_bind\n        )\n\n    @on_init\n    async def _setup_request_handler_hooks(self) -&gt; None:\n        \"\"\"Configure the reply client to handle requests for all @request_handler hook decorators.\"\"\"\n\n        def _register_request_handler(hook: Hook, message_type: MessageTypeT) -&gt; None:\n            self.debug(\n                lambda: f\"Registering request handler for message type: {message_type} for hook: {hook}\"\n            )\n            self.reply_client.register_request_handler(\n                service_id=self.id,\n                message_type=message_type,\n                handler=hook.func,\n            )\n\n        # For each @on_request hook, register a request handler for each message type.\n        self.for_each_hook_param(\n            AIPerfHook.ON_REQUEST,\n            self_obj=self,\n            param_type=MessageTypeT,\n            lambda_func=_register_request_handler,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmixinstask_manager_mixin","title":"aiperf.common.mixins.task_manager_mixin","text":""},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin","title":"<code>TaskManagerMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Mixin to manage a set of async tasks, and provide background task loop capabilities. Can be used standalone, but it is most useful as part of the :class:<code>AIPerfLifecycleMixin</code> mixin, where the lifecycle methods are automatically integrated with the task manager.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>@implements_protocol(TaskManagerProtocol)\nclass TaskManagerMixin(AIPerfLoggerMixin):\n    \"\"\"Mixin to manage a set of async tasks, and provide background task loop capabilities.\n    Can be used standalone, but it is most useful as part of the :class:`AIPerfLifecycleMixin`\n    mixin, where the lifecycle methods are automatically integrated with the task manager.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.tasks: set[asyncio.Task] = set()\n        super().__init__(**kwargs)\n\n    def execute_async(self, coro: Coroutine) -&gt; asyncio.Task:\n        \"\"\"Create a task from a coroutine and add it to the set of tasks, and return immediately.\n        The task will be automatically cleaned up when it completes.\n        \"\"\"\n        task = asyncio.create_task(coro)\n        self.tasks.add(task)\n        task.add_done_callback(self.tasks.discard)\n        return task\n\n    async def wait_for_tasks(self) -&gt; list[BaseException | None]:\n        \"\"\"Wait for all current tasks to complete.\"\"\"\n        return await asyncio.gather(*list(self.tasks), return_exceptions=True)\n\n    async def cancel_all_tasks(\n        self, timeout: float = TASK_CANCEL_TIMEOUT_SHORT\n    ) -&gt; None:\n        \"\"\"Cancel all tasks in the set and wait for up to timeout seconds for them to complete.\n\n        Args:\n            timeout: The timeout to wait for the tasks to complete.\n        \"\"\"\n        if not self.tasks:\n            return\n\n        task_list = list(self.tasks)\n        for task in task_list:\n            task.cancel()\n\n    def start_background_task(\n        self,\n        method: Callable,\n        interval: float | Callable[[TaskManagerProtocol], float] | None = None,\n        immediate: bool = False,\n        stop_on_error: bool = False,\n        stop_event: asyncio.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Run a task in the background, in a loop until cancelled.\"\"\"\n        self.execute_async(\n            self._background_task_loop(\n                method, interval, immediate, stop_on_error, stop_event\n            )\n        )\n\n    async def _background_task_loop(\n        self,\n        method: Callable,\n        interval: float | Callable[[TaskManagerProtocol], float] | None = None,\n        immediate: bool = False,\n        stop_on_error: bool = False,\n        stop_event: asyncio.Event | None = None,\n    ) -&gt; None:\n        \"\"\"Run a background task in a loop until cancelled.\n\n        Args:\n            method: The method to run as a background task.\n            interval: The interval to run the task in seconds. Can be a callable that returns the interval, and will be called with 'self' as the argument.\n            immediate: If True, run the task immediately on start, otherwise wait for the interval first.\n            stop_on_error: If True, stop the task on any exception, otherwise log and continue.\n        \"\"\"\n        while stop_event is None or not stop_event.is_set():\n            try:\n                if interval is None or immediate:\n                    await yield_to_event_loop()\n                    # Reset immediate flag for next iteration otherwise we will not sleep\n                    immediate = False\n                else:\n                    sleep_time = interval(self) if callable(interval) else interval\n                    await asyncio.sleep(sleep_time)\n\n                if inspect.iscoroutinefunction(method):\n                    await method()\n                else:\n                    await asyncio.to_thread(method)\n\n                if interval is None:\n                    break\n            except asyncio.CancelledError:\n                self.debug(f\"Background task {method.__name__} cancelled\")\n                break\n            except Exception as e:\n                self.exception(f\"Error in background task {method.__name__}: {e}\")\n                if stop_on_error:\n                    self.exception(\n                        f\"Background task {method.__name__} stopped due to error\"\n                    )\n                    break\n                # Give some time to recover, just in case\n                await asyncio.sleep(0.001)\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.cancel_all_tasks","title":"<code>cancel_all_tasks(timeout=TASK_CANCEL_TIMEOUT_SHORT)</code>  <code>async</code>","text":"<p>Cancel all tasks in the set and wait for up to timeout seconds for them to complete.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>The timeout to wait for the tasks to complete.</p> <code>TASK_CANCEL_TIMEOUT_SHORT</code> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>async def cancel_all_tasks(\n    self, timeout: float = TASK_CANCEL_TIMEOUT_SHORT\n) -&gt; None:\n    \"\"\"Cancel all tasks in the set and wait for up to timeout seconds for them to complete.\n\n    Args:\n        timeout: The timeout to wait for the tasks to complete.\n    \"\"\"\n    if not self.tasks:\n        return\n\n    task_list = list(self.tasks)\n    for task in task_list:\n        task.cancel()\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.execute_async","title":"<code>execute_async(coro)</code>","text":"<p>Create a task from a coroutine and add it to the set of tasks, and return immediately. The task will be automatically cleaned up when it completes.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>def execute_async(self, coro: Coroutine) -&gt; asyncio.Task:\n    \"\"\"Create a task from a coroutine and add it to the set of tasks, and return immediately.\n    The task will be automatically cleaned up when it completes.\n    \"\"\"\n    task = asyncio.create_task(coro)\n    self.tasks.add(task)\n    task.add_done_callback(self.tasks.discard)\n    return task\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.start_background_task","title":"<code>start_background_task(method, interval=None, immediate=False, stop_on_error=False, stop_event=None)</code>","text":"<p>Run a task in the background, in a loop until cancelled.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>def start_background_task(\n    self,\n    method: Callable,\n    interval: float | Callable[[TaskManagerProtocol], float] | None = None,\n    immediate: bool = False,\n    stop_on_error: bool = False,\n    stop_event: asyncio.Event | None = None,\n) -&gt; None:\n    \"\"\"Run a task in the background, in a loop until cancelled.\"\"\"\n    self.execute_async(\n        self._background_task_loop(\n            method, interval, immediate, stop_on_error, stop_event\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.common.mixins.task_manager_mixin.TaskManagerMixin.wait_for_tasks","title":"<code>wait_for_tasks()</code>  <code>async</code>","text":"<p>Wait for all current tasks to complete.</p> Source code in <code>aiperf/common/mixins/task_manager_mixin.py</code> <pre><code>async def wait_for_tasks(self) -&gt; list[BaseException | None]:\n    \"\"\"Wait for all current tasks to complete.\"\"\"\n    return await asyncio.gather(*list(self.tasks), return_exceptions=True)\n</code></pre>"},{"location":"api/#aiperfcommonmixinsworker_tracker_mixin","title":"aiperf.common.mixins.worker_tracker_mixin","text":""},{"location":"api/#aiperf.common.mixins.worker_tracker_mixin.WorkerTrackerMixin","title":"<code>WorkerTrackerMixin</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code></p> <p>A worker tracker that tracks the health and tasks of the workers.</p> Source code in <code>aiperf/common/mixins/worker_tracker_mixin.py</code> <pre><code>@provides_hooks(AIPerfHook.ON_WORKER_UPDATE, AIPerfHook.ON_WORKER_STATUS_SUMMARY)\nclass WorkerTrackerMixin(MessageBusClientMixin):\n    \"\"\"A worker tracker that tracks the health and tasks of the workers.\"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs):\n        super().__init__(service_config=service_config, **kwargs)\n        self._workers_stats: dict[str, WorkerStats] = {}\n        self._workers_stats_lock = asyncio.Lock()\n\n    @on_message(MessageType.WORKER_HEALTH)\n    async def _on_worker_health(self, message: WorkerHealthMessage):\n        \"\"\"Update the worker stats from a worker health message.\"\"\"\n        worker_id = message.service_id\n        async with self._workers_stats_lock:\n            if worker_id not in self._workers_stats:\n                self._workers_stats[worker_id] = WorkerStats(worker_id=worker_id)\n            self._workers_stats[worker_id].health = message.health\n            self._workers_stats[worker_id].task_stats = message.task_stats\n            await self.run_hooks(\n                AIPerfHook.ON_WORKER_UPDATE,\n                worker_id=worker_id,\n                worker_stats=self._workers_stats[worker_id],\n            )\n\n    @on_message(MessageType.WORKER_STATUS_SUMMARY)\n    async def _on_worker_status_summary(self, message: WorkerStatusSummaryMessage):\n        \"\"\"Update the worker stats from a worker status summary message.\"\"\"\n        async with self._workers_stats_lock:\n            for worker_id, status in message.worker_statuses.items():\n                if worker_id not in self._workers_stats:\n                    self.warning(f\"Worker {worker_id} not found in worker stats\")\n                    continue\n                self._workers_stats[worker_id].status = status\n        await self.run_hooks(\n            AIPerfHook.ON_WORKER_STATUS_SUMMARY,\n            worker_status_summary=message.worker_statuses,\n        )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsbase_models","title":"aiperf.common.models.base_models","text":""},{"location":"api/#aiperf.common.models.base_models.AIPerfBaseModel","title":"<code>AIPerfBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for all AIPerf Pydantic models. This class is configured to allow arbitrary types to be used as fields as to allow for more flexible model definitions by end users without breaking the existing code.</p> <p>The @exclude_if_none decorator can also be used to specify which fields should be excluded from the serialized model if they are None. This is a workaround for the fact that pydantic does not support specifying exclude_none on a per-field basis.</p> Source code in <code>aiperf/common/models/base_models.py</code> <pre><code>class AIPerfBaseModel(BaseModel):\n    \"\"\"Base model for all AIPerf Pydantic models. This class is configured to allow\n    arbitrary types to be used as fields as to allow for more flexible model definitions\n    by end users without breaking the existing code.\n\n    The @exclude_if_none decorator can also be used to specify which fields\n    should be excluded from the serialized model if they are None. This is a workaround\n    for the fact that pydantic does not support specifying exclude_none on a per-field basis.\n    \"\"\"\n\n    _exclude_if_none_fields: ClassVar[set[str]] = set()\n    \"\"\"Set of field names that should be excluded from the serialized model if they\n    are None. This is set by the @exclude_if_none decorator.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_serializer\n    def _serialize_model(self) -&gt; dict[str, Any]:\n        \"\"\"Serialize the model to a dictionary.\n\n        This method overrides the default serializer to exclude fields that with a\n        value of None and were marked with the @exclude_if_none decorator.\n        \"\"\"\n        return {\n            k: v\n            for k, v in self\n            if not (k in self._exclude_if_none_fields and v is None)\n        }\n</code></pre>"},{"location":"api/#aiperf.common.models.base_models.exclude_if_none","title":"<code>exclude_if_none(*field_names)</code>","text":"<p>Decorator to set the _exclude_if_none_fields class attribute to the set of field names that should be excluded if they are None.</p> Source code in <code>aiperf/common/models/base_models.py</code> <pre><code>def exclude_if_none(*field_names: str):\n    \"\"\"Decorator to set the _exclude_if_none_fields class attribute to the set of\n    field names that should be excluded if they are None.\n    \"\"\"\n\n    def decorator(model: type[AIPerfBaseModelT]) -&gt; type[AIPerfBaseModelT]:\n        # This attribute is defined by the AIPerfBaseModel class.\n        if not hasattr(model, \"_exclude_if_none_fields\"):\n            model._exclude_if_none_fields = set()\n        model._exclude_if_none_fields.update(set(field_names))\n        return model\n\n    return decorator\n</code></pre>"},{"location":"api/#aiperfcommonmodelscredit_models","title":"aiperf.common.models.credit_models","text":""},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseConfig","title":"<code>CreditPhaseConfig</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Model for phase credit config. This is used by the TimingManager to configure the credit phases.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>class CreditPhaseConfig(AIPerfBaseModel):\n    \"\"\"Model for phase credit config. This is used by the TimingManager to configure the credit phases.\"\"\"\n\n    type: CreditPhase = Field(..., description=\"The type of credit phase\")\n    total_expected_requests: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The total number of expected credits. If None, the phase is not request count based.\",\n    )\n    expected_duration_sec: float | None = Field(\n        default=None,\n        ge=1,\n        description=\"The expected duration of the credit phase in seconds. If None, the phase is not time based.\",\n    )\n\n    @property\n    def is_time_based(self) -&gt; bool:\n        return self.expected_duration_sec is not None\n\n    @property\n    def is_request_count_based(self) -&gt; bool:\n        return self.total_expected_requests is not None\n\n    @property\n    def is_valid(self) -&gt; bool:\n        \"\"\"A phase config is valid if it is exactly one of the following:\n        - is_time_based (expected_duration_sec is set and &gt; 0)\n        - is_request_count_based (total_expected_requests is set and &gt; 0)\n        \"\"\"\n        is_time_based = self.is_time_based\n        is_request_count_based = self.is_request_count_based\n        return (is_time_based and not is_request_count_based) or (\n            not is_time_based and is_request_count_based\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseConfig.is_valid","title":"<code>is_valid</code>  <code>property</code>","text":"<p>A phase config is valid if it is exactly one of the following: - is_time_based (expected_duration_sec is set and &gt; 0) - is_request_count_based (total_expected_requests is set and &gt; 0)</p>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats","title":"<code>CreditPhaseStats</code>","text":"<p>               Bases: <code>CreditPhaseConfig</code></p> <p>Model for phase credit stats. Extends the CreditPhaseConfig fields to track the progress of the credit phases. How many credits were dropped and how many were returned, as well as the progress percentage of the phase.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>class CreditPhaseStats(CreditPhaseConfig):\n    \"\"\"Model for phase credit stats. Extends the CreditPhaseConfig fields to track the progress of the credit phases.\n    How many credits were dropped and how many were returned, as well as the progress percentage of the phase.\"\"\"\n\n    start_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the credit phase in nanoseconds.\",\n    )\n    sent_end_ns: int | None = Field(\n        default=None,\n        description=\"The time of the last sent credit in nanoseconds. If None, the phase has not sent all credits.\",\n    )\n    end_ns: int | None = Field(\n        default=None,\n        ge=1,\n        description=\"The time in which the last credit was returned from the workers in nanoseconds. If None, the phase has not completed.\",\n    )\n    sent: int = Field(default=0, description=\"The number of sent credits\")\n    completed: int = Field(\n        default=0,\n        description=\"The number of completed credits (returned from the workers)\",\n    )\n\n    @property\n    def is_sending_complete(self) -&gt; bool:\n        return self.sent_end_ns is not None\n\n    @property\n    def is_complete(self) -&gt; bool:\n        return self.is_sending_complete and self.end_ns is not None\n\n    @property\n    def is_started(self) -&gt; bool:\n        return self.start_ns is not None\n\n    @property\n    def in_flight(self) -&gt; int:\n        \"\"\"Calculate the number of in-flight credits (sent but not completed).\"\"\"\n        return self.sent - self.completed\n\n    def should_send(self) -&gt; bool:\n        \"\"\"Whether the phase should send more credits.\"\"\"\n        if self.total_expected_requests is not None:\n            return self.sent &lt; self.total_expected_requests\n        elif self.expected_duration_sec is not None:\n            return time.time_ns() - self.start_ns &lt;= (\n                self.expected_duration_sec * NANOS_PER_SECOND\n            )\n        else:\n            raise InvalidStateError(\"Credit phase is not time or request count based\")\n\n    @property\n    def progress_percent(self) -&gt; float | None:\n        if self.start_ns is None:\n            return None\n\n        if self.is_complete:\n            return 100\n\n        if self.is_time_based:\n            # Time based, so progress is the percentage of time elapsed compared to the duration\n\n            return (\n                (time.time_ns() - self.start_ns)\n                / (self.expected_duration_sec * NANOS_PER_SECOND)  # type: ignore\n            ) * 100\n\n        elif self.total_expected_requests is not None:\n            # Credit count based, so progress is the percentage of credits returned\n            return (self.completed / self.total_expected_requests) * 100\n\n        # We don't know the progress\n        return None\n\n    @classmethod\n    def from_phase_config(cls, phase_config: CreditPhaseConfig) -&gt; \"CreditPhaseStats\":\n        \"\"\"Create a CreditPhaseStats from a CreditPhaseConfig. This is used to initialize the stats for a phase.\"\"\"\n        return cls(\n            type=phase_config.type,\n            total_expected_requests=phase_config.total_expected_requests,\n            expected_duration_sec=phase_config.expected_duration_sec,\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats.in_flight","title":"<code>in_flight</code>  <code>property</code>","text":"<p>Calculate the number of in-flight credits (sent but not completed).</p>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats.from_phase_config","title":"<code>from_phase_config(phase_config)</code>  <code>classmethod</code>","text":"<p>Create a CreditPhaseStats from a CreditPhaseConfig. This is used to initialize the stats for a phase.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>@classmethod\ndef from_phase_config(cls, phase_config: CreditPhaseConfig) -&gt; \"CreditPhaseStats\":\n    \"\"\"Create a CreditPhaseStats from a CreditPhaseConfig. This is used to initialize the stats for a phase.\"\"\"\n    return cls(\n        type=phase_config.type,\n        total_expected_requests=phase_config.total_expected_requests,\n        expected_duration_sec=phase_config.expected_duration_sec,\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.CreditPhaseStats.should_send","title":"<code>should_send()</code>","text":"<p>Whether the phase should send more credits.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>def should_send(self) -&gt; bool:\n    \"\"\"Whether the phase should send more credits.\"\"\"\n    if self.total_expected_requests is not None:\n        return self.sent &lt; self.total_expected_requests\n    elif self.expected_duration_sec is not None:\n        return time.time_ns() - self.start_ns &lt;= (\n            self.expected_duration_sec * NANOS_PER_SECOND\n        )\n    else:\n        raise InvalidStateError(\"Credit phase is not time or request count based\")\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.ProcessingStats","title":"<code>ProcessingStats</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Model for phase processing stats. How many requests were processed and how many errors were encountered.</p> Source code in <code>aiperf/common/models/credit_models.py</code> <pre><code>class ProcessingStats(AIPerfBaseModel):\n    \"\"\"Model for phase processing stats. How many requests were processed and\n    how many errors were encountered.\"\"\"\n\n    processed: int = Field(\n        default=0, description=\"The number of records processed successfully\"\n    )\n    errors: int = Field(\n        default=0, description=\"The number of record errors encountered\"\n    )\n    total_expected_requests: int | None = Field(\n        default=None,\n        description=\"The total number of expected requests to process. If None, the phase is not request count based.\",\n    )\n\n    @property\n    def total_records(self) -&gt; int:\n        \"\"\"The total number of records processed successfully or in error.\"\"\"\n        return self.processed + self.errors\n\n    @property\n    def is_complete(self) -&gt; bool:\n        return self.total_records == self.total_expected_requests\n</code></pre>"},{"location":"api/#aiperf.common.models.credit_models.ProcessingStats.total_records","title":"<code>total_records</code>  <code>property</code>","text":"<p>The total number of records processed successfully or in error.</p>"},{"location":"api/#aiperfcommonmodelsdataset_models","title":"aiperf.common.models.dataset_models","text":""},{"location":"api/#aiperf.common.models.dataset_models.Audio","title":"<code>Audio</code>","text":"<p>               Bases: <code>Media</code></p> <p>Media that contains audio data.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>class Audio(Media):\n    \"\"\"Media that contains audio data.\"\"\"\n\n    media_type: ClassVar[MediaTypeT] = MediaType.AUDIO\n</code></pre>"},{"location":"api/#aiperf.common.models.dataset_models.Conversation","title":"<code>Conversation</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>A dataset representation of a full conversation.</p> <p>A conversation is a sequence of turns between a user and an endpoint, and it contains the session ID and all the turns that consists the conversation.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>class Conversation(AIPerfBaseModel):\n    \"\"\"A dataset representation of a full conversation.\n\n    A conversation is a sequence of turns between a user and an endpoint,\n    and it contains the session ID and all the turns that consists the conversation.\n    \"\"\"\n\n    turns: list[Turn] = Field(\n        default=[], description=\"List of turns in the conversation.\"\n    )\n    session_id: str = Field(default=\"\", description=\"Session ID of the conversation.\")\n</code></pre>"},{"location":"api/#aiperf.common.models.dataset_models.Image","title":"<code>Image</code>","text":"<p>               Bases: <code>Media</code></p> <p>Media that contains image data.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>class Image(Media):\n    \"\"\"Media that contains image data.\"\"\"\n\n    media_type: ClassVar[MediaTypeT] = MediaType.IMAGE\n</code></pre>"},{"location":"api/#aiperf.common.models.dataset_models.Media","title":"<code>Media</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base class for all media fields. Contains name and contents of the media data.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>class Media(AIPerfBaseModel):\n    \"\"\"Base class for all media fields. Contains name and contents of the media data.\"\"\"\n\n    name: str = Field(default=\"\", description=\"Name of the media field.\")\n\n    contents: list[str] = Field(\n        default=[],\n        description=\"List of media contents. Supports batched media payload in a single turn.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.dataset_models.Text","title":"<code>Text</code>","text":"<p>               Bases: <code>Media</code></p> <p>Media that contains text/prompt data.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>class Text(Media):\n    \"\"\"Media that contains text/prompt data.\"\"\"\n\n    media_type: ClassVar[MediaTypeT] = MediaType.TEXT\n</code></pre>"},{"location":"api/#aiperf.common.models.dataset_models.Turn","title":"<code>Turn</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>A dataset representation of a single turn within a conversation.</p> <p>A turn is a single interaction between a user and an AI assistant, and it contains timestamp, delay, and raw data that user sends in each turn.</p> Source code in <code>aiperf/common/models/dataset_models.py</code> <pre><code>@exclude_if_none(\"role\")\nclass Turn(AIPerfBaseModel):\n    \"\"\"A dataset representation of a single turn within a conversation.\n\n    A turn is a single interaction between a user and an AI assistant,\n    and it contains timestamp, delay, and raw data that user sends in each turn.\n    \"\"\"\n\n    timestamp: int | None = Field(\n        default=None, description=\"Timestamp of the turn in milliseconds.\"\n    )\n    delay: int | None = Field(\n        default=None,\n        description=\"Amount of milliseconds to wait before sending the turn.\",\n    )\n    model: str | None = Field(default=None, description=\"Model name used for the turn.\")\n    role: str | None = Field(default=None, description=\"Role of the turn.\")\n    max_tokens: int | None = Field(\n        default=None, description=\"Maximum number of tokens to generate for this turn.\"\n    )\n    texts: list[Text] = Field(\n        default=[], description=\"Collection of text data in each turn.\"\n    )\n    images: list[Image] = Field(\n        default=[], description=\"Collection of image data in each turn.\"\n    )\n    audios: list[Audio] = Field(\n        default=[], description=\"Collection of audio data in each turn.\"\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelserror_models","title":"aiperf.common.models.error_models","text":""},{"location":"api/#aiperf.common.models.error_models.ErrorDetails","title":"<code>ErrorDetails</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Encapsulates details about an error.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>class ErrorDetails(AIPerfBaseModel):\n    \"\"\"Encapsulates details about an error.\"\"\"\n\n    code: int | None = Field(\n        default=None,\n        description=\"The error code.\",\n    )\n    type: str | None = Field(\n        default=None,\n        description=\"The error type.\",\n    )\n    message: str = Field(\n        ...,\n        description=\"The error message.\",\n    )\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Check if the error details are equal by comparing the code, type, and message.\"\"\"\n        if not isinstance(other, ErrorDetails):\n            return False\n        return (\n            self.code == other.code\n            and self.type == other.type\n            and self.message == other.message\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash the error details by hashing the code, type, and message.\"\"\"\n        return hash((self.code, self.type, self.message))\n\n    @classmethod\n    def from_exception(cls, e: BaseException) -&gt; \"ErrorDetails\":\n        \"\"\"Create an error details object from an exception.\"\"\"\n        return cls(\n            type=e.__class__.__name__,\n            message=str(e),\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetails.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check if the error details are equal by comparing the code, type, and message.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check if the error details are equal by comparing the code, type, and message.\"\"\"\n    if not isinstance(other, ErrorDetails):\n        return False\n    return (\n        self.code == other.code\n        and self.type == other.type\n        and self.message == other.message\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetails.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash the error details by hashing the code, type, and message.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash the error details by hashing the code, type, and message.\"\"\"\n    return hash((self.code, self.type, self.message))\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetails.from_exception","title":"<code>from_exception(e)</code>  <code>classmethod</code>","text":"<p>Create an error details object from an exception.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>@classmethod\ndef from_exception(cls, e: BaseException) -&gt; \"ErrorDetails\":\n    \"\"\"Create an error details object from an exception.\"\"\"\n    return cls(\n        type=e.__class__.__name__,\n        message=str(e),\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.error_models.ErrorDetailsCount","title":"<code>ErrorDetailsCount</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Count of error details.</p> Source code in <code>aiperf/common/models/error_models.py</code> <pre><code>class ErrorDetailsCount(AIPerfBaseModel):\n    \"\"\"Count of error details.\"\"\"\n\n    error_details: ErrorDetails\n    count: int = Field(\n        ...,\n        description=\"The count of the error details.\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelshealth_models","title":"aiperf.common.models.health_models","text":""},{"location":"api/#aiperf.common.models.health_models.ProcessHealth","title":"<code>ProcessHealth</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Model for process health data.</p> Source code in <code>aiperf/common/models/health_models.py</code> <pre><code>class ProcessHealth(AIPerfBaseModel):\n    \"\"\"Model for process health data.\"\"\"\n\n    pid: int | None = Field(\n        default=None,\n        description=\"The PID of the process\",\n    )\n    create_time: float = Field(\n        ..., description=\"The creation time of the process in seconds\"\n    )\n    uptime: float = Field(..., description=\"The uptime of the process in seconds\")\n    cpu_usage: float = Field(\n        ..., description=\"The current CPU usage of the process in %\"\n    )\n    memory_usage: int = Field(\n        ..., description=\"The current memory usage of the process in bytes (rss)\"\n    )\n    io_counters: IOCounters | tuple | None = Field(\n        default=None,\n        description=\"The current I/O counters of the process (read_count, write_count, read_bytes, write_bytes, read_chars, write_chars)\",\n    )\n    cpu_times: CPUTimes | tuple | None = Field(\n        default=None,\n        description=\"The current CPU times of the process (user, system, iowait)\",\n    )\n    num_ctx_switches: CtxSwitches | tuple | None = Field(\n        default=None,\n        description=\"The current number of context switches (voluntary, involuntary)\",\n    )\n    num_threads: int | None = Field(\n        default=None,\n        description=\"The current number of threads\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsprogress_models","title":"aiperf.common.models.progress_models","text":"<p>Models for tracking the progress of the benchmark suite.</p>"},{"location":"api/#aiperf.common.models.progress_models.ComputedStats","title":"<code>ComputedStats</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Computed info for a phase (can be used for requests or records).</p> Source code in <code>aiperf/common/models/progress_models.py</code> <pre><code>class ComputedStats(AIPerfBaseModel):\n    \"\"\"Computed info for a phase (can be used for requests or records).\"\"\"\n\n    per_second: float | None = Field(default=None, description=\"The average per second\")\n    eta: float | None = Field(\n        default=None, description=\"The estimated time for completion\"\n    )\n    last_update_ns: int | None = Field(\n        default=None, description=\"The time of the last update\"\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.progress_models.FullPhaseProgress","title":"<code>FullPhaseProgress</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Full state of the credit phase progress, including the progress of the phase, the processing stats, and the worker stats.</p> Source code in <code>aiperf/common/models/progress_models.py</code> <pre><code>class FullPhaseProgress(AIPerfBaseModel):\n    \"\"\"Full state of the credit phase progress, including the progress of the phase, the processing stats, and the worker stats.\"\"\"\n\n    requests: RequestsStats = Field(\n        ...,\n        description=\"The progress stats for the requests as reported by the TimingManager\",\n    )\n    records: RecordsStats = Field(\n        ...,\n        description=\"The progress stats for the records as reported by the RecordsManager\",\n    )\n\n    @property\n    def start_ns(self) -&gt; int:\n        \"\"\"Get the start time.\"\"\"\n        # NOTE: This will always be set, because ProgressTracker will always set the start_ns when the phase starts.\n        return self.requests.start_ns  # type: ignore\n\n    @property\n    def phase(self) -&gt; CreditPhase:\n        \"\"\"Get the phase.\"\"\"\n        return self.requests.type\n\n    @property\n    def elapsed_time(self) -&gt; float:\n        \"\"\"Get the elapsed time.\"\"\"\n        return (time.time_ns() - self.start_ns) / NANOS_PER_SECOND\n</code></pre>"},{"location":"api/#aiperf.common.models.progress_models.FullPhaseProgress.elapsed_time","title":"<code>elapsed_time</code>  <code>property</code>","text":"<p>Get the elapsed time.</p>"},{"location":"api/#aiperf.common.models.progress_models.FullPhaseProgress.phase","title":"<code>phase</code>  <code>property</code>","text":"<p>Get the phase.</p>"},{"location":"api/#aiperf.common.models.progress_models.FullPhaseProgress.start_ns","title":"<code>start_ns</code>  <code>property</code>","text":"<p>Get the start time.</p>"},{"location":"api/#aiperf.common.models.progress_models.RecordsStats","title":"<code>RecordsStats</code>","text":"<p>               Bases: <code>ComputedStats</code>, <code>ProcessingStats</code></p> <p>Stats for the records. Based on the RecordsManager data.</p> Source code in <code>aiperf/common/models/progress_models.py</code> <pre><code>@implements_protocol(StatsProtocol)\nclass RecordsStats(ComputedStats, ProcessingStats):\n    \"\"\"Stats for the records. Based on the RecordsManager data.\"\"\"\n\n    start_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the requests in nanoseconds.\",\n    )\n\n    @property\n    def finished(self) -&gt; int:\n        \"\"\"Get the number of finished records.\"\"\"\n        return self.processed + self.errors\n\n    @property\n    def progress_percent(self) -&gt; float | None:\n        \"\"\"Get the progress percent.\"\"\"\n        if not self.total_expected_requests:\n            return None\n        return (self.total_records / self.total_expected_requests) * 100\n\n    @property\n    def elapsed_time(self) -&gt; float | None:\n        \"\"\"Get the elapsed time in seconds.\"\"\"\n        if self.start_ns is None:\n            return None\n        return (time.time_ns() - self.start_ns) / NANOS_PER_SECOND\n</code></pre>"},{"location":"api/#aiperf.common.models.progress_models.RecordsStats.elapsed_time","title":"<code>elapsed_time</code>  <code>property</code>","text":"<p>Get the elapsed time in seconds.</p>"},{"location":"api/#aiperf.common.models.progress_models.RecordsStats.finished","title":"<code>finished</code>  <code>property</code>","text":"<p>Get the number of finished records.</p>"},{"location":"api/#aiperf.common.models.progress_models.RecordsStats.progress_percent","title":"<code>progress_percent</code>  <code>property</code>","text":"<p>Get the progress percent.</p>"},{"location":"api/#aiperf.common.models.progress_models.RequestsStats","title":"<code>RequestsStats</code>","text":"<p>               Bases: <code>ComputedStats</code>, <code>CreditPhaseStats</code></p> <p>Stats for the requests. Based on the TimingManager data.</p> Source code in <code>aiperf/common/models/progress_models.py</code> <pre><code>@implements_protocol(StatsProtocol)\nclass RequestsStats(ComputedStats, CreditPhaseStats):\n    \"\"\"Stats for the requests. Based on the TimingManager data.\"\"\"\n\n    @property\n    def finished(self) -&gt; int:\n        \"\"\"Get the number of finished requests.\"\"\"\n        return self.completed\n\n    @property\n    def elapsed_time(self) -&gt; float | None:\n        \"\"\"Get the elapsed time in seconds.\"\"\"\n        if self.start_ns is None:\n            return None\n        return (time.time_ns() - self.start_ns) / NANOS_PER_SECOND\n</code></pre>"},{"location":"api/#aiperf.common.models.progress_models.RequestsStats.elapsed_time","title":"<code>elapsed_time</code>  <code>property</code>","text":"<p>Get the elapsed time in seconds.</p>"},{"location":"api/#aiperf.common.models.progress_models.RequestsStats.finished","title":"<code>finished</code>  <code>property</code>","text":"<p>Get the number of finished requests.</p>"},{"location":"api/#aiperf.common.models.progress_models.StatsProtocol","title":"<code>StatsProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for stats.</p> Source code in <code>aiperf/common/models/progress_models.py</code> <pre><code>class StatsProtocol(Protocol):\n    \"\"\"Protocol for stats.\"\"\"\n\n    progress_percent: float | None\n    total_expected_requests: int | None\n    last_update_ns: int | None\n    per_second: float | None\n    eta: float | None\n    start_ns: int | None\n\n    @property\n    def finished(self) -&gt; int: ...\n\n    @property\n    def is_complete(self) -&gt; bool: ...\n</code></pre>"},{"location":"api/#aiperf.common.models.progress_models.WorkerStats","title":"<code>WorkerStats</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Stats for a worker.</p> Source code in <code>aiperf/common/models/progress_models.py</code> <pre><code>class WorkerStats(AIPerfBaseModel):\n    \"\"\"Stats for a worker.\"\"\"\n\n    worker_id: str = Field(\n        ...,\n        description=\"The ID of the worker\",\n    )\n    task_stats: WorkerTaskStats = Field(\n        default_factory=WorkerTaskStats,\n        description=\"The task stats for the worker as reported by the Workers (total, completed, failed)\",\n    )\n    processing_stats: ProcessingStats = Field(\n        default_factory=ProcessingStats,\n        description=\"The processing stats for the worker as reported by the RecordsManager (processed, errors)\",\n    )\n    health: ProcessHealth | None = Field(\n        default=None,\n        description=\"The health of the worker as reported by the Workers\",\n    )\n    status: WorkerStatus = Field(\n        default=WorkerStatus.IDLE,\n        description=\"The status of the worker\",\n    )\n    last_update_ns: int | None = Field(\n        default=None,\n        description=\"The last time the worker was updated in nanoseconds\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsrecord_models","title":"aiperf.common.models.record_models","text":""},{"location":"api/#aiperf.common.models.record_models.BaseResponseData","title":"<code>BaseResponseData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all response data.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class BaseResponseData(BaseModel):\n    \"\"\"Base class for all response data.\"\"\"\n\n    def get_text(self) -&gt; str:\n        \"\"\"Get the text of the response.\"\"\"\n        return \"\"\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.BaseResponseData.get_text","title":"<code>get_text()</code>","text":"<p>Get the text of the response.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Get the text of the response.\"\"\"\n    return \"\"\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.EmbeddingResponseData","title":"<code>EmbeddingResponseData</code>","text":"<p>               Bases: <code>BaseResponseData</code></p> <p>Parsed embedding response data.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class EmbeddingResponseData(BaseResponseData):\n    \"\"\"Parsed embedding response data.\"\"\"\n\n    embeddings: list[list[float]] = Field(\n        ..., description=\"The embedding vectors from the response.\"\n    )\n\n    def get_text(self) -&gt; str:\n        \"\"\"Get the text of the response (empty for embeddings).\"\"\"\n        return \"\"\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.EmbeddingResponseData.get_text","title":"<code>get_text()</code>","text":"<p>Get the text of the response (empty for embeddings).</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Get the text of the response (empty for embeddings).\"\"\"\n    return \"\"\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.InferenceServerResponse","title":"<code>InferenceServerResponse</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Response from a inference client.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class InferenceServerResponse(AIPerfBaseModel):\n    \"\"\"Response from a inference client.\"\"\"\n\n    perf_ns: int = Field(\n        ...,\n        description=\"The timestamp of the response in nanoseconds (perf_counter_ns).\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.MetricResult","title":"<code>MetricResult</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>The result values of a single metric.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class MetricResult(AIPerfBaseModel):\n    \"\"\"The result values of a single metric.\"\"\"\n\n    tag: MetricTagT = Field(description=\"The unique identifier of the metric\")\n    # NOTE: We do not use a MetricUnitT here, as that is harder to de-serialize from JSON strings with pydantic.\n    #       If we need an instance of a MetricUnitT, lookup the unit based on the tag in the MetricRegistry.\n    unit: str = Field(description=\"The unit of the metric, e.g. 'ms'\")\n    header: str = Field(\n        description=\"The user friendly name of the metric (e.g. 'Inter Token Latency')\"\n    )\n    avg: float | None = None\n    min: int | float | None = None\n    max: int | float | None = None\n    p1: float | None = None\n    p5: float | None = None\n    p25: float | None = None\n    p50: float | None = None\n    p75: float | None = None\n    p90: float | None = None\n    p95: float | None = None\n    p99: float | None = None\n    std: float | None = None\n    count: int | None = Field(\n        default=None,\n        description=\"The total number of records used to calculate the metric\",\n    )\n\n    def to_display_unit(self) -&gt; \"MetricResult\":\n        \"\"\"Convert the metric result to its display unit.\"\"\"\n        from aiperf.exporters.display_units_utils import to_display_unit\n        from aiperf.metrics.metric_registry import MetricRegistry\n\n        return to_display_unit(self, MetricRegistry)\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.MetricResult.to_display_unit","title":"<code>to_display_unit()</code>","text":"<p>Convert the metric result to its display unit.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def to_display_unit(self) -&gt; \"MetricResult\":\n    \"\"\"Convert the metric result to its display unit.\"\"\"\n    from aiperf.exporters.display_units_utils import to_display_unit\n    from aiperf.metrics.metric_registry import MetricRegistry\n\n    return to_display_unit(self, MetricRegistry)\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponse","title":"<code>ParsedResponse</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Parsed response from a inference client.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ParsedResponse(AIPerfBaseModel):\n    \"\"\"Parsed response from a inference client.\"\"\"\n\n    perf_ns: int = Field(description=\"The performance timestamp of the response.\")\n    data: SerializeAsAny[\n        ReasoningResponseData\n        | TextResponseData\n        | EmbeddingResponseData\n        | RankingsResponseData\n        | BaseResponseData\n    ] = Field(..., description=\"The parsed response data.\")\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord","title":"<code>ParsedResponseRecord</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Record of a request and its associated responses, already parsed and ready for metrics.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ParsedResponseRecord(AIPerfBaseModel):\n    \"\"\"Record of a request and its associated responses, already parsed and ready for metrics.\"\"\"\n\n    request: RequestRecord = Field(description=\"The original request record\")\n    responses: list[ParsedResponse] = Field(description=\"The parsed responses.\")\n    input_token_count: int | None = Field(\n        default=None,\n        description=\"The number of tokens in the input. If None, the number of tokens could not be calculated.\",\n    )\n    output_token_count: int | None = Field(\n        default=None,\n        description=\"The number of output tokens across all responses. If None, the number of tokens could not be calculated.\",\n    )\n    reasoning_token_count: int | None = Field(\n        default=None,\n        description=\"The number of reasoning tokens across all responses. If None, the number of tokens could not be calculated, or the model does not support reasoning.\",\n    )\n\n    @cached_property\n    def start_perf_ns(self) -&gt; int:\n        \"\"\"Get the start time of the request in nanoseconds (perf_counter_ns).\"\"\"\n        return self.request.start_perf_ns\n\n    @cached_property\n    def timestamp_ns(self) -&gt; int:\n        \"\"\"Get the wall clock timestamp of the request in nanoseconds. DO NOT USE FOR LATENCY CALCULATIONS. (time.time_ns).\"\"\"\n        return self.request.timestamp_ns\n\n    # TODO: How do we differentiate the end of the request vs the time of the last response?\n    #       Which one should we use for the latency metrics?\n    @cached_property\n    def end_perf_ns(self) -&gt; int:\n        \"\"\"Get the end time of the request in nanoseconds (perf_counter_ns).\n        If request.end_perf_ns is not set, use the time of the last response.\n        If there are no responses, use sys.maxsize.\n        \"\"\"\n        return (\n            self.request.end_perf_ns\n            if self.request.end_perf_ns\n            else self.responses[-1].perf_ns\n            if self.responses\n            else sys.maxsize\n        )\n\n    @cached_property\n    def request_duration_ns(self) -&gt; int:\n        \"\"\"Get the duration of the request in nanoseconds.\"\"\"\n        return self.end_perf_ns - self.start_perf_ns\n\n    @cached_property\n    def tokens_per_second(self) -&gt; float | None:\n        \"\"\"Get the number of tokens per second of the request.\"\"\"\n        if self.output_token_count is None or self.request_duration_ns == 0:\n            return None\n        return self.output_token_count / (self.request_duration_ns / NANOS_PER_SECOND)\n\n    @cached_property\n    def has_error(self) -&gt; bool:\n        \"\"\"Check if the response record has an error.\"\"\"\n        return self.request.has_error\n\n    @cached_property\n    def valid(self) -&gt; bool:\n        \"\"\"Check if the response record is valid.\n\n        Checks:\n        - Request has no errors\n        - Has at least one response\n        - Start time is before the end time\n        - Response timestamps are within valid ranges\n\n        Returns:\n            bool: True if the record is valid, False otherwise.\n        \"\"\"\n        return (\n            not self.has_error\n            and len(self.responses) &gt; 0\n            and 0 &lt;= self.start_perf_ns &lt; self.end_perf_ns &lt; sys.maxsize\n            and all(0 &lt; response.perf_ns &lt; sys.maxsize for response in self.responses)\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.end_perf_ns","title":"<code>end_perf_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the end time of the request in nanoseconds (perf_counter_ns). If request.end_perf_ns is not set, use the time of the last response. If there are no responses, use sys.maxsize.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.has_error","title":"<code>has_error</code>  <code>cached</code> <code>property</code>","text":"<p>Check if the response record has an error.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.request_duration_ns","title":"<code>request_duration_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the duration of the request in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.start_perf_ns","title":"<code>start_perf_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the start time of the request in nanoseconds (perf_counter_ns).</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.timestamp_ns","title":"<code>timestamp_ns</code>  <code>cached</code> <code>property</code>","text":"<p>Get the wall clock timestamp of the request in nanoseconds. DO NOT USE FOR LATENCY CALCULATIONS. (time.time_ns).</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.tokens_per_second","title":"<code>tokens_per_second</code>  <code>cached</code> <code>property</code>","text":"<p>Get the number of tokens per second of the request.</p>"},{"location":"api/#aiperf.common.models.record_models.ParsedResponseRecord.valid","title":"<code>valid</code>  <code>cached</code> <code>property</code>","text":"<p>Check if the response record is valid.</p> <p>Checks: - Request has no errors - Has at least one response - Start time is before the end time - Response timestamps are within valid ranges</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the record is valid, False otherwise.</p>"},{"location":"api/#aiperf.common.models.record_models.ProcessRecordsResult","title":"<code>ProcessRecordsResult</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Result of the process records command.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ProcessRecordsResult(AIPerfBaseModel):\n    \"\"\"Result of the process records command.\"\"\"\n\n    results: ProfileResults = Field(..., description=\"The profile results\")\n    errors: list[ErrorDetails] = Field(\n        default_factory=list,\n        description=\"Any error that occurred while processing the profile results\",\n    )\n\n    def get(self, tag: MetricTagT) -&gt; MetricResult | None:\n        \"\"\"Get a metric result by tag, if it exists.\"\"\"\n        return self.results.get(tag)\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ProcessRecordsResult.get","title":"<code>get(tag)</code>","text":"<p>Get a metric result by tag, if it exists.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get(self, tag: MetricTagT) -&gt; MetricResult | None:\n    \"\"\"Get a metric result by tag, if it exists.\"\"\"\n    return self.results.get(tag)\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ProfileResults","title":"<code>ProfileResults</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ProfileResults(AIPerfBaseModel):\n    records: list[MetricResult] | None = Field(\n        ..., description=\"The records of the profile results\"\n    )\n    total_expected: int | None = Field(\n        default=None,\n        description=\"The total number of inference requests expected to be made (if known)\",\n    )\n    completed: int = Field(\n        ..., description=\"The number of inference requests completed\"\n    )\n    start_ns: int = Field(\n        ..., description=\"The start time of the profile run in nanoseconds\"\n    )\n    end_ns: int = Field(\n        ..., description=\"The end time of the profile run in nanoseconds\"\n    )\n    was_cancelled: bool = Field(\n        default=False,\n        description=\"Whether the profile run was cancelled early\",\n    )\n    error_summary: list[ErrorDetailsCount] = Field(\n        default_factory=list,\n        description=\"A list of the unique error details and their counts\",\n    )\n\n    def get(self, tag: MetricTagT) -&gt; MetricResult | None:\n        \"\"\"Get a metric result by tag, if it exists.\"\"\"\n        for record in self.records or []:\n            if record.tag == tag:\n                return record\n        return None\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ProfileResults.get","title":"<code>get(tag)</code>","text":"<p>Get a metric result by tag, if it exists.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get(self, tag: MetricTagT) -&gt; MetricResult | None:\n    \"\"\"Get a metric result by tag, if it exists.\"\"\"\n    for record in self.records or []:\n        if record.tag == tag:\n            return record\n    return None\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.RankingsResponseData","title":"<code>RankingsResponseData</code>","text":"<p>               Bases: <code>BaseResponseData</code></p> <p>Parsed rankings response data.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class RankingsResponseData(BaseResponseData):\n    \"\"\"Parsed rankings response data.\"\"\"\n\n    rankings: list[dict[str, Any]] = Field(\n        ..., description=\"The rankings results from the response.\"\n    )\n\n    def get_text(self) -&gt; str:\n        \"\"\"Get the text of the response (empty for rankings).\"\"\"\n        return \"\"\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.RankingsResponseData.get_text","title":"<code>get_text()</code>","text":"<p>Get the text of the response (empty for rankings).</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Get the text of the response (empty for rankings).\"\"\"\n    return \"\"\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ReasoningResponseData","title":"<code>ReasoningResponseData</code>","text":"<p>               Bases: <code>BaseResponseData</code></p> <p>Parsed reasoning response data.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class ReasoningResponseData(BaseResponseData):\n    \"\"\"Parsed reasoning response data.\"\"\"\n\n    content: str | None = Field(\n        default=None, description=\"The parsed content of the response.\"\n    )\n    reasoning: str | None = Field(\n        default=None, description=\"The parsed reasoning of the response.\"\n    )\n\n    def get_text(self) -&gt; str:\n        \"\"\"Get the text of the response.\"\"\"\n        return \"\".join([self.reasoning or \"\", self.content or \"\"])\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.ReasoningResponseData.get_text","title":"<code>get_text()</code>","text":"<p>Get the text of the response.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Get the text of the response.\"\"\"\n    return \"\".join([self.reasoning or \"\", self.content or \"\"])\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord","title":"<code>RequestRecord</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Record of a request with its associated responses.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class RequestRecord(AIPerfBaseModel):\n    \"\"\"Record of a request with its associated responses.\"\"\"\n\n    turn: Turn | None = Field(\n        default=None,\n        description=\"The turn of the request, if applicable.\",\n    )\n    conversation_id: str | None = Field(\n        default=None,\n        description=\"The ID of the conversation (if applicable).\",\n    )\n    turn_index: int | None = Field(\n        default=None,\n        ge=0,\n        description=\"The index of the turn in the conversation (if applicable).\",\n    )\n    model_name: str | None = Field(\n        default=None,\n        description=\"The name of the model targeted by the request.\",\n    )\n    timestamp_ns: int = Field(\n        default_factory=time.time_ns,\n        description=\"The wall clock timestamp of the request in nanoseconds. DO NOT USE FOR LATENCY CALCULATIONS. (time.time_ns).\",\n    )\n    start_perf_ns: int = Field(\n        default_factory=time.perf_counter_ns,\n        description=\"The start reference time of the request in nanoseconds used for latency calculations (perf_counter_ns).\",\n    )\n    end_perf_ns: int | None = Field(\n        default=None,\n        description=\"The end time of the request in nanoseconds (perf_counter_ns).\",\n    )\n    recv_start_perf_ns: int | None = Field(\n        default=None,\n        description=\"The start time of the streaming response in nanoseconds (perf_counter_ns).\",\n    )\n    status: int | None = Field(\n        default=None,\n        description=\"The HTTP status code of the response.\",\n    )\n    # TODO: Maybe we could improve this with subclassing the responses to allow for more specific types.\n    #       This would allow us to remove the SerializeAsAny and use a more specific type. Look at how we handle\n    #       the CommandMessage and CommandResponse classes for an example.\n    # NOTE: We need to use SerializeAsAny to allow for generic subclass support\n    # NOTE: The order of the types is important, as that is the order they are type checked.\n    #       Start with the most specific types and work towards the most general types.\n    responses: SerializeAsAny[\n        list[SSEMessage | TextResponse | InferenceServerResponse | Any]\n    ] = Field(\n        default_factory=list,\n        description=\"The raw responses received from the request.\",\n    )\n    error: ErrorDetails | None = Field(\n        default=None,\n        description=\"The error details if the request failed.\",\n    )\n    delayed_ns: int | None = Field(\n        default=None,\n        ge=0,\n        description=\"The number of nanoseconds the request was delayed from when it was expected to be sent, \"\n        \"or None if the request was sent on time, or did not have a credit_drop_ns timestamp.\",\n    )\n    credit_phase: CreditPhase = Field(\n        default=CreditPhase.PROFILING,\n        description=\"The type of credit phase (either warmup or profiling)\",\n    )\n    credit_drop_latency: int | None = Field(\n        default=None,\n        description=\"The latency of the credit drop in nanoseconds from when it was first received by a Worker to when the inference request was actually sent. \"\n        \"This can be used to trace internal latency in order to identify bottlenecks or other issues.\",\n        ge=0,\n    )\n\n    @property\n    def delayed(self) -&gt; bool:\n        \"\"\"Check if the request was delayed.\"\"\"\n        return self.delayed_ns is not None and self.delayed_ns &gt; 0\n\n    # TODO: Most of these properties will be removed once we have proper record handling and metrics.\n\n    @property\n    def has_error(self) -&gt; bool:\n        \"\"\"Check if the request record has an error.\"\"\"\n        return self.error is not None\n\n    @property\n    def valid(self) -&gt; bool:\n        \"\"\"Check if the request record is valid by ensuring that the start time\n        and response timestamps are within valid ranges.\n\n        Returns:\n            bool: True if the record is valid, False otherwise.\n        \"\"\"\n        return not self.has_error and (\n            0 &lt;= self.start_perf_ns &lt; sys.maxsize\n            and len(self.responses) &gt; 0\n            and all(0 &lt; response.perf_ns &lt; sys.maxsize for response in self.responses)\n        )\n\n    @property\n    def time_to_first_response_ns(self) -&gt; int | None:\n        \"\"\"Get the time to the first response in nanoseconds.\"\"\"\n        if not self.valid:\n            return None\n        return (\n            self.responses[0].perf_ns - self.start_perf_ns\n            if self.start_perf_ns\n            else None\n        )\n\n    @property\n    def time_to_second_response_ns(self) -&gt; int | None:\n        \"\"\"Get the time to the second response in nanoseconds.\"\"\"\n        if not self.valid or len(self.responses) &lt; 2:\n            return None\n        return (\n            self.responses[1].perf_ns - self.responses[0].perf_ns\n            if self.responses[1].perf_ns and self.responses[0].perf_ns\n            else None\n        )\n\n    @property\n    def time_to_last_response_ns(self) -&gt; int | None:\n        \"\"\"Get the time to the last response in nanoseconds.\"\"\"\n        if not self.valid:\n            return None\n        if self.end_perf_ns is None or self.start_perf_ns is None:\n            return None\n        return self.end_perf_ns - self.start_perf_ns if self.start_perf_ns else None\n\n    @property\n    def inter_token_latency_ns(self) -&gt; float | None:\n        \"\"\"Get the interval between responses in nanoseconds.\"\"\"\n        if not self.valid or len(self.responses) &lt; 2:\n            return None\n\n        if (\n            isinstance(self.responses[-1], SSEMessage)\n            and self.responses[-1].packets[-1].value == \"[DONE]\"\n        ):\n            return (\n                (self.responses[-2].perf_ns - self.responses[0].perf_ns)\n                / (len(self.responses) - 2)\n                if self.responses[-2].perf_ns and self.responses[0].perf_ns\n                else None\n            )\n\n        return (\n            (self.responses[-1].perf_ns - self.responses[0].perf_ns)\n            / (len(self.responses) - 1)\n            if self.responses[-1].perf_ns and self.responses[0].perf_ns\n            else None\n        )\n\n    def token_latency_ns(self, index: int) -&gt; float | None:\n        \"\"\"Get the latency of a token in nanoseconds.\"\"\"\n        if not self.valid or len(self.responses) &lt; 1:\n            return None\n        if index == 0:\n            return (\n                self.responses[0].perf_ns - self.recv_start_perf_ns\n                if self.recv_start_perf_ns\n                else None\n            )\n        return (\n            self.responses[index].perf_ns - self.responses[index - 1].perf_ns\n            if self.responses[index].perf_ns and self.responses[index - 1].perf_ns\n            else None\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.delayed","title":"<code>delayed</code>  <code>property</code>","text":"<p>Check if the request was delayed.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.has_error","title":"<code>has_error</code>  <code>property</code>","text":"<p>Check if the request record has an error.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.inter_token_latency_ns","title":"<code>inter_token_latency_ns</code>  <code>property</code>","text":"<p>Get the interval between responses in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.time_to_first_response_ns","title":"<code>time_to_first_response_ns</code>  <code>property</code>","text":"<p>Get the time to the first response in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.time_to_last_response_ns","title":"<code>time_to_last_response_ns</code>  <code>property</code>","text":"<p>Get the time to the last response in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.time_to_second_response_ns","title":"<code>time_to_second_response_ns</code>  <code>property</code>","text":"<p>Get the time to the second response in nanoseconds.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.valid","title":"<code>valid</code>  <code>property</code>","text":"<p>Check if the request record is valid by ensuring that the start time and response timestamps are within valid ranges.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the record is valid, False otherwise.</p>"},{"location":"api/#aiperf.common.models.record_models.RequestRecord.token_latency_ns","title":"<code>token_latency_ns(index)</code>","text":"<p>Get the latency of a token in nanoseconds.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def token_latency_ns(self, index: int) -&gt; float | None:\n    \"\"\"Get the latency of a token in nanoseconds.\"\"\"\n    if not self.valid or len(self.responses) &lt; 1:\n        return None\n    if index == 0:\n        return (\n            self.responses[0].perf_ns - self.recv_start_perf_ns\n            if self.recv_start_perf_ns\n            else None\n        )\n    return (\n        self.responses[index].perf_ns - self.responses[index - 1].perf_ns\n        if self.responses[index].perf_ns and self.responses[index - 1].perf_ns\n        else None\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.SSEField","title":"<code>SSEField</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base model for a single field in an SSE message.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class SSEField(AIPerfBaseModel):\n    \"\"\"Base model for a single field in an SSE message.\"\"\"\n\n    name: SSEFieldType | str = Field(\n        ...,\n        description=\"The name of the field. e.g. 'data', 'event', 'id', 'retry', 'comment'.\",\n    )\n    value: str | None = Field(\n        default=None,\n        description=\"The value of the field.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.SSEMessage","title":"<code>SSEMessage</code>","text":"<p>               Bases: <code>InferenceServerResponse</code></p> <p>Individual SSE message from an SSE stream. Delimited by </p> <p>.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class SSEMessage(InferenceServerResponse):\n    \"\"\"Individual SSE message from an SSE stream. Delimited by \\n\\n.\"\"\"\n\n    # Note: \"fields\" is a restricted keyword in pydantic\n    packets: list[SSEField] = Field(\n        default_factory=list,\n        description=\"The fields contained in the message.\",\n    )\n\n    def extract_data_content(self) -&gt; str:\n        \"\"\"Extract the data contents from the SSE message as a list of strings. Note that the SSE spec specifies\n        that each data content should be combined and delimited by a single \\n.\n\n        Returns:\n            list[str]: A list of strings containing the data contents of the SSE message.\n        \"\"\"\n        return \"\\n\".join(\n            [\n                packet.value\n                for packet in self.packets\n                if packet.name == SSEFieldType.DATA and packet.value\n            ]\n        )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.SSEMessage.extract_data_content","title":"<code>extract_data_content()</code>","text":"<p>Extract the data contents from the SSE message as a list of strings. Note that the SSE spec specifies         that each data content should be combined and delimited by a single  .</p> <pre><code>    Returns:\n        list[str]: A list of strings containing the data contents of the SSE message.\n</code></pre> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def extract_data_content(self) -&gt; str:\n    \"\"\"Extract the data contents from the SSE message as a list of strings. Note that the SSE spec specifies\n    that each data content should be combined and delimited by a single \\n.\n\n    Returns:\n        list[str]: A list of strings containing the data contents of the SSE message.\n    \"\"\"\n    return \"\\n\".join(\n        [\n            packet.value\n            for packet in self.packets\n            if packet.name == SSEFieldType.DATA and packet.value\n        ]\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.TextResponse","title":"<code>TextResponse</code>","text":"<p>               Bases: <code>InferenceServerResponse</code></p> <p>Raw text response from a inference client including an optional content type.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class TextResponse(InferenceServerResponse):\n    \"\"\"Raw text response from a inference client including an optional content type.\"\"\"\n\n    content_type: str | None = Field(\n        default=None,\n        description=\"The content type of the response. e.g. 'text/plain', 'application/json'.\",\n    )\n    text: str = Field(\n        ...,\n        description=\"The text of the response.\",\n    )\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.TextResponseData","title":"<code>TextResponseData</code>","text":"<p>               Bases: <code>BaseResponseData</code></p> <p>Parsed text response data.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>class TextResponseData(BaseResponseData):\n    \"\"\"Parsed text response data.\"\"\"\n\n    text: str = Field(..., description=\"The parsed text of the response.\")\n\n    def get_text(self) -&gt; str:\n        \"\"\"Get the text of the response.\"\"\"\n        return self.text\n</code></pre>"},{"location":"api/#aiperf.common.models.record_models.TextResponseData.get_text","title":"<code>get_text()</code>","text":"<p>Get the text of the response.</p> Source code in <code>aiperf/common/models/record_models.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Get the text of the response.\"\"\"\n    return self.text\n</code></pre>"},{"location":"api/#aiperfcommonmodelsservice_models","title":"aiperf.common.models.service_models","text":""},{"location":"api/#aiperf.common.models.service_models.ServiceRunInfo","title":"<code>ServiceRunInfo</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Base model for tracking service run information.</p> Source code in <code>aiperf/common/models/service_models.py</code> <pre><code>class ServiceRunInfo(AIPerfBaseModel):\n    \"\"\"Base model for tracking service run information.\"\"\"\n\n    service_type: ServiceTypeT = Field(\n        ...,\n        description=\"The type of service\",\n    )\n    registration_status: ServiceRegistrationStatus = Field(\n        ...,\n        description=\"The registration status of the service\",\n    )\n    service_id: str = Field(\n        ...,\n        description=\"The ID of the service\",\n    )\n    first_seen: int | None = Field(\n        default_factory=time.time_ns,\n        description=\"The first time the service was seen\",\n    )\n    last_seen: int | None = Field(\n        default_factory=time.time_ns,\n        description=\"The last time the service was seen\",\n    )\n    state: LifecycleState = Field(\n        default=LifecycleState.CREATED,\n        description=\"The current state of the service\",\n    )\n</code></pre>"},{"location":"api/#aiperfcommonmodelsworker_models","title":"aiperf.common.models.worker_models","text":""},{"location":"api/#aiperf.common.models.worker_models.WorkerTaskStats","title":"<code>WorkerTaskStats</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Stats for the tasks that have been sent to the worker.</p> Source code in <code>aiperf/common/models/worker_models.py</code> <pre><code>class WorkerTaskStats(AIPerfBaseModel):\n    \"\"\"Stats for the tasks that have been sent to the worker.\"\"\"\n\n    total: int = Field(\n        default=0,\n        description=\"The total number of tasks that have been sent to the worker (not all tasks will be completed)\",\n    )\n    failed: int = Field(\n        default=0,\n        description=\"The number of tasks that returned an error\",\n    )\n    completed: int = Field(\n        default=0,\n        description=\"The number of tasks that were completed successfully\",\n    )\n\n    def task_finished(self, valid: bool) -&gt; None:\n        \"\"\"Increment the task stats based on success or failure.\"\"\"\n        if not valid:\n            self.failed += 1\n        else:\n            self.completed += 1\n\n    @property\n    def in_progress(self) -&gt; int:\n        \"\"\"The number of tasks that are currently in progress.\n\n        This is the total number of tasks sent to the worker minus the number of failed and successfully completed tasks.\n        \"\"\"\n        return self.total - self.completed - self.failed\n</code></pre>"},{"location":"api/#aiperf.common.models.worker_models.WorkerTaskStats.in_progress","title":"<code>in_progress</code>  <code>property</code>","text":"<p>The number of tasks that are currently in progress.</p> <p>This is the total number of tasks sent to the worker minus the number of failed and successfully completed tasks.</p>"},{"location":"api/#aiperf.common.models.worker_models.WorkerTaskStats.task_finished","title":"<code>task_finished(valid)</code>","text":"<p>Increment the task stats based on success or failure.</p> Source code in <code>aiperf/common/models/worker_models.py</code> <pre><code>def task_finished(self, valid: bool) -&gt; None:\n    \"\"\"Increment the task stats based on success or failure.\"\"\"\n    if not valid:\n        self.failed += 1\n    else:\n        self.completed += 1\n</code></pre>"},{"location":"api/#aiperfcommonprotocols","title":"aiperf.common.protocols","text":""},{"location":"api/#aiperf.common.protocols.AIPerfLifecycleProtocol","title":"<code>AIPerfLifecycleProtocol</code>","text":"<p>               Bases: <code>TaskManagerProtocol</code>, <code>Protocol</code></p> <p>Protocol for AIPerf lifecycle methods. see :class:<code>aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin</code> for more details.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass AIPerfLifecycleProtocol(TaskManagerProtocol, Protocol):\n    \"\"\"Protocol for AIPerf lifecycle methods.\n    see :class:`aiperf.common.mixins.aiperf_lifecycle_mixin.AIPerfLifecycleMixin` for more details.\n    \"\"\"\n\n    @property\n    def was_initialized(self) -&gt; bool: ...\n    @property\n    def was_started(self) -&gt; bool: ...\n    @property\n    def was_stopped(self) -&gt; bool: ...\n    @property\n    def is_running(self) -&gt; bool: ...\n\n    initialized_event: asyncio.Event\n    started_event: asyncio.Event\n    stopped_event: asyncio.Event\n\n    @property\n    def state(self) -&gt; LifecycleState: ...\n\n    async def initialize(self) -&gt; None: ...\n    async def start(self) -&gt; None: ...\n    async def initialize_and_start(self) -&gt; None: ...\n    async def stop(self) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.AIPerfUIProtocol","title":"<code>AIPerfUIProtocol</code>","text":"<p>               Bases: <code>AIPerfLifecycleProtocol</code>, <code>Protocol</code></p> <p>Protocol interface definition for AIPerf UI implementations.</p> <p>Basically a UI can be any class that implements the AIPerfLifecycleProtocol. However, in order to provide progress tracking and worker tracking, the simplest way would be to inherit from the :class:<code>aiperf.ui.base_ui.BaseAIPerfUI</code>.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass AIPerfUIProtocol(AIPerfLifecycleProtocol, Protocol):\n    \"\"\"Protocol interface definition for AIPerf UI implementations.\n\n    Basically a UI can be any class that implements the AIPerfLifecycleProtocol. However, in order to provide\n    progress tracking and worker tracking, the simplest way would be to inherit from the :class:`aiperf.ui.base_ui.BaseAIPerfUI`.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol","title":"<code>CommunicationProtocol</code>","text":"<p>               Bases: <code>AIPerfLifecycleProtocol</code>, <code>Protocol</code></p> <p>Protocol for the base communication layer. see :class:<code>aiperf.common.comms.base_comms.BaseCommunication</code> for more details.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass CommunicationProtocol(AIPerfLifecycleProtocol, Protocol):\n    \"\"\"Protocol for the base communication layer.\n    see :class:`aiperf.common.comms.base_comms.BaseCommunication` for more details.\n    \"\"\"\n\n    def get_address(self, address_type: CommAddressType) -&gt; str: ...\n\n    \"\"\"Get the address for the given address type can be an enum value for lookup, or a string for direct use.\"\"\"\n\n    def create_client(\n        self,\n        client_type: CommClientType,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; CommunicationClientProtocol:\n        \"\"\"Create a client for the given client type and address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_pub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PubClientProtocol:\n        \"\"\"Create a PUB client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_sub_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; SubClientProtocol:\n        \"\"\"Create a SUB client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_push_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; PushClientProtocol:\n        \"\"\"Create a PUSH client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_pull_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n    ) -&gt; PullClientProtocol:\n        \"\"\"Create a PULL client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_request_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; RequestClientProtocol:\n        \"\"\"Create a REQUEST client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n\n    def create_reply_client(\n        self,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n    ) -&gt; ReplyClientProtocol:\n        \"\"\"Create a REPLY client for the given address, which will be automatically\n        started and stopped with the CommunicationProtocol instance.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_client","title":"<code>create_client(client_type, address, bind=False, socket_ops=None, max_pull_concurrency=None)</code>","text":"<p>Create a client for the given client type and address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_client(\n    self,\n    client_type: CommClientType,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n) -&gt; CommunicationClientProtocol:\n    \"\"\"Create a client for the given client type and address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_pub_client","title":"<code>create_pub_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a PUB client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_pub_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; PubClientProtocol:\n    \"\"\"Create a PUB client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_pull_client","title":"<code>create_pull_client(address, bind=False, socket_ops=None, max_pull_concurrency=None)</code>","text":"<p>Create a PULL client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_pull_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n) -&gt; PullClientProtocol:\n    \"\"\"Create a PULL client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_push_client","title":"<code>create_push_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a PUSH client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_push_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; PushClientProtocol:\n    \"\"\"Create a PUSH client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_reply_client","title":"<code>create_reply_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a REPLY client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_reply_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; ReplyClientProtocol:\n    \"\"\"Create a REPLY client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_request_client","title":"<code>create_request_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a REQUEST client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_request_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; RequestClientProtocol:\n    \"\"\"Create a REQUEST client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.CommunicationProtocol.create_sub_client","title":"<code>create_sub_client(address, bind=False, socket_ops=None)</code>","text":"<p>Create a SUB client for the given address, which will be automatically started and stopped with the CommunicationProtocol instance.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def create_sub_client(\n    self,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n) -&gt; SubClientProtocol:\n    \"\"\"Create a SUB client for the given address, which will be automatically\n    started and stopped with the CommunicationProtocol instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ConsoleExporterProtocol","title":"<code>ConsoleExporterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for console exporters. Any class implementing this protocol will be provided an ExporterConfig and must provide an <code>export</code> method that takes a rich Console and handles exporting them appropriately.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ConsoleExporterProtocol(Protocol):\n    \"\"\"Protocol for console exporters.\n    Any class implementing this protocol will be provided an ExporterConfig and must provide an\n    `export` method that takes a rich Console and handles exporting them appropriately.\n    \"\"\"\n\n    def __init__(self, exporter_config: \"ExporterConfig\") -&gt; None: ...\n\n    async def export(self, console: \"Console\") -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.DataExporterProtocol","title":"<code>DataExporterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for data exporters. Any class implementing this protocol will be provided an ExporterConfig and must provide an <code>export</code> method that handles exporting the data appropriately.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass DataExporterProtocol(Protocol):\n    \"\"\"\n    Protocol for data exporters.\n    Any class implementing this protocol will be provided an ExporterConfig and must provide an\n    `export` method that handles exporting the data appropriately.\n    \"\"\"\n\n    def __init__(self, exporter_config: \"ExporterConfig\") -&gt; None: ...\n\n    def get_export_info(self) -&gt; \"FileExportInfo\": ...\n\n    async def export(self) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.HooksProtocol","title":"<code>HooksProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for hooks methods provided by the HooksMixin.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass HooksProtocol(Protocol):\n    \"\"\"Protocol for hooks methods provided by the HooksMixin.\"\"\"\n\n    def get_hooks(self, *hook_types: HookType, reversed: bool = False) -&gt; list[Hook]:\n        \"\"\"Get the hooks for the given hook type(s), optionally reversed.\"\"\"\n        ...\n\n    async def run_hooks(\n        self, *hook_types: HookType, reversed: bool = False, **kwargs\n    ) -&gt; None:\n        \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n        If reversed is True, the hooks will be run in reverse order. This is useful for stop/cleanup starting with\n        the children and ending with the parent.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.HooksProtocol.get_hooks","title":"<code>get_hooks(*hook_types, reversed=False)</code>","text":"<p>Get the hooks for the given hook type(s), optionally reversed.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def get_hooks(self, *hook_types: HookType, reversed: bool = False) -&gt; list[Hook]:\n    \"\"\"Get the hooks for the given hook type(s), optionally reversed.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.HooksProtocol.run_hooks","title":"<code>run_hooks(*hook_types, reversed=False, **kwargs)</code>  <code>async</code>","text":"<p>Run the hooks for the given hook type, waiting for each hook to complete before running the next one. If reversed is True, the hooks will be run in reverse order. This is useful for stop/cleanup starting with the children and ending with the parent.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def run_hooks(\n    self, *hook_types: HookType, reversed: bool = False, **kwargs\n) -&gt; None:\n    \"\"\"Run the hooks for the given hook type, waiting for each hook to complete before running the next one.\n    If reversed is True, the hooks will be run in reverse order. This is useful for stop/cleanup starting with\n    the children and ending with the parent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol","title":"<code>InferenceClientProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for an inference server client.</p> <p>This protocol defines the methods that must be implemented by any inference server client implementation that is compatible with the AIPerf framework.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass InferenceClientProtocol(Protocol):\n    \"\"\"Protocol for an inference server client.\n\n    This protocol defines the methods that must be implemented by any inference server client\n    implementation that is compatible with the AIPerf framework.\n    \"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfoT) -&gt; None:\n        \"\"\"Create a new inference server client based on the provided configuration.\"\"\"\n        ...\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the inference server client in an asynchronous context.\"\"\"\n        ...\n\n    async def send_request(\n        self,\n        model_endpoint: ModelEndpointInfoT,\n        payload: RequestInputT,\n    ) -&gt; RequestRecord:\n        \"\"\"Send a request to the inference server.\n\n        This method is used to send a request to the inference server.\n\n        Args:\n            model_endpoint: The endpoint to send the request to.\n            payload: The payload to send to the inference server.\n        Returns:\n            The raw response from the inference server.\n        \"\"\"\n        ...\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the client.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.__init__","title":"<code>__init__(model_endpoint)</code>","text":"<p>Create a new inference server client based on the provided configuration.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def __init__(self, model_endpoint: ModelEndpointInfoT) -&gt; None:\n    \"\"\"Create a new inference server client based on the provided configuration.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the client.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the client.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the inference server client in an asynchronous context.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the inference server client in an asynchronous context.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.InferenceClientProtocol.send_request","title":"<code>send_request(model_endpoint, payload)</code>  <code>async</code>","text":"<p>Send a request to the inference server.</p> <p>This method is used to send a request to the inference server.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>ModelEndpointInfoT</code> <p>The endpoint to send the request to.</p> required <code>payload</code> <code>RequestInputT</code> <p>The payload to send to the inference server.</p> required <p>Returns:     The raw response from the inference server.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def send_request(\n    self,\n    model_endpoint: ModelEndpointInfoT,\n    payload: RequestInputT,\n) -&gt; RequestRecord:\n    \"\"\"Send a request to the inference server.\n\n    This method is used to send a request to the inference server.\n\n    Args:\n        model_endpoint: The endpoint to send the request to.\n        payload: The payload to send to the inference server.\n    Returns:\n        The raw response from the inference server.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.MessageBusClientProtocol","title":"<code>MessageBusClientProtocol</code>","text":"<p>               Bases: <code>PubClientProtocol</code>, <code>SubClientProtocol</code>, <code>Protocol</code></p> <p>A message bus client is a client that can publish and subscribe to messages on the event bus/message bus.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass MessageBusClientProtocol(PubClientProtocol, SubClientProtocol, Protocol):\n    \"\"\"A message bus client is a client that can publish and subscribe to messages\n    on the event bus/message bus.\"\"\"\n\n    comms: CommunicationProtocol\n    sub_client: SubClientProtocol\n    pub_client: PubClientProtocol\n</code></pre>"},{"location":"api/#aiperf.common.protocols.OpenAIObjectParserProtocol","title":"<code>OpenAIObjectParserProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for an OpenAI object parser that parses a raw OpenAI object into a BaseResponseData object.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass OpenAIObjectParserProtocol(Protocol):\n    \"\"\"Protocol for an OpenAI object parser that parses a raw OpenAI object into a BaseResponseData object.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse the raw text of an OpenAI object into a BaseResponseData object.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.OpenAIObjectParserProtocol.parse","title":"<code>parse(obj)</code>","text":"<p>Parse the raw text of an OpenAI object into a BaseResponseData object.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse the raw text of an OpenAI object into a BaseResponseData object.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.RecordProcessorProtocol","title":"<code>RecordProcessorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a record processor that processes the incoming records and returns the results of the post processing.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass RecordProcessorProtocol(Protocol):\n    \"\"\"Protocol for a record processor that processes the incoming records and returns the results of the post processing.\"\"\"\n\n    async def process_record(\n        self, record: ParsedResponseRecord\n    ) -&gt; \"MetricRecordDict\": ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.RequestConverterProtocol","title":"<code>RequestConverterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a request converter that converts a raw request to a formatted request for the inference server.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass RequestConverterProtocol(Protocol):\n    \"\"\"Protocol for a request converter that converts a raw request to a formatted request for the inference server.\"\"\"\n\n    async def format_payload(\n        self, model_endpoint: ModelEndpointInfoT, turn: Turn\n    ) -&gt; RequestOutputT:\n        \"\"\"Format the turn for the inference server.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.RequestConverterProtocol.format_payload","title":"<code>format_payload(model_endpoint, turn)</code>  <code>async</code>","text":"<p>Format the turn for the inference server.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def format_payload(\n    self, model_endpoint: ModelEndpointInfoT, turn: Turn\n) -&gt; RequestOutputT:\n    \"\"\"Format the turn for the inference server.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.RequestRateGeneratorProtocol","title":"<code>RequestRateGeneratorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a request rate generator that generates the next interval for a request rate.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass RequestRateGeneratorProtocol(Protocol):\n    \"\"\"Protocol for a request rate generator that generates the next interval for a request rate.\"\"\"\n\n    def __init__(self, config: \"TimingManagerConfig\") -&gt; None: ...\n\n    def next_interval(self) -&gt; float: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ResponseExtractorProtocol","title":"<code>ResponseExtractorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a response extractor that extracts the response data from a raw inference server response and converts it to a list of ParsedResponse objects.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ResponseExtractorProtocol(Protocol):\n    \"\"\"Protocol for a response extractor that extracts the response data from a raw inference server\n    response and converts it to a list of ParsedResponse objects.\"\"\"\n\n    async def extract_response_data(\n        self, record: RequestRecord\n    ) -&gt; list[ParsedResponse]:\n        \"\"\"Extract the response data from a raw inference server response and convert it to a list of ParsedResponse objects.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ResponseExtractorProtocol.extract_response_data","title":"<code>extract_response_data(record)</code>  <code>async</code>","text":"<p>Extract the response data from a raw inference server response and convert it to a list of ParsedResponse objects.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>async def extract_response_data(\n    self, record: RequestRecord\n) -&gt; list[ParsedResponse]:\n    \"\"\"Extract the response data from a raw inference server response and convert it to a list of ParsedResponse objects.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ResultsProcessorProtocol","title":"<code>ResultsProcessorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a results processor that processes the results of multiple record processors, and provides the ability to summarize the results.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ResultsProcessorProtocol(Protocol):\n    \"\"\"Protocol for a results processor that processes the results of multiple\n    record processors, and provides the ability to summarize the results.\"\"\"\n\n    async def process_result(\n        self, result: dict[MetricTagT, \"MetricValueTypeT\"]\n    ) -&gt; None: ...\n\n    async def summarize(self) -&gt; list[\"MetricResult\"]: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ServiceManagerProtocol","title":"<code>ServiceManagerProtocol</code>","text":"<p>               Bases: <code>AIPerfLifecycleProtocol</code>, <code>Protocol</code></p> <p>Protocol for a service manager that manages the running of services using the specific ServiceRunType. Abstracts away the details of service deployment and management. see :class:<code>aiperf.controller.base_service_manager.BaseServiceManager</code> for more details.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ServiceManagerProtocol(AIPerfLifecycleProtocol, Protocol):\n    \"\"\"Protocol for a service manager that manages the running of services using the specific ServiceRunType.\n    Abstracts away the details of service deployment and management.\n    see :class:`aiperf.controller.base_service_manager.BaseServiceManager` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: \"ServiceConfig\",\n        user_config: \"UserConfig\",\n        log_queue: \"multiprocessing.Queue | None\" = None,\n    ): ...\n\n    required_services: dict[ServiceTypeT, int]\n    service_map: dict[ServiceTypeT, list[ServiceRunInfo]]\n    service_id_map: dict[str, ServiceRunInfo]\n\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None: ...\n\n    async def run_services(self, service_types: dict[ServiceTypeT, int]) -&gt; None: ...\n    async def run_required_services(self) -&gt; None: ...\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]: ...\n    async def kill_all_services(self) -&gt; list[BaseException | None]: ...\n    async def stop_service(\n        self, service_type: ServiceTypeT, service_id: str | None = None\n    ) -&gt; list[BaseException | None]: ...\n    async def stop_services_by_type(\n        self, service_types: list[ServiceTypeT]\n    ) -&gt; list[BaseException | None]: ...\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None: ...\n\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.common.protocols.ServiceProtocol","title":"<code>ServiceProtocol</code>","text":"<p>               Bases: <code>MessageBusClientProtocol</code>, <code>Protocol</code></p> <p>Protocol for a service. Essentially a MessageBusClientProtocol with a service_type and service_id attributes.</p> Source code in <code>aiperf/common/protocols.py</code> <pre><code>@runtime_checkable\nclass ServiceProtocol(MessageBusClientProtocol, Protocol):\n    \"\"\"Protocol for a service. Essentially a MessageBusClientProtocol with a service_type and service_id attributes.\"\"\"\n\n    def __init__(\n        self,\n        user_config: \"UserConfig\",\n        service_config: \"ServiceConfig\",\n        service_id: str | None = None,\n        **kwargs,\n    ) -&gt; None: ...\n\n    service_type: ServiceTypeT\n    service_id: str\n</code></pre>"},{"location":"api/#aiperfcommontokenizer","title":"aiperf.common.tokenizer","text":""},{"location":"api/#aiperf.common.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>This class provides a simplified interface for using Huggingface tokenizers, with default arguments for common operations.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>class Tokenizer:\n    \"\"\"\n    This class provides a simplified interface for using Huggingface\n    tokenizers, with default arguments for common operations.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the tokenizer with default values for call, encode, and decode.\n        \"\"\"\n        self._tokenizer = None\n        self._call_args = {\"add_special_tokens\": False}\n        self._encode_args = {\"add_special_tokens\": False}\n        self._decode_args = {\"skip_special_tokens\": True}\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        name: str,\n        trust_remote_code: bool = False,\n        revision: str = \"main\",\n    ) -&gt; \"Tokenizer\":\n        \"\"\"\n        Factory to load a tokenizer for the given pretrained model name.\n\n        Args:\n            name: The name or path of the pretrained tokenizer model.\n            trust_remote_code: Whether to trust remote code when loading the tokenizer.\n            revision: The specific model version to use.\n        \"\"\"\n        try:\n            tokenizer_cls = cls()\n            tokenizer_cls._tokenizer = AutoTokenizer.from_pretrained(\n                name, trust_remote_code=trust_remote_code, revision=revision\n            )\n        except Exception as e:\n            raise InitializationError(e) from e\n        return tokenizer_cls\n\n    def __call__(self, text, **kwargs) -&gt; \"BatchEncoding\":\n        \"\"\"\n        Call the underlying Huggingface tokenizer with default arguments,\n        which can be overridden by kwargs.\n\n        Args:\n            text: The input text to tokenize.\n\n        Returns:\n            A BatchEncoding object containing the tokenized output.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer(text, **{**self._call_args, **kwargs})\n\n    def encode(self, text, **kwargs) -&gt; list[int]:\n        \"\"\"\n        Encode the input text into a list of token IDs.\n\n        This method calls the underlying Huggingface tokenizer's encode\n        method with default arguments, which can be overridden by kwargs.\n\n        Args:\n            text: The input text to encode.\n\n        Returns:\n            A list of token IDs.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.encode(text, **{**self._encode_args, **kwargs})\n\n    def decode(self, token_ids, **kwargs) -&gt; str:\n        \"\"\"\n        Decode a list of token IDs back into a string.\n\n        This method calls the underlying Huggingface tokenizer's decode\n        method with default arguments, which can be overridden by kwargs.\n\n        Args:\n            token_ids: A list of token IDs to decode.\n\n        Returns:\n            The decoded string.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.decode(token_ids, **{**self._decode_args, **kwargs})\n\n    @property\n    def bos_token_id(self) -&gt; int:\n        \"\"\"\n        Return the beginning-of-sequence (BOS) token ID.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.bos_token_id\n\n    @property\n    def eos_token_id(self) -&gt; int:\n        \"\"\"\n        Return the end-of-sequence (EOS) token ID.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n        return self._tokenizer.eos_token_id\n\n    @property\n    def block_separation_token_id(self) -&gt; int | None:\n        \"\"\"\n        Returns BOS, EOS, or None if none areavailable.\n        \"\"\"\n        if self._tokenizer is None:\n            raise NotInitializedError(\"Tokenizer is not initialized.\")\n\n        if self.bos_token_id is not None:\n            return self.bos_token_id\n        if self.eos_token_id is not None:\n            return self.eos_token_id\n        return None\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the underlying tokenizer.\n\n        Returns:\n            The string representation of the tokenizer.\n        \"\"\"\n        return self._tokenizer.__repr__()\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Return a user-friendly string representation of the underlying tokenizer.\n\n        Returns:\n            The string representation of the tokenizer.\n        \"\"\"\n        return self._tokenizer.__str__()\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.block_separation_token_id","title":"<code>block_separation_token_id</code>  <code>property</code>","text":"<p>Returns BOS, EOS, or None if none areavailable.</p>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>Return the beginning-of-sequence (BOS) token ID.</p>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>Return the end-of-sequence (EOS) token ID.</p>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__call__","title":"<code>__call__(text, **kwargs)</code>","text":"<p>Call the underlying Huggingface tokenizer with default arguments, which can be overridden by kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The input text to tokenize.</p> required <p>Returns:</p> Type Description <code>BatchEncoding</code> <p>A BatchEncoding object containing the tokenized output.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __call__(self, text, **kwargs) -&gt; \"BatchEncoding\":\n    \"\"\"\n    Call the underlying Huggingface tokenizer with default arguments,\n    which can be overridden by kwargs.\n\n    Args:\n        text: The input text to tokenize.\n\n    Returns:\n        A BatchEncoding object containing the tokenized output.\n    \"\"\"\n    if self._tokenizer is None:\n        raise NotInitializedError(\"Tokenizer is not initialized.\")\n    return self._tokenizer(text, **{**self._call_args, **kwargs})\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the tokenizer with default values for call, encode, and decode.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the tokenizer with default values for call, encode, and decode.\n    \"\"\"\n    self._tokenizer = None\n    self._call_args = {\"add_special_tokens\": False}\n    self._encode_args = {\"add_special_tokens\": False}\n    self._decode_args = {\"skip_special_tokens\": True}\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the underlying tokenizer.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the tokenizer.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the underlying tokenizer.\n\n    Returns:\n        The string representation of the tokenizer.\n    \"\"\"\n    return self._tokenizer.__repr__()\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.__str__","title":"<code>__str__()</code>","text":"<p>Return a user-friendly string representation of the underlying tokenizer.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the tokenizer.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Return a user-friendly string representation of the underlying tokenizer.\n\n    Returns:\n        The string representation of the tokenizer.\n    \"\"\"\n    return self._tokenizer.__str__()\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.decode","title":"<code>decode(token_ids, **kwargs)</code>","text":"<p>Decode a list of token IDs back into a string.</p> <p>This method calls the underlying Huggingface tokenizer's decode method with default arguments, which can be overridden by kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <p>A list of token IDs to decode.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The decoded string.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def decode(self, token_ids, **kwargs) -&gt; str:\n    \"\"\"\n    Decode a list of token IDs back into a string.\n\n    This method calls the underlying Huggingface tokenizer's decode\n    method with default arguments, which can be overridden by kwargs.\n\n    Args:\n        token_ids: A list of token IDs to decode.\n\n    Returns:\n        The decoded string.\n    \"\"\"\n    if self._tokenizer is None:\n        raise NotInitializedError(\"Tokenizer is not initialized.\")\n    return self._tokenizer.decode(token_ids, **{**self._decode_args, **kwargs})\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.encode","title":"<code>encode(text, **kwargs)</code>","text":"<p>Encode the input text into a list of token IDs.</p> <p>This method calls the underlying Huggingface tokenizer's encode method with default arguments, which can be overridden by kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The input text to encode.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of token IDs.</p> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>def encode(self, text, **kwargs) -&gt; list[int]:\n    \"\"\"\n    Encode the input text into a list of token IDs.\n\n    This method calls the underlying Huggingface tokenizer's encode\n    method with default arguments, which can be overridden by kwargs.\n\n    Args:\n        text: The input text to encode.\n\n    Returns:\n        A list of token IDs.\n    \"\"\"\n    if self._tokenizer is None:\n        raise NotInitializedError(\"Tokenizer is not initialized.\")\n    return self._tokenizer.encode(text, **{**self._encode_args, **kwargs})\n</code></pre>"},{"location":"api/#aiperf.common.tokenizer.Tokenizer.from_pretrained","title":"<code>from_pretrained(name, trust_remote_code=False, revision='main')</code>  <code>classmethod</code>","text":"<p>Factory to load a tokenizer for the given pretrained model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name or path of the pretrained tokenizer model.</p> required <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading the tokenizer.</p> <code>False</code> <code>revision</code> <code>str</code> <p>The specific model version to use.</p> <code>'main'</code> Source code in <code>aiperf/common/tokenizer.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    name: str,\n    trust_remote_code: bool = False,\n    revision: str = \"main\",\n) -&gt; \"Tokenizer\":\n    \"\"\"\n    Factory to load a tokenizer for the given pretrained model name.\n\n    Args:\n        name: The name or path of the pretrained tokenizer model.\n        trust_remote_code: Whether to trust remote code when loading the tokenizer.\n        revision: The specific model version to use.\n    \"\"\"\n    try:\n        tokenizer_cls = cls()\n        tokenizer_cls._tokenizer = AutoTokenizer.from_pretrained(\n            name, trust_remote_code=trust_remote_code, revision=revision\n        )\n    except Exception as e:\n        raise InitializationError(e) from e\n    return tokenizer_cls\n</code></pre>"},{"location":"api/#aiperfcommontypes","title":"aiperf.common.types","text":"<p>This module defines common used alias types for AIPerf. This both helps prevent circular imports and helps with type hinting.</p>"},{"location":"api/#aiperfcommonutils","title":"aiperf.common.utils","text":""},{"location":"api/#aiperf.common.utils.call_all_functions","title":"<code>call_all_functions(funcs, *args, **kwargs)</code>  <code>async</code>","text":"<p>Call all functions in the list with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>The object to call the functions on.</p> required <code>func_names</code> <p>The names of the functions to call.</p> required <code>*args</code> <p>The arguments to pass to the functions.</p> <code>()</code> <code>**kwargs</code> <p>The keyword arguments to pass to the functions.</p> <code>{}</code> <p>Raises:</p> Type Description <code>AIPerfMultiError</code> <p>If any of the functions raise an exception.</p> Source code in <code>aiperf/common/utils.py</code> <pre><code>async def call_all_functions(funcs: list[Callable], *args, **kwargs) -&gt; None:\n    \"\"\"Call all functions in the list with the given name.\n\n    Args:\n        obj: The object to call the functions on.\n        func_names: The names of the functions to call.\n        *args: The arguments to pass to the functions.\n        **kwargs: The keyword arguments to pass to the functions.\n\n    Raises:\n        AIPerfMultiError: If any of the functions raise an exception.\n    \"\"\"\n\n    exceptions = []\n    for func in funcs:\n        try:\n            if inspect.iscoroutinefunction(func):\n                await func(*args, **kwargs)\n            else:\n                func(*args, **kwargs)\n        except Exception as e:\n            # TODO: error handling, logging\n            traceback.print_exc()\n            exceptions.append(e)\n\n    if len(exceptions) &gt; 0:\n        raise AIPerfMultiError(\"Errors calling functions\", exceptions)\n</code></pre>"},{"location":"api/#aiperf.common.utils.call_all_functions_self","title":"<code>call_all_functions_self(self_, funcs, *args, **kwargs)</code>  <code>async</code>","text":"<p>Call all functions in the list with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>The object to call the functions on.</p> required <code>func_names</code> <p>The names of the functions to call.</p> required <code>*args</code> <p>The arguments to pass to the functions.</p> <code>()</code> <code>**kwargs</code> <p>The keyword arguments to pass to the functions.</p> <code>{}</code> <p>Raises:</p> Type Description <code>AIPerfMultiError</code> <p>If any of the functions raise an exception.</p> Source code in <code>aiperf/common/utils.py</code> <pre><code>async def call_all_functions_self(\n    self_: object, funcs: list[Callable], *args, **kwargs\n) -&gt; None:\n    \"\"\"Call all functions in the list with the given name.\n\n    Args:\n        obj: The object to call the functions on.\n        func_names: The names of the functions to call.\n        *args: The arguments to pass to the functions.\n        **kwargs: The keyword arguments to pass to the functions.\n\n    Raises:\n        AIPerfMultiError: If any of the functions raise an exception.\n    \"\"\"\n\n    exceptions = []\n    for func in funcs:\n        try:\n            if inspect.iscoroutinefunction(func):\n                await func(self_, *args, **kwargs)\n            else:\n                func(self_, *args, **kwargs)\n        except Exception as e:\n            # TODO: error handling, logging\n            traceback.print_exc()\n            exceptions.append(e)\n\n    if len(exceptions) &gt; 0:\n        raise AIPerfMultiError(\"Errors calling functions\", exceptions)\n</code></pre>"},{"location":"api/#aiperf.common.utils.load_json_str","title":"<code>load_json_str(json_str, func=lambda x: x)</code>","text":"<p>Deserializes JSON encoded string into Python object.</p> <p>Parameters:</p> Name Type Description Default <code>- json_str</code> <p>string   JSON encoded string</p> required <code>- func</code> <p>callable   A function that takes deserialized JSON object. This can be used to   run validation checks on the object. Defaults to identity function.</p> required Source code in <code>aiperf/common/utils.py</code> <pre><code>def load_json_str(json_str: str, func: Callable = lambda x: x) -&gt; dict[str, Any]:\n    \"\"\"\n    Deserializes JSON encoded string into Python object.\n\n    Args:\n      - json_str: string\n          JSON encoded string\n      - func: callable\n          A function that takes deserialized JSON object. This can be used to\n          run validation checks on the object. Defaults to identity function.\n    \"\"\"\n    try:\n        # Note: orjson may not parse JSON the same way as Python's standard json library,\n        # notably being stricter on UTF-8 conformance.\n        # Refer to https://github.com/ijl/orjson?tab=readme-ov-file#str for details.\n        return func(orjson.loads(json_str))\n    except orjson.JSONDecodeError:\n        snippet = json_str[:200] + (\"...\" if len(json_str) &gt; 200 else \"\")\n        _logger.error(f\"Failed to parse JSON string: '{snippet}'\")\n        raise\n</code></pre>"},{"location":"api/#aiperf.common.utils.yield_to_event_loop","title":"<code>yield_to_event_loop()</code>  <code>async</code>","text":"<p>Yield to the event loop. This forces the current coroutine to yield and allow other coroutines to run, preventing starvation. Use this when you do not want to delay your coroutine via sleep, but still want to allow other coroutines to run if there is a potential for an infinite loop.</p> Source code in <code>aiperf/common/utils.py</code> <pre><code>async def yield_to_event_loop() -&gt; None:\n    \"\"\"Yield to the event loop. This forces the current coroutine to yield and allow\n    other coroutines to run, preventing starvation. Use this when you do not want to\n    delay your coroutine via sleep, but still want to allow other coroutines to run if\n    there is a potential for an infinite loop.\n    \"\"\"\n    await asyncio.sleep(0)\n</code></pre>"},{"location":"api/#aiperfcontrollerbase_service_manager","title":"aiperf.controller.base_service_manager","text":""},{"location":"api/#aiperf.controller.base_service_manager.BaseServiceManager","title":"<code>BaseServiceManager</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>Base class for service managers. It provides a common interface for managing services.</p> Source code in <code>aiperf/controller/base_service_manager.py</code> <pre><code>@implements_protocol(ServiceManagerProtocol)\nclass BaseServiceManager(AIPerfLifecycleMixin, ABC):\n    \"\"\"\n    Base class for service managers. It provides a common interface for managing services.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        **kwargs,\n    ):\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            **kwargs,\n        )\n        self.required_services = required_services\n        self.service_config = service_config\n        self.user_config = user_config\n        self.kwargs = kwargs\n        # Maps to track service information\n        self.service_map: dict[ServiceTypeT, list[ServiceRunInfo]] = {}\n\n        # Create service ID map for component lookups\n        self.service_id_map: dict[str, ServiceRunInfo] = {}\n\n    @on_start\n    async def _start_service_manager(self) -&gt; None:\n        await self.run_required_services()\n\n    @on_stop\n    async def _stop_service_manager(self) -&gt; None:\n        await self.shutdown_all_services()\n\n    async def run_services(\n        self, service_types: dict[ServiceTypeT, int]\n    ) -&gt; list[BaseException | None]:\n        return await asyncio.gather(\n            *[\n                self.run_service(service_type, num_replicas)\n                for service_type, num_replicas in service_types.items()\n            ],\n            return_exceptions=True,\n        )\n\n    @abstractmethod\n    async def stop_service(\n        self, service_type: ServiceTypeT, service_id: str | None = None\n    ) -&gt; list[BaseException | None]: ...\n\n    # TODO: This stuff needs some major cleanup\n\n    async def stop_services_by_type(\n        self, service_types: list[ServiceTypeT]\n    ) -&gt; list[BaseException | None]:\n        \"\"\"Stop a set of services.\"\"\"\n        results = await asyncio.gather(\n            *[self.stop_service(service_type) for service_type in service_types],\n            return_exceptions=True,\n        )\n        output: list[BaseException | None] = []\n        for result in results:\n            if isinstance(result, list):\n                output.extend(result)\n            else:\n                output.append(result)\n        return output\n\n    async def run_required_services(self) -&gt; None:\n        await self.run_services(self.required_services)\n\n    @abstractmethod\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n        pass\n\n    @abstractmethod\n    async def kill_all_services(self) -&gt; list[BaseException | None]:\n        pass\n\n    @abstractmethod\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None:\n        pass\n</code></pre>"},{"location":"api/#aiperf.controller.base_service_manager.BaseServiceManager.stop_services_by_type","title":"<code>stop_services_by_type(service_types)</code>  <code>async</code>","text":"<p>Stop a set of services.</p> Source code in <code>aiperf/controller/base_service_manager.py</code> <pre><code>async def stop_services_by_type(\n    self, service_types: list[ServiceTypeT]\n) -&gt; list[BaseException | None]:\n    \"\"\"Stop a set of services.\"\"\"\n    results = await asyncio.gather(\n        *[self.stop_service(service_type) for service_type in service_types],\n        return_exceptions=True,\n    )\n    output: list[BaseException | None] = []\n    for result in results:\n        if isinstance(result, list):\n            output.extend(result)\n        else:\n            output.append(result)\n    return output\n</code></pre>"},{"location":"api/#aiperfcontrollerkubernetes_service_manager","title":"aiperf.controller.kubernetes_service_manager","text":""},{"location":"api/#aiperf.controller.kubernetes_service_manager.KubernetesServiceManager","title":"<code>KubernetesServiceManager</code>","text":"<p>               Bases: <code>BaseServiceManager</code></p> <p>Service Manager for starting and stopping services in a Kubernetes cluster.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>@implements_protocol(ServiceManagerProtocol)\n@ServiceManagerFactory.register(ServiceRunType.KUBERNETES)\nclass KubernetesServiceManager(BaseServiceManager):\n    \"\"\"\n    Service Manager for starting and stopping services in a Kubernetes cluster.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        **kwargs,\n    ):\n        super().__init__(required_services, service_config, user_config, **kwargs)\n\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None:\n        \"\"\"Run a service as a Kubernetes pod.\"\"\"\n        self.logger.debug(f\"Running service {service_type} as a Kubernetes pod\")\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.run_service not implemented\"\n        )\n\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Stop all required services as Kubernetes pods.\"\"\"\n        self.logger.debug(\"Stopping all required services as Kubernetes pods\")\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.stop_all_services not implemented\"\n        )\n\n    async def kill_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Kill all required services as Kubernetes pods.\"\"\"\n        self.logger.debug(\"Killing all required services as Kubernetes pods\")\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.kill_all_services not implemented\"\n        )\n\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be registered in Kubernetes.\"\"\"\n        self.logger.debug(\n            \"Waiting for all required services to be registered in Kubernetes\"\n        )\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.wait_for_all_services_registration not implemented\"\n        )\n\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be started in Kubernetes.\"\"\"\n        self.logger.debug(\n            \"Waiting for all required services to be started in Kubernetes\"\n        )\n        # TODO: Implement Kubernetes\n        raise NotImplementedError(\n            \"KubernetesServiceManager.wait_for_all_services_start not implemented\"\n        )\n</code></pre>"},{"location":"api/#aiperf.controller.kubernetes_service_manager.KubernetesServiceManager.kill_all_services","title":"<code>kill_all_services()</code>  <code>async</code>","text":"<p>Kill all required services as Kubernetes pods.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>async def kill_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Kill all required services as Kubernetes pods.\"\"\"\n    self.logger.debug(\"Killing all required services as Kubernetes pods\")\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.kill_all_services not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.kubernetes_service_manager.KubernetesServiceManager.run_service","title":"<code>run_service(service_type, num_replicas=1)</code>  <code>async</code>","text":"<p>Run a service as a Kubernetes pod.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>async def run_service(\n    self, service_type: ServiceTypeT, num_replicas: int = 1\n) -&gt; None:\n    \"\"\"Run a service as a Kubernetes pod.\"\"\"\n    self.logger.debug(f\"Running service {service_type} as a Kubernetes pod\")\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.run_service not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.kubernetes_service_manager.KubernetesServiceManager.shutdown_all_services","title":"<code>shutdown_all_services()</code>  <code>async</code>","text":"<p>Stop all required services as Kubernetes pods.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Stop all required services as Kubernetes pods.\"\"\"\n    self.logger.debug(\"Stopping all required services as Kubernetes pods\")\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.stop_all_services not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.kubernetes_service_manager.KubernetesServiceManager.wait_for_all_services_registration","title":"<code>wait_for_all_services_registration(stop_event, timeout_seconds=DEFAULT_SERVICE_REGISTRATION_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be registered in Kubernetes.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>async def wait_for_all_services_registration(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be registered in Kubernetes.\"\"\"\n    self.logger.debug(\n        \"Waiting for all required services to be registered in Kubernetes\"\n    )\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.wait_for_all_services_registration not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.kubernetes_service_manager.KubernetesServiceManager.wait_for_all_services_start","title":"<code>wait_for_all_services_start(stop_event, timeout_seconds=DEFAULT_SERVICE_START_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be started in Kubernetes.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>async def wait_for_all_services_start(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be started in Kubernetes.\"\"\"\n    self.logger.debug(\n        \"Waiting for all required services to be started in Kubernetes\"\n    )\n    # TODO: Implement Kubernetes\n    raise NotImplementedError(\n        \"KubernetesServiceManager.wait_for_all_services_start not implemented\"\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.kubernetes_service_manager.ServiceKubernetesRunInfo","title":"<code>ServiceKubernetesRunInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a service running in a Kubernetes pod.</p> Source code in <code>aiperf/controller/kubernetes_service_manager.py</code> <pre><code>class ServiceKubernetesRunInfo(BaseModel):\n    \"\"\"Information about a service running in a Kubernetes pod.\"\"\"\n\n    pod_name: str\n    node_name: str\n    namespace: str\n</code></pre>"},{"location":"api/#aiperfcontrollermultiprocess_service_manager","title":"aiperf.controller.multiprocess_service_manager","text":""},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessRunInfo","title":"<code>MultiProcessRunInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a service running as a multiprocessing process.</p> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>class MultiProcessRunInfo(BaseModel):\n    \"\"\"Information about a service running as a multiprocessing process.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    process: Process | SpawnProcess | ForkProcess | None = Field(default=None)\n    service_type: ServiceTypeT = Field(\n        ...,\n        description=\"Type of service running in the process\",\n    )\n    service_id: str = Field(\n        ...,\n        description=\"ID of the service running in the process\",\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessServiceManager","title":"<code>MultiProcessServiceManager</code>","text":"<p>               Bases: <code>BaseServiceManager</code></p> <p>Service Manager for starting and stopping services as multiprocessing processes.</p> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>@implements_protocol(ServiceManagerProtocol)\n@ServiceManagerFactory.register(ServiceRunType.MULTIPROCESSING)\nclass MultiProcessServiceManager(BaseServiceManager):\n    \"\"\"\n    Service Manager for starting and stopping services as multiprocessing processes.\n    \"\"\"\n\n    def __init__(\n        self,\n        required_services: dict[ServiceTypeT, int],\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        log_queue: \"multiprocessing.Queue | None\" = None,\n        **kwargs,\n    ):\n        super().__init__(required_services, service_config, user_config, **kwargs)\n        self.multi_process_info: list[MultiProcessRunInfo] = []\n        self.log_queue = log_queue\n\n    async def run_service(\n        self, service_type: ServiceTypeT, num_replicas: int = 1\n    ) -&gt; None:\n        \"\"\"Run a service with the given number of replicas.\"\"\"\n        service_class = ServiceFactory.get_class_from_type(service_type)\n\n        for _ in range(num_replicas):\n            service_id = f\"{service_type}_{uuid.uuid4().hex[:8]}\"\n            process = Process(\n                target=bootstrap_and_run_service,\n                name=f\"{service_type}_process\",\n                kwargs={\n                    \"service_class\": service_class,\n                    \"service_id\": service_id,\n                    \"service_config\": self.service_config,\n                    \"user_config\": self.user_config,\n                    \"log_queue\": self.log_queue,\n                },\n                daemon=True,\n            )\n\n            process.start()\n\n            self.debug(\n                lambda pid=process.pid,\n                type=service_type: f\"Service {type} started as process (pid: {pid})\"\n            )\n\n            self.multi_process_info.append(\n                MultiProcessRunInfo(\n                    process=process,\n                    service_type=service_type,\n                    service_id=service_id,\n                )\n            )\n\n    async def stop_service(\n        self, service_type: ServiceTypeT, service_id: str | None = None\n    ) -&gt; list[BaseException | None]:\n        self.debug(lambda: f\"Stopping {service_type} process(es) with id: {service_id}\")\n        tasks = []\n        for info in list(self.multi_process_info):\n            if info.service_type == service_type and (\n                service_id is None or info.service_id == service_id\n            ):\n                task = asyncio.create_task(self._wait_for_process(info))\n                task.add_done_callback(\n                    lambda _, info=info: self.multi_process_info.remove(info)\n                )\n                tasks.append(task)\n        return await asyncio.gather(*tasks, return_exceptions=True)\n\n    async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Stop all required services as multiprocessing processes.\"\"\"\n        self.debug(\"Stopping all service processes\")\n\n        # Wait for all to finish in parallel\n        return await asyncio.gather(\n            *[self._wait_for_process(info) for info in self.multi_process_info],\n            return_exceptions=True,\n        )\n\n    async def kill_all_services(self) -&gt; list[BaseException | None]:\n        \"\"\"Kill all required services as multiprocessing processes.\"\"\"\n        self.debug(\"Killing all service processes\")\n\n        # Kill all processes\n        for info in self.multi_process_info:\n            if info.process:\n                info.process.kill()\n\n        # Wait for all to finish in parallel\n        return await asyncio.gather(\n            *[self._wait_for_process(info) for info in self.multi_process_info],\n            return_exceptions=True,\n        )\n\n    async def wait_for_all_services_registration(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be registered.\n\n        Args:\n            stop_event: Event to check if operation should be cancelled\n            timeout_seconds: Maximum time to wait in seconds\n\n        Raises:\n            Exception if any service failed to register, None otherwise\n        \"\"\"\n        self.debug(\"Waiting for all required services to register...\")\n\n        # Get the set of required service types for checking completion\n        required_types = set(self.required_services.keys())\n\n        # TODO: Can this be done better by using asyncio.Event()?\n\n        async def _wait_for_registration():\n            while not stop_event.is_set():\n                # Get all registered service types from the id map\n                registered_types = {\n                    service_info.service_type\n                    for service_info in self.service_id_map.values()\n                    if service_info.registration_status\n                    == ServiceRegistrationStatus.REGISTERED\n                }\n\n                # Check if all required types are registered\n                if required_types.issubset(registered_types):\n                    return\n\n                # Wait a bit before checking again\n                await asyncio.sleep(0.5)\n\n        try:\n            await asyncio.wait_for(_wait_for_registration(), timeout=timeout_seconds)\n        except asyncio.TimeoutError as e:\n            # Log which services didn't register in time\n            registered_types_set = set(\n                service_info.service_type\n                for service_info in self.service_id_map.values()\n                if service_info.registration_status\n                == ServiceRegistrationStatus.REGISTERED\n            )\n\n            for service_type in required_types:\n                if service_type not in registered_types_set:\n                    self.error(\n                        f\"Service {service_type} failed to register within timeout\"\n                    )\n\n            raise AIPerfError(\"Some services failed to register within timeout\") from e\n\n    async def _wait_for_process(self, info: MultiProcessRunInfo) -&gt; None:\n        \"\"\"Wait for a process to terminate with timeout handling.\"\"\"\n        if not info.process or not info.process.is_alive():\n            return\n\n        try:\n            info.process.terminate()\n            await asyncio.to_thread(\n                info.process.join, timeout=TASK_CANCEL_TIMEOUT_SHORT\n            )\n            self.debug(\n                f\"Service {info.service_type} process stopped (pid: {info.process.pid})\"\n            )\n        except asyncio.TimeoutError:\n            self.warning(\n                f\"Service {info.service_type} process (pid: {info.process.pid}) did not terminate gracefully, killing\"\n            )\n            info.process.kill()\n\n    async def wait_for_all_services_start(\n        self,\n        stop_event: asyncio.Event,\n        timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n    ) -&gt; None:\n        \"\"\"Wait for all required services to be started.\"\"\"\n        self.debug(\"Waiting for all required services to start...\")\n        self.warning(\n            \"Waiting for all required services to start is not implemented for multiprocessing\"\n        )\n</code></pre>"},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessServiceManager.kill_all_services","title":"<code>kill_all_services()</code>  <code>async</code>","text":"<p>Kill all required services as multiprocessing processes.</p> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>async def kill_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Kill all required services as multiprocessing processes.\"\"\"\n    self.debug(\"Killing all service processes\")\n\n    # Kill all processes\n    for info in self.multi_process_info:\n        if info.process:\n            info.process.kill()\n\n    # Wait for all to finish in parallel\n    return await asyncio.gather(\n        *[self._wait_for_process(info) for info in self.multi_process_info],\n        return_exceptions=True,\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessServiceManager.run_service","title":"<code>run_service(service_type, num_replicas=1)</code>  <code>async</code>","text":"<p>Run a service with the given number of replicas.</p> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>async def run_service(\n    self, service_type: ServiceTypeT, num_replicas: int = 1\n) -&gt; None:\n    \"\"\"Run a service with the given number of replicas.\"\"\"\n    service_class = ServiceFactory.get_class_from_type(service_type)\n\n    for _ in range(num_replicas):\n        service_id = f\"{service_type}_{uuid.uuid4().hex[:8]}\"\n        process = Process(\n            target=bootstrap_and_run_service,\n            name=f\"{service_type}_process\",\n            kwargs={\n                \"service_class\": service_class,\n                \"service_id\": service_id,\n                \"service_config\": self.service_config,\n                \"user_config\": self.user_config,\n                \"log_queue\": self.log_queue,\n            },\n            daemon=True,\n        )\n\n        process.start()\n\n        self.debug(\n            lambda pid=process.pid,\n            type=service_type: f\"Service {type} started as process (pid: {pid})\"\n        )\n\n        self.multi_process_info.append(\n            MultiProcessRunInfo(\n                process=process,\n                service_type=service_type,\n                service_id=service_id,\n            )\n        )\n</code></pre>"},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessServiceManager.shutdown_all_services","title":"<code>shutdown_all_services()</code>  <code>async</code>","text":"<p>Stop all required services as multiprocessing processes.</p> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>async def shutdown_all_services(self) -&gt; list[BaseException | None]:\n    \"\"\"Stop all required services as multiprocessing processes.\"\"\"\n    self.debug(\"Stopping all service processes\")\n\n    # Wait for all to finish in parallel\n    return await asyncio.gather(\n        *[self._wait_for_process(info) for info in self.multi_process_info],\n        return_exceptions=True,\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessServiceManager.wait_for_all_services_registration","title":"<code>wait_for_all_services_registration(stop_event, timeout_seconds=DEFAULT_SERVICE_REGISTRATION_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be registered.</p> <p>Parameters:</p> Name Type Description Default <code>stop_event</code> <code>Event</code> <p>Event to check if operation should be cancelled</p> required <code>timeout_seconds</code> <code>float</code> <p>Maximum time to wait in seconds</p> <code>DEFAULT_SERVICE_REGISTRATION_TIMEOUT</code> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>async def wait_for_all_services_registration(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_REGISTRATION_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be registered.\n\n    Args:\n        stop_event: Event to check if operation should be cancelled\n        timeout_seconds: Maximum time to wait in seconds\n\n    Raises:\n        Exception if any service failed to register, None otherwise\n    \"\"\"\n    self.debug(\"Waiting for all required services to register...\")\n\n    # Get the set of required service types for checking completion\n    required_types = set(self.required_services.keys())\n\n    # TODO: Can this be done better by using asyncio.Event()?\n\n    async def _wait_for_registration():\n        while not stop_event.is_set():\n            # Get all registered service types from the id map\n            registered_types = {\n                service_info.service_type\n                for service_info in self.service_id_map.values()\n                if service_info.registration_status\n                == ServiceRegistrationStatus.REGISTERED\n            }\n\n            # Check if all required types are registered\n            if required_types.issubset(registered_types):\n                return\n\n            # Wait a bit before checking again\n            await asyncio.sleep(0.5)\n\n    try:\n        await asyncio.wait_for(_wait_for_registration(), timeout=timeout_seconds)\n    except asyncio.TimeoutError as e:\n        # Log which services didn't register in time\n        registered_types_set = set(\n            service_info.service_type\n            for service_info in self.service_id_map.values()\n            if service_info.registration_status\n            == ServiceRegistrationStatus.REGISTERED\n        )\n\n        for service_type in required_types:\n            if service_type not in registered_types_set:\n                self.error(\n                    f\"Service {service_type} failed to register within timeout\"\n                )\n\n        raise AIPerfError(\"Some services failed to register within timeout\") from e\n</code></pre>"},{"location":"api/#aiperf.controller.multiprocess_service_manager.MultiProcessServiceManager.wait_for_all_services_start","title":"<code>wait_for_all_services_start(stop_event, timeout_seconds=DEFAULT_SERVICE_START_TIMEOUT)</code>  <code>async</code>","text":"<p>Wait for all required services to be started.</p> Source code in <code>aiperf/controller/multiprocess_service_manager.py</code> <pre><code>async def wait_for_all_services_start(\n    self,\n    stop_event: asyncio.Event,\n    timeout_seconds: float = DEFAULT_SERVICE_START_TIMEOUT,\n) -&gt; None:\n    \"\"\"Wait for all required services to be started.\"\"\"\n    self.debug(\"Waiting for all required services to start...\")\n    self.warning(\n        \"Waiting for all required services to start is not implemented for multiprocessing\"\n    )\n</code></pre>"},{"location":"api/#aiperfcontrollerproxy_manager","title":"aiperf.controller.proxy_manager","text":""},{"location":"api/#aiperfcontrollersystem_controller","title":"aiperf.controller.system_controller","text":""},{"location":"api/#aiperf.controller.system_controller.SystemController","title":"<code>SystemController</code>","text":"<p>               Bases: <code>SignalHandlerMixin</code>, <code>BaseService</code></p> <p>System Controller service.</p> <p>This service is responsible for managing the lifecycle of all other services. It will start, stop, and configure all other services.</p> Source code in <code>aiperf/controller/system_controller.py</code> <pre><code>@ServiceFactory.register(ServiceType.SYSTEM_CONTROLLER)\nclass SystemController(SignalHandlerMixin, BaseService):\n    \"\"\"System Controller service.\n\n    This service is responsible for managing the lifecycle of all other services.\n    It will start, stop, and configure all other services.\n    \"\"\"\n\n    def __init__(\n        self,\n        user_config: UserConfig,\n        service_config: ServiceConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n        )\n        self.debug(\"Creating System Controller\")\n        self._was_cancelled = False\n        # List of required service types, in no particular order\n        # These are services that must be running before the system controller can start profiling\n        self.required_services: dict[ServiceTypeT, int] = {\n            ServiceType.DATASET_MANAGER: 1,\n            ServiceType.TIMING_MANAGER: 1,\n            ServiceType.WORKER_MANAGER: 1,\n            ServiceType.RECORDS_MANAGER: 1,\n        }\n        if self.service_config.record_processor_service_count is not None:\n            self.required_services[ServiceType.RECORD_PROCESSOR] = (\n                self.service_config.record_processor_service_count\n            )\n            self.scale_record_processors_with_workers = False\n        else:\n            self.scale_record_processors_with_workers = True\n\n        self.proxy_manager: ProxyManager = ProxyManager(\n            service_config=self.service_config\n        )\n        self.service_manager: ServiceManagerProtocol = (\n            ServiceManagerFactory.create_instance(\n                self.service_config.service_run_type.value,\n                required_services=self.required_services,\n                user_config=self.user_config,\n                service_config=self.service_config,\n                log_queue=get_global_log_queue(),\n            )\n        )\n        self.ui: AIPerfUIProtocol = AIPerfUIFactory.create_instance(\n            self.service_config.ui_type,\n            service_config=self.service_config,\n            user_config=self.user_config,\n            log_queue=get_global_log_queue(),\n            controller=self,\n        )\n        self.attach_child_lifecycle(self.ui)\n        self._stop_tasks: set[asyncio.Task] = set()\n        self._profile_results: ProcessRecordsResult | None = None\n        self.debug(\"System Controller created\")\n\n    async def request_realtime_metrics(self) -&gt; None:\n        \"\"\"Request real-time metrics from the RecordsManager.\"\"\"\n        await self.send_command_and_wait_for_response(\n            RealtimeMetricsCommand(\n                service_id=self.service_id,\n                target_service_type=ServiceType.RECORDS_MANAGER,\n            )\n        )\n\n    async def initialize(self) -&gt; None:\n        \"\"\"We need to override the initialize method to run the proxy manager before the base service initialize.\n        This is because the proxies need to be running before we can subscribe to the message bus.\n        \"\"\"\n        self.debug(\"Running ZMQ Proxy Manager Before Initialize\")\n        await self.proxy_manager.initialize_and_start()\n        # Once the proxies are running, call the original initialize method\n        await super().initialize()\n\n    @on_init\n    async def _initialize_system_controller(self) -&gt; None:\n        self.debug(\"Initializing System Controller\")\n\n        self.setup_signal_handlers(self._handle_signal)\n        self.debug(\"Setup signal handlers\")\n        await self.service_manager.initialize()\n\n    @on_start\n    async def _start_services(self) -&gt; None:\n        \"\"\"Bootstrap the system services.\n\n        This method will:\n        - Initialize all required services\n        - Wait for all required services to be registered\n        - Start all required services\n        \"\"\"\n        self.debug(\"System Controller is bootstrapping services\")\n        # Start all required services\n        await self.service_manager.start()\n        await self.service_manager.wait_for_all_services_registration(\n            stop_event=self._stop_requested_event,\n        )\n\n        self.info(\"AIPerf System is CONFIGURING\")\n        await self._profile_configure_all_services()\n        self.info(\"AIPerf System is CONFIGURED\")\n        await self._start_profiling_all_services()\n        self.info(\"AIPerf System is PROFILING\")\n\n    async def _profile_configure_all_services(self) -&gt; None:\n        \"\"\"Configure all services to start profiling.\n\n        This is a blocking call that will wait for all services to be configured before returning. This way\n        we can ensure that all services are configured before we start profiling.\n        \"\"\"\n        self.info(\"Configuring all services to start profiling\")\n        begin = time.perf_counter()\n        await self.send_command_and_wait_for_all_responses(\n            ProfileConfigureCommand(\n                service_id=self.service_id,\n                config=self.user_config,\n            ),\n            list(self.service_manager.service_id_map.keys()),\n            timeout=DEFAULT_PROFILE_CONFIGURE_TIMEOUT,\n        )\n        duration = time.perf_counter() - begin\n        self.info(f\"All services configured in {duration:.2f} seconds\")\n\n    async def _start_profiling_all_services(self) -&gt; None:\n        \"\"\"Tell all services to start profiling.\"\"\"\n        self.debug(\"Sending PROFILE_START command to all services\")\n        await self.send_command_and_wait_for_all_responses(\n            ProfileStartCommand(\n                service_id=self.service_id,\n            ),\n            list(self.service_manager.service_id_map.keys()),\n            timeout=DEFAULT_PROFILE_START_TIMEOUT,\n        )\n        self.info(\"All services started profiling successfully\")\n\n    @on_command(CommandType.REGISTER_SERVICE)\n    async def _handle_register_service_command(\n        self, message: RegisterServiceCommand\n    ) -&gt; None:\n        \"\"\"Process a registration message from a service. It will\n        add the service to the service manager and send a configure command\n        to the service.\n\n        Args:\n            message: The registration message to process\n        \"\"\"\n\n        self.debug(\n            lambda: f\"Processing registration from {message.service_type} with ID: {message.service_id}\"\n        )\n\n        service_info = ServiceRunInfo(\n            registration_status=ServiceRegistrationStatus.REGISTERED,\n            service_type=message.service_type,\n            service_id=message.service_id,\n            first_seen=time.time_ns(),\n            state=message.state,\n            last_seen=time.time_ns(),\n        )\n\n        self.service_manager.service_id_map[message.service_id] = service_info\n        if message.service_type not in self.service_manager.service_map:\n            self.service_manager.service_map[message.service_type] = []\n        self.service_manager.service_map[message.service_type].append(service_info)\n\n        try:\n            type_name = ServiceType(message.service_type).name.title().replace(\"_\", \" \")\n        except (TypeError, ValueError):\n            type_name = message.service_type\n        self.info(lambda: f\"Registered {type_name} (id: '{message.service_id}')\")\n\n    @on_message(MessageType.HEARTBEAT)\n    async def _process_heartbeat_message(self, message: HeartbeatMessage) -&gt; None:\n        \"\"\"Process a heartbeat message from a service. It will\n        update the last seen timestamp and state of the service.\n\n        Args:\n            message: The heartbeat message to process\n        \"\"\"\n        service_id = message.service_id\n        service_type = message.service_type\n        timestamp = message.request_ns\n\n        self.debug(lambda: f\"Received heartbeat from {service_type} (ID: {service_id})\")\n\n        # Update the last heartbeat timestamp if the component exists\n        try:\n            service_info = self.service_manager.service_id_map[service_id]\n            service_info.last_seen = timestamp\n            service_info.state = message.state\n            self.debug(f\"Updated heartbeat for {service_id} to {timestamp}\")\n        except Exception:\n            self.warning(\n                f\"Received heartbeat from unknown service: {service_id} ({service_type})\"\n            )\n\n    @on_message(MessageType.CREDITS_COMPLETE)\n    async def _process_credits_complete_message(\n        self, message: CreditsCompleteMessage\n    ) -&gt; None:\n        \"\"\"Process a credits complete message from a service. It will\n        update the state of the service with the service manager.\n\n        Args:\n            message: The credits complete message to process\n        \"\"\"\n        service_id = message.service_id\n        self.info(f\"Received credits complete from {service_id}\")\n\n    @on_message(MessageType.STATUS)\n    async def _process_status_message(self, message: StatusMessage) -&gt; None:\n        \"\"\"Process a status message from a service. It will\n        update the state of the service with the service manager.\n\n        Args:\n            message: The status message to process\n        \"\"\"\n        service_id = message.service_id\n        service_type = message.service_type\n        state = message.state\n\n        self.debug(\n            lambda: f\"Received status update from {service_type} (ID: {service_id}): {state}\"\n        )\n\n        # Update the component state if the component exists\n        if service_id not in self.service_manager.service_id_map:\n            self.debug(\n                lambda: f\"Received status update from un-registered service: {service_id} ({service_type})\"\n            )\n            return\n\n        service_info = self.service_manager.service_id_map.get(service_id)\n        if service_info is None:\n            return\n\n        service_info.state = message.state\n\n        self.debug(f\"Updated state for {service_id} to {message.state}\")\n\n    @on_message(MessageType.COMMAND_RESPONSE)\n    async def _process_command_response_message(self, message: CommandResponse) -&gt; None:\n        \"\"\"Process a command response message.\"\"\"\n        self.debug(lambda: f\"Received command response message: {message}\")\n        if message.status == CommandResponseStatus.SUCCESS:\n            self.debug(f\"Command {message.command} succeeded from {message.service_id}\")\n        elif message.status == CommandResponseStatus.ACKNOWLEDGED:\n            self.debug(\n                f\"Command {message.command} acknowledged from {message.service_id}\"\n            )\n        elif message.status == CommandResponseStatus.UNHANDLED:\n            self.debug(f\"Command {message.command} unhandled from {message.service_id}\")\n        elif message.status == CommandResponseStatus.FAILURE:\n            message = cast(CommandErrorResponse, message)\n            self.error(\n                f\"Command {message.command} failed from {message.service_id}: {message.error}\"\n            )\n\n    @on_command(CommandType.SPAWN_WORKERS)\n    async def _handle_spawn_workers_command(self, message: SpawnWorkersCommand) -&gt; None:\n        \"\"\"Handle a spawn workers command.\"\"\"\n        self.debug(lambda: f\"Received spawn workers command: {message}\")\n        # Spawn the workers\n        await self.service_manager.run_service(ServiceType.WORKER, message.num_workers)\n        # If we are scaling the record processor service count with the number of workers, spawn the record processors\n        if self.scale_record_processors_with_workers:\n            await self.service_manager.run_service(\n                ServiceType.RECORD_PROCESSOR, max(1, message.num_workers // 2)\n            )\n\n    @on_command(CommandType.SHUTDOWN_WORKERS)\n    async def _handle_shutdown_workers_command(\n        self, message: ShutdownWorkersCommand\n    ) -&gt; None:\n        \"\"\"Handle a shutdown workers command.\"\"\"\n        self.debug(lambda: f\"Received shutdown workers command: {message}\")\n        # TODO: Handle individual worker shutdowns via worker id\n        await self.service_manager.stop_service(ServiceType.WORKER)\n        if self.scale_record_processors_with_workers:\n            await self.service_manager.stop_service(ServiceType.RECORD_PROCESSOR)\n\n    @on_message(MessageType.PROCESS_RECORDS_RESULT)\n    async def _on_process_records_result_message(\n        self, message: ProcessRecordsResultMessage\n    ) -&gt; None:\n        \"\"\"Handle a profile results message.\"\"\"\n        self.debug(lambda: f\"Received profile results message: {message}\")\n        if message.results.errors:\n            self.error(\n                f\"Received process records result message with errors: {message.results.errors}\"\n            )\n\n        # This data will also be displayed by the console error exporter\n        self.debug(lambda: f\"Error summary: {message.results.results.error_summary}\")\n\n        self._profile_results = message.results\n\n        if message.results.results:\n            await ExporterManager(\n                results=message.results.results,\n                input_config=self.user_config,\n                service_config=self.service_config,\n            ).export_data()\n        else:\n            self.error(\n                f\"Received process records result message with no records: {message.results.results}\"\n            )\n\n        # TODO: HACK: Stop the system controller after exporting the records\n        self.debug(\"Stopping system controller after exporting records\")\n        await asyncio.shield(self.stop())\n\n    async def _handle_signal(self, sig: int) -&gt; None:\n        \"\"\"Handle received signals by triggering graceful shutdown.\n\n        Args:\n            sig: The signal number received\n        \"\"\"\n        if self.stop_requested:\n            # If we are already in a stopping state, we need to kill the process to be safe.\n            self.warning(f\"Received signal {sig}, killing\")\n            await self._kill()\n            return\n\n        self.debug(lambda: f\"Received signal {sig}, initiating graceful shutdown\")\n        await self._cancel_profiling()\n\n    async def _cancel_profiling(self) -&gt; None:\n        self.debug(\"Cancelling profiling of all services\")\n        self._was_cancelled = True\n        await self.publish(ProfileCancelCommand(service_id=self.service_id))\n\n        # TODO: HACK: Wait for 2 seconds to ensure the profiling is cancelled\n        # Wait for the profiling to be cancelled\n        await asyncio.sleep(2)\n        self.debug(\"Stopping system controller after profiling cancelled\")\n        await asyncio.shield(self.stop())\n\n    @on_stop\n    async def _stop_system_controller(self) -&gt; None:\n        \"\"\"Stop the system controller and all running services.\"\"\"\n        # Broadcast a shutdown command to all services\n        await self.publish(ShutdownCommand(service_id=self.service_id))\n\n        # TODO: HACK: Wait for 0.5 seconds to ensure the shutdown command is received\n        await asyncio.sleep(0.5)\n\n        await self.service_manager.shutdown_all_services()\n        await self.comms.stop()\n        await self.proxy_manager.stop()\n\n        # Wait for the UI to stop before exporting any results to the console\n        await self.ui.stop()\n        await self.ui.wait_for_tasks()\n\n        await self._print_post_benchmark_info_and_metrics()\n\n        if AIPERF_DEV_MODE:\n            # Print a warning message to the console if developer mode is enabled, on exit after results\n            print_developer_mode_warning()\n\n        # Exit the process in a more explicit way, to ensure that it stops\n        os._exit(0)\n\n    async def _print_post_benchmark_info_and_metrics(self) -&gt; None:\n        \"\"\"Print post benchmark info and metrics to the console.\"\"\"\n        if not self._profile_results or not self._profile_results.results.records:\n            self.warning(\"No profile results to export\")\n            return\n\n        console = Console()\n        if console.width &lt; 100:\n            console.width = 100\n\n        exporter_manager = ExporterManager(\n            results=self._profile_results.results,\n            input_config=self.user_config,\n            service_config=self.service_config,\n        )\n        await exporter_manager.export_console(console=console)\n\n        console.print()\n        self._print_cli_command(console)\n        self._print_benchmark_duration(console)\n        self._print_exported_file_infos(exporter_manager, console)\n        if self._was_cancelled:\n            console.print(\n                \"[italic yellow]The profile run was cancelled early. Results shown may be incomplete or inaccurate.[/italic yellow]\"\n            )\n\n        console.print()\n        console.file.flush()\n\n    def _print_exported_file_infos(\n        self, exporter_manager: ExporterManager, console: Console\n    ) -&gt; None:\n        \"\"\"Print the exported file infos.\"\"\"\n        file_infos = exporter_manager.get_exported_file_infos()\n\n        for file_info in file_infos:\n            console.print(\n                f\"[bold green]{file_info.export_type}[/bold green]: [cyan]{file_info.file_path.resolve()}[/cyan]\"\n            )\n\n    def _print_cli_command(self, console: Console) -&gt; None:\n        \"\"\"Print the CLI command that was used to run the benchmark.\"\"\"\n        console.print(\n            f\"[bold green]CLI Command:[/bold green] [italic]{self.user_config.cli_command}[/italic]\"\n        )\n\n    def _print_benchmark_duration(self, console: Console) -&gt; None:\n        \"\"\"Print the duration of the benchmark.\"\"\"\n        from aiperf.metrics.types.benchmark_duration_metric import (\n            BenchmarkDurationMetric,\n        )\n\n        duration = self._profile_results.get(BenchmarkDurationMetric.tag)\n        if duration:\n            duration = duration.to_display_unit()\n            duration_str = f\"[bold green]{BenchmarkDurationMetric.header}[/bold green]: {duration.avg:.2f} {duration.unit}\"\n            if self._was_cancelled:\n                duration_str += \" [italic yellow](cancelled early)[/italic yellow]\"\n            console.print(duration_str)\n\n    async def _kill(self):\n        \"\"\"Kill the system controller.\"\"\"\n        try:\n            await self.service_manager.kill_all_services()\n        except Exception as e:\n            raise self._service_error(\"Failed to stop all services\") from e\n\n        await super()._kill()\n</code></pre>"},{"location":"api/#aiperf.controller.system_controller.SystemController.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>We need to override the initialize method to run the proxy manager before the base service initialize. This is because the proxies need to be running before we can subscribe to the message bus.</p> Source code in <code>aiperf/controller/system_controller.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"We need to override the initialize method to run the proxy manager before the base service initialize.\n    This is because the proxies need to be running before we can subscribe to the message bus.\n    \"\"\"\n    self.debug(\"Running ZMQ Proxy Manager Before Initialize\")\n    await self.proxy_manager.initialize_and_start()\n    # Once the proxies are running, call the original initialize method\n    await super().initialize()\n</code></pre>"},{"location":"api/#aiperf.controller.system_controller.SystemController.request_realtime_metrics","title":"<code>request_realtime_metrics()</code>  <code>async</code>","text":"<p>Request real-time metrics from the RecordsManager.</p> Source code in <code>aiperf/controller/system_controller.py</code> <pre><code>async def request_realtime_metrics(self) -&gt; None:\n    \"\"\"Request real-time metrics from the RecordsManager.\"\"\"\n    await self.send_command_and_wait_for_response(\n        RealtimeMetricsCommand(\n            service_id=self.service_id,\n            target_service_type=ServiceType.RECORDS_MANAGER,\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.controller.system_controller.main","title":"<code>main()</code>","text":"<p>Main entry point for the system controller.</p> Source code in <code>aiperf/controller/system_controller.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the system controller.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(SystemController)\n</code></pre>"},{"location":"api/#aiperfcontrollersystem_mixins","title":"aiperf.controller.system_mixins","text":""},{"location":"api/#aiperf.controller.system_mixins.SignalHandlerMixin","title":"<code>SignalHandlerMixin</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Mixin for services that need to handle system signals.</p> Source code in <code>aiperf/controller/system_mixins.py</code> <pre><code>class SignalHandlerMixin(AIPerfLoggerMixin):\n    \"\"\"Mixin for services that need to handle system signals.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        # Set to store signal handler tasks to prevent them from being garbage collected\n        self._signal_tasks = set()\n        super().__init__(**kwargs)\n\n    def setup_signal_handlers(self, callback: Callable[[int], Coroutine]) -&gt; None:\n        \"\"\"This method will set up signal handlers for the SIGTERM and SIGINT signals\n        in order to trigger a graceful shutdown of the service.\n\n        Args:\n            callback: The callback to call when a signal is received\n        \"\"\"\n        loop = asyncio.get_running_loop()\n\n        def signal_handler(sig: int) -&gt; None:\n            task = asyncio.create_task(callback(sig))\n            self._signal_tasks.add(task)\n            task.add_done_callback(self._signal_tasks.discard)\n\n        loop.add_signal_handler(signal.SIGINT, signal_handler, signal.SIGINT)\n</code></pre>"},{"location":"api/#aiperf.controller.system_mixins.SignalHandlerMixin.setup_signal_handlers","title":"<code>setup_signal_handlers(callback)</code>","text":"<p>This method will set up signal handlers for the SIGTERM and SIGINT signals in order to trigger a graceful shutdown of the service.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[int], Coroutine]</code> <p>The callback to call when a signal is received</p> required Source code in <code>aiperf/controller/system_mixins.py</code> <pre><code>def setup_signal_handlers(self, callback: Callable[[int], Coroutine]) -&gt; None:\n    \"\"\"This method will set up signal handlers for the SIGTERM and SIGINT signals\n    in order to trigger a graceful shutdown of the service.\n\n    Args:\n        callback: The callback to call when a signal is received\n    \"\"\"\n    loop = asyncio.get_running_loop()\n\n    def signal_handler(sig: int) -&gt; None:\n        task = asyncio.create_task(callback(sig))\n        self._signal_tasks.add(task)\n        task.add_done_callback(self._signal_tasks.discard)\n\n    loop.add_signal_handler(signal.SIGINT, signal_handler, signal.SIGINT)\n</code></pre>"},{"location":"api/#aiperfdatasetcomposerbase","title":"aiperf.dataset.composer.base","text":""},{"location":"api/#aiperf.dataset.composer.base.BaseDatasetComposer","title":"<code>BaseDatasetComposer</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> Source code in <code>aiperf/dataset/composer/base.py</code> <pre><code>class BaseDatasetComposer(AIPerfLoggerMixin, ABC):\n    def __init__(self, config: UserConfig, tokenizer: Tokenizer, **kwargs):\n        self.config = config\n        super().__init__(config=config, tokenizer=tokenizer, **kwargs)\n        self.prompt_generator = PromptGenerator(config.input.prompt, tokenizer)\n        self.image_generator = ImageGenerator(config.input.image)\n        self.audio_generator = AudioGenerator(config.input.audio)\n        self.turn_count = 0\n\n    @abstractmethod\n    def create_dataset(self) -&gt; list[Conversation]:\n        \"\"\"\n        Create a set of conversation objects from the given configuration.\n\n        Returns:\n            list[Conversation]: A list of conversation objects.\n        \"\"\"\n        ...\n\n    def _select_model_name(self) -&gt; str:\n        if (\n            self.config.endpoint.model_selection_strategy\n            == ModelSelectionStrategy.RANDOM\n        ):\n            return random.choice(self.config.endpoint.model_names)\n        elif (\n            self.config.endpoint.model_selection_strategy\n            == ModelSelectionStrategy.ROUND_ROBIN\n        ):\n            model_name = self.config.endpoint.model_names[\n                self.turn_count % len(self.config.endpoint.model_names)\n            ]\n            self.turn_count += 1\n            return model_name\n        else:\n            raise ValueError(\n                f\"Invalid model selection strategy: {self.config.endpoint.model_selection_strategy}.\"\n            )\n\n    def _set_max_tokens(self, turn: Turn) -&gt; None:\n        \"\"\"Set max_tokens for the turn based on the output configuration.\n\n        Args:\n            turn: The turn object to finalize.\n        \"\"\"\n        output_tokens_config = self.config.input.prompt.output_tokens\n        if output_tokens_config.mean is not None:\n            stddev = output_tokens_config.stddev\n            turn.max_tokens = utils.sample_positive_normal_integer(\n                output_tokens_config.mean, stddev\n            )\n\n    def _finalize_turn(self, turn: Turn) -&gt; None:\n        \"\"\"Finalize a turn by populating all required metadata fields.\n\n        This method handles:\n        - Model name selection\n        - Max tokens sampling based on output configuration\n        - Any other turn-level metadata that needs to be set\n\n        Args:\n            turn: The turn object to finalize.\n        \"\"\"\n        turn.model = self._select_model_name()\n        self._set_max_tokens(turn)\n\n    @property\n    def prefix_prompt_enabled(self) -&gt; bool:\n        return self.config.input.prompt.prefix_prompt.length &gt; 0\n</code></pre>"},{"location":"api/#aiperf.dataset.composer.base.BaseDatasetComposer.create_dataset","title":"<code>create_dataset()</code>  <code>abstractmethod</code>","text":"<p>Create a set of conversation objects from the given configuration.</p> <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of conversation objects.</p> Source code in <code>aiperf/dataset/composer/base.py</code> <pre><code>@abstractmethod\ndef create_dataset(self) -&gt; list[Conversation]:\n    \"\"\"\n    Create a set of conversation objects from the given configuration.\n\n    Returns:\n        list[Conversation]: A list of conversation objects.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperfdatasetcomposercustom","title":"aiperf.dataset.composer.custom","text":""},{"location":"api/#aiperf.dataset.composer.custom.CustomDatasetComposer","title":"<code>CustomDatasetComposer</code>","text":"<p>               Bases: <code>BaseDatasetComposer</code></p> Source code in <code>aiperf/dataset/composer/custom.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ComposerFactory.register(ComposerType.CUSTOM)\nclass CustomDatasetComposer(BaseDatasetComposer):\n    def __init__(self, config: UserConfig, tokenizer: Tokenizer):\n        super().__init__(config, tokenizer)\n\n    def create_dataset(self) -&gt; list[Conversation]:\n        \"\"\"Create conversations from a file or directory.\n\n        Returns:\n            list[Conversation]: A list of conversation objects.\n        \"\"\"\n        # TODO: (future) for K8s, we need to transfer file data from SC (across node)\n        utils.check_file_exists(self.config.input.file)\n\n        self._create_loader_instance(self.config.input.custom_dataset_type)\n        dataset = self.loader.load_dataset()\n        conversations = self.loader.convert_to_conversations(dataset)\n        self._finalize_conversations(conversations)\n        return conversations\n\n    def _create_loader_instance(self, dataset_type: CustomDatasetType) -&gt; None:\n        \"\"\"Initializes the dataset loader based on the custom dataset type.\n\n        Args:\n            dataset_type: The type of custom dataset to create.\n        \"\"\"\n        kwargs = {\"filename\": self.config.input.file}\n        if dataset_type == CustomDatasetType.MOONCAKE_TRACE:\n            kwargs[\"prompt_generator\"] = self.prompt_generator\n            kwargs[\"user_config\"] = self.config\n        elif dataset_type == CustomDatasetType.RANDOM_POOL:\n            kwargs[\"num_conversations\"] = self.config.input.conversation.num\n\n        self.loader = CustomDatasetFactory.create_instance(dataset_type, **kwargs)\n\n    def _finalize_conversations(self, conversations: list[Conversation]) -&gt; None:\n        \"\"\"Finalize all turns in conversations by adding metadata.\"\"\"\n        for conversation in conversations:\n            for turn in conversation.turns:\n                self._finalize_turn(turn)\n</code></pre>"},{"location":"api/#aiperf.dataset.composer.custom.CustomDatasetComposer.create_dataset","title":"<code>create_dataset()</code>","text":"<p>Create conversations from a file or directory.</p> <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of conversation objects.</p> Source code in <code>aiperf/dataset/composer/custom.py</code> <pre><code>def create_dataset(self) -&gt; list[Conversation]:\n    \"\"\"Create conversations from a file or directory.\n\n    Returns:\n        list[Conversation]: A list of conversation objects.\n    \"\"\"\n    # TODO: (future) for K8s, we need to transfer file data from SC (across node)\n    utils.check_file_exists(self.config.input.file)\n\n    self._create_loader_instance(self.config.input.custom_dataset_type)\n    dataset = self.loader.load_dataset()\n    conversations = self.loader.convert_to_conversations(dataset)\n    self._finalize_conversations(conversations)\n    return conversations\n</code></pre>"},{"location":"api/#aiperfdatasetcomposersynthetic","title":"aiperf.dataset.composer.synthetic","text":""},{"location":"api/#aiperf.dataset.composer.synthetic.SyntheticDatasetComposer","title":"<code>SyntheticDatasetComposer</code>","text":"<p>               Bases: <code>BaseDatasetComposer</code></p> Source code in <code>aiperf/dataset/composer/synthetic.py</code> <pre><code>@ComposerFactory.register(ComposerType.SYNTHETIC)\nclass SyntheticDatasetComposer(BaseDatasetComposer):\n    def __init__(self, config: UserConfig, tokenizer: Tokenizer):\n        super().__init__(config, tokenizer)\n\n        if (\n            not self.include_prompt\n            and not self.include_image\n            and not self.include_audio\n        ):\n            raise ValueError(\n                \"All synthetic data are disabled. \"\n                \"Please enable at least one of prompt, image, or audio by \"\n                \"setting the mean to a positive value.\"\n            )\n\n    def create_dataset(self) -&gt; list[Conversation]:\n        \"\"\"Create a synthetic conversation dataset from the given configuration.\n\n        It generates a set of conversations with a varying number of turns,\n        where each turn contains synthetic text, image, and audio payloads.\n\n        Returns:\n            list[Conversation]: A list of conversation objects.\n        \"\"\"\n        conversations = []\n        for _ in range(self.config.input.conversation.num):\n            conversation = Conversation(session_id=str(uuid.uuid4()))\n\n            num_turns = utils.sample_positive_normal_integer(\n                self.config.input.conversation.turn.mean,\n                self.config.input.conversation.turn.stddev,\n            )\n            self.logger.debug(\"Creating conversation with %d turns\", num_turns)\n\n            for turn_idx in range(num_turns):\n                turn = self._create_turn(is_first=(turn_idx == 0))\n                conversation.turns.append(turn)\n            conversations.append(conversation)\n        return conversations\n\n    def _create_turn(self, is_first: bool) -&gt; Turn:\n        \"\"\"Create a turn object that contains synthetic payloads to send.\n\n        It generates multi-modal data (e.g. text, image, audio) using synthetic\n        generators and also the delay between turns.\n\n        Args:\n            is_first: Whether the turn is the first turn in the conversation.\n\n        Returns:\n            Turn: A dataset representation of a single turn.\n        \"\"\"\n        turn = Turn()\n\n        if self.include_prompt:\n            turn.texts.append(self._generate_text_payloads(is_first))\n        if self.include_image:\n            turn.images.append(self._generate_image_payloads())\n        if self.include_audio:\n            turn.audios.append(self._generate_audio_payloads())\n\n        # Add randomized delays between each turn. Skip if first turn.\n        if not is_first:\n            turn.delay = utils.sample_positive_normal_integer(\n                self.config.input.conversation.turn.delay.mean,\n                self.config.input.conversation.turn.delay.stddev,\n            )\n\n        if not turn.texts and not turn.images and not turn.audios:\n            self.logger.warning(\n                \"There were no synthetic payloads generated. \"\n                \"Please enable at least one of prompt, image, or audio by \"\n                \"setting the mean to a positive value.\"\n            )\n\n        self._finalize_turn(turn)\n\n        return turn\n\n    def _generate_text_payloads(self, is_first: bool) -&gt; Text:\n        \"\"\"Generate synthetic text payloads.\n\n        If the turn is the first turn in the conversation, it could add a prefix prompt\n        to the prompt.\n\n        Args:\n            is_first: Whether the turn is the first turn in the conversation.\n\n        Returns:\n            Text: A text payload object.\n        \"\"\"\n        text = Text(name=\"text\")\n        for _ in range(self.config.input.prompt.batch_size):\n            prompt = self.prompt_generator.generate(\n                mean=self.config.input.prompt.input_tokens.mean,\n                stddev=self.config.input.prompt.input_tokens.stddev,\n            )\n\n            if self.prefix_prompt_enabled and is_first:\n                # TODO: Rename\n                prefix_prompt = self.prompt_generator.get_random_prefix_prompt()\n                prompt = f\"{prefix_prompt} {prompt}\"\n\n            text.contents.append(prompt)\n        return text\n\n    def _generate_image_payloads(self) -&gt; Image:\n        \"\"\"\n        Generate synthetic images if the image width and height are specified.\n\n        Returns:\n            Image: An image payload object.\n        \"\"\"\n        image = Image(name=\"image_url\")\n        for _ in range(self.config.input.image.batch_size):\n            data = self.image_generator.generate()\n            image.contents.append(data)\n        return image\n\n    def _generate_audio_payloads(self) -&gt; Audio:\n        \"\"\"\n        Generate synthetic audios if the audio length is specified.\n\n        Returns:\n            Audio: An audio payload object.\n        \"\"\"\n        audio = Audio(name=\"input_audio\")\n        for _ in range(self.config.input.audio.batch_size):\n            data = self.audio_generator.generate()\n            audio.contents.append(data)\n        return audio\n\n    @property\n    def include_prompt(self) -&gt; bool:\n        return self.config.input.prompt.input_tokens.mean &gt; 0\n\n    @property\n    def include_image(self) -&gt; bool:\n        return (\n            self.config.input.image.width.mean &gt; 0\n            and self.config.input.image.height.mean &gt; 0\n        )\n\n    @property\n    def include_audio(self) -&gt; bool:\n        return self.config.input.audio.length.mean &gt; 0\n</code></pre>"},{"location":"api/#aiperf.dataset.composer.synthetic.SyntheticDatasetComposer.create_dataset","title":"<code>create_dataset()</code>","text":"<p>Create a synthetic conversation dataset from the given configuration.</p> <p>It generates a set of conversations with a varying number of turns, where each turn contains synthetic text, image, and audio payloads.</p> <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of conversation objects.</p> Source code in <code>aiperf/dataset/composer/synthetic.py</code> <pre><code>def create_dataset(self) -&gt; list[Conversation]:\n    \"\"\"Create a synthetic conversation dataset from the given configuration.\n\n    It generates a set of conversations with a varying number of turns,\n    where each turn contains synthetic text, image, and audio payloads.\n\n    Returns:\n        list[Conversation]: A list of conversation objects.\n    \"\"\"\n    conversations = []\n    for _ in range(self.config.input.conversation.num):\n        conversation = Conversation(session_id=str(uuid.uuid4()))\n\n        num_turns = utils.sample_positive_normal_integer(\n            self.config.input.conversation.turn.mean,\n            self.config.input.conversation.turn.stddev,\n        )\n        self.logger.debug(\"Creating conversation with %d turns\", num_turns)\n\n        for turn_idx in range(num_turns):\n            turn = self._create_turn(is_first=(turn_idx == 0))\n            conversation.turns.append(turn)\n        conversations.append(conversation)\n    return conversations\n</code></pre>"},{"location":"api/#aiperfdatasetdataset_manager","title":"aiperf.dataset.dataset_manager","text":""},{"location":"api/#aiperf.dataset.dataset_manager.DatasetManager","title":"<code>DatasetManager</code>","text":"<p>               Bases: <code>ReplyClientMixin</code>, <code>BaseComponentService</code></p> <p>The DatasetManager primary responsibility is to manage the data generation or acquisition. For synthetic generation, it contains the code to generate the prompts or tokens. It will have an API for dataset acquisition of a dataset if available in a remote repository or database.</p> Source code in <code>aiperf/dataset/dataset_manager.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ServiceFactory.register(ServiceType.DATASET_MANAGER)\nclass DatasetManager(ReplyClientMixin, BaseComponentService):\n    \"\"\"\n    The DatasetManager primary responsibility is to manage the data generation or acquisition.\n    For synthetic generation, it contains the code to generate the prompts or tokens.\n    It will have an API for dataset acquisition of a dataset if available in a remote repository or database.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            reply_client_address=CommAddress.DATASET_MANAGER_PROXY_BACKEND,\n            reply_client_bind=False,\n        )\n        self.debug(\"Dataset manager __init__\")\n        self.user_config = user_config\n        self.tokenizer: Tokenizer | None = None\n        self.dataset: dict[str, Conversation] = {}  # session ID -&gt; Conversation mapping\n        self._session_ids_cache: list[str] = []\n        self._conversation_query_random = random.Random(\n            self.user_config.input.random_seed\n        )\n        self.dataset_configured = asyncio.Event()\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize the dataset manager.\"\"\"\n        self.debug(\"Initializing dataset manager\")\n        tokenizer_name = self.user_config.tokenizer.name\n        if tokenizer_name is None:\n            # TODO: What do we do if there are multiple models?\n            # How will we know which tokenizer to use?\n            tokenizer_name = self.user_config.endpoint.model_names[0]\n\n        self.tokenizer = Tokenizer.from_pretrained(\n            tokenizer_name,\n            trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n            revision=self.user_config.tokenizer.revision,\n        )\n\n    @on_command(CommandType.PROFILE_CONFIGURE)\n    async def _profile_configure_command(\n        self, message: ProfileConfigureCommand\n    ) -&gt; None:\n        \"\"\"Configure the dataset.\"\"\"\n        self.info(lambda: f\"Configuring dataset for {self.service_id}\")\n        begin = time.perf_counter()\n        await self._configure_dataset()\n        duration = time.perf_counter() - begin\n        self.info(lambda: f\"Dataset configured in {duration:.2f} seconds\")\n\n    async def _configure_dataset(self) -&gt; None:\n        if self.user_config is None:\n            raise self._service_error(\"User config is required for dataset manager\")\n\n        self.dataset_configured.clear()\n\n        # Temporary as this will change with the following dataset processor service PR\n        if self.user_config.input.public_dataset is not None:\n            loader = ShareGPTLoader(self.user_config, self.tokenizer)\n            dataset = await loader.load_dataset()\n            conversations = await loader.convert_to_conversations(dataset)\n        elif self.user_config.input.custom_dataset_type is not None:\n            composer = ComposerFactory.create_instance(\n                ComposerType.CUSTOM,\n                config=self.user_config,\n                tokenizer=self.tokenizer,\n            )\n            conversations = composer.create_dataset()\n        else:\n            composer = ComposerFactory.create_instance(\n                ComposerType.SYNTHETIC,\n                config=self.user_config,\n                tokenizer=self.tokenizer,\n            )\n            conversations = composer.create_dataset()\n\n        self.dataset = {conv.session_id: conv for conv in conversations}\n        self._session_ids_cache = list(self.dataset.keys())\n\n        self.dataset_configured.set()\n        await self.publish(\n            DatasetConfiguredNotification(\n                service_id=self.service_id,\n            ),\n        )\n\n    @on_request(MessageType.CONVERSATION_REQUEST)\n    async def _handle_conversation_request(\n        self, message: ConversationRequestMessage\n    ) -&gt; ConversationResponseMessage:\n        \"\"\"Handle a conversation request.\"\"\"\n        self.debug(lambda: f\"Handling conversation request: {message}\")\n\n        await self._wait_for_dataset_configuration()\n\n        if not self.dataset:\n            raise self._service_error(\n                \"Dataset is empty and must be configured before handling requests.\",\n            )\n\n        if message.conversation_id is None:\n            return self._return_any_conversation(\n                request_id=message.request_id,\n            )\n        else:\n            return self._return_conversation_by_id(\n                request_id=message.request_id,\n                conversation_id=message.conversation_id,\n            )\n\n    def _return_any_conversation(\n        self, request_id: str | None\n    ) -&gt; ConversationResponseMessage:\n        \"\"\"Return any conversation from the dataset based on the user specified method.\"\"\"\n\n        # TODO: Implement the user specified method (random, round robin, etc.)\n        session_id = self._conversation_query_random.choice(self._session_ids_cache)\n        conversation = self.dataset[session_id]\n        self.trace_or_debug(\n            lambda: f\"Sending random conversation response: {conversation}\",\n            lambda: f\"Sending random conversation response with id: {conversation.session_id}\",\n        )\n        return ConversationResponseMessage(\n            service_id=self.service_id,\n            request_id=request_id,\n            conversation=conversation,\n        )\n\n    def _return_conversation_by_id(\n        self, request_id: str | None, conversation_id: str\n    ) -&gt; ConversationResponseMessage:\n        \"\"\"Return a conversation if it exists, otherwise raise an error.\"\"\"\n\n        if conversation_id not in self.dataset:\n            raise self._service_error(\n                f\"Conversation {conversation_id} not found in dataset.\",\n            )\n\n        conversation = self.dataset[conversation_id]\n        self.trace_or_debug(\n            lambda: f\"Sending conversation response: {conversation}\",\n            lambda: f\"Sending conversation response with id: {conversation.session_id}\",\n        )\n        return ConversationResponseMessage(\n            service_id=self.service_id,\n            request_id=request_id,\n            conversation=conversation,\n        )\n\n    @on_request(MessageType.CONVERSATION_TURN_REQUEST)\n    async def _handle_conversation_turn_request(\n        self, message: ConversationTurnRequestMessage\n    ) -&gt; ConversationTurnResponseMessage:\n        \"\"\"Handle a turn request.\"\"\"\n        self.debug(lambda: f\"Handling turn request: {message}\")\n\n        if message.conversation_id not in self.dataset:\n            raise self._service_error(\n                f\"Conversation {message.conversation_id} not found in dataset.\",\n            )\n\n        conversation = self.dataset[message.conversation_id]\n        if message.turn_index &gt;= len(conversation.turns):\n            raise self._service_error(\n                f\"Turn index {message.turn_index} is out of range for conversation {message.conversation_id}.\",\n            )\n\n        turn = conversation.turns[message.turn_index]\n\n        self.trace_or_debug(\n            lambda: f\"Sending turn response: {turn}\",\n            \"Sending turn response\",\n        )\n        return ConversationTurnResponseMessage(\n            service_id=self.service_id,\n            request_id=message.request_id,\n            turn=turn,\n        )\n\n    @on_request(MessageType.DATASET_TIMING_REQUEST)\n    async def _handle_dataset_timing_request(\n        self, message: DatasetTimingRequest\n    ) -&gt; DatasetTimingResponse:\n        \"\"\"Handle a dataset timing request.\"\"\"\n        self.trace_or_debug(\n            lambda: f\"Handling dataset timing request: {message}\",\n            \"Handling dataset timing request\",\n        )\n\n        await self._wait_for_dataset_configuration()\n\n        if not self.dataset:\n            raise self._service_error(\n                \"Dataset is empty and must be configured before handling timing requests.\",\n            )\n\n        timing_dataset = []\n        for conversation_id, conversation in self.dataset.items():\n            for turn in conversation.turns:\n                timing_dataset.append((turn.timestamp, conversation_id))\n\n        return DatasetTimingResponse(\n            service_id=self.service_id,\n            request_id=message.request_id,\n            timing_data=timing_dataset,\n        )\n\n    async def _wait_for_dataset_configuration(self) -&gt; None:\n        \"\"\"Wait for the dataset to be configured if it is not already.\"\"\"\n        if not self.dataset_configured.is_set():\n            self.debug(\n                \"Dataset not configured. Waiting for dataset to be configured...\"\n            )\n            await asyncio.wait_for(\n                self.dataset_configured.wait(), timeout=DATASET_CONFIGURATION_TIMEOUT\n            )\n</code></pre>"},{"location":"api/#aiperf.dataset.dataset_manager.main","title":"<code>main()</code>","text":"<p>Main entry point for the dataset manager.</p> Source code in <code>aiperf/dataset/dataset_manager.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the dataset manager.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(DatasetManager)\n</code></pre>"},{"location":"api/#aiperfdatasetgeneratoraudio","title":"aiperf.dataset.generator.audio","text":""},{"location":"api/#aiperf.dataset.generator.audio.AudioGenerator","title":"<code>AudioGenerator</code>","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>A class for generating synthetic audio data.</p> <p>This class provides methods to create audio samples with specified characteristics such as format (WAV, MP3), length, sampling rate, bit depth, and number of channels. It supports validation of audio parameters to ensure compatibility with chosen formats.</p> Source code in <code>aiperf/dataset/generator/audio.py</code> <pre><code>class AudioGenerator(BaseGenerator):\n    \"\"\"\n    A class for generating synthetic audio data.\n\n    This class provides methods to create audio samples with specified\n    characteristics such as format (WAV, MP3), length, sampling rate,\n    bit depth, and number of channels. It supports validation of audio\n    parameters to ensure compatibility with chosen formats.\n    \"\"\"\n\n    def __init__(self, config: AudioConfig):\n        super().__init__()\n        self.config = config\n\n    def _validate_sampling_rate(\n        self, sampling_rate_hz: int, audio_format: AudioFormat\n    ) -&gt; None:\n        \"\"\"\n        Validate sampling rate for the given output format.\n\n        Args:\n            sampling_rate_hz: Sampling rate in Hz\n            audio_format: Audio format\n\n        Raises:\n            ConfigurationError: If sampling rate is not supported for the given format\n        \"\"\"\n        if (\n            audio_format == AudioFormat.MP3\n            and sampling_rate_hz not in MP3_SUPPORTED_SAMPLE_RATES\n        ):\n            supported_rates = sorted(MP3_SUPPORTED_SAMPLE_RATES)\n            raise ConfigurationError(\n                f\"MP3 format only supports the following sample rates (in Hz): {supported_rates}. \"\n                f\"Got {sampling_rate_hz} Hz. Please choose a supported rate from the list.\"\n            )\n\n    def _validate_bit_depth(self, bit_depth: int) -&gt; None:\n        \"\"\"\n        Validate bit depth is supported.\n\n        Args:\n            bit_depth: Bit depth in bits\n\n        Raises:\n            ConfigurationError: If bit depth is not supported\n        \"\"\"\n        if bit_depth not in SUPPORTED_BIT_DEPTHS:\n            supported_depths = sorted(SUPPORTED_BIT_DEPTHS.keys())\n            raise ConfigurationError(\n                f\"Unsupported bit depth: {bit_depth}. \"\n                f\"Supported bit depths are: {supported_depths}\"\n            )\n\n    def generate(self, *args, **kwargs) -&gt; str:\n        \"\"\"Generate audio data with specified parameters.\n\n        Returns:\n            Data URI containing base64-encoded audio data with format specification\n\n        Raises:\n            ConfigurationError: If any of the following conditions are met:\n                - audio length is less than 0.01 seconds\n                - channels is not 1 (mono) or 2 (stereo)\n                - sampling rate is not supported for MP3 format\n                - bit depth is not supported (must be 8, 16, 24, or 32)\n                - audio format is not supported (must be 'wav' or 'mp3')\n        \"\"\"\n        if self.config.num_channels not in (1, 2):\n            raise ConfigurationError(\n                \"Only mono (1) and stereo (2) channels are supported\"\n            )\n\n        if self.config.length.mean &lt; 0.01:\n            raise ConfigurationError(\"Audio length must be greater than 0.01 seconds\")\n\n        # Sample audio length (in seconds) using rejection sampling\n        audio_length = utils.sample_normal(\n            self.config.length.mean, self.config.length.stddev, lower=0.01\n        )\n\n        # Randomly select sampling rate and bit depth\n        sampling_rate_hz = int(\n            np.random.choice(self.config.sample_rates) * 1000\n        )  # Convert kHz to Hz\n        bit_depth = np.random.choice(self.config.depths)\n\n        # Validate sampling rate and bit depth\n        self._validate_sampling_rate(sampling_rate_hz, self.config.format)\n        self._validate_bit_depth(bit_depth)\n\n        # Generate synthetic audio data (gaussian noise)\n        num_samples = int(audio_length * sampling_rate_hz)\n        audio_data = np.random.normal(\n            0,\n            0.3,\n            (\n                (num_samples, self.config.num_channels)\n                if self.config.num_channels &gt; 1\n                else num_samples\n            ),\n        )\n\n        # Ensure the signal is within [-1, 1] range\n        audio_data = np.clip(audio_data, -1, 1)\n\n        # Scale to the appropriate bit depth range\n        max_val = 2 ** (bit_depth - 1) - 1\n        numpy_type, _ = SUPPORTED_BIT_DEPTHS[bit_depth]\n        audio_data = (audio_data * max_val).astype(numpy_type)\n\n        # Write audio using soundfile\n        output_buffer = io.BytesIO()\n\n        # Select appropriate subtype based on format\n        if self.config.format == AudioFormat.MP3:\n            subtype = \"MPEG_LAYER_III\"\n        elif self.config.format == AudioFormat.WAV:\n            _, subtype = SUPPORTED_BIT_DEPTHS[bit_depth]\n        else:\n            raise ConfigurationError(\n                f\"Unsupported audio format: {self.config.format}. \"\n                f\"Supported formats are: {AudioFormat.WAV.name}, {AudioFormat.MP3.name}\"\n            )\n\n        sf.write(\n            output_buffer,\n            audio_data,\n            sampling_rate_hz,\n            format=self.config.format,\n            subtype=subtype,\n        )\n        audio_bytes = output_buffer.getvalue()\n\n        # Encode to base64 with data URI scheme: \"{format},{data}\"\n        base64_data = base64.b64encode(audio_bytes).decode(\"utf-8\")\n        return f\"{self.config.format.lower()},{base64_data}\"\n</code></pre>"},{"location":"api/#aiperf.dataset.generator.audio.AudioGenerator.generate","title":"<code>generate(*args, **kwargs)</code>","text":"<p>Generate audio data with specified parameters.</p> <p>Returns:</p> Type Description <code>str</code> <p>Data URI containing base64-encoded audio data with format specification</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If any of the following conditions are met: - audio length is less than 0.01 seconds - channels is not 1 (mono) or 2 (stereo) - sampling rate is not supported for MP3 format - bit depth is not supported (must be 8, 16, 24, or 32) - audio format is not supported (must be 'wav' or 'mp3')</p> Source code in <code>aiperf/dataset/generator/audio.py</code> <pre><code>def generate(self, *args, **kwargs) -&gt; str:\n    \"\"\"Generate audio data with specified parameters.\n\n    Returns:\n        Data URI containing base64-encoded audio data with format specification\n\n    Raises:\n        ConfigurationError: If any of the following conditions are met:\n            - audio length is less than 0.01 seconds\n            - channels is not 1 (mono) or 2 (stereo)\n            - sampling rate is not supported for MP3 format\n            - bit depth is not supported (must be 8, 16, 24, or 32)\n            - audio format is not supported (must be 'wav' or 'mp3')\n    \"\"\"\n    if self.config.num_channels not in (1, 2):\n        raise ConfigurationError(\n            \"Only mono (1) and stereo (2) channels are supported\"\n        )\n\n    if self.config.length.mean &lt; 0.01:\n        raise ConfigurationError(\"Audio length must be greater than 0.01 seconds\")\n\n    # Sample audio length (in seconds) using rejection sampling\n    audio_length = utils.sample_normal(\n        self.config.length.mean, self.config.length.stddev, lower=0.01\n    )\n\n    # Randomly select sampling rate and bit depth\n    sampling_rate_hz = int(\n        np.random.choice(self.config.sample_rates) * 1000\n    )  # Convert kHz to Hz\n    bit_depth = np.random.choice(self.config.depths)\n\n    # Validate sampling rate and bit depth\n    self._validate_sampling_rate(sampling_rate_hz, self.config.format)\n    self._validate_bit_depth(bit_depth)\n\n    # Generate synthetic audio data (gaussian noise)\n    num_samples = int(audio_length * sampling_rate_hz)\n    audio_data = np.random.normal(\n        0,\n        0.3,\n        (\n            (num_samples, self.config.num_channels)\n            if self.config.num_channels &gt; 1\n            else num_samples\n        ),\n    )\n\n    # Ensure the signal is within [-1, 1] range\n    audio_data = np.clip(audio_data, -1, 1)\n\n    # Scale to the appropriate bit depth range\n    max_val = 2 ** (bit_depth - 1) - 1\n    numpy_type, _ = SUPPORTED_BIT_DEPTHS[bit_depth]\n    audio_data = (audio_data * max_val).astype(numpy_type)\n\n    # Write audio using soundfile\n    output_buffer = io.BytesIO()\n\n    # Select appropriate subtype based on format\n    if self.config.format == AudioFormat.MP3:\n        subtype = \"MPEG_LAYER_III\"\n    elif self.config.format == AudioFormat.WAV:\n        _, subtype = SUPPORTED_BIT_DEPTHS[bit_depth]\n    else:\n        raise ConfigurationError(\n            f\"Unsupported audio format: {self.config.format}. \"\n            f\"Supported formats are: {AudioFormat.WAV.name}, {AudioFormat.MP3.name}\"\n        )\n\n    sf.write(\n        output_buffer,\n        audio_data,\n        sampling_rate_hz,\n        format=self.config.format,\n        subtype=subtype,\n    )\n    audio_bytes = output_buffer.getvalue()\n\n    # Encode to base64 with data URI scheme: \"{format},{data}\"\n    base64_data = base64.b64encode(audio_bytes).decode(\"utf-8\")\n    return f\"{self.config.format.lower()},{base64_data}\"\n</code></pre>"},{"location":"api/#aiperfdatasetgeneratorbase","title":"aiperf.dataset.generator.base","text":""},{"location":"api/#aiperf.dataset.generator.base.BaseGenerator","title":"<code>BaseGenerator</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> <p>Abstract base class for all data generators.</p> <p>Provides a consistent interface for generating synthetic data while allowing each generator type to use its own specific configuration and runtime parameters.</p> Source code in <code>aiperf/dataset/generator/base.py</code> <pre><code>class BaseGenerator(AIPerfLoggerMixin, ABC):\n    \"\"\"Abstract base class for all data generators.\n\n    Provides a consistent interface for generating synthetic data while allowing\n    each generator type to use its own specific configuration and runtime parameters.\n    \"\"\"\n\n    @abstractmethod\n    def generate(self, *args, **kwargs) -&gt; str:\n        \"\"\"Generate synthetic data.\n\n        Args:\n            *args: Variable length argument list (subclass-specific)\n            **kwargs: Arbitrary keyword arguments (subclass-specific)\n\n        Returns:\n            Generated data as a string (could be text, base64 encoded media, etc.)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#aiperf.dataset.generator.base.BaseGenerator.generate","title":"<code>generate(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list (subclass-specific)</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments (subclass-specific)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated data as a string (could be text, base64 encoded media, etc.)</p> Source code in <code>aiperf/dataset/generator/base.py</code> <pre><code>@abstractmethod\ndef generate(self, *args, **kwargs) -&gt; str:\n    \"\"\"Generate synthetic data.\n\n    Args:\n        *args: Variable length argument list (subclass-specific)\n        **kwargs: Arbitrary keyword arguments (subclass-specific)\n\n    Returns:\n        Generated data as a string (could be text, base64 encoded media, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#aiperfdatasetgeneratorimage","title":"aiperf.dataset.generator.image","text":""},{"location":"api/#aiperf.dataset.generator.image.ImageGenerator","title":"<code>ImageGenerator</code>","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>A class that generates images from source images.</p> <p>This class provides methods to create synthetic images by resizing source images (located in the 'assets/source_images' directory) to specified dimensions and converting them to a chosen image format (e.g., PNG, JPEG). The dimensions can be randomized based on mean and standard deviation values.</p> Source code in <code>aiperf/dataset/generator/image.py</code> <pre><code>class ImageGenerator(BaseGenerator):\n    \"\"\"A class that generates images from source images.\n\n    This class provides methods to create synthetic images by resizing\n    source images (located in the 'assets/source_images' directory)\n    to specified dimensions and converting them to a chosen image format (e.g., PNG, JPEG).\n    The dimensions can be randomized based on mean and standard deviation values.\n    \"\"\"\n\n    def __init__(self, config: ImageConfig):\n        super().__init__()\n        self.config = config\n\n    def generate(self, *args, **kwargs) -&gt; str:\n        \"\"\"Generate an image with the configured parameters.\n\n        Returns:\n            A base64 encoded string of the generated image.\n        \"\"\"\n        image_format = self.config.format\n        if image_format == ImageFormat.RANDOM:\n            image_format = random.choice(\n                [f for f in ImageFormat if f != ImageFormat.RANDOM]\n            )\n\n        width = utils.sample_positive_normal_integer(\n            self.config.width.mean, self.config.width.stddev\n        )\n        height = utils.sample_positive_normal_integer(\n            self.config.height.mean, self.config.height.stddev\n        )\n\n        self.logger.debug(\n            \"Generating image with width=%d, height=%d\",\n            width,\n            height,\n        )\n\n        image = self._sample_source_image()\n        image = image.resize(size=(width, height))\n        base64_image = utils.encode_image(image, image_format)\n        return f\"data:image/{image_format.name.lower()};base64,{base64_image}\"\n\n    def _sample_source_image(self):\n        \"\"\"Sample one image among the source images.\n\n        Returns:\n            A PIL Image object randomly selected from the source images.\n        \"\"\"\n        filepath = Path(__file__).parent.resolve() / \"assets\" / \"source_images\" / \"*\"\n        filenames = glob.glob(str(filepath))\n        if not filenames:\n            raise ValueError(f\"No source images found in '{filepath}'\")\n\n        self.debug(lambda: f\"Found {len(filenames)} source images in '{filepath}'\")\n        return Image.open(random.choice(filenames))\n</code></pre>"},{"location":"api/#aiperf.dataset.generator.image.ImageGenerator.generate","title":"<code>generate(*args, **kwargs)</code>","text":"<p>Generate an image with the configured parameters.</p> <p>Returns:</p> Type Description <code>str</code> <p>A base64 encoded string of the generated image.</p> Source code in <code>aiperf/dataset/generator/image.py</code> <pre><code>def generate(self, *args, **kwargs) -&gt; str:\n    \"\"\"Generate an image with the configured parameters.\n\n    Returns:\n        A base64 encoded string of the generated image.\n    \"\"\"\n    image_format = self.config.format\n    if image_format == ImageFormat.RANDOM:\n        image_format = random.choice(\n            [f for f in ImageFormat if f != ImageFormat.RANDOM]\n        )\n\n    width = utils.sample_positive_normal_integer(\n        self.config.width.mean, self.config.width.stddev\n    )\n    height = utils.sample_positive_normal_integer(\n        self.config.height.mean, self.config.height.stddev\n    )\n\n    self.logger.debug(\n        \"Generating image with width=%d, height=%d\",\n        width,\n        height,\n    )\n\n    image = self._sample_source_image()\n    image = image.resize(size=(width, height))\n    base64_image = utils.encode_image(image, image_format)\n    return f\"data:image/{image_format.name.lower()};base64,{base64_image}\"\n</code></pre>"},{"location":"api/#aiperfdatasetgeneratorprompt","title":"aiperf.dataset.generator.prompt","text":""},{"location":"api/#aiperf.dataset.generator.prompt.PromptGenerator","title":"<code>PromptGenerator</code>","text":"<p>               Bases: <code>BaseGenerator</code></p> <p>A class for generating synthetic prompts from a text corpus.</p> <p>This class loads a text corpus (e.g., Shakespearean text), tokenizes it, and uses the tokenized corpus to generate synthetic prompts of specified lengths. It supports generating prompts with a target number of tokens (with optional randomization around a mean and standard deviation) and can reuse previously generated token blocks to optimize generation for certain use cases. It also allows for the creation of a pool of prefix prompts that can be randomly selected.</p> Source code in <code>aiperf/dataset/generator/prompt.py</code> <pre><code>class PromptGenerator(BaseGenerator):\n    \"\"\"A class for generating synthetic prompts from a text corpus.\n\n    This class loads a text corpus (e.g., Shakespearean text), tokenizes it,\n    and uses the tokenized corpus to generate synthetic prompts of specified\n    lengths. It supports generating prompts with a target number of tokens\n    (with optional randomization around a mean and standard deviation) and\n    can reuse previously generated token blocks to optimize generation for\n    certain use cases. It also allows for the creation of a pool of prefix\n    prompts that can be randomly selected.\n    \"\"\"\n\n    def __init__(self, config: PromptConfig, tokenizer: Tokenizer, **kwargs):\n        self.config = config\n        self.tokenizer = tokenizer\n        self._tokenized_corpus = None\n        self._corpus_size = 0\n        self._prefix_prompts: list[str] = []\n        super().__init__(config=config, tokenizer=tokenizer, **kwargs)\n\n        # Cached prompts: block ID -&gt; list of tokens\n        self._cache: dict[int, list[int]] = {}\n\n        # TODO: move this under initialize() method\n        # Initialize corpus if not already done\n        if self._tokenized_corpus is None:\n            self._initialize_corpus()\n\n        # Initialize prefix prompts pool if the pool size &gt; 0\n        if self.config.prefix_prompt.pool_size &gt; 0:\n            self._create_prefix_prompt_pool()\n\n    def _initialize_corpus(self) -&gt; None:\n        \"\"\"Load and tokenize the corpus once, storing it for reuse.\"\"\"\n        corpus_path = Path(__file__).parent / DEFAULT_CORPUS_FILE\n\n        with open(corpus_path) as f:\n            lines = f.readlines()\n\n        def tokenize_chunk(chunk):\n            cleaned_text = \" \".join(line.strip() for line in chunk if line.strip())\n            tokens = self.tokenizer.encode(cleaned_text)\n            return tokens\n\n        num_threads = os.cpu_count()\n        if num_threads is None:\n            num_threads = 4\n\n        # Ensure chunk_size is at least 1 to avoid division by zero in range()\n        chunk_size = max(1, len(lines) // num_threads)\n        chunks = [lines[i : i + chunk_size] for i in range(0, len(lines), chunk_size)]\n\n        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n            tokenized_chunks = list(executor.map(tokenize_chunk, chunks))\n\n        self._tokenized_corpus = [\n            token for chunk in tokenized_chunks for token in chunk\n        ]\n        self._corpus_size = len(self._tokenized_corpus)\n        self.debug(lambda: f\"Initialized corpus with {self._corpus_size} tokens\")\n\n    def _create_prefix_prompt_pool(self) -&gt; None:\n        \"\"\"Generate a pool of prefix prompts to sample from.\"\"\"\n        if self._tokenized_corpus is None:\n            raise NotInitializedError(\"Tokenized corpus is not initialized.\")\n\n        self._prefix_prompts = [\n            self._generate_prompt(self.config.prefix_prompt.length)\n            for _ in range(self.config.prefix_prompt.pool_size)\n        ]\n        self.debug(\n            lambda: f\"Initialized prefix prompts pool with {len(self._prefix_prompts)} prompts\"\n        )\n\n    def generate(\n        self,\n        mean: int | None = None,\n        stddev: int | None = None,\n        hash_ids: list[int] | None = None,\n    ) -&gt; str:\n        \"\"\"Generate a synthetic prompt with the configuration parameters.\n\n        Args:\n            mean: The mean of the normal distribution.\n            stddev: The standard deviation of the normal distribution.\n            hash_ids: A list of hash indices used for token reuse.\n\n        Returns:\n            A synthetic prompt as a string.\n        \"\"\"\n        if hash_ids:\n            return self._generate_cached_prompt(\n                mean, hash_ids, self.config.input_tokens.block_size\n            )\n\n        num_tokens = utils.sample_positive_normal_integer(mean, stddev)\n        return self._generate_prompt(num_tokens)\n\n    def _generate_prompt(self, num_tokens: int) -&gt; str:\n        \"\"\"Generate a prompt containing exactly `num_tokens` number of tokens.\n\n        Args:\n            num_tokens: Number of tokens required in the prompt.\n\n        Returns:\n            A synthetic prompt as a string.\n        \"\"\"\n        return self.tokenizer.decode(self._sample_tokens(num_tokens))\n\n    def _generate_cached_prompt(\n        self,\n        num_tokens: int,\n        hash_ids: list[int],\n        block_size: int,\n    ) -&gt; str:\n        \"\"\"\n        Generate a prompt containing exactly `num_tokens` by reusing previously generated prompts\n        stored in `_cache`. Each hash index in `hash_ids` corresponds to a block of\n        `block_size` tokens. If a hash index is found in `_cache`, its stored prompt is reused.\n        Otherwise, a new prompt is generated using `_generate_prompt()` and stored in `_cache`.\n\n        Args:\n            num_tokens: The number of tokens required in the prompt.\n            hash_ids: A list of hash IDs to use for token reuse.\n            block_size: The number of tokens allocated per hash block.\n\n        Returns:\n            str: A synthetic prompt as a string.\n\n        Raises:\n            ConfigurationError: If the input parameters are not compatible.\n        \"\"\"\n        final_prompt: list[int] = []\n        current_block_size = block_size\n\n        # Sanity check the final block size\n        final_block_size = num_tokens - ((len(hash_ids) - 1) * block_size)\n        if final_block_size &lt;= 0 or block_size &lt; final_block_size:\n            raise ConfigurationError(\n                f\"Input length: {num_tokens}, Hash IDs: {hash_ids}, Block size: {block_size} \"\n                f\"are not compatible. The final hash block size: {final_block_size} must be \"\n                f\"greater than 0 and less than or equal to {block_size}.\"\n            )\n\n        for index, hash_id in enumerate(hash_ids):\n            # For the last hash ID, use the remaining tokens as the block size\n            if index == len(hash_ids) - 1:\n                current_block_size = final_block_size\n\n            if hash_id not in self._cache:\n                # To ensure that the prompt doesn't merge chunks, we insert a BOS or EOS token\n                # at the beginning. Length is maintained and the prompt generates the expected\n                # number of tokens. If no BOS or EOS token is available, we don't insert one.\n                prompt_tokens: list[int] = []\n                if self.tokenizer.block_separation_token_id is not None:\n                    prompt_tokens += [self.tokenizer.block_separation_token_id]\n                    prompt_tokens += self._sample_tokens(current_block_size - 1)\n                else:\n                    prompt_tokens += self._sample_tokens(current_block_size)\n\n                self._cache[hash_id] = prompt_tokens  # store to cache\n\n            final_prompt.extend(self._cache[hash_id])\n\n        return self.tokenizer.decode(final_prompt, skip_special_tokens=False)\n\n    def _sample_tokens(self, num_tokens: int) -&gt; list[int]:\n        \"\"\"Generate a list of token IDs containing exactly `num_tokens` number of tokens\n        using the preloaded tokenized corpus.\n\n        Args:\n            num_tokens: Number of tokens required in the prompt.\n\n        Returns:\n            A list of token IDs.\n\n        Raises:\n            NotInitializedError: If the tokenized corpus is not initialized\n        \"\"\"\n        if not self._tokenized_corpus:\n            raise NotInitializedError(\"Tokenized corpus is not initialized.\")\n        if num_tokens &gt; self._corpus_size:\n            self.warning(\n                f\"Requested prompt length {num_tokens} is longer than the corpus. \"\n                f\"Returning a prompt of length {self._corpus_size}.\"\n            )\n\n        start_idx = random.randrange(self._corpus_size)\n\n        end_idx = start_idx + num_tokens\n        prompt_tokens = self._tokenized_corpus[start_idx:end_idx]\n        if end_idx &gt; self._corpus_size:\n            prompt_tokens += self._tokenized_corpus[: end_idx - self._corpus_size]\n\n        self.trace(lambda: f\"Sampled {len(prompt_tokens)} tokens from corpus\")\n        return prompt_tokens\n\n    def get_random_prefix_prompt(self) -&gt; str:\n        \"\"\"\n        Fetch a random prefix prompt from the pool.\n\n        Returns:\n            A random prefix prompt.\n\n        Raises:\n            InvalidStateError: If the prefix prompts pool is empty.\n        \"\"\"\n        if not self._prefix_prompts:\n            raise InvalidStateError(\n                \"Attempted to sample a prefix prompt but the prefix prompts pool is empty. \"\n                \"Please ensure that the prefix prompts pool is initialized.\"\n            )\n        return random.choice(self._prefix_prompts)\n</code></pre>"},{"location":"api/#aiperf.dataset.generator.prompt.PromptGenerator.generate","title":"<code>generate(mean=None, stddev=None, hash_ids=None)</code>","text":"<p>Generate a synthetic prompt with the configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>int | None</code> <p>The mean of the normal distribution.</p> <code>None</code> <code>stddev</code> <code>int | None</code> <p>The standard deviation of the normal distribution.</p> <code>None</code> <code>hash_ids</code> <code>list[int] | None</code> <p>A list of hash indices used for token reuse.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A synthetic prompt as a string.</p> Source code in <code>aiperf/dataset/generator/prompt.py</code> <pre><code>def generate(\n    self,\n    mean: int | None = None,\n    stddev: int | None = None,\n    hash_ids: list[int] | None = None,\n) -&gt; str:\n    \"\"\"Generate a synthetic prompt with the configuration parameters.\n\n    Args:\n        mean: The mean of the normal distribution.\n        stddev: The standard deviation of the normal distribution.\n        hash_ids: A list of hash indices used for token reuse.\n\n    Returns:\n        A synthetic prompt as a string.\n    \"\"\"\n    if hash_ids:\n        return self._generate_cached_prompt(\n            mean, hash_ids, self.config.input_tokens.block_size\n        )\n\n    num_tokens = utils.sample_positive_normal_integer(mean, stddev)\n    return self._generate_prompt(num_tokens)\n</code></pre>"},{"location":"api/#aiperf.dataset.generator.prompt.PromptGenerator.get_random_prefix_prompt","title":"<code>get_random_prefix_prompt()</code>","text":"<p>Fetch a random prefix prompt from the pool.</p> <p>Returns:</p> Type Description <code>str</code> <p>A random prefix prompt.</p> <p>Raises:</p> Type Description <code>InvalidStateError</code> <p>If the prefix prompts pool is empty.</p> Source code in <code>aiperf/dataset/generator/prompt.py</code> <pre><code>def get_random_prefix_prompt(self) -&gt; str:\n    \"\"\"\n    Fetch a random prefix prompt from the pool.\n\n    Returns:\n        A random prefix prompt.\n\n    Raises:\n        InvalidStateError: If the prefix prompts pool is empty.\n    \"\"\"\n    if not self._prefix_prompts:\n        raise InvalidStateError(\n            \"Attempted to sample a prefix prompt but the prefix prompts pool is empty. \"\n            \"Please ensure that the prefix prompts pool is initialized.\"\n        )\n    return random.choice(self._prefix_prompts)\n</code></pre>"},{"location":"api/#aiperfdatasetloaderbase_public_dataset","title":"aiperf.dataset.loader.base_public_dataset","text":""},{"location":"api/#aiperf.dataset.loader.base_public_dataset.BasePublicDatasetLoader","title":"<code>BasePublicDatasetLoader</code>","text":"<p>               Bases: <code>AioHttpClientMixin</code></p> <p>Base class for loading public datasets from remote URLs with caching support.</p> <p>This abstract base class provides a common interface and implementation for downloading, caching, and loading public datasets. It handles the HTTP download logic, local caching to avoid redundant downloads, and provides utilities for validating dataset entries.</p> <p>The class follows a two-step process: 1. Load/download the raw dataset (with automatic caching) 2. Convert the dataset to AIPerf's standardized Conversation format</p> Example <p>class MyDatasetLoader(BasePublicDatasetLoader): ...     tag = \"MyDataset\" ...     url = \"https://example.com/dataset.json\" ...     filename = \"my_dataset.json\" ... ...     async def load_dataset(self) -&gt; list[Conversation]: ...         # Custom dataset loading logic here ...         return dataset</p> <p>loader = MyDatasetLoader() conversations = await loader.load_dataset() print(f\"Loaded {len(conversations)} conversations\")</p> Source code in <code>aiperf/dataset/loader/base_public_dataset.py</code> <pre><code>class BasePublicDatasetLoader(AioHttpClientMixin):\n    \"\"\"Base class for loading public datasets from remote URLs with caching support.\n\n    This abstract base class provides a common interface and implementation for downloading,\n    caching, and loading public datasets. It handles the HTTP download logic, local caching\n    to avoid redundant downloads, and provides utilities for validating dataset entries.\n\n    The class follows a two-step process:\n    1. Load/download the raw dataset (with automatic caching)\n    2. Convert the dataset to AIPerf's standardized Conversation format\n\n    Example:\n        &gt;&gt;&gt; class MyDatasetLoader(BasePublicDatasetLoader):\n        ...     tag = \"MyDataset\"\n        ...     url = \"https://example.com/dataset.json\"\n        ...     filename = \"my_dataset.json\"\n        ...\n        ...     async def load_dataset(self) -&gt; list[Conversation]:\n        ...         # Custom dataset loading logic here\n        ...         return dataset\n\n        &gt;&gt;&gt; loader = MyDatasetLoader()\n        &gt;&gt;&gt; conversations = await loader.load_dataset()\n        &gt;&gt;&gt; print(f\"Loaded {len(conversations)} conversations\")\n    \"\"\"\n\n    tag: ClassVar[str]\n    url: ClassVar[str]\n    filename: ClassVar[str] = \"dataset.json\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.cache_filepath = AIPERF_DATASET_CACHE_DIR / self.filename\n\n    async def load_dataset(self) -&gt; dict[str, Any]:\n        \"\"\"Load the dataset and convert it to AIPerf Conversation format.\n\n        This is the main entry point for dataset loading. Subclasses must implement\n        this method to define their specific loading logic.\n\n        Returns:\n            dict[str, Any]: A dictionary containing the loaded dataset data.\n\n        Raises:\n            NotImplementedError: Always raised as this is an abstract method.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    async def convert_to_conversations(\n        self, dataset: dict[str, Any]\n    ) -&gt; list[Conversation]:\n        \"\"\"\n        Convert the loaded dataset into a list of AIPerf Conversation objects.\n\n        This abstract method must be implemented by subclasses to define how the\n        specific dataset format should be converted to AIPerf's standardized\n        conversation format.\n\n        Args:\n            dataset: The parsed dataset object (typically from JSON). The exact\n                    structure depends on the specific dataset being loaded.\n\n        Returns:\n            list[Conversation]: A list of Conversation objects ready for use\n                               in AIPerf benchmarking.\n\n        Raises:\n            NotImplementedError: Always raised as this is an abstract method.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    async def _load_dataset(self, headers: dict[str, str]) -&gt; str:\n        \"\"\"\n        Load the dataset from the local cache or download it from the URL.\n\n        This method first checks if a cached version exists locally. If not, it downloads\n        the dataset from the configured URL and saves it to the cache for future use.\n\n        Args:\n            headers: Optional HTTP headers to include in the download request.\n                    Useful for setting Accept headers or authentication tokens.\n\n        Returns:\n            str: The raw dataset content as a string. Subclasses are responsible\n                 for parsing this content (e.g., JSON parsing).\n        \"\"\"\n        if not self.cache_filepath.exists():\n            self.info(f\"No local dataset cache found, downloading from {self.url}\")\n            record: RequestRecord = await self.get_request(self.url, headers=headers)\n            await self.close()\n\n            dataset = record.responses[0].text\n            self._save_to_local(dataset)\n            return dataset\n\n        return self._load_from_local()\n\n    def _save_to_local(self, dataset: str):\n        \"\"\"\n        Save the dataset to the local cache.\n\n        Args:\n            dataset: The raw dataset payload downloaded from the URL.\n        \"\"\"\n        self.info(f\"Saving {self.tag} dataset to local cache {self.cache_filepath}\")\n        try:\n            self.cache_filepath.parent.mkdir(parents=True, exist_ok=True)\n            with open(self.cache_filepath, \"w\") as f:\n                f.write(dataset)\n        except Exception as e:\n            raise DatasetLoaderError(f\"Error saving dataset to local cache: {e}\") from e\n\n    def _load_from_local(self) -&gt; str:\n        \"\"\"\n        Load the raw dataset payload from the local cache.\n\n        Returns:\n            str: The raw dataset payload.\n        \"\"\"\n        self.info(f\"Loading {self.tag} dataset from local cache {self.cache_filepath}\")\n        try:\n            with open(self.cache_filepath) as f:\n                return f.read()\n        except Exception as e:\n            raise DatasetLoaderError(\n                f\"Error loading dataset from local cache: {e}\"\n            ) from e\n\n    def is_valid_sequence(\n        self,\n        prompt_len: int,\n        output_len: int,\n        min_seq_len: int = 4,\n        max_prompt_len: int = 1024,\n        max_total_len: int = 2048,\n        skip_min_output_len_check: bool = False,\n    ) -&gt; bool:\n        \"\"\"\n        Validate a sequence based on prompt and output lengths.\n        Adopted from `vllm/benchmarks/benchmark_dataset.py`.\n\n        Args:\n            prompt_len: The length of the prompt.\n            output_len: The length of the output.\n            min_seq_len: The minimum length of the sequence.\n            max_prompt_len: The maximum length of the prompt.\n            max_total_len: The maximum length of the total sequence.\n            skip_min_output_len_check: Whether to skip the minimum output length check.\n\n        Returns:\n            True if the sequence is valid, False otherwise.\n        \"\"\"\n        # Check for invalid conditions\n        prompt_too_short = prompt_len &lt; min_seq_len\n        prompt_too_long = prompt_len &gt; max_prompt_len\n        output_too_short = (not skip_min_output_len_check) and (\n            output_len &lt; min_seq_len\n        )\n        combined_too_long = (prompt_len + output_len) &gt; max_total_len\n\n        return not (\n            prompt_too_short or output_too_short or prompt_too_long or combined_too_long\n        )\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.base_public_dataset.BasePublicDatasetLoader.convert_to_conversations","title":"<code>convert_to_conversations(dataset)</code>  <code>async</code>","text":"<p>Convert the loaded dataset into a list of AIPerf Conversation objects.</p> <p>This abstract method must be implemented by subclasses to define how the specific dataset format should be converted to AIPerf's standardized conversation format.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>dict[str, Any]</code> <p>The parsed dataset object (typically from JSON). The exact     structure depends on the specific dataset being loaded.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: A list of Conversation objects ready for use                in AIPerf benchmarking.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised as this is an abstract method.</p> Source code in <code>aiperf/dataset/loader/base_public_dataset.py</code> <pre><code>async def convert_to_conversations(\n    self, dataset: dict[str, Any]\n) -&gt; list[Conversation]:\n    \"\"\"\n    Convert the loaded dataset into a list of AIPerf Conversation objects.\n\n    This abstract method must be implemented by subclasses to define how the\n    specific dataset format should be converted to AIPerf's standardized\n    conversation format.\n\n    Args:\n        dataset: The parsed dataset object (typically from JSON). The exact\n                structure depends on the specific dataset being loaded.\n\n    Returns:\n        list[Conversation]: A list of Conversation objects ready for use\n                           in AIPerf benchmarking.\n\n    Raises:\n        NotImplementedError: Always raised as this is an abstract method.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement this method\")\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.base_public_dataset.BasePublicDatasetLoader.is_valid_sequence","title":"<code>is_valid_sequence(prompt_len, output_len, min_seq_len=4, max_prompt_len=1024, max_total_len=2048, skip_min_output_len_check=False)</code>","text":"<p>Validate a sequence based on prompt and output lengths. Adopted from <code>vllm/benchmarks/benchmark_dataset.py</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_len</code> <code>int</code> <p>The length of the prompt.</p> required <code>output_len</code> <code>int</code> <p>The length of the output.</p> required <code>min_seq_len</code> <code>int</code> <p>The minimum length of the sequence.</p> <code>4</code> <code>max_prompt_len</code> <code>int</code> <p>The maximum length of the prompt.</p> <code>1024</code> <code>max_total_len</code> <code>int</code> <p>The maximum length of the total sequence.</p> <code>2048</code> <code>skip_min_output_len_check</code> <code>bool</code> <p>Whether to skip the minimum output length check.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the sequence is valid, False otherwise.</p> Source code in <code>aiperf/dataset/loader/base_public_dataset.py</code> <pre><code>def is_valid_sequence(\n    self,\n    prompt_len: int,\n    output_len: int,\n    min_seq_len: int = 4,\n    max_prompt_len: int = 1024,\n    max_total_len: int = 2048,\n    skip_min_output_len_check: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Validate a sequence based on prompt and output lengths.\n    Adopted from `vllm/benchmarks/benchmark_dataset.py`.\n\n    Args:\n        prompt_len: The length of the prompt.\n        output_len: The length of the output.\n        min_seq_len: The minimum length of the sequence.\n        max_prompt_len: The maximum length of the prompt.\n        max_total_len: The maximum length of the total sequence.\n        skip_min_output_len_check: Whether to skip the minimum output length check.\n\n    Returns:\n        True if the sequence is valid, False otherwise.\n    \"\"\"\n    # Check for invalid conditions\n    prompt_too_short = prompt_len &lt; min_seq_len\n    prompt_too_long = prompt_len &gt; max_prompt_len\n    output_too_short = (not skip_min_output_len_check) and (\n        output_len &lt; min_seq_len\n    )\n    combined_too_long = (prompt_len + output_len) &gt; max_total_len\n\n    return not (\n        prompt_too_short or output_too_short or prompt_too_long or combined_too_long\n    )\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.base_public_dataset.BasePublicDatasetLoader.load_dataset","title":"<code>load_dataset()</code>  <code>async</code>","text":"<p>Load the dataset and convert it to AIPerf Conversation format.</p> <p>This is the main entry point for dataset loading. Subclasses must implement this method to define their specific loading logic.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the loaded dataset data.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always raised as this is an abstract method.</p> Source code in <code>aiperf/dataset/loader/base_public_dataset.py</code> <pre><code>async def load_dataset(self) -&gt; dict[str, Any]:\n    \"\"\"Load the dataset and convert it to AIPerf Conversation format.\n\n    This is the main entry point for dataset loading. Subclasses must implement\n    this method to define their specific loading logic.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the loaded dataset data.\n\n    Raises:\n        NotImplementedError: Always raised as this is an abstract method.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement this method\")\n</code></pre>"},{"location":"api/#aiperfdatasetloadermixins","title":"aiperf.dataset.loader.mixins","text":""},{"location":"api/#aiperf.dataset.loader.mixins.MediaConversionMixin","title":"<code>MediaConversionMixin</code>","text":"<p>Mixin providing shared media conversion functionality for dataset loaders. It is used to construct text, image, and audio data from a CustomDatasetT object.</p> Source code in <code>aiperf/dataset/loader/mixins.py</code> <pre><code>class MediaConversionMixin:\n    \"\"\"Mixin providing shared media conversion functionality for dataset loaders.\n    It is used to construct text, image, and audio data from a CustomDatasetT object.\n    \"\"\"\n\n    @property\n    def _media_classes(self) -&gt; list[type[MediaT]]:\n        \"\"\"Dynamically get all Media subclasses.\"\"\"\n        return Media.__subclasses__()\n\n    def convert_to_media_objects(\n        self, data: CustomDatasetT, name: str = \"\"\n    ) -&gt; dict[str, list[MediaT]]:\n        \"\"\"Convert all custom dataset into media objects.\n\n        Args:\n            data: The custom dataset to convert into media objects.\n            name: The name of the media field.\n\n        Returns:\n            A dictionary of media objects.\n        \"\"\"\n        media_objects: dict[str, list[MediaT]] = {}\n        for media_class in self._media_classes:\n            media_objects[media_class.media_type] = self._convert_to_media_objects(\n                data,\n                media_class=media_class,\n                field=media_class.media_type,\n                name=name,\n            )\n        return media_objects\n\n    def _convert_to_media_objects(\n        self,\n        data: CustomDatasetT,\n        media_class: type[MediaT],\n        field: str,\n        name: str = \"\",\n    ) -&gt; list[MediaT]:\n        \"\"\"Generic method to construct media objects from a CustomDatasetT object.\n\n        Args:\n            data: The custom dataset to construct media objects from.\n            media_class: The target media class (Text, Image, or Audio).\n            field: The name of the field (e.g., 'text', 'image', 'audio').\n            name: The name of the media field.\n\n        Returns:\n            A list of media objects.\n        \"\"\"\n        # Check singular field first\n        value = getattr(data, field, None)\n        if value is not None:\n            return [media_class(name=name, contents=[value])]\n\n        # Check plural field\n        values = getattr(data, f\"{field}s\", None)\n        if values is None or not isinstance(values, Iterable):\n            return []\n\n        # If already correct media objects, return as is\n        if all(isinstance(v, media_class) for v in values):\n            return values\n\n        return [media_class(name=name, contents=values)]\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.mixins.MediaConversionMixin.convert_to_media_objects","title":"<code>convert_to_media_objects(data, name='')</code>","text":"<p>Convert all custom dataset into media objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>CustomDatasetT</code> <p>The custom dataset to convert into media objects.</p> required <code>name</code> <code>str</code> <p>The name of the media field.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict[str, list[MediaT]]</code> <p>A dictionary of media objects.</p> Source code in <code>aiperf/dataset/loader/mixins.py</code> <pre><code>def convert_to_media_objects(\n    self, data: CustomDatasetT, name: str = \"\"\n) -&gt; dict[str, list[MediaT]]:\n    \"\"\"Convert all custom dataset into media objects.\n\n    Args:\n        data: The custom dataset to convert into media objects.\n        name: The name of the media field.\n\n    Returns:\n        A dictionary of media objects.\n    \"\"\"\n    media_objects: dict[str, list[MediaT]] = {}\n    for media_class in self._media_classes:\n        media_objects[media_class.media_type] = self._convert_to_media_objects(\n            data,\n            media_class=media_class,\n            field=media_class.media_type,\n            name=name,\n        )\n    return media_objects\n</code></pre>"},{"location":"api/#aiperfdatasetloadermodels","title":"aiperf.dataset.loader.models","text":""},{"location":"api/#aiperf.dataset.loader.models.CustomDatasetT","title":"<code>CustomDatasetT = TypeVar('CustomDatasetT', bound=(SingleTurn | MultiTurn | RandomPool | MooncakeTrace))</code>  <code>module-attribute</code>","text":"<p>A union type of all custom data types.</p>"},{"location":"api/#aiperf.dataset.loader.models.MooncakeTrace","title":"<code>MooncakeTrace</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for Mooncake trace data.</p> <p>See https://github.com/kvcache-ai/Mooncake for more details.</p> <p>Example:</p> <pre><code>{\"timestamp\": 1000, \"input_length\": 10, \"output_length\": 4, \"hash_ids\": [123, 456]}\n</code></pre> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>class MooncakeTrace(AIPerfBaseModel):\n    \"\"\"Defines the schema for Mooncake trace data.\n\n    See https://github.com/kvcache-ai/Mooncake for more details.\n\n    Example:\n    ```json\n    {\"timestamp\": 1000, \"input_length\": 10, \"output_length\": 4, \"hash_ids\": [123, 456]}\n    ```\n    \"\"\"\n\n    type: Literal[CustomDatasetType.MOONCAKE_TRACE] = CustomDatasetType.MOONCAKE_TRACE\n\n    input_length: int = Field(..., description=\"The input sequence length of a request\")\n    output_length: int = Field(\n        ..., description=\"The output sequence length of a request\"\n    )\n    hash_ids: list[int] = Field(..., description=\"The hash ids of a request\")\n    timestamp: int = Field(..., description=\"The timestamp of a request\")\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.MultiTurn","title":"<code>MultiTurn</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for multi-turn conversations.</p> <p>The multi-turn custom dataset   - supports multi-modal data (e.g. text, image, audio)   - supports multi-turn features (e.g. delay, sessions, etc.)   - supports client-side batching for each data (e.g. batch size &gt; 1)</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>class MultiTurn(AIPerfBaseModel):\n    \"\"\"Defines the schema for multi-turn conversations.\n\n    The multi-turn custom dataset\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports multi-turn features (e.g. delay, sessions, etc.)\n      - supports client-side batching for each data (e.g. batch size &gt; 1)\n    \"\"\"\n\n    type: Literal[CustomDatasetType.MULTI_TURN] = CustomDatasetType.MULTI_TURN\n\n    session_id: str | None = Field(\n        None, description=\"Unique identifier for the conversation session\"\n    )\n    turns: list[SingleTurn] = Field(\n        ..., description=\"List of turns in the conversation\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_turns_not_empty(self) -&gt; \"MultiTurn\":\n        \"\"\"Ensure at least one turn is provided\"\"\"\n        if not self.turns:\n            raise ValueError(\"At least one turn must be provided\")\n        return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.MultiTurn.validate_turns_not_empty","title":"<code>validate_turns_not_empty()</code>","text":"<p>Ensure at least one turn is provided</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_turns_not_empty(self) -&gt; \"MultiTurn\":\n    \"\"\"Ensure at least one turn is provided\"\"\"\n    if not self.turns:\n        raise ValueError(\"At least one turn must be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.RandomPool","title":"<code>RandomPool</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for random pool data entry.</p> <p>The random pool custom dataset   - supports multi-modal data (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch size &gt; 1)   - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)   - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>class RandomPool(AIPerfBaseModel):\n    \"\"\"Defines the schema for random pool data entry.\n\n    The random pool custom dataset\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch size &gt; 1)\n      - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)\n      - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)\n    \"\"\"\n\n    type: Literal[CustomDatasetType.RANDOM_POOL] = CustomDatasetType.RANDOM_POOL\n\n    text: str | None = Field(None, description=\"Simple text string content\")\n    texts: list[str] | list[Text] | None = Field(\n        None,\n        description=\"List of text strings or Text objects format\",\n    )\n    image: str | None = Field(None, description=\"Simple image string content\")\n    images: list[str] | list[Image] | None = Field(\n        None,\n        description=\"List of image strings or Image objects format\",\n    )\n    audio: str | None = Field(None, description=\"Simple audio string content\")\n    audios: list[str] | list[Audio] | None = Field(\n        None,\n        description=\"List of audio strings or Audio objects format\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_mutually_exclusive_fields(self) -&gt; \"RandomPool\":\n        \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n        if self.text and self.texts:\n            raise ValueError(\"text and texts cannot be set together\")\n        if self.image and self.images:\n            raise ValueError(\"image and images cannot be set together\")\n        if self.audio and self.audios:\n            raise ValueError(\"audio and audios cannot be set together\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_at_least_one_modality(self) -&gt; \"RandomPool\":\n        \"\"\"Ensure at least one modality is provided\"\"\"\n        if not any(\n            [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n        ):\n            raise ValueError(\"At least one modality must be provided\")\n        return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.RandomPool.validate_at_least_one_modality","title":"<code>validate_at_least_one_modality()</code>","text":"<p>Ensure at least one modality is provided</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_at_least_one_modality(self) -&gt; \"RandomPool\":\n    \"\"\"Ensure at least one modality is provided\"\"\"\n    if not any(\n        [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n    ):\n        raise ValueError(\"At least one modality must be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.RandomPool.validate_mutually_exclusive_fields","title":"<code>validate_mutually_exclusive_fields()</code>","text":"<p>Ensure mutually exclusive fields are not set together</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_mutually_exclusive_fields(self) -&gt; \"RandomPool\":\n    \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n    if self.text and self.texts:\n        raise ValueError(\"text and texts cannot be set together\")\n    if self.image and self.images:\n        raise ValueError(\"image and images cannot be set together\")\n    if self.audio and self.audios:\n        raise ValueError(\"audio and audios cannot be set together\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.SingleTurn","title":"<code>SingleTurn</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Defines the schema for single-turn data.</p> <p>User can use this format to quickly provide a custom single turn dataset. Each line in the file will be treated as a single turn conversation.</p> <p>The single turn type   - supports multi-modal (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch_size &gt; 1)   - DOES NOT support multi-turn features (e.g. session_id)</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>class SingleTurn(AIPerfBaseModel):\n    \"\"\"Defines the schema for single-turn data.\n\n    User can use this format to quickly provide a custom single turn dataset.\n    Each line in the file will be treated as a single turn conversation.\n\n    The single turn type\n      - supports multi-modal (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch_size &gt; 1)\n      - DOES NOT support multi-turn features (e.g. session_id)\n    \"\"\"\n\n    type: Literal[CustomDatasetType.SINGLE_TURN] = CustomDatasetType.SINGLE_TURN\n\n    # TODO (TL-89): investigate if we only want to support single field for each modality\n    text: str | None = Field(None, description=\"Simple text string content\")\n    texts: list[str] | list[Text] | None = Field(\n        None,\n        description=\"List of text strings or Text objects format\",\n    )\n    image: str | None = Field(None, description=\"Simple image string content\")\n    images: list[str] | list[Image] | None = Field(\n        None,\n        description=\"List of image strings or Image objects format\",\n    )\n    audio: str | None = Field(None, description=\"Simple audio string content\")\n    audios: list[str] | list[Audio] | None = Field(\n        None,\n        description=\"List of audio strings or Audio objects format\",\n    )\n    timestamp: int | None = Field(\n        default=None, description=\"Timestamp of the turn in milliseconds.\"\n    )\n    delay: int | None = Field(\n        default=None,\n        description=\"Amount of milliseconds to wait before sending the turn.\",\n    )\n    role: str | None = Field(default=None, description=\"Role of the turn.\")\n\n    @model_validator(mode=\"after\")\n    def validate_mutually_exclusive_fields(self) -&gt; \"SingleTurn\":\n        \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n        if self.text and self.texts:\n            raise ValueError(\"text and texts cannot be set together\")\n        if self.image and self.images:\n            raise ValueError(\"image and images cannot be set together\")\n        if self.audio and self.audios:\n            raise ValueError(\"audio and audios cannot be set together\")\n        if self.timestamp and self.delay:\n            raise ValueError(\"timestamp and delay cannot be set together\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_at_least_one_modality(self) -&gt; \"SingleTurn\":\n        \"\"\"Ensure at least one modality is provided\"\"\"\n        if not any(\n            [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n        ):\n            raise ValueError(\"At least one modality must be provided\")\n        return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.SingleTurn.validate_at_least_one_modality","title":"<code>validate_at_least_one_modality()</code>","text":"<p>Ensure at least one modality is provided</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_at_least_one_modality(self) -&gt; \"SingleTurn\":\n    \"\"\"Ensure at least one modality is provided\"\"\"\n    if not any(\n        [self.text, self.texts, self.image, self.images, self.audio, self.audios]\n    ):\n        raise ValueError(\"At least one modality must be provided\")\n    return self\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.models.SingleTurn.validate_mutually_exclusive_fields","title":"<code>validate_mutually_exclusive_fields()</code>","text":"<p>Ensure mutually exclusive fields are not set together</p> Source code in <code>aiperf/dataset/loader/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_mutually_exclusive_fields(self) -&gt; \"SingleTurn\":\n    \"\"\"Ensure mutually exclusive fields are not set together\"\"\"\n    if self.text and self.texts:\n        raise ValueError(\"text and texts cannot be set together\")\n    if self.image and self.images:\n        raise ValueError(\"image and images cannot be set together\")\n    if self.audio and self.audios:\n        raise ValueError(\"audio and audios cannot be set together\")\n    if self.timestamp and self.delay:\n        raise ValueError(\"timestamp and delay cannot be set together\")\n    return self\n</code></pre>"},{"location":"api/#aiperfdatasetloadermooncake_trace","title":"aiperf.dataset.loader.mooncake_trace","text":""},{"location":"api/#aiperf.dataset.loader.mooncake_trace.MooncakeTraceDatasetLoader","title":"<code>MooncakeTraceDatasetLoader</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>A dataset loader that loads Mooncake trace data from a file.</p> <p>Loads Mooncake trace data from a file and converts the data into a list of conversations for dataset manager.</p> <p>Each line in the file represents a single trace entry and will be converted to a separate conversation with a unique session ID.</p> <p>Example: Fixed schedule version (Each line is a distinct session. Multi-turn is NOT supported)</p> <pre><code>{\"timestamp\": 1000, \"input_length\": 300, \"output_length\": 40, \"hash_ids\": [123, 456]}\n</code></pre> Source code in <code>aiperf/dataset/loader/mooncake_trace.py</code> <pre><code>@implements_protocol(CustomDatasetLoaderProtocol)\n@CustomDatasetFactory.register(CustomDatasetType.MOONCAKE_TRACE)\nclass MooncakeTraceDatasetLoader(AIPerfLoggerMixin):\n    \"\"\"A dataset loader that loads Mooncake trace data from a file.\n\n    Loads Mooncake trace data from a file and converts the data into\n    a list of conversations for dataset manager.\n\n    Each line in the file represents a single trace entry and will be\n    converted to a separate conversation with a unique session ID.\n\n    Example:\n    Fixed schedule version (Each line is a distinct session. Multi-turn is NOT supported)\n    ```json\n    {\"timestamp\": 1000, \"input_length\": 300, \"output_length\": 40, \"hash_ids\": [123, 456]}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str,\n        prompt_generator: PromptGenerator,\n        user_config: UserConfig,\n        **kwargs,\n    ):\n        self.filename = filename\n        self.prompt_generator = prompt_generator\n        self.user_config = user_config\n        self._skipped_traces = 0\n        self._start_offset = user_config.input.fixed_schedule_start_offset\n        self._end_offset = user_config.input.fixed_schedule_end_offset\n        super().__init__(user_config=user_config, **kwargs)\n\n    def load_dataset(self) -&gt; dict[str, list[MooncakeTrace]]:\n        \"\"\"Load Mooncake trace data from a file.\n\n        Returns:\n            A dictionary of session_id and list of Mooncake trace data.\n        \"\"\"\n        data: dict[str, list[MooncakeTrace]] = defaultdict(list)\n\n        with open(self.filename) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                trace_data = MooncakeTrace.model_validate_json(line)\n\n                if not self._timestamp_within_offsets(trace_data.timestamp):\n                    self._skipped_traces += 1\n                    continue  # Skip traces before or after the fixed schedule offset\n\n                session_id = str(uuid.uuid4())\n                data[session_id].append(trace_data)\n\n        if self._skipped_traces &gt; 0:\n            self.info(\n                f\"Skipped {self._skipped_traces:,} traces because they were \"\n                f\"before the start offset of {self._start_offset} or \"\n                f\"after the end offset of {self._end_offset}\"\n            )\n        self.debug(lambda: f\"Loaded {len(data):,} traces from {self.filename}\")\n\n        return data\n\n    def _timestamp_within_offsets(self, timestamp: int) -&gt; bool:\n        return (self._start_offset is None or timestamp &gt;= self._start_offset) and (\n            self._end_offset is None or timestamp &lt;= self._end_offset\n        )\n\n    def convert_to_conversations(\n        self, data: dict[str, list[MooncakeTrace]]\n    ) -&gt; list[Conversation]:\n        \"\"\"Convert all the Mooncake trace data to conversation objects.\n\n        Args:\n            data: A dictionary of session_id and list of Mooncake trace data.\n\n        Returns:\n            A list of conversations.\n        \"\"\"\n        conversations = []\n        for session_id, traces in data.items():\n            conversation = Conversation(session_id=session_id)\n            for trace in traces:\n                prompt = self.prompt_generator.generate(\n                    mean=trace.input_length,\n                    stddev=0,\n                    hash_ids=trace.hash_ids,\n                )\n                turn = Turn(\n                    timestamp=trace.timestamp,\n                    texts=[Text(name=\"text\", contents=[prompt])],\n                    max_tokens=trace.output_length,\n                )\n                conversation.turns.append(turn)\n            conversations.append(conversation)\n        return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.mooncake_trace.MooncakeTraceDatasetLoader.convert_to_conversations","title":"<code>convert_to_conversations(data)</code>","text":"<p>Convert all the Mooncake trace data to conversation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[MooncakeTrace]]</code> <p>A dictionary of session_id and list of Mooncake trace data.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>A list of conversations.</p> Source code in <code>aiperf/dataset/loader/mooncake_trace.py</code> <pre><code>def convert_to_conversations(\n    self, data: dict[str, list[MooncakeTrace]]\n) -&gt; list[Conversation]:\n    \"\"\"Convert all the Mooncake trace data to conversation objects.\n\n    Args:\n        data: A dictionary of session_id and list of Mooncake trace data.\n\n    Returns:\n        A list of conversations.\n    \"\"\"\n    conversations = []\n    for session_id, traces in data.items():\n        conversation = Conversation(session_id=session_id)\n        for trace in traces:\n            prompt = self.prompt_generator.generate(\n                mean=trace.input_length,\n                stddev=0,\n                hash_ids=trace.hash_ids,\n            )\n            turn = Turn(\n                timestamp=trace.timestamp,\n                texts=[Text(name=\"text\", contents=[prompt])],\n                max_tokens=trace.output_length,\n            )\n            conversation.turns.append(turn)\n        conversations.append(conversation)\n    return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.mooncake_trace.MooncakeTraceDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load Mooncake trace data from a file.</p> <p>Returns:</p> Type Description <code>dict[str, list[MooncakeTrace]]</code> <p>A dictionary of session_id and list of Mooncake trace data.</p> Source code in <code>aiperf/dataset/loader/mooncake_trace.py</code> <pre><code>def load_dataset(self) -&gt; dict[str, list[MooncakeTrace]]:\n    \"\"\"Load Mooncake trace data from a file.\n\n    Returns:\n        A dictionary of session_id and list of Mooncake trace data.\n    \"\"\"\n    data: dict[str, list[MooncakeTrace]] = defaultdict(list)\n\n    with open(self.filename) as f:\n        for line in f:\n            if (line := line.strip()) == \"\":\n                continue  # Skip empty lines\n\n            trace_data = MooncakeTrace.model_validate_json(line)\n\n            if not self._timestamp_within_offsets(trace_data.timestamp):\n                self._skipped_traces += 1\n                continue  # Skip traces before or after the fixed schedule offset\n\n            session_id = str(uuid.uuid4())\n            data[session_id].append(trace_data)\n\n    if self._skipped_traces &gt; 0:\n        self.info(\n            f\"Skipped {self._skipped_traces:,} traces because they were \"\n            f\"before the start offset of {self._start_offset} or \"\n            f\"after the end offset of {self._end_offset}\"\n        )\n    self.debug(lambda: f\"Loaded {len(data):,} traces from {self.filename}\")\n\n    return data\n</code></pre>"},{"location":"api/#aiperfdatasetloadermulti_turn","title":"aiperf.dataset.loader.multi_turn","text":""},{"location":"api/#aiperf.dataset.loader.multi_turn.MultiTurnDatasetLoader","title":"<code>MultiTurnDatasetLoader</code>","text":"<p>               Bases: <code>MediaConversionMixin</code></p> <p>A dataset loader that loads multi-turn data from a file.</p> <p>The multi-turn type   - supports multi-modal data (e.g. text, image, audio)   - supports multi-turn features (e.g. delay, sessions, etc.)   - supports client-side batching for each data (e.g. batch_size &gt; 1)</p> <p>NOTE: If the user specifies multiple multi-turn entries with same session ID, the loader will group them together. If the timestamps are specified, they will be sorted in ascending order later in the timing manager.</p> <p>Examples: 1. Simple version</p> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"text\": \"Hello\", \"image\": \"url\", \"delay\": 0},\n        {\"text\": \"Hi there\", \"delay\": 1000}\n    ]\n}\n</code></pre> <ol> <li>Batched version</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/1.png\", \"/path/2.png\"]},\n        {\"texts\": [\"What is in the image?\", \"What is AI?\"], \"images\": [\"/path/3.png\", \"/path/4.png\"]}\n    ]\n}\n</code></pre> <ol> <li>Fixed schedule version</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n        {\"timestamp\": 1000, \"text\": \"Who are you?\"}\n    ]\n}\n</code></pre> <ol> <li>Time delayed version</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\"delay\": 0, \"text\": \"What is deep learning?\"},\n        {\"delay\": 1000, \"text\": \"Who are you?\"}\n    ]\n}\n</code></pre> <ol> <li>full-featured version (multi-batch, multi-modal, multi-fielded, session-based, etc.)</li> </ol> <pre><code>{\n    \"session_id\": \"session_123\",\n    \"turns\": [\n        {\n            \"timestamp\": 1234,\n            \"texts\": [\n                {\"name\": \"text_field_a\", \"contents\": [\"hello\", \"world\"]},\n                {\"name\": \"text_field_b\", \"contents\": [\"hi there\"]}\n            ],\n            \"images\": [\n                {\"name\": \"image_field_a\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n                {\"name\": \"image_field_b\", \"contents\": [\"/path/3.png\"]}\n            ]\n        }\n    ]\n}\n</code></pre> Source code in <code>aiperf/dataset/loader/multi_turn.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.MULTI_TURN)\nclass MultiTurnDatasetLoader(MediaConversionMixin):\n    \"\"\"A dataset loader that loads multi-turn data from a file.\n\n    The multi-turn type\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports multi-turn features (e.g. delay, sessions, etc.)\n      - supports client-side batching for each data (e.g. batch_size &gt; 1)\n\n    NOTE: If the user specifies multiple multi-turn entries with same session ID,\n    the loader will group them together. If the timestamps are specified, they will\n    be sorted in ascending order later in the timing manager.\n\n    Examples:\n    1. Simple version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"text\": \"Hello\", \"image\": \"url\", \"delay\": 0},\n            {\"text\": \"Hi there\", \"delay\": 1000}\n        ]\n    }\n    ```\n\n    2. Batched version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/1.png\", \"/path/2.png\"]},\n            {\"texts\": [\"What is in the image?\", \"What is AI?\"], \"images\": [\"/path/3.png\", \"/path/4.png\"]}\n        ]\n    }\n    ```\n\n    3. Fixed schedule version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n            {\"timestamp\": 1000, \"text\": \"Who are you?\"}\n        ]\n    }\n    ```\n\n    4. Time delayed version\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\"delay\": 0, \"text\": \"What is deep learning?\"},\n            {\"delay\": 1000, \"text\": \"Who are you?\"}\n        ]\n    }\n    ```\n\n    5. full-featured version (multi-batch, multi-modal, multi-fielded, session-based, etc.)\n    ```json\n    {\n        \"session_id\": \"session_123\",\n        \"turns\": [\n            {\n                \"timestamp\": 1234,\n                \"texts\": [\n                    {\"name\": \"text_field_a\", \"contents\": [\"hello\", \"world\"]},\n                    {\"name\": \"text_field_b\", \"contents\": [\"hi there\"]}\n                ],\n                \"images\": [\n                    {\"name\": \"image_field_a\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n                    {\"name\": \"image_field_b\", \"contents\": [\"/path/3.png\"]}\n                ]\n            }\n        ]\n    }\n    ```\n    \"\"\"\n\n    def __init__(self, filename: str):\n        self.filename = filename\n\n    def load_dataset(self) -&gt; dict[str, list[MultiTurn]]:\n        \"\"\"Load multi-turn data from a JSONL file.\n\n        Each line represents a complete multi-turn conversation with its own\n        session_id and multiple turns.\n\n        Returns:\n            A dictionary mapping session_id to list of MultiTurn objects.\n        \"\"\"\n        data: dict[str, list[MultiTurn]] = defaultdict(list)\n\n        with open(self.filename) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                multi_turn_data = MultiTurn.model_validate_json(line)\n                session_id = multi_turn_data.session_id or str(uuid.uuid4())\n                data[session_id].append(multi_turn_data)\n\n        return data\n\n    def convert_to_conversations(\n        self, data: dict[str, list[MultiTurn]]\n    ) -&gt; list[Conversation]:\n        \"\"\"Convert multi-turn data to conversation objects.\n\n        Args:\n            data: A dictionary mapping session_id to list of MultiTurn objects.\n\n        Returns:\n            A list of conversations.\n        \"\"\"\n        conversations = []\n        for session_id, multi_turns in data.items():\n            conversation = Conversation(session_id=session_id)\n\n            # Process all MultiTurn objects for this session\n            for multi_turn in multi_turns:\n                for single_turn in multi_turn.turns:\n                    media = self.convert_to_media_objects(single_turn)\n                    conversation.turns.append(\n                        Turn(\n                            texts=media[MediaType.TEXT],\n                            images=media[MediaType.IMAGE],\n                            audios=media[MediaType.AUDIO],\n                            timestamp=single_turn.timestamp,\n                            delay=single_turn.delay,\n                            role=single_turn.role,\n                        )\n                    )\n            conversations.append(conversation)\n        return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.multi_turn.MultiTurnDatasetLoader.convert_to_conversations","title":"<code>convert_to_conversations(data)</code>","text":"<p>Convert multi-turn data to conversation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[MultiTurn]]</code> <p>A dictionary mapping session_id to list of MultiTurn objects.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>A list of conversations.</p> Source code in <code>aiperf/dataset/loader/multi_turn.py</code> <pre><code>def convert_to_conversations(\n    self, data: dict[str, list[MultiTurn]]\n) -&gt; list[Conversation]:\n    \"\"\"Convert multi-turn data to conversation objects.\n\n    Args:\n        data: A dictionary mapping session_id to list of MultiTurn objects.\n\n    Returns:\n        A list of conversations.\n    \"\"\"\n    conversations = []\n    for session_id, multi_turns in data.items():\n        conversation = Conversation(session_id=session_id)\n\n        # Process all MultiTurn objects for this session\n        for multi_turn in multi_turns:\n            for single_turn in multi_turn.turns:\n                media = self.convert_to_media_objects(single_turn)\n                conversation.turns.append(\n                    Turn(\n                        texts=media[MediaType.TEXT],\n                        images=media[MediaType.IMAGE],\n                        audios=media[MediaType.AUDIO],\n                        timestamp=single_turn.timestamp,\n                        delay=single_turn.delay,\n                        role=single_turn.role,\n                    )\n                )\n        conversations.append(conversation)\n    return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.multi_turn.MultiTurnDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load multi-turn data from a JSONL file.</p> <p>Each line represents a complete multi-turn conversation with its own session_id and multiple turns.</p> <p>Returns:</p> Type Description <code>dict[str, list[MultiTurn]]</code> <p>A dictionary mapping session_id to list of MultiTurn objects.</p> Source code in <code>aiperf/dataset/loader/multi_turn.py</code> <pre><code>def load_dataset(self) -&gt; dict[str, list[MultiTurn]]:\n    \"\"\"Load multi-turn data from a JSONL file.\n\n    Each line represents a complete multi-turn conversation with its own\n    session_id and multiple turns.\n\n    Returns:\n        A dictionary mapping session_id to list of MultiTurn objects.\n    \"\"\"\n    data: dict[str, list[MultiTurn]] = defaultdict(list)\n\n    with open(self.filename) as f:\n        for line in f:\n            if (line := line.strip()) == \"\":\n                continue  # Skip empty lines\n\n            multi_turn_data = MultiTurn.model_validate_json(line)\n            session_id = multi_turn_data.session_id or str(uuid.uuid4())\n            data[session_id].append(multi_turn_data)\n\n    return data\n</code></pre>"},{"location":"api/#aiperfdatasetloaderprotocol","title":"aiperf.dataset.loader.protocol","text":""},{"location":"api/#aiperfdatasetloaderrandom_pool","title":"aiperf.dataset.loader.random_pool","text":""},{"location":"api/#aiperf.dataset.loader.random_pool.RandomPoolDatasetLoader","title":"<code>RandomPoolDatasetLoader</code>","text":"<p>               Bases: <code>MediaConversionMixin</code></p> <p>A dataset loader that loads data from a single file or a directory.</p> <p>Each line in the file represents single-turn conversation data, and files create individual pools for random sampling:   - Single file: All lines form one single pool (to be randomly sampled from)   - Directory: Each file becomes a separate pool, then pools are randomly sampled                and merged into conversations later.</p> <p>The random pool custom dataset   - supports multi-modal data (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch size &gt; 1)   - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)   - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)</p> <p>Example:</p> <ol> <li>Single file</li> </ol> <pre><code>{\"text\": \"Who are you?\", \"image\": \"/path/to/image1.png\"}\n{\"text\": \"Explain what is the meaning of life.\", \"image\": \"/path/to/image2.png\"}\n...\n</code></pre> <p>The file will form a single pool of text and image data that will be used to generate conversations.</p> <ol> <li>Directory</li> </ol> <p>Directory will be useful if user wants to   - create multiple pools of different modalities separately (e.g. text, image)   - specify different field names for the same modality.</p> <p>data/queries.jsonl</p> <pre><code>{\"texts\": [{\"name\": \"query\", \"contents\": [\"Who are you?\"]}]}\n{\"texts\": [{\"name\": \"query\", \"contents\": [\"What is the meaning of life?\"]}]}\n...\n</code></pre> <p>data/passages.jsonl</p> <pre><code>{\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a cat.\"]}]}\n{\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a dog.\"]}]}\n...\n</code></pre> <p>The loader will create two separate pools for each file: queries and passages. Each pool is a text dataset with a different field name (e.g. query, passage), and loader will later sample from these two pools to create conversations.</p> Source code in <code>aiperf/dataset/loader/random_pool.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.RANDOM_POOL)\nclass RandomPoolDatasetLoader(MediaConversionMixin):\n    \"\"\"A dataset loader that loads data from a single file or a directory.\n\n    Each line in the file represents single-turn conversation data,\n    and files create individual pools for random sampling:\n      - Single file: All lines form one single pool (to be randomly sampled from)\n      - Directory: Each file becomes a separate pool, then pools are randomly sampled\n                   and merged into conversations later.\n\n    The random pool custom dataset\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch size &gt; 1)\n      - supports named fields for each modality (e.g. text_field_a, text_field_b, etc.)\n      - DOES NOT support multi-turn or its features (e.g. delay, sessions, etc.)\n\n    Example:\n\n    1. Single file\n    ```jsonl\n    {\"text\": \"Who are you?\", \"image\": \"/path/to/image1.png\"}\n    {\"text\": \"Explain what is the meaning of life.\", \"image\": \"/path/to/image2.png\"}\n    ...\n    ```\n    The file will form a single pool of text and image data that will be used\n    to generate conversations.\n\n    2. Directory\n\n    Directory will be useful if user wants to\n      - create multiple pools of different modalities separately (e.g. text, image)\n      - specify different field names for the same modality.\n\n    data/queries.jsonl\n    ```jsonl\n    {\"texts\": [{\"name\": \"query\", \"contents\": [\"Who are you?\"]}]}\n    {\"texts\": [{\"name\": \"query\", \"contents\": [\"What is the meaning of life?\"]}]}\n    ...\n    ```\n\n    data/passages.jsonl\n    ```jsonl\n    {\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a cat.\"]}]}\n    {\"texts\": [{\"name\": \"passage\", \"contents\": [\"I am a dog.\"]}]}\n    ...\n    ```\n\n    The loader will create two separate pools for each file: queries and passages.\n    Each pool is a text dataset with a different field name (e.g. query, passage),\n    and loader will later sample from these two pools to create conversations.\n    \"\"\"\n\n    def __init__(self, filename: str, num_conversations: int = 1):\n        self.filename = filename\n        self.num_conversations = num_conversations\n\n    def load_dataset(self) -&gt; dict[Filename, list[RandomPool]]:\n        \"\"\"Load random pool data from a file or directory.\n\n        If filename is a file, reads and parses using the RandomPool model.\n        If filename is a directory, reads each file in the directory and merges\n        items with different modality names into combined RandomPool objects.\n\n        Returns:\n            A dictionary mapping filename to list of RandomPool objects.\n        \"\"\"\n        path = Path(self.filename)\n\n        if path.is_file():\n            dataset_pool = self._load_dataset_from_file(path)\n            return {path.name: dataset_pool}\n\n        return self._load_dataset_from_dir(path)\n\n    def _load_dataset_from_file(self, file_path: Path) -&gt; list[RandomPool]:\n        \"\"\"Load random pool data from a single file.\n\n        Args:\n            file_path: The path to the file containing the data.\n\n        Returns:\n            A list of RandomPool objects.\n        \"\"\"\n        dataset_pool: list[RandomPool] = []\n\n        with open(file_path) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                random_pool_data = RandomPool.model_validate_json(line)\n                dataset_pool.append(random_pool_data)\n\n        return dataset_pool\n\n    def _load_dataset_from_dir(\n        self, dir_path: Path\n    ) -&gt; dict[Filename, list[RandomPool]]:\n        \"\"\"Load random pool data from all files in a directory.\n\n        Args:\n            dir_path: The path to the directory containing the files.\n\n        Returns:\n            A dictionary mapping filename to list of RandomPool objects.\n        \"\"\"\n        data: dict[Filename, list[RandomPool]] = defaultdict(list)\n\n        for file_path in dir_path.iterdir():\n            if file_path.is_file():\n                dataset_pool = self._load_dataset_from_file(file_path)\n                data[file_path.name].extend(dataset_pool)\n\n        return data\n\n    def convert_to_conversations(\n        self, data: dict[Filename, list[RandomPool]]\n    ) -&gt; list[Conversation]:\n        \"\"\"Convert random pool data to conversation objects.\n\n        Each RandomPool entry becomes a single-turn conversation with a unique session ID.\n\n        Args:\n            data: A dictionary mapping filename to list of RandomPool objects.\n\n        Returns:\n            A list of conversations.\n        \"\"\"\n        conversations = [\n            Conversation(session_id=str(uuid.uuid4()))\n            for _ in range(self.num_conversations)\n        ]\n\n        # F x N (F: num of files, N: num of conversations)\n        sampled_dataset: dict[Filename, list[Turn]] = {}\n\n        # Randomly sample (with replacement) from each dataset pool\n        for filename, dataset_pool in data.items():\n            samples = random.choices(dataset_pool, k=self.num_conversations)\n            turns: list[Turn] = []\n            for sample in samples:\n                media = self.convert_to_media_objects(sample, name=Path(filename).stem)\n                turns.append(\n                    Turn(\n                        texts=media[MediaType.TEXT],\n                        images=media[MediaType.IMAGE],\n                        audios=media[MediaType.AUDIO],\n                    )\n                )\n            sampled_dataset[filename] = turns\n\n        # Merge turns for each conversation\n        for i, batched_turns in enumerate(zip(*sampled_dataset.values(), strict=False)):\n            turn = self._merge_turns(batched_turns)\n            conversations[i].turns.append(turn)\n\n        return conversations\n\n    def _merge_turns(self, turns: list[Turn]) -&gt; Turn:\n        \"\"\"Merge turns into a single turn.\n\n        Args:\n            turns: A list of turns.\n\n        Returns:\n            A single turn.\n        \"\"\"\n        merged_turn = Turn(\n            texts=[text for turn in turns for text in turn.texts],\n            images=[image for turn in turns for image in turn.images],\n            audios=[audio for turn in turns for audio in turn.audios],\n        )\n        return merged_turn\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.random_pool.RandomPoolDatasetLoader.convert_to_conversations","title":"<code>convert_to_conversations(data)</code>","text":"<p>Convert random pool data to conversation objects.</p> <p>Each RandomPool entry becomes a single-turn conversation with a unique session ID.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Filename, list[RandomPool]]</code> <p>A dictionary mapping filename to list of RandomPool objects.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>A list of conversations.</p> Source code in <code>aiperf/dataset/loader/random_pool.py</code> <pre><code>def convert_to_conversations(\n    self, data: dict[Filename, list[RandomPool]]\n) -&gt; list[Conversation]:\n    \"\"\"Convert random pool data to conversation objects.\n\n    Each RandomPool entry becomes a single-turn conversation with a unique session ID.\n\n    Args:\n        data: A dictionary mapping filename to list of RandomPool objects.\n\n    Returns:\n        A list of conversations.\n    \"\"\"\n    conversations = [\n        Conversation(session_id=str(uuid.uuid4()))\n        for _ in range(self.num_conversations)\n    ]\n\n    # F x N (F: num of files, N: num of conversations)\n    sampled_dataset: dict[Filename, list[Turn]] = {}\n\n    # Randomly sample (with replacement) from each dataset pool\n    for filename, dataset_pool in data.items():\n        samples = random.choices(dataset_pool, k=self.num_conversations)\n        turns: list[Turn] = []\n        for sample in samples:\n            media = self.convert_to_media_objects(sample, name=Path(filename).stem)\n            turns.append(\n                Turn(\n                    texts=media[MediaType.TEXT],\n                    images=media[MediaType.IMAGE],\n                    audios=media[MediaType.AUDIO],\n                )\n            )\n        sampled_dataset[filename] = turns\n\n    # Merge turns for each conversation\n    for i, batched_turns in enumerate(zip(*sampled_dataset.values(), strict=False)):\n        turn = self._merge_turns(batched_turns)\n        conversations[i].turns.append(turn)\n\n    return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.random_pool.RandomPoolDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load random pool data from a file or directory.</p> <p>If filename is a file, reads and parses using the RandomPool model. If filename is a directory, reads each file in the directory and merges items with different modality names into combined RandomPool objects.</p> <p>Returns:</p> Type Description <code>dict[Filename, list[RandomPool]]</code> <p>A dictionary mapping filename to list of RandomPool objects.</p> Source code in <code>aiperf/dataset/loader/random_pool.py</code> <pre><code>def load_dataset(self) -&gt; dict[Filename, list[RandomPool]]:\n    \"\"\"Load random pool data from a file or directory.\n\n    If filename is a file, reads and parses using the RandomPool model.\n    If filename is a directory, reads each file in the directory and merges\n    items with different modality names into combined RandomPool objects.\n\n    Returns:\n        A dictionary mapping filename to list of RandomPool objects.\n    \"\"\"\n    path = Path(self.filename)\n\n    if path.is_file():\n        dataset_pool = self._load_dataset_from_file(path)\n        return {path.name: dataset_pool}\n\n    return self._load_dataset_from_dir(path)\n</code></pre>"},{"location":"api/#aiperfdatasetloadersharegpt","title":"aiperf.dataset.loader.sharegpt","text":""},{"location":"api/#aiperf.dataset.loader.sharegpt.ShareGPTLoader","title":"<code>ShareGPTLoader</code>","text":"<p>               Bases: <code>BasePublicDatasetLoader</code></p> <p>ShareGPT dataset loader for loading and processing ShareGPT conversation data.</p> <p>This loader downloads and processes the ShareGPT dataset from HuggingFace. It handles downloading, caching, validation, and conversion of ShareGPT conversations into the AIPerf conversation format.</p> <p>The loader filters conversations based on: - Minimum conversation length (at least 2 turns required) - Sequence length validation for prompt and completion tokens - Configurable max prompt length and total sequence length</p> Example <p>loader = ShareGPTLoader(user_config, tokenizer) dataset = await loader.load_dataset() conversations = await loader.convert_to_conversations(dataset) print(f\"Loaded {len(conversations)} valid conversations\")</p> Source code in <code>aiperf/dataset/loader/sharegpt.py</code> <pre><code>class ShareGPTLoader(BasePublicDatasetLoader):\n    \"\"\"ShareGPT dataset loader for loading and processing ShareGPT conversation data.\n\n    This loader downloads and processes the ShareGPT dataset from HuggingFace.\n    It handles downloading, caching, validation, and conversion of ShareGPT\n    conversations into the AIPerf conversation format.\n\n    The loader filters conversations based on:\n    - Minimum conversation length (at least 2 turns required)\n    - Sequence length validation for prompt and completion tokens\n    - Configurable max prompt length and total sequence length\n\n    Example:\n        &gt;&gt;&gt; loader = ShareGPTLoader(user_config, tokenizer)\n        &gt;&gt;&gt; dataset = await loader.load_dataset()\n        &gt;&gt;&gt; conversations = await loader.convert_to_conversations(dataset)\n        &gt;&gt;&gt; print(f\"Loaded {len(conversations)} valid conversations\")\n    \"\"\"\n\n    tag = \"ShareGPT\"\n    url = \"https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\"\n    filename = \"ShareGPT_V3_unfiltered_cleaned_split.json\"\n\n    def __init__(self, user_config: UserConfig, tokenizer: Tokenizer, **kwargs):\n        self.tokenizer = tokenizer\n        self.user_config = user_config\n        self.output_tokens_mean = self.user_config.input.prompt.output_tokens.mean\n        self.turn_count = 0\n\n        # TODO: Temporary placeholder for AioHttpClientMixin.\n        model_endpoint = ModelEndpointInfo(\n            models=ModelListInfo(\n                models=[ModelInfo(name=\"ShareGPT\")],\n                model_selection_strategy=ModelSelectionStrategy.ROUND_ROBIN,\n            ),\n            endpoint=EndpointInfo(base_url=self.url),\n        )\n        super().__init__(model_endpoint=model_endpoint, **kwargs)\n\n    async def load_dataset(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Load the dataset from the local cache or download it from the URL.\n\n        Returns:\n            dict[str, Any]: The loaded dataset.\n        \"\"\"\n        loaded_dataset = await self._load_dataset(\n            headers={\"Accept\": \"application/json\"}\n        )\n        return load_json_str(loaded_dataset)\n\n    # TODO: distribute this work across the processors\n    async def convert_to_conversations(\n        self, dataset: dict[str, Any]\n    ) -&gt; list[Conversation]:\n        \"\"\"\n        Convert the loaded dataset to conversations.\n\n        This method will construct `Conversation` objects from the dataset by filtering the dataset\n        depending on the sequence lengths and the content sizes.\n\n        Args:\n            dataset (dict[str, Any]): The loaded dataset.\n\n        Returns:\n            list[Conversation]: The list of conversations.\n        \"\"\"\n        self.info(\n            f\"Validating {self.tag} dataset and constructing conversation dataset\"\n        )\n        filtered_dataset = []\n        skipped_entries = 0\n        for entry in dataset:\n            conversations = entry.get(\"conversations\", [])\n            if not conversations or len(conversations) &lt; 2:\n                skipped_entries += 1\n                continue\n\n            prompt, completion = conversations[0][\"value\"], conversations[1][\"value\"]\n            prompt_length = len(self.tokenizer.encode(prompt))\n            completion_length = len(self.tokenizer.encode(completion))\n\n            if not self.is_valid_sequence(\n                prompt_len=prompt_length,\n                output_len=completion_length,\n                skip_min_output_len_check=self.output_tokens_mean is not None,\n            ):\n                skipped_entries += 1\n                continue\n\n            filtered_dataset.append(\n                Conversation(\n                    session_id=str(uuid.uuid4()),\n                    turns=[\n                        Turn(\n                            model=self._select_model_name(),\n                            texts=[Text(contents=[prompt])],\n                            max_tokens=completion_length,\n                        )\n                    ],\n                )\n            )\n\n        self.debug(\n            lambda: f\"Filtered to {len(filtered_dataset)} dataset entries out of {len(dataset)} (skipped {skipped_entries})\"\n        )\n        return filtered_dataset\n\n    def _select_model_name(self) -&gt; str:\n        selection_strategy = self.user_config.endpoint.model_selection_strategy\n        if selection_strategy == ModelSelectionStrategy.RANDOM:\n            return random.choice(self.user_config.endpoint.model_names)\n        elif selection_strategy == ModelSelectionStrategy.ROUND_ROBIN:\n            model_name = self.user_config.endpoint.model_names[\n                self.turn_count % len(self.user_config.endpoint.model_names)\n            ]\n            self.turn_count += 1\n            return model_name\n        else:\n            raise ValueError(f\"Invalid model selection strategy: {selection_strategy}.\")\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.sharegpt.ShareGPTLoader.convert_to_conversations","title":"<code>convert_to_conversations(dataset)</code>  <code>async</code>","text":"<p>Convert the loaded dataset to conversations.</p> <p>This method will construct <code>Conversation</code> objects from the dataset by filtering the dataset depending on the sequence lengths and the content sizes.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>dict[str, Any]</code> <p>The loaded dataset.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>list[Conversation]: The list of conversations.</p> Source code in <code>aiperf/dataset/loader/sharegpt.py</code> <pre><code>async def convert_to_conversations(\n    self, dataset: dict[str, Any]\n) -&gt; list[Conversation]:\n    \"\"\"\n    Convert the loaded dataset to conversations.\n\n    This method will construct `Conversation` objects from the dataset by filtering the dataset\n    depending on the sequence lengths and the content sizes.\n\n    Args:\n        dataset (dict[str, Any]): The loaded dataset.\n\n    Returns:\n        list[Conversation]: The list of conversations.\n    \"\"\"\n    self.info(\n        f\"Validating {self.tag} dataset and constructing conversation dataset\"\n    )\n    filtered_dataset = []\n    skipped_entries = 0\n    for entry in dataset:\n        conversations = entry.get(\"conversations\", [])\n        if not conversations or len(conversations) &lt; 2:\n            skipped_entries += 1\n            continue\n\n        prompt, completion = conversations[0][\"value\"], conversations[1][\"value\"]\n        prompt_length = len(self.tokenizer.encode(prompt))\n        completion_length = len(self.tokenizer.encode(completion))\n\n        if not self.is_valid_sequence(\n            prompt_len=prompt_length,\n            output_len=completion_length,\n            skip_min_output_len_check=self.output_tokens_mean is not None,\n        ):\n            skipped_entries += 1\n            continue\n\n        filtered_dataset.append(\n            Conversation(\n                session_id=str(uuid.uuid4()),\n                turns=[\n                    Turn(\n                        model=self._select_model_name(),\n                        texts=[Text(contents=[prompt])],\n                        max_tokens=completion_length,\n                    )\n                ],\n            )\n        )\n\n    self.debug(\n        lambda: f\"Filtered to {len(filtered_dataset)} dataset entries out of {len(dataset)} (skipped {skipped_entries})\"\n    )\n    return filtered_dataset\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.sharegpt.ShareGPTLoader.load_dataset","title":"<code>load_dataset()</code>  <code>async</code>","text":"<p>Load the dataset from the local cache or download it from the URL.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The loaded dataset.</p> Source code in <code>aiperf/dataset/loader/sharegpt.py</code> <pre><code>async def load_dataset(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the dataset from the local cache or download it from the URL.\n\n    Returns:\n        dict[str, Any]: The loaded dataset.\n    \"\"\"\n    loaded_dataset = await self._load_dataset(\n        headers={\"Accept\": \"application/json\"}\n    )\n    return load_json_str(loaded_dataset)\n</code></pre>"},{"location":"api/#aiperfdatasetloadersingle_turn","title":"aiperf.dataset.loader.single_turn","text":""},{"location":"api/#aiperf.dataset.loader.single_turn.SingleTurnDatasetLoader","title":"<code>SingleTurnDatasetLoader</code>","text":"<p>               Bases: <code>MediaConversionMixin</code></p> <p>A dataset loader that loads single turn data from a file.</p> <p>The single turn type   - supports multi-modal data (e.g. text, image, audio)   - supports client-side batching for each data (e.g. batch_size &gt; 1)   - DOES NOT support multi-turn features (e.g. delay, sessions, etc.)</p> <p>Examples: 1. Single-batch, text only</p> <pre><code>{\"text\": \"What is deep learning?\"}\n</code></pre> <ol> <li>Single-batch, multi-modal</li> </ol> <pre><code>{\"text\": \"What is in the image?\", \"image\": \"/path/to/image.png\"}\n</code></pre> <ol> <li>Multi-batch, multi-modal</li> </ol> <pre><code>{\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/to/image.png\", \"/path/to/image2.png\"]}\n</code></pre> <ol> <li>Fixed schedule version</li> </ol> <pre><code>{\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n{\"timestamp\": 1000, \"text\": \"Who are you?\"},\n{\"timestamp\": 2000, \"text\": \"What is AI?\"}\n</code></pre> <ol> <li>Time delayed version</li> </ol> <pre><code>{\"delay\": 0, \"text\": \"What is deep learning?\"},\n{\"delay\": 1234, \"text\": \"Who are you?\"}\n</code></pre> <ol> <li>Full-featured version (Multi-batch, multi-modal, multi-fielded)</li> </ol> <pre><code>{\n    \"texts\": [\n        {\"name\": \"text_field_A\", \"contents\": [\"Hello\", \"World\"]},\n        {\"name\": \"text_field_B\", \"contents\": [\"Hi there\"]}\n    ],\n    \"images\": [\n        {\"name\": \"image_field_A\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n        {\"name\": \"image_field_B\", \"contents\": [\"/path/3.png\"]}\n    ]\n}\n</code></pre> Source code in <code>aiperf/dataset/loader/single_turn.py</code> <pre><code>@CustomDatasetFactory.register(CustomDatasetType.SINGLE_TURN)\nclass SingleTurnDatasetLoader(MediaConversionMixin):\n    \"\"\"A dataset loader that loads single turn data from a file.\n\n    The single turn type\n      - supports multi-modal data (e.g. text, image, audio)\n      - supports client-side batching for each data (e.g. batch_size &gt; 1)\n      - DOES NOT support multi-turn features (e.g. delay, sessions, etc.)\n\n    Examples:\n    1. Single-batch, text only\n    ```json\n    {\"text\": \"What is deep learning?\"}\n    ```\n\n    2. Single-batch, multi-modal\n    ```json\n    {\"text\": \"What is in the image?\", \"image\": \"/path/to/image.png\"}\n    ```\n\n    3. Multi-batch, multi-modal\n    ```json\n    {\"texts\": [\"Who are you?\", \"Hello world\"], \"images\": [\"/path/to/image.png\", \"/path/to/image2.png\"]}\n    ```\n\n    4. Fixed schedule version\n    ```json\n    {\"timestamp\": 0, \"text\": \"What is deep learning?\"},\n    {\"timestamp\": 1000, \"text\": \"Who are you?\"},\n    {\"timestamp\": 2000, \"text\": \"What is AI?\"}\n    ```\n\n    5. Time delayed version\n    ```json\n    {\"delay\": 0, \"text\": \"What is deep learning?\"},\n    {\"delay\": 1234, \"text\": \"Who are you?\"}\n    ```\n\n    6. Full-featured version (Multi-batch, multi-modal, multi-fielded)\n    ```json\n    {\n        \"texts\": [\n            {\"name\": \"text_field_A\", \"contents\": [\"Hello\", \"World\"]},\n            {\"name\": \"text_field_B\", \"contents\": [\"Hi there\"]}\n        ],\n        \"images\": [\n            {\"name\": \"image_field_A\", \"contents\": [\"/path/1.png\", \"/path/2.png\"]},\n            {\"name\": \"image_field_B\", \"contents\": [\"/path/3.png\"]}\n        ]\n    }\n    ```\n    \"\"\"\n\n    def __init__(self, filename: str):\n        self.filename = filename\n\n    def load_dataset(self) -&gt; dict[str, list[SingleTurn]]:\n        \"\"\"Load single-turn data from a JSONL file.\n\n        Each line represents a single turn conversation. Multiple turns with\n        the same session_id (or generated UUID) are grouped together.\n\n        Returns:\n            A dictionary mapping session_id to list of CustomData.\n        \"\"\"\n        data: dict[str, list[SingleTurn]] = defaultdict(list)\n\n        with open(self.filename) as f:\n            for line in f:\n                if (line := line.strip()) == \"\":\n                    continue  # Skip empty lines\n\n                single_turn_data = SingleTurn.model_validate_json(line)\n                session_id = str(uuid.uuid4())\n                data[session_id].append(single_turn_data)\n\n        return data\n\n    def convert_to_conversations(\n        self, data: dict[str, list[SingleTurn]]\n    ) -&gt; list[Conversation]:\n        \"\"\"Convert single turn data to conversation objects.\n\n        Args:\n            data: A dictionary mapping session_id to list of SingleTurn objects.\n\n        Returns:\n            A list of conversations.\n        \"\"\"\n        conversations = []\n        for session_id, single_turns in data.items():\n            conversation = Conversation(session_id=session_id)\n            for single_turn in single_turns:\n                media = self.convert_to_media_objects(single_turn)\n                conversation.turns.append(\n                    Turn(\n                        texts=media[MediaType.TEXT],\n                        images=media[MediaType.IMAGE],\n                        audios=media[MediaType.AUDIO],\n                        timestamp=single_turn.timestamp,\n                        delay=single_turn.delay,\n                        role=single_turn.role,\n                    )\n                )\n            conversations.append(conversation)\n        return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.single_turn.SingleTurnDatasetLoader.convert_to_conversations","title":"<code>convert_to_conversations(data)</code>","text":"<p>Convert single turn data to conversation objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[SingleTurn]]</code> <p>A dictionary mapping session_id to list of SingleTurn objects.</p> required <p>Returns:</p> Type Description <code>list[Conversation]</code> <p>A list of conversations.</p> Source code in <code>aiperf/dataset/loader/single_turn.py</code> <pre><code>def convert_to_conversations(\n    self, data: dict[str, list[SingleTurn]]\n) -&gt; list[Conversation]:\n    \"\"\"Convert single turn data to conversation objects.\n\n    Args:\n        data: A dictionary mapping session_id to list of SingleTurn objects.\n\n    Returns:\n        A list of conversations.\n    \"\"\"\n    conversations = []\n    for session_id, single_turns in data.items():\n        conversation = Conversation(session_id=session_id)\n        for single_turn in single_turns:\n            media = self.convert_to_media_objects(single_turn)\n            conversation.turns.append(\n                Turn(\n                    texts=media[MediaType.TEXT],\n                    images=media[MediaType.IMAGE],\n                    audios=media[MediaType.AUDIO],\n                    timestamp=single_turn.timestamp,\n                    delay=single_turn.delay,\n                    role=single_turn.role,\n                )\n            )\n        conversations.append(conversation)\n    return conversations\n</code></pre>"},{"location":"api/#aiperf.dataset.loader.single_turn.SingleTurnDatasetLoader.load_dataset","title":"<code>load_dataset()</code>","text":"<p>Load single-turn data from a JSONL file.</p> <p>Each line represents a single turn conversation. Multiple turns with the same session_id (or generated UUID) are grouped together.</p> <p>Returns:</p> Type Description <code>dict[str, list[SingleTurn]]</code> <p>A dictionary mapping session_id to list of CustomData.</p> Source code in <code>aiperf/dataset/loader/single_turn.py</code> <pre><code>def load_dataset(self) -&gt; dict[str, list[SingleTurn]]:\n    \"\"\"Load single-turn data from a JSONL file.\n\n    Each line represents a single turn conversation. Multiple turns with\n    the same session_id (or generated UUID) are grouped together.\n\n    Returns:\n        A dictionary mapping session_id to list of CustomData.\n    \"\"\"\n    data: dict[str, list[SingleTurn]] = defaultdict(list)\n\n    with open(self.filename) as f:\n        for line in f:\n            if (line := line.strip()) == \"\":\n                continue  # Skip empty lines\n\n            single_turn_data = SingleTurn.model_validate_json(line)\n            session_id = str(uuid.uuid4())\n            data[session_id].append(single_turn_data)\n\n    return data\n</code></pre>"},{"location":"api/#aiperfdatasetutils","title":"aiperf.dataset.utils","text":""},{"location":"api/#aiperf.dataset.utils.check_file_exists","title":"<code>check_file_exists(filename)</code>","text":"<p>Verifies that the file exists.</p> <p>Parameters:</p> Name Type Description Default <code>filename </code> <p>The file path to verify.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>aiperf/dataset/utils.py</code> <pre><code>def check_file_exists(filename: Path) -&gt; None:\n    \"\"\"Verifies that the file exists.\n\n    Args:\n        filename : The file path to verify.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    if not filename.exists():\n        raise FileNotFoundError(f\"The file '{filename}' does not exist.\")\n</code></pre>"},{"location":"api/#aiperf.dataset.utils.encode_image","title":"<code>encode_image(img, format)</code>","text":"<p>Encodes an image into base64 encoded string.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Image</code> <p>The PIL Image object to encode.</p> required <code>format</code> <code>str</code> <p>The image format to use (e.g., \"JPEG\", \"PNG\").</p> required <p>Returns:</p> Type Description <code>str</code> <p>A base64 encoded string representation of the image.</p> Source code in <code>aiperf/dataset/utils.py</code> <pre><code>def encode_image(img: Image, format: str) -&gt; str:\n    \"\"\"Encodes an image into base64 encoded string.\n\n    Args:\n        img: The PIL Image object to encode.\n        format: The image format to use (e.g., \"JPEG\", \"PNG\").\n\n    Returns:\n        A base64 encoded string representation of the image.\n    \"\"\"\n    # JPEG does not support P or RGBA mode (commonly used for PNG) so it needs\n    # to be converted to RGB before an image can be saved as JPEG format.\n    if format == \"JPEG\" and img.mode != \"RGB\":\n        img = img.convert(\"RGB\")\n\n    buffer = BytesIO()\n    img.save(buffer, format=format)\n    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n</code></pre>"},{"location":"api/#aiperf.dataset.utils.open_image","title":"<code>open_image(filename)</code>","text":"<p>Opens an image file.</p> <p>Parameters:</p> Name Type Description Default <code>filename </code> <p>The file path to open.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The opened PIL Image object.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>aiperf/dataset/utils.py</code> <pre><code>def open_image(filename: str) -&gt; Image:\n    \"\"\"Opens an image file.\n\n    Args:\n        filename : The file path to open.\n\n    Returns:\n        The opened PIL Image object.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    check_file_exists(Path(filename))\n    img = Image.open(filename)\n\n    if img.format is None:\n        raise RuntimeError(f\"Failed to determine image format of '{filename}'.\")\n\n    if img.format.upper() not in list(ImageFormat):\n        raise RuntimeError(\n            f\"'{img.format}' is not one of the supported image formats: \"\n            f\"{', '.join(ImageFormat)}\"\n        )\n    return img\n</code></pre>"},{"location":"api/#aiperf.dataset.utils.sample_normal","title":"<code>sample_normal(mean, stddev, lower=-np.inf, upper=np.inf)</code>","text":"<p>Sample from a normal distribution with support for bounds using rejection sampling.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>stddev</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <code>lower</code> <code>float</code> <p>The lower bound of the distribution.</p> <code>-inf</code> <code>upper</code> <code>float</code> <p>The upper bound of the distribution.</p> <code>inf</code> <p>Returns:</p> Type Description <code>int</code> <p>An integer sampled from the distribution.</p> Source code in <code>aiperf/dataset/utils.py</code> <pre><code>def sample_normal(\n    mean: float, stddev: float, lower: float = -np.inf, upper: float = np.inf\n) -&gt; int:\n    \"\"\"Sample from a normal distribution with support for bounds using rejection sampling.\n\n    Args:\n        mean: The mean of the normal distribution.\n        stddev: The standard deviation of the normal distribution.\n        lower: The lower bound of the distribution.\n        upper: The upper bound of the distribution.\n\n    Returns:\n        An integer sampled from the distribution.\n    \"\"\"\n    while True:\n        n = np.random.normal(mean, stddev)\n        if lower &lt;= n &lt;= upper:\n            return n\n</code></pre>"},{"location":"api/#aiperf.dataset.utils.sample_positive_normal","title":"<code>sample_positive_normal(mean, stddev)</code>","text":"<p>Sample from a normal distribution ensuring positive values without distorting the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>Mean value for the normal distribution</p> required <code>stddev</code> <code>float</code> <p>Standard deviation for the normal distribution</p> required <p>Returns:</p> Type Description <code>float</code> <p>A positive sample from the normal distribution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean is less than 0</p> Source code in <code>aiperf/dataset/utils.py</code> <pre><code>def sample_positive_normal(mean: float, stddev: float) -&gt; float:\n    \"\"\"Sample from a normal distribution ensuring positive values\n    without distorting the distribution.\n\n    Args:\n        mean: Mean value for the normal distribution\n        stddev: Standard deviation for the normal distribution\n\n    Returns:\n        A positive sample from the normal distribution\n\n    Raises:\n        ValueError: If mean is less than 0\n    \"\"\"\n    if mean &lt; 0:\n        raise ValueError(f\"Mean value ({mean}) should be greater than 0\")\n    return sample_normal(mean, stddev, lower=0)\n</code></pre>"},{"location":"api/#aiperf.dataset.utils.sample_positive_normal_integer","title":"<code>sample_positive_normal_integer(mean, stddev)</code>","text":"<p>Sample a random positive integer from a normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>stddev</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <p>Returns:</p> Type Description <code>int</code> <p>A positive integer sampled from the distribution. If the sampled</p> <code>int</code> <p>number is less than 1, it returns 1.</p> Source code in <code>aiperf/dataset/utils.py</code> <pre><code>def sample_positive_normal_integer(mean: float, stddev: float) -&gt; int:\n    \"\"\"Sample a random positive integer from a normal distribution.\n\n    Args:\n        mean: The mean of the normal distribution.\n        stddev: The standard deviation of the normal distribution.\n\n    Returns:\n        A positive integer sampled from the distribution. If the sampled\n        number is less than 1, it returns 1.\n    \"\"\"\n    return math.ceil(sample_positive_normal(mean, stddev))\n</code></pre>"},{"location":"api/#aiperfexportersconsole_error_exporter","title":"aiperf.exporters.console_error_exporter","text":""},{"location":"api/#aiperf.exporters.console_error_exporter.ConsoleErrorExporter","title":"<code>ConsoleErrorExporter</code>","text":"<p>A class that exports error data to the console</p> Source code in <code>aiperf/exporters/console_error_exporter.py</code> <pre><code>@implements_protocol(ConsoleExporterProtocol)\n@ConsoleExporterFactory.register(ConsoleExporterType.ERRORS)\nclass ConsoleErrorExporter:\n    \"\"\"A class that exports error data to the console\"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs):\n        self._results = exporter_config.results\n\n    async def export(self, console: Console) -&gt; None:\n        if not self._results.error_summary:\n            return\n\n        table = Table(title=self._get_title())\n        table.add_column(\"Code\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Type\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Message\", justify=\"left\", style=\"yellow\")\n        table.add_column(\"Count\", justify=\"right\", style=\"yellow\")\n        self._construct_table(table, self._results.error_summary)\n\n        console.print(\"\\n\")\n        console.print(table)\n        console.file.flush()\n\n    def _construct_table(\n        self, table: Table, errors_by_type: list[ErrorDetailsCount]\n    ) -&gt; None:\n        for error_details_count in errors_by_type:\n            table.add_row(*self._format_row(error_details_count))\n\n    def _format_row(self, error_details_count: ErrorDetailsCount) -&gt; list[str]:\n        details = error_details_count.error_details\n        count = error_details_count.count\n\n        return [\n            str(details.code) if details.code else \"[dim]N/A[/dim]\",\n            str(details.type) if details.type else \"[dim]N/A[/dim]\",\n            str(details.message),\n            f\"{count:,}\",\n        ]\n\n    def _get_title(self) -&gt; str:\n        return \"[red]NVIDIA AIPerf | Error Summary[/red]\"\n</code></pre>"},{"location":"api/#aiperfexportersconsole_metrics_exporter","title":"aiperf.exporters.console_metrics_exporter","text":""},{"location":"api/#aiperf.exporters.console_metrics_exporter.ConsoleMetricsExporter","title":"<code>ConsoleMetricsExporter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>A class that exports data to the console</p> Source code in <code>aiperf/exporters/console_metrics_exporter.py</code> <pre><code>@implements_protocol(ConsoleExporterProtocol)\n@ConsoleExporterFactory.register(ConsoleExporterType.METRICS)\nclass ConsoleMetricsExporter(AIPerfLoggerMixin):\n    \"\"\"A class that exports data to the console\"\"\"\n\n    STAT_COLUMN_KEYS = [\"avg\", \"min\", \"max\", \"p99\", \"p90\", \"p75\", \"std\"]\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self._results = exporter_config.results\n        self._endpoint_type = exporter_config.user_config.endpoint.type\n\n    async def export(self, console: Console) -&gt; None:\n        if not self._results.records:\n            self.warning(\"No records to export\")\n            return\n\n        self._print_renderable(\n            console, self.get_renderable(self._results.records, console)\n        )\n\n    def _print_renderable(self, console: Console, renderable: RenderableType) -&gt; None:\n        console.print(\"\\n\")\n        console.print(renderable)\n        console.file.flush()\n\n    def get_renderable(\n        self, records: list[MetricResult], console: Console\n    ) -&gt; RenderableType:\n        table = Table(title=self._get_title())\n        table.add_column(\"Metric\", justify=\"right\", style=\"cyan\")\n        for key in self.STAT_COLUMN_KEYS:\n            table.add_column(key, justify=\"right\", style=\"green\")\n        self._construct_table(table, records)\n        return table\n\n    def _construct_table(self, table: Table, records: list[MetricResult]) -&gt; None:\n        records = sorted(\n            (to_display_unit(r, MetricRegistry) for r in records),\n            key=lambda x: MetricRegistry.get_class(x.tag).display_order or sys.maxsize,\n        )\n        for record in records:\n            if not self._should_show(record):\n                continue\n            table.add_row(*self._format_row(record))\n\n    def _should_show(self, record: MetricResult) -&gt; bool:\n        # Only show metrics that are not error-only or hidden\n        metric_class = MetricRegistry.get_class(record.tag)\n        return metric_class.missing_flags(MetricFlags.ERROR_ONLY | MetricFlags.HIDDEN)\n\n    def _format_row(self, record: MetricResult) -&gt; list[str]:\n        metric_class = MetricRegistry.get_class(record.tag)\n        display_unit = metric_class.display_unit or metric_class.unit\n        delimiter = \"\\n\" if len(record.header) &gt; 30 else \" \"\n        row = [f\"{record.header}{delimiter}({display_unit})\"]\n        for stat in self.STAT_COLUMN_KEYS:\n            value = getattr(record, stat, None)\n            if value is None:\n                row.append(\"[dim]N/A[/dim]\")\n                continue\n\n            if isinstance(value, datetime):\n                value = value.strftime(\"%Y-%m-%d %H:%M:%S\")\n            elif isinstance(value, int | float):\n                value = f\"{value:,.2f}\"\n            else:\n                value = str(value)\n            row.append(value)\n        return row\n\n    def _get_title(self) -&gt; str:\n        return f\"NVIDIA AIPerf | {self._endpoint_type.metrics_title}\"\n</code></pre>"},{"location":"api/#aiperfexporterscsv_exporter","title":"aiperf.exporters.csv_exporter","text":""},{"location":"api/#aiperf.exporters.csv_exporter.CsvExporter","title":"<code>CsvExporter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Exports records to a CSV file in a legacy, two-section format.</p> Source code in <code>aiperf/exporters/csv_exporter.py</code> <pre><code>@DataExporterFactory.register(DataExporterType.CSV)\n@implements_protocol(DataExporterProtocol)\nclass CsvExporter(AIPerfLoggerMixin):\n    \"\"\"Exports records to a CSV file in a legacy, two-section format.\"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.debug(lambda: f\"Initializing CsvExporter with config: {exporter_config}\")\n        self._results = exporter_config.results\n        self._output_directory = exporter_config.user_config.output.artifact_directory\n        self._metric_registry = MetricRegistry\n        self._file_path = self._output_directory / \"profile_export_aiperf.csv\"\n        self._percentile_keys = _percentile_keys_from(STAT_KEYS)\n\n    def get_export_info(self) -&gt; FileExportInfo:\n        return FileExportInfo(\n            export_type=\"CSV Export\",\n            file_path=self._file_path,\n        )\n\n    async def export(self) -&gt; None:\n        self._output_directory.mkdir(parents=True, exist_ok=True)\n\n        self.debug(lambda: f\"Exporting data to CSV file: {self._file_path}\")\n\n        try:\n            records: Mapping[str, MetricResult] = {}\n            if self._results.records:\n                records = convert_all_metrics_to_display_units(\n                    self._results.records, self._metric_registry\n                )\n\n            csv_content = self._generate_csv_content(records)\n\n            async with aiofiles.open(\n                self._file_path, \"w\", newline=\"\", encoding=\"utf-8\"\n            ) as f:\n                await f.write(csv_content)\n\n        except Exception as e:\n            self.error(f\"Failed to export CSV to {self._file_path}: {e}\")\n            raise\n\n    def _generate_csv_content(self, records: Mapping[str, MetricResult]) -&gt; str:\n        buf = io.StringIO()\n        writer = csv.writer(buf)\n\n        request_metrics, system_metrics = self._split_metrics(records)\n\n        if request_metrics:\n            self._write_request_metrics(writer, request_metrics)\n            if system_metrics:  # blank line between sections\n                writer.writerow([])\n\n        if system_metrics:\n            self._write_system_metrics(writer, system_metrics)\n\n        return buf.getvalue()\n\n    def _split_metrics(\n        self, records: Mapping[str, MetricResult]\n    ) -&gt; tuple[dict[str, MetricResult], dict[str, MetricResult]]:\n        \"\"\"Split metrics into request metrics (with percentiles) and system metrics (single values).\"\"\"\n        request_metrics: dict[str, MetricResult] = {}\n        system_metrics: dict[str, MetricResult] = {}\n\n        for tag, metric in records.items():\n            if self._has_percentiles(metric):\n                request_metrics[tag] = metric\n            else:\n                system_metrics[tag] = metric\n\n        return request_metrics, system_metrics\n\n    def _has_percentiles(self, metric: MetricResult) -&gt; bool:\n        \"\"\"Check if a metric has any percentile data.\"\"\"\n        return any(getattr(metric, k, None) is not None for k in self._percentile_keys)\n\n    def _write_request_metrics(\n        self,\n        writer: csv.writer,\n        records: Mapping[str, MetricResult],  # type: ignore\n    ) -&gt; None:\n        header = [\"Metric\"] + list(STAT_KEYS)\n        writer.writerow(header)\n\n        for _, metric in sorted(records.items(), key=lambda kv: kv[0]):\n            if not self._should_export(metric):\n                continue\n            row = [self._format_metric_name(metric)]\n            for stat_name in STAT_KEYS:\n                value = getattr(metric, stat_name, None)\n                row.append(self._format_number(value))\n            writer.writerow(row)\n\n    def _should_export(self, metric: MetricResult) -&gt; bool:\n        \"\"\"Check if a metric should be exported.\"\"\"\n        metric_class = MetricRegistry.get_class(metric.tag)\n        res = metric_class.missing_flags(\n            MetricFlags.EXPERIMENTAL | MetricFlags.INTERNAL\n        )\n        self.debug(lambda: f\"Metric '{metric.tag}' should be exported: {res}\")\n        return res\n\n    def _write_system_metrics(\n        self,\n        writer: csv.writer,\n        records: Mapping[str, MetricResult],  # type: ignore\n    ) -&gt; None:\n        writer.writerow([\"Metric\", \"Value\"])\n        for _, metric in sorted(records.items(), key=lambda kv: kv[0]):\n            if not self._should_export(metric):\n                continue\n            writer.writerow(\n                [self._format_metric_name(metric), self._format_number(metric.avg)]\n            )\n\n    def _format_metric_name(self, metric: MetricResult) -&gt; str:\n        \"\"\"Format metric name with its unit.\"\"\"\n        name = metric.header or \"\"\n        if metric.unit and metric.unit.lower() not in {\"count\", \"requests\"}:\n            name = f\"{name} ({metric.unit})\" if name else f\"({metric.unit})\"\n        return name\n\n    def _format_number(self, value) -&gt; str:\n        \"\"\"Format a number for CSV output.\"\"\"\n        if value is None:\n            return \"\"\n        if isinstance(value, int):\n            return str(value)\n        if isinstance(value, float):\n            return f\"{float(value):.2f}\"\n        return str(value)\n</code></pre>"},{"location":"api/#aiperfexportersdisplay_units_utils","title":"aiperf.exporters.display_units_utils","text":""},{"location":"api/#aiperf.exporters.display_units_utils.convert_all_metrics_to_display_units","title":"<code>convert_all_metrics_to_display_units(records, registry)</code>","text":"<p>Helper for exporters that want a tag-&gt;result mapping in display units.</p> Source code in <code>aiperf/exporters/display_units_utils.py</code> <pre><code>def convert_all_metrics_to_display_units(\n    records: Iterable[MetricResult], registry: MetricRegistry\n) -&gt; dict[MetricTagT, MetricResult]:\n    \"\"\"Helper for exporters that want a tag-&gt;result mapping in display units.\"\"\"\n    out: dict[MetricTagT, MetricResult] = {}\n    for r in records:\n        out[r.tag] = to_display_unit(r, registry)\n    return out\n</code></pre>"},{"location":"api/#aiperf.exporters.display_units_utils.to_display_unit","title":"<code>to_display_unit(result, registry)</code>","text":"<p>Return a new MetricResult converted to its display unit (if different).</p> Source code in <code>aiperf/exporters/display_units_utils.py</code> <pre><code>def to_display_unit(result: MetricResult, registry: MetricRegistry) -&gt; MetricResult:\n    \"\"\"\n    Return a new MetricResult converted to its display unit (if different).\n    \"\"\"\n    metric_cls = registry.get_class(result.tag)\n    if result.unit and result.unit != metric_cls.unit.value:\n        _logger.error(\n            f\"Metric {result.tag} has a unit ({result.unit}) that does not match the expected unit ({metric_cls.unit.value}). \"\n            f\"({metric_cls.unit.value}) will be used for conversion.\"\n        )\n\n    display_unit = metric_cls.display_unit or metric_cls.unit\n\n    if display_unit == metric_cls.unit:\n        return result\n\n    record = result.model_copy(deep=True)\n    record.unit = display_unit.value\n\n    for stat in STAT_KEYS:\n        val = getattr(record, stat, None)\n        if val is None:\n            continue\n        # Only convert numeric values\n        if isinstance(val, int | float):\n            try:\n                new_value = metric_cls.unit.convert_to(display_unit, val)\n            except MetricUnitError as e:\n                _logger.warning(\n                    f\"Error converting {stat} for {result.tag} from {metric_cls.unit.value} to {display_unit.value}: {e}\"\n                )\n                continue\n            setattr(record, stat, new_value)\n    return record\n</code></pre>"},{"location":"api/#aiperfexportersexperimental_metrics_console_exporter","title":"aiperf.exporters.experimental_metrics_console_exporter","text":""},{"location":"api/#aiperf.exporters.experimental_metrics_console_exporter.ConsoleExperimentalMetricsExporter","title":"<code>ConsoleExperimentalMetricsExporter</code>","text":"<p>               Bases: <code>ConsoleMetricsExporter</code></p> <p>A class that exports experimental metrics to the console.</p> <p>This is a special exporter that is used to export experimental metrics to the console.</p> Source code in <code>aiperf/exporters/experimental_metrics_console_exporter.py</code> <pre><code>@implements_protocol(ConsoleExporterProtocol)\n@ConsoleExporterFactory.register(ConsoleExporterType.EXPERIMENTAL_METRICS)\nclass ConsoleExperimentalMetricsExporter(ConsoleMetricsExporter):\n    \"\"\"A class that exports experimental metrics to the console.\n\n    This is a special exporter that is used to export experimental metrics to the console.\n    \"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs) -&gt; None:\n        super().__init__(exporter_config=exporter_config, **kwargs)\n        self._show_experimental_metrics = AIPERF_DEV_MODE and (\n            exporter_config.service_config.developer.show_internal_metrics\n        )\n\n    async def export(self, console: Console) -&gt; None:\n        if not self._show_experimental_metrics:\n            self.debug(\"Experimental metrics are not enabled, skipping export\")\n            return\n\n        await super().export(console)\n\n    def _should_show(self, record: MetricResult) -&gt; bool:\n        metric_class = MetricRegistry.get_class(record.tag)\n        # Only show experimental or hidden metrics that are not internal\n        return (\n            metric_class.has_flags(MetricFlags.EXPERIMENTAL)\n            or metric_class.has_flags(MetricFlags.HIDDEN)\n            and metric_class.missing_flags(MetricFlags.INTERNAL)\n        )\n\n    def _get_title(self) -&gt; str:\n        return \"[yellow]NVIDIA AIPerf | Experimental Metrics[/yellow]\"\n</code></pre>"},{"location":"api/#aiperfexportersexporter_config","title":"aiperf.exporters.exporter_config","text":""},{"location":"api/#aiperfexportersexporter_manager","title":"aiperf.exporters.exporter_manager","text":""},{"location":"api/#aiperf.exporters.exporter_manager.ExporterManager","title":"<code>ExporterManager</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>ExporterManager is responsible for exporting records using all registered data exporters.</p> Source code in <code>aiperf/exporters/exporter_manager.py</code> <pre><code>class ExporterManager(AIPerfLoggerMixin):\n    \"\"\"\n    ExporterManager is responsible for exporting records using all\n    registered data exporters.\n    \"\"\"\n\n    def __init__(\n        self,\n        results: ProfileResults,\n        input_config: UserConfig,\n        service_config: ServiceConfig,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        self._results = results\n        self._input_config = input_config\n        self._tasks: set[asyncio.Task] = set()\n        self._service_config = service_config\n        self._exporter_config = ExporterConfig(\n            results=self._results,\n            user_config=self._input_config,\n            service_config=self._service_config,\n        )\n\n    def _task_done_callback(self, task: asyncio.Task) -&gt; None:\n        self.debug(lambda: f\"Task done: {task}\")\n        if task.exception():\n            self.error(f\"Error exporting records: {task.exception()}\")\n        else:\n            self.debug(f\"Exported records: {task.result()}\")\n        self._tasks.discard(task)\n\n    async def export_data(self) -&gt; None:\n        self.info(\"Exporting all records\")\n\n        for exporter_type in DataExporterFactory.get_all_class_types():\n            exporter = DataExporterFactory.create_instance(\n                exporter_type, exporter_config=self._exporter_config\n            )\n            self.debug(f\"Creating task for exporter: {exporter_type}\")\n            task = asyncio.create_task(exporter.export())\n            self._tasks.add(task)\n            task.add_done_callback(self._task_done_callback)\n\n        await asyncio.gather(*self._tasks, return_exceptions=True)\n        self._tasks.clear()\n        self.debug(\"Exporting all records completed\")\n\n    def get_exported_file_infos(self) -&gt; list[FileExportInfo]:\n        \"\"\"Get the file infos for all exported files.\"\"\"\n        file_infos = []\n        for exporter_type in DataExporterFactory.get_all_class_types():\n            exporter = DataExporterFactory.create_instance(\n                exporter_type, exporter_config=self._exporter_config\n            )\n            file_infos.append(exporter.get_export_info())\n        return file_infos\n\n    async def export_console(self, console: Console) -&gt; None:\n        self.info(\"Exporting console data\")\n\n        for exporter_type in ConsoleExporterFactory.get_all_class_types():\n            exporter = ConsoleExporterFactory.create_instance(\n                exporter_type, exporter_config=self._exporter_config\n            )\n            self.debug(f\"Creating task for exporter: {exporter_type}\")\n            task = asyncio.create_task(exporter.export(console=console))\n            self._tasks.add(task)\n            task.add_done_callback(self._task_done_callback)\n\n        await asyncio.gather(*self._tasks, return_exceptions=True)\n        self._tasks.clear()\n        self.debug(\"Exporting console data completed\")\n</code></pre>"},{"location":"api/#aiperf.exporters.exporter_manager.ExporterManager.get_exported_file_infos","title":"<code>get_exported_file_infos()</code>","text":"<p>Get the file infos for all exported files.</p> Source code in <code>aiperf/exporters/exporter_manager.py</code> <pre><code>def get_exported_file_infos(self) -&gt; list[FileExportInfo]:\n    \"\"\"Get the file infos for all exported files.\"\"\"\n    file_infos = []\n    for exporter_type in DataExporterFactory.get_all_class_types():\n        exporter = DataExporterFactory.create_instance(\n            exporter_type, exporter_config=self._exporter_config\n        )\n        file_infos.append(exporter.get_export_info())\n    return file_infos\n</code></pre>"},{"location":"api/#aiperfexportersinternal_metrics_console_exporter","title":"aiperf.exporters.internal_metrics_console_exporter","text":""},{"location":"api/#aiperf.exporters.internal_metrics_console_exporter.ConsoleInternalMetricsExporter","title":"<code>ConsoleInternalMetricsExporter</code>","text":"<p>               Bases: <code>ConsoleMetricsExporter</code></p> <p>A class that exports internal metrics to the console.</p> <p>This is a special exporter that is used to export internal metrics to the console. It is only applicable to internal metrics and is not applicable to user-facing metrics.</p> Source code in <code>aiperf/exporters/internal_metrics_console_exporter.py</code> <pre><code>@implements_protocol(ConsoleExporterProtocol)\n@ConsoleExporterFactory.register(ConsoleExporterType.INTERNAL_METRICS)\nclass ConsoleInternalMetricsExporter(ConsoleMetricsExporter):\n    \"\"\"A class that exports internal metrics to the console.\n\n    This is a special exporter that is used to export internal metrics to the console.\n    It is only applicable to internal metrics and is not applicable to user-facing metrics.\n    \"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs) -&gt; None:\n        super().__init__(exporter_config=exporter_config, **kwargs)\n        self._show_internal_metrics = AIPERF_DEV_MODE and (\n            exporter_config.service_config.developer.show_internal_metrics\n        )\n\n    async def export(self, console: Console) -&gt; None:\n        if not self._show_internal_metrics:\n            self.debug(\"Internal metrics are not enabled, skipping export\")\n            return\n        await super().export(console)\n\n    def _should_show(self, record: MetricResult) -&gt; bool:\n        metric_class = MetricRegistry.get_class(record.tag)\n        # Only show internal metrics\n        return metric_class.has_flags(MetricFlags.INTERNAL)\n\n    def _get_title(self) -&gt; str:\n        return \"[yellow]NVIDIA AIPerf | Internal Metrics[/yellow]\"\n</code></pre>"},{"location":"api/#aiperfexportersjson_exporter","title":"aiperf.exporters.json_exporter","text":""},{"location":"api/#aiperf.exporters.json_exporter.JsonExportData","title":"<code>JsonExportData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data to be exported to a JSON file.</p> Source code in <code>aiperf/exporters/json_exporter.py</code> <pre><code>class JsonExportData(BaseModel):\n    \"\"\"Data to be exported to a JSON file.\"\"\"\n\n    records: dict[MetricTagT, MetricResult] | None = None\n    input_config: UserConfig | None = None\n    was_cancelled: bool | None = None\n    error_summary: list[ErrorDetailsCount] | None = None\n    start_time: datetime | None = None\n    end_time: datetime | None = None\n</code></pre>"},{"location":"api/#aiperf.exporters.json_exporter.JsonExporter","title":"<code>JsonExporter</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>A class to export records to a JSON file.</p> Source code in <code>aiperf/exporters/json_exporter.py</code> <pre><code>@DataExporterFactory.register(DataExporterType.JSON)\n@implements_protocol(DataExporterProtocol)\nclass JsonExporter(AIPerfLoggerMixin):\n    \"\"\"\n    A class to export records to a JSON file.\n    \"\"\"\n\n    def __init__(self, exporter_config: ExporterConfig, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.debug(lambda: f\"Initializing JsonExporter with config: {exporter_config}\")\n        self._results = exporter_config.results\n        self._output_directory = exporter_config.user_config.output.artifact_directory\n        self._input_config = exporter_config.user_config\n        self._metric_registry = MetricRegistry\n        self._file_path = self._output_directory / \"profile_export_aiperf.json\"\n\n    def get_export_info(self) -&gt; FileExportInfo:\n        return FileExportInfo(\n            export_type=\"JSON Export\",\n            file_path=self._file_path,\n        )\n\n    def _should_export(self, metric: MetricResult) -&gt; bool:\n        \"\"\"Check if a metric should be exported.\"\"\"\n        metric_class = MetricRegistry.get_class(metric.tag)\n        res = metric_class.missing_flags(\n            MetricFlags.EXPERIMENTAL | MetricFlags.INTERNAL\n        )\n        self.debug(lambda: f\"Metric '{metric.tag}' should be exported: {res}\")\n        return res\n\n    async def export(self) -&gt; None:\n        self._output_directory.mkdir(parents=True, exist_ok=True)\n\n        start_time = (\n            datetime.fromtimestamp(self._results.start_ns / NANOS_PER_SECOND)\n            if self._results.start_ns\n            else None\n        )\n        end_time = (\n            datetime.fromtimestamp(self._results.end_ns / NANOS_PER_SECOND)\n            if self._results.end_ns\n            else None\n        )\n\n        converted_records: dict[MetricTagT, MetricResult] = {}\n        if self._results.records:\n            converted_records = convert_all_metrics_to_display_units(\n                self._results.records, self._metric_registry\n            )\n            converted_records = {\n                k: v for k, v in converted_records.items() if self._should_export(v)\n            }\n\n        export_data = JsonExportData(\n            input_config=self._input_config,\n            records=converted_records,\n            was_cancelled=self._results.was_cancelled,\n            error_summary=self._results.error_summary,\n            start_time=start_time,\n            end_time=end_time,\n        )\n\n        self.debug(lambda: f\"Exporting data to JSON file: {export_data}\")\n        export_data_json = export_data.model_dump_json(indent=2, exclude_unset=True)\n        async with aiofiles.open(self._file_path, \"w\") as f:\n            await f.write(export_data_json)\n</code></pre>"},{"location":"api/#aiperfmetricsbase_aggregate_counter_metric","title":"aiperf.metrics.base_aggregate_counter_metric","text":""},{"location":"api/#aiperf.metrics.base_aggregate_counter_metric.BaseAggregateCounterMetric","title":"<code>BaseAggregateCounterMetric</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT]</code>, <code>BaseAggregateMetric[MetricValueTypeVarT]</code>, <code>ABC</code></p> <p>A base class for aggregate counter metrics. These metrics increment a counter for each record.</p> <p>Examples:</p> <pre><code>class RequestCountMetric(BaseAggregateCounterMetric[int]):\n    # ... Metric attributes ...\n</code></pre> Source code in <code>aiperf/metrics/base_aggregate_counter_metric.py</code> <pre><code>class BaseAggregateCounterMetric(\n    Generic[MetricValueTypeVarT], BaseAggregateMetric[MetricValueTypeVarT], ABC\n):\n    \"\"\"\n    A base class for aggregate counter metrics. These metrics increment a counter for each record.\n\n    Examples:\n    ```python\n    class RequestCountMetric(BaseAggregateCounterMetric[int]):\n        # ... Metric attributes ...\n    ```\n    \"\"\"\n\n    __is_abstract__: ClassVar[bool] = True\n\n    def __init_subclass__(cls, **kwargs) -&gt; None:\n        cls.__is_abstract__ = False\n        return super().__init_subclass__(**kwargs)\n\n    def _parse_record(\n        self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n    ) -&gt; MetricValueTypeVarT:\n        \"\"\"Return the value of the counter for this record.\"\"\"\n        return 1  # type: ignore\n\n    def _aggregate_value(self, value: MetricValueTypeVarT) -&gt; None:\n        \"\"\"Aggregate the metric value.\"\"\"\n        self._value += value  # type: ignore\n</code></pre>"},{"location":"api/#aiperfmetricsbase_aggregate_metric","title":"aiperf.metrics.base_aggregate_metric","text":""},{"location":"api/#aiperf.metrics.base_aggregate_metric.BaseAggregateMetric","title":"<code>BaseAggregateMetric</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT]</code>, <code>BaseMetric[MetricValueTypeVarT]</code>, <code>ABC</code></p> <p>A base class for aggregate metrics. These metrics keep track of a value or list of values over time.</p> <p>This metric type is unique in the fact that it is split into 2 distinct phases of processing, in order to support distributed processing.</p> <p>For each distributed RecordProcessor, an instance of this class is created. This instance is passed the record and the existing record metrics, and is responsible for returning the individual value for that record. It should not use or update the aggregate value here.</p> <p>The ResultsProcessor creates a singleton instance of this class, which will be used to aggregate the results from the distributed RecordProcessors. It calls the <code>_aggregate_value</code> method, which each metric class must implement to define how values from different processes are aggregated, such as summing the values, or taking the min/max/average, etc.</p> <p>Examples:</p> <pre><code>class RequestCountMetric(BaseAggregateMetric[int]):\n    # ... Metric attributes ...\n\n    def _parse_record(self, record: ParsedResponseRecord, record_metrics: MetricRecordDict) -&gt; int:\n        # We just return 1 since we are tracking the total count, and this is a single request.\n        return 1\n\n    def _aggregate_value(self, value: int) -&gt; None:\n        # We add the value to the aggregate value.\n        self._value += value\n</code></pre> Source code in <code>aiperf/metrics/base_aggregate_metric.py</code> <pre><code>class BaseAggregateMetric(\n    Generic[MetricValueTypeVarT], BaseMetric[MetricValueTypeVarT], ABC\n):\n    \"\"\"A base class for aggregate metrics. These metrics keep track of a value or list of values over time.\n\n    This metric type is unique in the fact that it is split into 2 distinct phases of processing, in order to support distributed processing.\n\n    For each distributed RecordProcessor, an instance of this class is created. This instance is passed the record and the existing record metrics,\n    and is responsible for returning the individual value for that record. It should not use or update the aggregate value here.\n\n    The ResultsProcessor creates a singleton instance of this class, which will be used to aggregate the results from the distributed\n    RecordProcessors. It calls the `_aggregate_value` method, which each metric class must implement to define how values from different\n    processes are aggregated, such as summing the values, or taking the min/max/average, etc.\n\n    Examples:\n    ```python\n    class RequestCountMetric(BaseAggregateMetric[int]):\n        # ... Metric attributes ...\n\n        def _parse_record(self, record: ParsedResponseRecord, record_metrics: MetricRecordDict) -&gt; int:\n            # We just return 1 since we are tracking the total count, and this is a single request.\n            return 1\n\n        def _aggregate_value(self, value: int) -&gt; None:\n            # We add the value to the aggregate value.\n            self._value += value\n    ```\n    \"\"\"\n\n    type = MetricType.AGGREGATE\n\n    def __init__(self, default_value: MetricValueTypeVarT | None = None) -&gt; None:\n        \"\"\"Initialize the metric with optionally with a default value. If no default value is provided,\n        the default value is automatically set based on the value type.\"\"\"\n        self._value: MetricValueTypeVarT = (  # type: ignore\n            default_value\n            if default_value is not None\n            else self.value_type.default_factory()\n        )\n        self.aggregate_value: Callable[[MetricValueTypeVarT], None] = (\n            self._aggregate_value\n        )\n        super().__init__()\n\n    @property\n    def current_value(self) -&gt; MetricValueTypeVarT:\n        \"\"\"Get the current value of the metric.\"\"\"\n        return self._value\n\n    def parse_record(\n        self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n    ) -&gt; MetricValueTypeVarT:\n        \"\"\"Parse the record and return the individual value.\n\n        Raises:\n            ValueError: If the metric cannot be computed for the given inputs.\n        \"\"\"\n        self._require_valid_record(record)\n        self._check_metrics(record_metrics)\n        return self._parse_record(record, record_metrics)\n\n    @abstractmethod\n    def _parse_record(\n        self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n    ) -&gt; MetricValueTypeVarT:\n        \"\"\"Parse the record and *return* the individual value base on this record, and this record alone. This\n        method is implemented by subclasses.\n\n        NOTE: Do not use or update the aggregate value here.\n\n        This method is called after the required metrics are checked, so it can assume that the required metrics are available.\n        This method is called after the record is checked, so it can assume that the record is valid.\n\n        Raises:\n            ValueError: If the metric cannot be computed for the given inputs.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    # NOTE: This method does not return a value on purpose, as a hint to the user that the\n    #       internal value is supposed to be updated.\n    @abstractmethod\n    def _aggregate_value(self, value: MetricValueTypeVarT) -&gt; None:\n        \"\"\"Aggregate the metric value. This method is implemented by subclasses.\n\n        This method is called with the result value from the `_parse_record` method, from each distributed record processor.\n        It is the responsibility of each metric class to implement how values from different processes are aggregated, such\n        as summing the values, or taking the min/max/average, etc.\n\n        NOTE: The order of the values is not guaranteed.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n</code></pre>"},{"location":"api/#aiperf.metrics.base_aggregate_metric.BaseAggregateMetric.current_value","title":"<code>current_value</code>  <code>property</code>","text":"<p>Get the current value of the metric.</p>"},{"location":"api/#aiperf.metrics.base_aggregate_metric.BaseAggregateMetric.__init__","title":"<code>__init__(default_value=None)</code>","text":"<p>Initialize the metric with optionally with a default value. If no default value is provided, the default value is automatically set based on the value type.</p> Source code in <code>aiperf/metrics/base_aggregate_metric.py</code> <pre><code>def __init__(self, default_value: MetricValueTypeVarT | None = None) -&gt; None:\n    \"\"\"Initialize the metric with optionally with a default value. If no default value is provided,\n    the default value is automatically set based on the value type.\"\"\"\n    self._value: MetricValueTypeVarT = (  # type: ignore\n        default_value\n        if default_value is not None\n        else self.value_type.default_factory()\n    )\n    self.aggregate_value: Callable[[MetricValueTypeVarT], None] = (\n        self._aggregate_value\n    )\n    super().__init__()\n</code></pre>"},{"location":"api/#aiperf.metrics.base_aggregate_metric.BaseAggregateMetric.parse_record","title":"<code>parse_record(record, record_metrics)</code>","text":"<p>Parse the record and return the individual value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the metric cannot be computed for the given inputs.</p> Source code in <code>aiperf/metrics/base_aggregate_metric.py</code> <pre><code>def parse_record(\n    self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n) -&gt; MetricValueTypeVarT:\n    \"\"\"Parse the record and return the individual value.\n\n    Raises:\n        ValueError: If the metric cannot be computed for the given inputs.\n    \"\"\"\n    self._require_valid_record(record)\n    self._check_metrics(record_metrics)\n    return self._parse_record(record, record_metrics)\n</code></pre>"},{"location":"api/#aiperfmetricsbase_derived_metric","title":"aiperf.metrics.base_derived_metric","text":""},{"location":"api/#aiperf.metrics.base_derived_metric.BaseDerivedMetric","title":"<code>BaseDerivedMetric</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT]</code>, <code>BaseMetric[MetricValueTypeVarT]</code>, <code>ABC</code></p> <p>A base class for derived metrics. These metrics are computed from other metrics, and do not require any knowledge of the individual records. The final results will be a single computed value (or list of values).</p> <p>NOTE: The generic type can be a list of values, or a single value.</p> <p>Examples:</p> <pre><code>class RequestThroughputMetric(BaseDerivedMetric[float]):\n    # ... Metric attributes ...\n\n    def _derive_value(self, metric_results: MetricResultsDict) -&gt; float:\n        request_count = metric_results[RequestCountMetric.tag]\n        benchmark_duration = metric_results[BenchmarkDurationMetric.tag]\n        return request_count / (benchmark_duration / NANOS_PER_SECOND)\n</code></pre> Source code in <code>aiperf/metrics/base_derived_metric.py</code> <pre><code>class BaseDerivedMetric(\n    Generic[MetricValueTypeVarT], BaseMetric[MetricValueTypeVarT], ABC\n):\n    \"\"\"A base class for derived metrics. These metrics are computed from other metrics,\n    and do not require any knowledge of the individual records. The final results will be a single computed value (or list of values).\n\n    NOTE: The generic type can be a list of values, or a single value.\n\n    Examples:\n    ```python\n    class RequestThroughputMetric(BaseDerivedMetric[float]):\n        # ... Metric attributes ...\n\n        def _derive_value(self, metric_results: MetricResultsDict) -&gt; float:\n            request_count = metric_results[RequestCountMetric.tag]\n            benchmark_duration = metric_results[BenchmarkDurationMetric.tag]\n            return request_count / (benchmark_duration / NANOS_PER_SECOND)\n    ```\n    \"\"\"\n\n    type = MetricType.DERIVED\n\n    def derive_value(self, metric_results: MetricResultsDict) -&gt; MetricValueTypeVarT:\n        \"\"\"Derive the metric value.\"\"\"\n        self._check_metrics(metric_results)\n        return self._derive_value(metric_results)\n\n    @abstractmethod\n    def _derive_value(self, metric_results: MetricResultsDict) -&gt; MetricValueTypeVarT:\n        \"\"\"Derive the metric value. This method is implemented by subclasses.\n        This method is called after the required metrics are checked, so it can assume that the required metrics are available.\n\n        Raises:\n            ValueError: If the metric cannot be computed for the given inputs.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n</code></pre>"},{"location":"api/#aiperf.metrics.base_derived_metric.BaseDerivedMetric.derive_value","title":"<code>derive_value(metric_results)</code>","text":"<p>Derive the metric value.</p> Source code in <code>aiperf/metrics/base_derived_metric.py</code> <pre><code>def derive_value(self, metric_results: MetricResultsDict) -&gt; MetricValueTypeVarT:\n    \"\"\"Derive the metric value.\"\"\"\n    self._check_metrics(metric_results)\n    return self._derive_value(metric_results)\n</code></pre>"},{"location":"api/#aiperfmetricsbase_metric","title":"aiperf.metrics.base_metric","text":""},{"location":"api/#aiperf.metrics.base_metric.BaseMetric","title":"<code>BaseMetric</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT]</code>, <code>ABC</code></p> <p>A definition of a metric type.</p> <p>This class is not meant to be instantiated directly or subclassed directly. It is meant to be a common base for all of the base metric classes by type.</p> <p>The class attributes are: - tag: The tag of the metric. This must be a non-empty string that uniquely identifies the metric type. - header: The header of the metric. This is the user-friendly name of the metric that will be displayed in the Console Export. - short_header: The short header of the metric. This is the shortened user-friendly name of the metric for display in the Dashboard. - unit: The unit of the internal representation of the metric. This is used for converting to other units and for display. - display_unit: The unit of the metric that is used for display (if different from the unit). None means use the unit for display. - short_header_hide_unit: If True, the unit will not be displayed in the Dashboard short header. - display_order: The display order in the ConsoleExporter. Lower numbers are displayed first. None means unordered after any ordered metrics. - flags: The flags of the metric that determine how and when it is computed and displayed. - required_metrics: The metrics that must be available to compute the metric. This is a set of metric tags.</p> Source code in <code>aiperf/metrics/base_metric.py</code> <pre><code>class BaseMetric(Generic[MetricValueTypeVarT], ABC):\n    \"\"\"A definition of a metric type.\n\n    This class is not meant to be instantiated directly or subclassed directly.\n    It is meant to be a common base for all of the base metric classes by type.\n\n    The class attributes are:\n    - tag: The tag of the metric. This must be a non-empty string that uniquely identifies the metric type.\n    - header: The header of the metric. This is the user-friendly name of the metric that will be displayed in the Console Export.\n    - short_header: The short header of the metric. This is the shortened user-friendly name of the metric for display in the Dashboard.\n    - unit: The unit of the internal representation of the metric. This is used for converting to other units and for display.\n    - display_unit: The unit of the metric that is used for display (if different from the unit). None means use the unit for display.\n    - short_header_hide_unit: If True, the unit will not be displayed in the Dashboard short header.\n    - display_order: The display order in the ConsoleExporter. Lower numbers are displayed first. None means unordered after any ordered metrics.\n    - flags: The flags of the metric that determine how and when it is computed and displayed.\n    - required_metrics: The metrics that must be available to compute the metric. This is a set of metric tags.\n    \"\"\"\n\n    # User-defined attributes to be overridden by subclasses\n    tag: ClassVar[MetricTagT]\n    header: ClassVar[str] = \"\"\n    short_header: ClassVar[str | None] = None\n    short_header_hide_unit: ClassVar[bool] = False\n    unit: ClassVar[MetricUnitT]\n    display_unit: ClassVar[MetricUnitT | None] = None\n    display_order: ClassVar[int | None] = None\n    flags: ClassVar[MetricFlags] = MetricFlags.NONE\n    required_metrics: ClassVar[set[MetricTagT] | None] = None\n\n    # Auto-derived attributes\n    value_type: ClassVar[MetricValueType]  # Auto set based on generic type parameter\n    type: ClassVar[MetricType]  # Set by base subclasses\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"\n        This method is called when a class is subclassed from Metric.\n        It automatically registers the subclass in the MetricRegistry\n        dictionary using the `tag` class attribute.\n        The `tag` attribute must be a non-empty string that uniquely identifies the\n        metric type. Only concrete (non-abstract) classes will be registered.\n        \"\"\"\n\n        super().__init_subclass__(**kwargs)\n\n        # Only register concrete classes (not abstract ones)\n        if inspect.isabstract(cls) or (\n            hasattr(cls, \"__is_abstract__\") and cls.__is_abstract__\n        ):\n            return\n\n        # Verify that the class is a valid metric type\n        # Make sure to do this after checking for abstractness, so that the imports are available\n        cls._verify_base_class()\n\n        # Import MetricRegistry here to avoid circular imports\n        from aiperf.metrics.metric_registry import MetricRegistry\n\n        # Enforce that subclasses define a non-empty tag\n        if not cls.tag or not isinstance(cls.tag, str):\n            raise TypeError(\n                f\"Concrete metric class {cls.__name__} must define a non-empty 'tag' class attribute\"\n            )\n\n        # Auto-detect value type from generic parameter\n        cls.value_type = cls._detect_value_type()\n\n        MetricRegistry.register_metric(cls)\n\n    @classmethod\n    def _verify_base_class(cls) -&gt; None:\n        \"\"\"Verify that the class is a subclass of BaseRecordMetric, BaseAggregateMetric, or BaseDerivedMetric.\n        This is done to ensure that the class is a valid metric type.\n        \"\"\"\n        # Note: this is valid because the below imports are abstract, so they will not get here\n        from aiperf.metrics import (\n            BaseAggregateMetric,\n            BaseDerivedMetric,\n            BaseRecordMetric,\n        )\n\n        # Enforce that concrete subclasses are a subclass of BaseRecordMetric, BaseAggregateMetric, or BaseDerivedMetric\n        valid_base_classes = {\n            BaseRecordMetric,\n            BaseAggregateMetric,\n            BaseDerivedMetric,\n        }\n        if not any(issubclass(cls, base) for base in valid_base_classes):\n            raise TypeError(\n                f\"Concrete metric class {cls.__name__} must be a subclass of BaseRecordMetric, BaseAggregateMetric, or BaseDerivedMetric\"\n            )\n\n    @classmethod\n    def _detect_value_type(cls) -&gt; MetricValueType:\n        \"\"\"Automatically detect the MetricValueType from the generic type parameter.\"\"\"\n        # Look through the class hierarchy for the first Generic[Type] definition\n        for base in cls.__orig_bases__:  # type: ignore\n            if get_origin(base) is not None:\n                args = get_args(base)\n                if args:\n                    # the first argument is the generic type\n                    generic_type = args[0]\n                    return MetricValueType.from_python_type(generic_type)\n\n        raise ValueError(\n            f\"Unable to detect the value type for {cls.__name__}. Please check the generic type parameter.\"\n        )\n\n    def _require_valid_record(self, record: ParsedResponseRecord) -&gt; None:\n        \"\"\"Check that the record is valid.\"\"\"\n        if (not record or not record.valid) and not self.has_flags(\n            MetricFlags.ERROR_ONLY\n        ):\n            raise NoMetricValue(\"Invalid Record\")\n\n    def _check_metrics(self, metrics: MetricRecordDict | MetricResultsDict) -&gt; None:\n        \"\"\"Check that the required metrics are available.\"\"\"\n        if self.required_metrics is None:\n            return\n        for tag in self.required_metrics:\n            if tag not in metrics:\n                raise NoMetricValue(f\"Missing required metric: '{tag}'\")\n\n    @classmethod\n    def has_flags(cls, flags: MetricFlags) -&gt; bool:\n        \"\"\"Return True if the metric has the given flag(s) (regardless of other flags).\"\"\"\n        return cls.flags.has_flags(flags)\n\n    @classmethod\n    def has_any_flags(cls, flags: MetricFlags) -&gt; bool:\n        \"\"\"Return True if the metric has ANY of the given flag(s) (regardless of other flags).\"\"\"\n        return cls.flags.has_any_flags(flags)\n\n    @classmethod\n    def missing_flags(cls, flags: MetricFlags) -&gt; bool:\n        \"\"\"Return True if the metric does not have the given flag(s) (regardless of other flags). It will\n        return False if the metric has ANY of the given flags.\"\"\"\n        return cls.flags.missing_flags(flags)\n</code></pre>"},{"location":"api/#aiperf.metrics.base_metric.BaseMetric.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>This method is called when a class is subclassed from Metric. It automatically registers the subclass in the MetricRegistry dictionary using the <code>tag</code> class attribute. The <code>tag</code> attribute must be a non-empty string that uniquely identifies the metric type. Only concrete (non-abstract) classes will be registered.</p> Source code in <code>aiperf/metrics/base_metric.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"\n    This method is called when a class is subclassed from Metric.\n    It automatically registers the subclass in the MetricRegistry\n    dictionary using the `tag` class attribute.\n    The `tag` attribute must be a non-empty string that uniquely identifies the\n    metric type. Only concrete (non-abstract) classes will be registered.\n    \"\"\"\n\n    super().__init_subclass__(**kwargs)\n\n    # Only register concrete classes (not abstract ones)\n    if inspect.isabstract(cls) or (\n        hasattr(cls, \"__is_abstract__\") and cls.__is_abstract__\n    ):\n        return\n\n    # Verify that the class is a valid metric type\n    # Make sure to do this after checking for abstractness, so that the imports are available\n    cls._verify_base_class()\n\n    # Import MetricRegistry here to avoid circular imports\n    from aiperf.metrics.metric_registry import MetricRegistry\n\n    # Enforce that subclasses define a non-empty tag\n    if not cls.tag or not isinstance(cls.tag, str):\n        raise TypeError(\n            f\"Concrete metric class {cls.__name__} must define a non-empty 'tag' class attribute\"\n        )\n\n    # Auto-detect value type from generic parameter\n    cls.value_type = cls._detect_value_type()\n\n    MetricRegistry.register_metric(cls)\n</code></pre>"},{"location":"api/#aiperf.metrics.base_metric.BaseMetric.has_any_flags","title":"<code>has_any_flags(flags)</code>  <code>classmethod</code>","text":"<p>Return True if the metric has ANY of the given flag(s) (regardless of other flags).</p> Source code in <code>aiperf/metrics/base_metric.py</code> <pre><code>@classmethod\ndef has_any_flags(cls, flags: MetricFlags) -&gt; bool:\n    \"\"\"Return True if the metric has ANY of the given flag(s) (regardless of other flags).\"\"\"\n    return cls.flags.has_any_flags(flags)\n</code></pre>"},{"location":"api/#aiperf.metrics.base_metric.BaseMetric.has_flags","title":"<code>has_flags(flags)</code>  <code>classmethod</code>","text":"<p>Return True if the metric has the given flag(s) (regardless of other flags).</p> Source code in <code>aiperf/metrics/base_metric.py</code> <pre><code>@classmethod\ndef has_flags(cls, flags: MetricFlags) -&gt; bool:\n    \"\"\"Return True if the metric has the given flag(s) (regardless of other flags).\"\"\"\n    return cls.flags.has_flags(flags)\n</code></pre>"},{"location":"api/#aiperf.metrics.base_metric.BaseMetric.missing_flags","title":"<code>missing_flags(flags)</code>  <code>classmethod</code>","text":"<p>Return True if the metric does not have the given flag(s) (regardless of other flags). It will return False if the metric has ANY of the given flags.</p> Source code in <code>aiperf/metrics/base_metric.py</code> <pre><code>@classmethod\ndef missing_flags(cls, flags: MetricFlags) -&gt; bool:\n    \"\"\"Return True if the metric does not have the given flag(s) (regardless of other flags). It will\n    return False if the metric has ANY of the given flags.\"\"\"\n    return cls.flags.missing_flags(flags)\n</code></pre>"},{"location":"api/#aiperfmetricsbase_record_metric","title":"aiperf.metrics.base_record_metric","text":""},{"location":"api/#aiperf.metrics.base_record_metric.BaseRecordMetric","title":"<code>BaseRecordMetric</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT]</code>, <code>BaseMetric[MetricValueTypeVarT]</code>, <code>ABC</code></p> <p>A base class for record-based metrics. These metrics are computed for each record, and are independent of other records. The final results will be a list of values, one for each record.</p> <p>NOTE: Set the generic type to be the type of the individual values, and NOT a list, unless the metric produces a list for every record. In that case, the result will be a list of lists.</p> <p>Examples:</p> <pre><code>class InputSequenceLengthMetric(BaseRecordMetric[int]):\n    # ... Metric attributes ...\n    # ... Input validation ...\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        return record.input_token_count\n</code></pre> Source code in <code>aiperf/metrics/base_record_metric.py</code> <pre><code>class BaseRecordMetric(\n    Generic[MetricValueTypeVarT], BaseMetric[MetricValueTypeVarT], ABC\n):\n    \"\"\"A base class for record-based metrics. These metrics are computed for each record,\n    and are independent of other records. The final results will be a list of values, one for each record.\n\n    NOTE: Set the generic type to be the type of the individual values, and NOT a list, unless the metric produces\n    a list *for every record*. In that case, the result will be a list of lists.\n\n    Examples:\n    ```python\n    class InputSequenceLengthMetric(BaseRecordMetric[int]):\n        # ... Metric attributes ...\n        # ... Input validation ...\n\n        def _parse_record(\n            self,\n            record: ParsedResponseRecord,\n            record_metrics: MetricRecordDict,\n        ) -&gt; int:\n            return record.input_token_count\n    ```\n    \"\"\"\n\n    type = MetricType.RECORD\n\n    def parse_record(\n        self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n    ) -&gt; MetricValueTypeVarT:\n        \"\"\"Parse a single record and return the metric value.\"\"\"\n        self._require_valid_record(record)\n        self._check_metrics(record_metrics)\n        return self._parse_record(record, record_metrics)\n\n    @abstractmethod\n    def _parse_record(\n        self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n    ) -&gt; MetricValueTypeVarT:\n        \"\"\"Parse a single record and return the metric value. This method is implemented by subclasses.\n        This method is called after the required metrics are checked, so it can assume that the required metrics are available.\n        This method is called after the record is checked, so it can assume that the record is valid.\n\n        Raises:\n            ValueError: If the metric cannot be computed for the given inputs.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n</code></pre>"},{"location":"api/#aiperf.metrics.base_record_metric.BaseRecordMetric.parse_record","title":"<code>parse_record(record, record_metrics)</code>","text":"<p>Parse a single record and return the metric value.</p> Source code in <code>aiperf/metrics/base_record_metric.py</code> <pre><code>def parse_record(\n    self, record: ParsedResponseRecord, record_metrics: MetricRecordDict\n) -&gt; MetricValueTypeVarT:\n    \"\"\"Parse a single record and return the metric value.\"\"\"\n    self._require_valid_record(record)\n    self._check_metrics(record_metrics)\n    return self._parse_record(record, record_metrics)\n</code></pre>"},{"location":"api/#aiperfmetricsderived_sum_metric","title":"aiperf.metrics.derived_sum_metric","text":""},{"location":"api/#aiperf.metrics.derived_sum_metric.DerivedSumMetric","title":"<code>DerivedSumMetric</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT, RecordMetricT]</code>, <code>BaseDerivedMetric[MetricValueTypeVarT]</code>, <code>ABC</code></p> <p>This class defines the base class for derived sum metrics. These metrics are automatically derived from a record metric, by returning the sum of the values of the record metric.</p> <p>Examples:</p> <pre><code>class TotalReasoningTokensMetric(DerivedSumMetric[ReasoningTokenCountMetric, int]):\n    # ... Metric attributes ...\n</code></pre> Source code in <code>aiperf/metrics/derived_sum_metric.py</code> <pre><code>class DerivedSumMetric(\n    Generic[MetricValueTypeVarT, RecordMetricT],\n    BaseDerivedMetric[MetricValueTypeVarT],\n    ABC,\n):\n    \"\"\"\n    This class defines the base class for derived sum metrics. These metrics are automatically derived from a record metric,\n    by returning the sum of the values of the record metric.\n\n    Examples:\n    ```python\n    class TotalReasoningTokensMetric(DerivedSumMetric[ReasoningTokenCountMetric, int]):\n        # ... Metric attributes ...\n    ```\n    \"\"\"\n\n    record_metric_type: ClassVar[type[BaseRecordMetric]]\n    __is_abstract__: ClassVar[bool] = True\n\n    def __init_subclass__(cls, **kwargs):\n        # Look through the class hierarchy for the first Generic[Type] definition\n        for base in cls.__orig_bases__:  # type: ignore\n            if get_origin(base) is not None:\n                args = get_args(base)\n                if args:\n                    # the second argument is the record metric type\n                    generic_type = args[1]\n                    cls.record_metric_type = generic_type\n                    cls.required_metrics = {generic_type.tag}\n                    cls.flags = (\n                        generic_type.flags\n                        if cls.flags is MetricFlags.NONE\n                        else cls.flags\n                    )\n                    cls.unit = generic_type.unit\n                    cls.__is_abstract__ = False\n                    break\n\n        super().__init_subclass__(**kwargs)\n\n    def _derive_value(self, metric_results: MetricResultsDict) -&gt; MetricValueTypeVarT:\n        metric_values = metric_results.get(self.record_metric_type.tag)\n        if not metric_values:\n            raise ValueError(\n                f\"{self.record_metric_type.tag} is missing in the metrics.\"\n            )\n        if not isinstance(metric_values, MetricArray):\n            raise ValueError(f\"{self.record_metric_type.tag} is not a MetricArray.\")\n        return metric_values.sum\n</code></pre>"},{"location":"api/#aiperfmetricsmetric_dicts","title":"aiperf.metrics.metric_dicts","text":""},{"location":"api/#aiperf.metrics.metric_dicts.BaseMetricDict","title":"<code>BaseMetricDict</code>","text":"<p>               Bases: <code>Generic[MetricDictValueTypeVarT]</code>, <code>dict[MetricTagT, MetricDictValueTypeVarT]</code></p> <p>Base class for all metric dicts.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>class BaseMetricDict(\n    Generic[MetricDictValueTypeVarT], dict[MetricTagT, MetricDictValueTypeVarT]\n):\n    \"\"\"Base class for all metric dicts.\"\"\"\n\n    def get_or_raise(self, metric: type[\"BaseMetric\"]) -&gt; MetricDictValueTypeT:\n        \"\"\"Get the value of a metric, or raise NoMetricValue if it is not available.\"\"\"\n        value = self.get(metric.tag)\n        if not value:\n            raise NoMetricValue(f\"Metric {metric.tag} is not available for the record.\")\n        return value\n\n    def get_converted_or_raise(\n        self, metric: type[\"BaseMetric\"], other_unit: MetricUnitT\n    ) -&gt; float:\n        \"\"\"Get the value of a metric, but converted to a different unit, or raise NoMetricValue if it is not available.\"\"\"\n        return metric.unit.convert_to(other_unit, self.get_or_raise(metric))  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.BaseMetricDict.get_converted_or_raise","title":"<code>get_converted_or_raise(metric, other_unit)</code>","text":"<p>Get the value of a metric, but converted to a different unit, or raise NoMetricValue if it is not available.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>def get_converted_or_raise(\n    self, metric: type[\"BaseMetric\"], other_unit: MetricUnitT\n) -&gt; float:\n    \"\"\"Get the value of a metric, but converted to a different unit, or raise NoMetricValue if it is not available.\"\"\"\n    return metric.unit.convert_to(other_unit, self.get_or_raise(metric))  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.BaseMetricDict.get_or_raise","title":"<code>get_or_raise(metric)</code>","text":"<p>Get the value of a metric, or raise NoMetricValue if it is not available.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>def get_or_raise(self, metric: type[\"BaseMetric\"]) -&gt; MetricDictValueTypeT:\n    \"\"\"Get the value of a metric, or raise NoMetricValue if it is not available.\"\"\"\n    value = self.get(metric.tag)\n    if not value:\n        raise NoMetricValue(f\"Metric {metric.tag} is not available for the record.\")\n    return value\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricArray","title":"<code>MetricArray</code>","text":"<p>               Bases: <code>Generic[MetricValueTypeVarT]</code></p> <p>NumPy backed array for metric data.</p> <p>This is used to store the values of a metric over time.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>class MetricArray(Generic[MetricValueTypeVarT]):\n    \"\"\"NumPy backed array for metric data.\n\n    This is used to store the values of a metric over time.\n    \"\"\"\n\n    def __init__(self, initial_capacity: int = 10000):\n        self._capacity = initial_capacity\n        self._data = np.empty(self._capacity)\n        self._size = 0\n        self._sum: MetricValueTypeVarT = 0  # type: ignore\n\n    def append(self, value: MetricValueTypeVarT) -&gt; None:\n        \"\"\"Append a value to the array.\"\"\"\n        if self._size &gt;= self._capacity:\n            # Double capacity when full\n            self._capacity *= 2\n            new_data = np.empty(self._capacity)\n            new_data[: self._size] = self._data[: self._size]\n            self._data = new_data\n\n        self._data[self._size] = value\n        self._size += 1\n        self._sum += value  # type: ignore\n\n    @property\n    def sum(self) -&gt; MetricValueTypeVarT:\n        \"\"\"Get the sum of the array.\"\"\"\n        return self._sum\n\n    @property\n    def data(self) -&gt; np.ndarray:\n        \"\"\"Return view of actual data\"\"\"\n        return self._data[: self._size]\n\n    def to_result(self, tag: MetricTagT, header: str, unit: str) -&gt; MetricResult:\n        \"\"\"Compute metric stats with zero-copy\"\"\"\n\n        arr = self.data\n        p1, p5, p25, p50, p75, p90, p95, p99 = np.percentile(\n            arr, [1, 5, 25, 50, 75, 90, 95, 99]\n        )\n        return MetricResult(\n            tag=tag,\n            header=header,\n            unit=unit,\n            min=np.min(arr),\n            max=np.max(arr),\n            avg=float(np.mean(arr)),\n            std=float(np.std(arr)),\n            p1=p1,\n            p5=p5,\n            p25=p25,\n            p50=p50,\n            p75=p75,\n            p90=p90,\n            p95=p95,\n            p99=p99,\n            count=self._size,\n        )\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Return view of actual data</p>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricArray.sum","title":"<code>sum</code>  <code>property</code>","text":"<p>Get the sum of the array.</p>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricArray.append","title":"<code>append(value)</code>","text":"<p>Append a value to the array.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>def append(self, value: MetricValueTypeVarT) -&gt; None:\n    \"\"\"Append a value to the array.\"\"\"\n    if self._size &gt;= self._capacity:\n        # Double capacity when full\n        self._capacity *= 2\n        new_data = np.empty(self._capacity)\n        new_data[: self._size] = self._data[: self._size]\n        self._data = new_data\n\n    self._data[self._size] = value\n    self._size += 1\n    self._sum += value  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricArray.to_result","title":"<code>to_result(tag, header, unit)</code>","text":"<p>Compute metric stats with zero-copy</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>def to_result(self, tag: MetricTagT, header: str, unit: str) -&gt; MetricResult:\n    \"\"\"Compute metric stats with zero-copy\"\"\"\n\n    arr = self.data\n    p1, p5, p25, p50, p75, p90, p95, p99 = np.percentile(\n        arr, [1, 5, 25, 50, 75, 90, 95, 99]\n    )\n    return MetricResult(\n        tag=tag,\n        header=header,\n        unit=unit,\n        min=np.min(arr),\n        max=np.max(arr),\n        avg=float(np.mean(arr)),\n        std=float(np.std(arr)),\n        p1=p1,\n        p5=p5,\n        p25=p25,\n        p50=p50,\n        p75=p75,\n        p90=p90,\n        p95=p95,\n        p99=p99,\n        count=self._size,\n    )\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricRecordDict","title":"<code>MetricRecordDict</code>","text":"<p>               Bases: <code>BaseMetricDict[MetricValueTypeT]</code></p> <p>A dict of metrics for a single record. This is used to store the current values of all metrics that have been computed for a single record.</p> <p>This will include: - The current value of any <code>BaseRecordMetric</code> that has been computed for this record. - The new value of any <code>BaseAggregateMetric</code> that has been computed for this record. - No <code>BaseDerivedMetric</code>s will be included.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>class MetricRecordDict(BaseMetricDict[MetricValueTypeT]):\n    \"\"\"\n    A dict of metrics for a single record. This is used to store the current values\n    of all metrics that have been computed for a single record.\n\n    This will include:\n    - The current value of any `BaseRecordMetric` that has been computed for this record.\n    - The new value of any `BaseAggregateMetric` that has been computed for this record.\n    - No `BaseDerivedMetric`s will be included.\n    \"\"\"\n\n    pass  # Everything is handled by the BaseMetricDict class.\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricResultsDict","title":"<code>MetricResultsDict</code>","text":"<p>               Bases: <code>BaseMetricDict[MetricDictValueTypeT]</code></p> <p>A dict of metrics over an entire run. This is used to store the final values of all metrics that have been computed for an entire run.</p> <p>This will include: - All <code>BaseRecordMetric</code>s as a MetricArray of their values. - The most recent value of each <code>BaseAggregateMetric</code>. - The value of any <code>BaseDerivedMetric</code> that has already been computed.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>class MetricResultsDict(BaseMetricDict[MetricDictValueTypeT]):\n    \"\"\"\n    A dict of metrics over an entire run. This is used to store the final values\n    of all metrics that have been computed for an entire run.\n\n    This will include:\n    - All `BaseRecordMetric`s as a MetricArray of their values.\n    - The most recent value of each `BaseAggregateMetric`.\n    - The value of any `BaseDerivedMetric` that has already been computed.\n    \"\"\"\n\n    def get_converted_or_raise(\n        self, metric: type[\"BaseMetric\"], other_unit: MetricUnitT\n    ) -&gt; float:\n        \"\"\"Get the value of a metric, but converted to a different unit, or raise NoMetricValue if it is not available.\"\"\"\n        if metric.type == MetricType.RECORD:\n            # Record metrics are a MetricArray of values, so we can't convert them directly.\n            raise ValueError(\n                f\"Cannot convert a record metric to a different unit: {metric.tag}\"\n            )\n        return super().get_converted_or_raise(metric, other_unit)\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_dicts.MetricResultsDict.get_converted_or_raise","title":"<code>get_converted_or_raise(metric, other_unit)</code>","text":"<p>Get the value of a metric, but converted to a different unit, or raise NoMetricValue if it is not available.</p> Source code in <code>aiperf/metrics/metric_dicts.py</code> <pre><code>def get_converted_or_raise(\n    self, metric: type[\"BaseMetric\"], other_unit: MetricUnitT\n) -&gt; float:\n    \"\"\"Get the value of a metric, but converted to a different unit, or raise NoMetricValue if it is not available.\"\"\"\n    if metric.type == MetricType.RECORD:\n        # Record metrics are a MetricArray of values, so we can't convert them directly.\n        raise ValueError(\n            f\"Cannot convert a record metric to a different unit: {metric.tag}\"\n        )\n    return super().get_converted_or_raise(metric, other_unit)\n</code></pre>"},{"location":"api/#aiperfmetricsmetric_registry","title":"aiperf.metrics.metric_registry","text":""},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry","title":"<code>MetricRegistry</code>","text":"<p>A registry for metrics.</p> <p>This is used to store all the metrics that are available to the system. It is used to lookup metrics by their tag, and to get all the metrics that are available. It also provides methods to get metrics by their type, flag, and to create a dependency order for the metrics. It is also used to create instances of metrics.</p> <p>This class is not meant to be instantiated directly. It is meant to be used as a singleton via classmethods.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>class MetricRegistry:\n    \"\"\"\n    A registry for metrics.\n\n    This is used to store all the metrics that are available to the system.\n    It is used to lookup metrics by their tag, and to get all the metrics that are available.\n    It also provides methods to get metrics by their type, flag, and to create a dependency order for the metrics.\n    It is also used to create instances of metrics.\n\n    This class is not meant to be instantiated directly. It is meant to be used as a singleton via classmethods.\n    \"\"\"\n\n    # Map of metric tags to their classes\n    _metrics_map: dict[MetricTagT, type[\"BaseMetric\"]] = {}\n\n    # Map of metric tags to their instances\n    _instances_map: dict[MetricTagT, \"BaseMetric\"] = {}\n    _instance_lock = Lock()\n\n    def __init__(self) -&gt; None:\n        raise TypeError(\n            \"MetricRegistry is a singleton and cannot be instantiated directly\"\n        )\n\n    @classmethod\n    def _discover_metrics(cls) -&gt; None:\n        \"\"\"\n        This method dynamically imports all metric type modules from the 'types' directory to ensure\n        all metric classes are registered via __init_subclass__. This will be called once when the\n        module is imported.\n        \"\"\"\n        # Get the types directory path\n        types_dir = Path(__file__).parent / \"types\"\n\n        # Ensure that the types directory exists\n        if not types_dir.exists() or not types_dir.is_dir():\n            raise MetricTypeError(\n                f\"Types directory '{types_dir.resolve()}' does not exist or is not a directory\"\n            )\n\n        # Get the module prefix for the types directory, which is the parent of\n        # this module, plus the types directory name.\n        # For example, `aiperf.metrics.metric_registry` will become `aiperf.metrics.types`\n        module_prefix = \".\".join([*cls.__module__.split(\".\")[:-1], \"types\"])\n        _logger.debug(\n            f\"Importing metric type modules from '{types_dir.resolve()}' with module prefix '{module_prefix}'\"\n        )\n        # Import all metric type modules to trigger registration\n        cls._import_metric_type_modules(types_dir, module_prefix)\n\n    @classmethod\n    def _import_metric_type_modules(cls, types_dir: Path, module_prefix: str) -&gt; None:\n        \"\"\"Import all metric type modules from the given directory. This will raise an error if the module cannot be imported.\"\"\"\n        for python_file in types_dir.glob(\"*.py\"):\n            if python_file.name != \"__init__.py\":\n                module_name = python_file.stem  # Get filename without extension\n                module_path = f\"{module_prefix}.{module_name}\"\n                try:\n                    _logger.debug(\n                        f\"Importing metric type module: '{module_path}' from '{python_file.resolve()}'\"\n                    )\n                    importlib.import_module(module_path)\n                except ImportError as err:\n                    raise MetricTypeError(\n                        f\"Error importing metric type module '{module_path}' from '{python_file.resolve()}'\"\n                    ) from err\n\n    @classmethod\n    def register_metric(cls, metric: type[\"BaseMetric\"]):\n        \"\"\"Register a metric class with the registry. This will raise a MetricTypeError if the class is already registered.\n\n        This method is called automatically via the __init_subclass__ method of the BaseMetric class, so there is no need\n        to call it manually.\n        \"\"\"\n        if metric.tag in cls._metrics_map:\n            # TODO: Should we consider adding an override_priority parameter to the metric class similar to AIPerfFactory?\n            #       This would allow the user to override built-in metrics with custom implementations, without requiring\n            #       them to modify the built-in metric classes.\n            raise MetricTypeError(\n                f\"Metric class with tag {metric.tag} already registered by {cls._metrics_map[metric.tag].__name__}\"\n            )\n\n        cls._metrics_map[metric.tag] = metric\n\n    @classmethod\n    def get_class(cls, tag: MetricTagT) -&gt; type[\"BaseMetric\"]:\n        \"\"\"Get a metric class by its tag.\n\n        Raises:\n            MetricTypeError: If the metric class is not found.\n        \"\"\"\n        try:\n            return cls._metrics_map[tag]\n        except KeyError as e:\n            raise MetricTypeError(f\"Metric class with tag '{tag}' not found\") from e\n\n    @classmethod\n    def get_instance(cls, tag: MetricTagT) -&gt; \"BaseMetric\":\n        \"\"\"Get an instance of a metric class by its tag. This will create a new instance if it does not exist.\n\n        Raises:\n            MetricTypeError: If the metric class is not found.\n        \"\"\"\n        # Check first without acquiring the lock for performance reasons. Since this is a hot path, we want to avoid\n        # acquiring the lock if we can. We can do this because we have added a secondary check after acquiring the lock.\n        if tag not in cls._instances_map:\n            with cls._instance_lock:\n                # Check again after acquiring the lock\n                if tag not in cls._instances_map:\n                    metric_class = cls.get_class(tag)\n                    cls._instances_map[tag] = metric_class()\n        return cls._instances_map[tag]\n\n    @classmethod\n    def tags_applicable_to(\n        cls,\n        required_flags: MetricFlags,\n        disallowed_flags: MetricFlags,\n        *types: MetricType,\n    ) -&gt; list[MetricTagT]:\n        \"\"\"Get metrics tags that are applicable to the given arguments.\n\n        This method is used to filter the metrics that are applicable to a given set of flags and types.\n        For instance, this can be used to only get all DERIVED metrics, or only get metrics that are\n        applicable to non-streaming endpoints, etc.\n\n        Arguments:\n            required_flags: The flags that the metric must have.\n            disallowed_flags: The flags that the metric must not have.\n            types: The types of metrics to include. If not provided, all types will be included.\n\n        Returns:\n            A list of metric tags that are applicable to the given arguments.\n        \"\"\"\n        return [\n            tag\n            for tag, metric_class in cls._metrics_map.items()\n            if metric_class.has_flags(required_flags)\n            and metric_class.missing_flags(disallowed_flags)\n            and (not types or metric_class.type in types)\n        ]\n\n    @classmethod\n    def all_tags(cls) -&gt; list[MetricTagT]:\n        \"\"\"Get all of the tags of the defined metric classes.\"\"\"\n        return list(cls._metrics_map.keys())\n\n    @classmethod\n    def all_classes(cls) -&gt; list[type[\"BaseMetric\"]]:\n        \"\"\"Get all of the classes of the defined metric classes.\"\"\"\n        return list(cls._metrics_map.values())\n\n    @classmethod\n    def classes_for(cls, tags: Iterable[MetricTagT]) -&gt; list[type[\"BaseMetric\"]]:\n        \"\"\"Get the classes for the given tags.\n\n        Raises:\n            MetricTypeError: If a tag is not found.\n        \"\"\"\n        return [cls.get_class(tag) for tag in tags]\n\n    @classmethod\n    def _validate_dependencies(cls) -&gt; None:\n        \"\"\"Validate that all dependencies are registered.\n\n        Raises:\n            MetricTypeError: If a dependency is not registered.\n        \"\"\"\n        all_tags = cls._metrics_map.keys()\n        all_classes = cls._metrics_map.values()\n\n        # Map of metric types to the types of metrics they can have dependencies on\n        _allowed_dependencies_by_type = {\n            # Record metrics can only depend on other record metrics\n            MetricType.RECORD: {MetricType.RECORD},\n            # Aggregate metrics can depend on other record or aggregate metrics\n            MetricType.AGGREGATE: {MetricType.RECORD, MetricType.AGGREGATE},\n            # Sum aggregate metrics can only depend on record metrics\n            MetricType.SUM_AGGREGATE: {MetricType.RECORD},\n            # Derived metrics can depend on any other metric type\n            MetricType.DERIVED: {\n                MetricType.RECORD,\n                MetricType.AGGREGATE,\n                MetricType.DERIVED,\n            },\n        }\n\n        # Validate that all required metrics are registered, and that the dependencies are allowed\n        for metric in all_classes:\n            for required_tag in metric.required_metrics or set():\n                # Validate that the dependency is registered\n                if required_tag not in all_tags:\n                    raise MetricTypeError(\n                        f\"Metric '{metric.tag}' depends on '{required_tag}', which is not registered\"\n                    )\n\n                # Validate that the dependency is allowed\n                required_metric_type = cls._metrics_map[required_tag].type\n                if (\n                    required_metric_type\n                    not in _allowed_dependencies_by_type[metric.type]\n                ):\n                    raise MetricTypeError(\n                        f\"Metric '{metric.tag}' is a {metric.type} metric, but depends on '{required_tag}', which is a {required_metric_type} metric\"\n                    )\n\n    @classmethod\n    def _get_all_required_tags(cls, tags: Iterable[MetricTagT]) -&gt; set[MetricTagT]:\n        \"\"\"Get all of the required tags, recursively, for a given list of metric tags.\"\"\"\n        required_tags = set(tags)\n        for metric_class in cls.classes_for(tags):\n            for required_tag in metric_class.required_metrics or set():\n                if required_tag not in required_tags:\n                    required_tags.add(required_tag)\n                    required_tags.update(cls._get_all_required_tags([required_tag]))\n        return required_tags\n\n    @classmethod\n    def create_dependency_order(cls) -&gt; list[MetricTagT]:\n        \"\"\"\n        Create a dependency order for all available metrics using topological sort.\n\n        See :meth:`create_dependency_order_for` for more details.\n        \"\"\"\n        return cls.create_dependency_order_for()\n\n    @classmethod\n    def create_dependency_order_for(\n        cls,\n        tags: Iterable[MetricTagT] | None = None,\n    ) -&gt; list[MetricTagT]:\n        \"\"\"\n        Create a dependency order for the given metrics using topological sort.\n\n        This ensures that all dependencies are computed before their dependents.\n        If `tags` is provided, only the tags present in `tags` will be included in the order.\n\n        Arguments:\n            tags: The tags of the metrics to compute the dependency order for. If not provided, all metrics will be included.\n\n        Returns:\n            List of metric tags in dependency order (dependencies first).\n\n        Raises:\n            MetricTypeError: If there are unregistered dependencies or circular dependencies.\n        \"\"\"\n        if tags is None:\n            tags = cls._metrics_map.keys()\n\n        # Build the dependency graph\n        sorter = graphlib.TopologicalSorter()\n\n        for metric in cls.classes_for(tags):\n            # Add the metric with its required dependencies\n            sorter.add(metric.tag, *(metric.required_metrics or set()))\n\n        try:\n            # Get the dependency order\n            order = list(sorter.static_order())\n\n            # Make sure we only return the tags that were requested\n            tags_set = set(tags)\n            return [tag for tag in order if tag in tags_set]\n\n        except graphlib.CycleError as e:\n            raise MetricTypeError(\n                f\"Circular dependency detected among metrics: {e}\"\n            ) from e\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.all_classes","title":"<code>all_classes()</code>  <code>classmethod</code>","text":"<p>Get all of the classes of the defined metric classes.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef all_classes(cls) -&gt; list[type[\"BaseMetric\"]]:\n    \"\"\"Get all of the classes of the defined metric classes.\"\"\"\n    return list(cls._metrics_map.values())\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.all_tags","title":"<code>all_tags()</code>  <code>classmethod</code>","text":"<p>Get all of the tags of the defined metric classes.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef all_tags(cls) -&gt; list[MetricTagT]:\n    \"\"\"Get all of the tags of the defined metric classes.\"\"\"\n    return list(cls._metrics_map.keys())\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.classes_for","title":"<code>classes_for(tags)</code>  <code>classmethod</code>","text":"<p>Get the classes for the given tags.</p> <p>Raises:</p> Type Description <code>MetricTypeError</code> <p>If a tag is not found.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef classes_for(cls, tags: Iterable[MetricTagT]) -&gt; list[type[\"BaseMetric\"]]:\n    \"\"\"Get the classes for the given tags.\n\n    Raises:\n        MetricTypeError: If a tag is not found.\n    \"\"\"\n    return [cls.get_class(tag) for tag in tags]\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.create_dependency_order","title":"<code>create_dependency_order()</code>  <code>classmethod</code>","text":"<p>Create a dependency order for all available metrics using topological sort.</p> <p>See :meth:<code>create_dependency_order_for</code> for more details.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef create_dependency_order(cls) -&gt; list[MetricTagT]:\n    \"\"\"\n    Create a dependency order for all available metrics using topological sort.\n\n    See :meth:`create_dependency_order_for` for more details.\n    \"\"\"\n    return cls.create_dependency_order_for()\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.create_dependency_order_for","title":"<code>create_dependency_order_for(tags=None)</code>  <code>classmethod</code>","text":"<p>Create a dependency order for the given metrics using topological sort.</p> <p>This ensures that all dependencies are computed before their dependents. If <code>tags</code> is provided, only the tags present in <code>tags</code> will be included in the order.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Iterable[MetricTagT] | None</code> <p>The tags of the metrics to compute the dependency order for. If not provided, all metrics will be included.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[MetricTagT]</code> <p>List of metric tags in dependency order (dependencies first).</p> <p>Raises:</p> Type Description <code>MetricTypeError</code> <p>If there are unregistered dependencies or circular dependencies.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef create_dependency_order_for(\n    cls,\n    tags: Iterable[MetricTagT] | None = None,\n) -&gt; list[MetricTagT]:\n    \"\"\"\n    Create a dependency order for the given metrics using topological sort.\n\n    This ensures that all dependencies are computed before their dependents.\n    If `tags` is provided, only the tags present in `tags` will be included in the order.\n\n    Arguments:\n        tags: The tags of the metrics to compute the dependency order for. If not provided, all metrics will be included.\n\n    Returns:\n        List of metric tags in dependency order (dependencies first).\n\n    Raises:\n        MetricTypeError: If there are unregistered dependencies or circular dependencies.\n    \"\"\"\n    if tags is None:\n        tags = cls._metrics_map.keys()\n\n    # Build the dependency graph\n    sorter = graphlib.TopologicalSorter()\n\n    for metric in cls.classes_for(tags):\n        # Add the metric with its required dependencies\n        sorter.add(metric.tag, *(metric.required_metrics or set()))\n\n    try:\n        # Get the dependency order\n        order = list(sorter.static_order())\n\n        # Make sure we only return the tags that were requested\n        tags_set = set(tags)\n        return [tag for tag in order if tag in tags_set]\n\n    except graphlib.CycleError as e:\n        raise MetricTypeError(\n            f\"Circular dependency detected among metrics: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.get_class","title":"<code>get_class(tag)</code>  <code>classmethod</code>","text":"<p>Get a metric class by its tag.</p> <p>Raises:</p> Type Description <code>MetricTypeError</code> <p>If the metric class is not found.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef get_class(cls, tag: MetricTagT) -&gt; type[\"BaseMetric\"]:\n    \"\"\"Get a metric class by its tag.\n\n    Raises:\n        MetricTypeError: If the metric class is not found.\n    \"\"\"\n    try:\n        return cls._metrics_map[tag]\n    except KeyError as e:\n        raise MetricTypeError(f\"Metric class with tag '{tag}' not found\") from e\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.get_instance","title":"<code>get_instance(tag)</code>  <code>classmethod</code>","text":"<p>Get an instance of a metric class by its tag. This will create a new instance if it does not exist.</p> <p>Raises:</p> Type Description <code>MetricTypeError</code> <p>If the metric class is not found.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef get_instance(cls, tag: MetricTagT) -&gt; \"BaseMetric\":\n    \"\"\"Get an instance of a metric class by its tag. This will create a new instance if it does not exist.\n\n    Raises:\n        MetricTypeError: If the metric class is not found.\n    \"\"\"\n    # Check first without acquiring the lock for performance reasons. Since this is a hot path, we want to avoid\n    # acquiring the lock if we can. We can do this because we have added a secondary check after acquiring the lock.\n    if tag not in cls._instances_map:\n        with cls._instance_lock:\n            # Check again after acquiring the lock\n            if tag not in cls._instances_map:\n                metric_class = cls.get_class(tag)\n                cls._instances_map[tag] = metric_class()\n    return cls._instances_map[tag]\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.register_metric","title":"<code>register_metric(metric)</code>  <code>classmethod</code>","text":"<p>Register a metric class with the registry. This will raise a MetricTypeError if the class is already registered.</p> <p>This method is called automatically via the init_subclass method of the BaseMetric class, so there is no need to call it manually.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef register_metric(cls, metric: type[\"BaseMetric\"]):\n    \"\"\"Register a metric class with the registry. This will raise a MetricTypeError if the class is already registered.\n\n    This method is called automatically via the __init_subclass__ method of the BaseMetric class, so there is no need\n    to call it manually.\n    \"\"\"\n    if metric.tag in cls._metrics_map:\n        # TODO: Should we consider adding an override_priority parameter to the metric class similar to AIPerfFactory?\n        #       This would allow the user to override built-in metrics with custom implementations, without requiring\n        #       them to modify the built-in metric classes.\n        raise MetricTypeError(\n            f\"Metric class with tag {metric.tag} already registered by {cls._metrics_map[metric.tag].__name__}\"\n        )\n\n    cls._metrics_map[metric.tag] = metric\n</code></pre>"},{"location":"api/#aiperf.metrics.metric_registry.MetricRegistry.tags_applicable_to","title":"<code>tags_applicable_to(required_flags, disallowed_flags, *types)</code>  <code>classmethod</code>","text":"<p>Get metrics tags that are applicable to the given arguments.</p> <p>This method is used to filter the metrics that are applicable to a given set of flags and types. For instance, this can be used to only get all DERIVED metrics, or only get metrics that are applicable to non-streaming endpoints, etc.</p> <p>Parameters:</p> Name Type Description Default <code>required_flags</code> <code>MetricFlags</code> <p>The flags that the metric must have.</p> required <code>disallowed_flags</code> <code>MetricFlags</code> <p>The flags that the metric must not have.</p> required <code>types</code> <code>MetricType</code> <p>The types of metrics to include. If not provided, all types will be included.</p> <code>()</code> <p>Returns:</p> Type Description <code>list[MetricTagT]</code> <p>A list of metric tags that are applicable to the given arguments.</p> Source code in <code>aiperf/metrics/metric_registry.py</code> <pre><code>@classmethod\ndef tags_applicable_to(\n    cls,\n    required_flags: MetricFlags,\n    disallowed_flags: MetricFlags,\n    *types: MetricType,\n) -&gt; list[MetricTagT]:\n    \"\"\"Get metrics tags that are applicable to the given arguments.\n\n    This method is used to filter the metrics that are applicable to a given set of flags and types.\n    For instance, this can be used to only get all DERIVED metrics, or only get metrics that are\n    applicable to non-streaming endpoints, etc.\n\n    Arguments:\n        required_flags: The flags that the metric must have.\n        disallowed_flags: The flags that the metric must not have.\n        types: The types of metrics to include. If not provided, all types will be included.\n\n    Returns:\n        A list of metric tags that are applicable to the given arguments.\n    \"\"\"\n    return [\n        tag\n        for tag, metric_class in cls._metrics_map.items()\n        if metric_class.has_flags(required_flags)\n        and metric_class.missing_flags(disallowed_flags)\n        and (not types or metric_class.type in types)\n    ]\n</code></pre>"},{"location":"api/#aiperfmetricstypesbenchmark_duration_metric","title":"aiperf.metrics.types.benchmark_duration_metric","text":""},{"location":"api/#aiperf.metrics.types.benchmark_duration_metric.BenchmarkDurationMetric","title":"<code>BenchmarkDurationMetric</code>","text":"<p>               Bases: <code>BaseDerivedMetric[int]</code></p> <p>This is the duration of the benchmark, from the first request to the last response.</p> Formula <pre><code>Benchmark Duration = Maximum Response Timestamp - Minimum Request Timestamp\n</code></pre> Source code in <code>aiperf/metrics/types/benchmark_duration_metric.py</code> <pre><code>class BenchmarkDurationMetric(BaseDerivedMetric[int]):\n    \"\"\"\n    This is the duration of the benchmark, from the first request to the last response.\n\n    Formula:\n        ```\n        Benchmark Duration = Maximum Response Timestamp - Minimum Request Timestamp\n        ```\n    \"\"\"\n\n    tag = \"benchmark_duration\"\n    header = \"Benchmark Duration\"\n    short_header = \"Duration\"\n    short_header_hide_unit = True\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.SECONDS\n    flags = MetricFlags.HIDDEN\n    required_metrics = {\n        MinRequestTimestampMetric.tag,\n        MaxResponseTimestampMetric.tag,\n    }\n\n    def _derive_value(\n        self,\n        metric_results: MetricResultsDict,\n    ) -&gt; int:\n        min_req_time = metric_results.get_or_raise(MinRequestTimestampMetric)\n        max_res_time = metric_results.get_or_raise(MaxResponseTimestampMetric)\n\n        if min_req_time &gt;= max_res_time:  # type: ignore\n            raise ValueError(\n                \"Min request must be less than max response to calculate benchmark duration.\"\n            )\n\n        return max_res_time - min_req_time  # type: ignore\n</code></pre>"},{"location":"api/#aiperfmetricstypescredit_drop_latency_metric","title":"aiperf.metrics.types.credit_drop_latency_metric","text":""},{"location":"api/#aiperf.metrics.types.credit_drop_latency_metric.CreditDropLatencyMetric","title":"<code>CreditDropLatencyMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Credit Drop Latency metrics from records. This is an internal metric that is intended to be used for debugging and performance analysis of the AIPerf internal system.</p> <p>It exposes how long it took from when a credit was dropped, to when the actual request was sent. This will include the time it took to query the DatasetManager to get the Turn, as well as the time it took to format the request.</p> Formula <p>Credit Drop Latency = Request Start Time - Credit Drop Received Time</p> Source code in <code>aiperf/metrics/types/credit_drop_latency_metric.py</code> <pre><code>class CreditDropLatencyMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Credit Drop Latency metrics from records. This is an internal metric that is\n    intended to be used for debugging and performance analysis of the AIPerf internal system.\n\n    It exposes how long it took from when a credit was dropped, to when the actual request was sent. This will\n    include the time it took to query the DatasetManager to get the Turn, as well as the time it took to format\n    the request.\n\n    Formula:\n        Credit Drop Latency = Request Start Time - Credit Drop Received Time\n    \"\"\"\n\n    tag = \"credit_drop_latency\"\n    header = \"Credit Drop Latency\"\n    short_header = \"Credit Latency\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    flags = MetricFlags.INTERNAL\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the credit drop latency from the record and returns it.\n\n        Raises:\n            ValueError: If the record does not include a credit drop latency.\n        \"\"\"\n        if not record.request.credit_drop_latency:\n            raise NoMetricValue(\"Credit Drop Latency is not included in the record.\")\n\n        return record.request.credit_drop_latency\n</code></pre>"},{"location":"api/#aiperfmetricstypeserror_request_count","title":"aiperf.metrics.types.error_request_count","text":""},{"location":"api/#aiperf.metrics.types.error_request_count.ErrorRequestCountMetric","title":"<code>ErrorRequestCountMetric</code>","text":"<p>               Bases: <code>BaseAggregateCounterMetric[int]</code></p> <p>This is the total number of error requests processed by the benchmark. It is incremented for each error request.</p> Formula <pre><code>Error Request Count = Sum(Error Requests)\n</code></pre> Source code in <code>aiperf/metrics/types/error_request_count.py</code> <pre><code>class ErrorRequestCountMetric(BaseAggregateCounterMetric[int]):\n    \"\"\"\n    This is the total number of error requests processed by the benchmark.\n    It is incremented for each error request.\n\n    Formula:\n        ```\n        Error Request Count = Sum(Error Requests)\n        ```\n    \"\"\"\n\n    tag = \"error_request_count\"\n    header = \"Error Request Count\"\n    short_header = \"Error Count\"\n    short_header_hide_unit = True\n    unit = GenericMetricUnit.REQUESTS\n    flags = MetricFlags.ERROR_ONLY\n    required_metrics = None\n</code></pre>"},{"location":"api/#aiperfmetricstypesinput_sequence_length_metric","title":"aiperf.metrics.types.input_sequence_length_metric","text":""},{"location":"api/#aiperf.metrics.types.input_sequence_length_metric.InputSequenceLengthMetric","title":"<code>InputSequenceLengthMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Input Sequence Length (ISL) metrics from records.</p> Formula <p>Input Sequence Length = Sum of Input Token Counts</p> Source code in <code>aiperf/metrics/types/input_sequence_length_metric.py</code> <pre><code>class InputSequenceLengthMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Input Sequence Length (ISL) metrics from records.\n\n    Formula:\n        Input Sequence Length = Sum of Input Token Counts\n    \"\"\"\n\n    tag = \"input_sequence_length\"\n    header = \"Input Sequence Length\"\n    short_header = \"ISL\"\n    unit = GenericMetricUnit.TOKENS\n    display_order = 700\n    flags = MetricFlags.PRODUCES_TOKENS_ONLY | MetricFlags.LARGER_IS_BETTER\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the input token count from the record and returns it.\n\n        Raises:\n            ValueError: If the record does not have an input token count.\n        \"\"\"\n        if record.input_token_count is None:\n            raise NoMetricValue(\"Input Token Count is not available for the record.\")\n\n        return record.input_token_count\n</code></pre>"},{"location":"api/#aiperfmetricstypesinter_token_latency_metric","title":"aiperf.metrics.types.inter_token_latency_metric","text":""},{"location":"api/#aiperf.metrics.types.inter_token_latency_metric.InterTokenLatencyMetric","title":"<code>InterTokenLatencyMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[float]</code></p> <p>Post Processor for calculating Inter Token Latency (ITL) metric.</p> Formula <p>Inter Token Latency = (Request Latency - Time to First Token) / (Output Sequence Length - 1)</p> Source code in <code>aiperf/metrics/types/inter_token_latency_metric.py</code> <pre><code>class InterTokenLatencyMetric(BaseRecordMetric[float]):\n    \"\"\"\n    Post Processor for calculating Inter Token Latency (ITL) metric.\n\n    Formula:\n        Inter Token Latency = (Request Latency - Time to First Token) / (Output Sequence Length - 1)\n    \"\"\"\n\n    tag = \"inter_token_latency\"\n    header = \"Inter Token Latency\"\n    short_header = \"ITL\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    display_order = 400\n    flags = MetricFlags.STREAMING_TOKENS_ONLY | MetricFlags.LARGER_IS_BETTER\n    required_metrics = {\n        RequestLatencyMetric.tag,\n        TTFTMetric.tag,\n        OutputSequenceLengthMetric.tag,\n    }\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; float:\n        \"\"\"\n        Calculates the Inter Token Latency (ITL) metric.\n        \"\"\"\n        osl = record_metrics.get_or_raise(OutputSequenceLengthMetric)\n        if osl &lt; 2:  # type: ignore\n            raise NoMetricValue(f\"Output sequence length must be at least 2, got {osl}\")\n\n        ttft = record_metrics.get_or_raise(TTFTMetric)\n        request_latency = record_metrics.get_or_raise(RequestLatencyMetric)\n\n        return (request_latency - ttft) / (osl - 1)  # type: ignore\n</code></pre>"},{"location":"api/#aiperfmetricstypesmax_response_metric","title":"aiperf.metrics.types.max_response_metric","text":""},{"location":"api/#aiperf.metrics.types.max_response_metric.MaxResponseTimestampMetric","title":"<code>MaxResponseTimestampMetric</code>","text":"<p>               Bases: <code>BaseAggregateMetric[int]</code></p> <p>Post-processor for calculating the maximum response time stamp metric from records.</p> Formula <p>Maximum Response Timestamp = Max(Final Response Timestamps)</p> Source code in <code>aiperf/metrics/types/max_response_metric.py</code> <pre><code>class MaxResponseTimestampMetric(BaseAggregateMetric[int]):\n    \"\"\"\n    Post-processor for calculating the maximum response time stamp metric from records.\n\n    Formula:\n        Maximum Response Timestamp = Max(Final Response Timestamps)\n    \"\"\"\n\n    tag = \"max_response_timestamp\"\n    header = \"Maximum Response Timestamp\"\n    short_header = \"Max Resp\"\n    short_header_hide_unit = True\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricDateTimeUnit.DATE_TIME\n    flags = MetricFlags.HIDDEN\n    required_metrics = {\n        RequestLatencyMetric.tag,\n    }\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        Updates the maximum response timestamp metric.\n        \"\"\"\n        # Compute the final response timestamp by adding the request latency to the request timestamp.\n        # We do this because we want wall-clock timestamps, and the only one we have that is wall-clock\n        # time is the timestamp_ns for the start of the request, so we need to use that and work from there.\n        request_latency: int = record_metrics.get_or_raise(RequestLatencyMetric)  # type: ignore\n        final_response_ts = record.timestamp_ns + request_latency\n        return final_response_ts\n\n    def _aggregate_value(self, value: int) -&gt; None:\n        \"\"\"Aggregate the metric value. For this metric, we just take the max of the values from the different processes.\"\"\"\n        if value &gt; self._value:\n            self._value = value\n</code></pre>"},{"location":"api/#aiperfmetricstypesmin_request_metric","title":"aiperf.metrics.types.min_request_metric","text":""},{"location":"api/#aiperf.metrics.types.min_request_metric.MinRequestTimestampMetric","title":"<code>MinRequestTimestampMetric</code>","text":"<p>               Bases: <code>BaseAggregateMetric[int]</code></p> <p>Post-processor for calculating the minimum request time stamp metric from records.</p> Formula <p>Minimum Request Timestamp = Min(Request Timestamps)</p> Source code in <code>aiperf/metrics/types/min_request_metric.py</code> <pre><code>class MinRequestTimestampMetric(BaseAggregateMetric[int]):\n    \"\"\"\n    Post-processor for calculating the minimum request time stamp metric from records.\n\n    Formula:\n        Minimum Request Timestamp = Min(Request Timestamps)\n    \"\"\"\n\n    tag = \"min_request_timestamp\"\n    header = \"Minimum Request Timestamp\"\n    short_header = \"Min Req\"\n    short_header_hide_unit = True\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricDateTimeUnit.DATE_TIME\n    flags = MetricFlags.HIDDEN\n    required_metrics = None\n\n    def __init__(self) -&gt; None:\n        # Default to a large value, so that any request timestamp will be smaller.\n        super().__init__(default_value=sys.maxsize)\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"Return the request timestamp.\"\"\"\n        # NOTE: Use the request timestamp_ns, not the start_perf_ns, because we want wall-clock timestamps,\n        return record.timestamp_ns\n\n    def _aggregate_value(self, value: int) -&gt; None:\n        \"\"\"Aggregate the metric value. For this metric, we just take the min of the values from the different processes.\"\"\"\n        if value &lt; self._value:\n            self._value = value\n</code></pre>"},{"location":"api/#aiperfmetricstypesoutput_sequence_length_metric","title":"aiperf.metrics.types.output_sequence_length_metric","text":""},{"location":"api/#aiperf.metrics.types.output_sequence_length_metric.BenchmarkTokenCountMetric","title":"<code>BenchmarkTokenCountMetric</code>","text":"<p>               Bases: <code>DerivedSumMetric[int, OutputSequenceLengthMetric]</code></p> <p>This is the total number of completion tokens processed by the benchmark.</p> Formula <pre><code>Benchmark Token Count = Sum(Output Sequence Lengths)\n</code></pre> Source code in <code>aiperf/metrics/types/output_sequence_length_metric.py</code> <pre><code>class BenchmarkTokenCountMetric(DerivedSumMetric[int, OutputSequenceLengthMetric]):\n    \"\"\"\n    This is the total number of completion tokens processed by the benchmark.\n\n    Formula:\n        ```\n        Benchmark Token Count = Sum(Output Sequence Lengths)\n        ```\n    \"\"\"\n\n    tag = \"benchmark_token_count\"\n    header = \"Benchmark Token Count\"\n    short_header = \"Tokens\"\n    short_header_hide_unit = True\n    flags = (\n        MetricFlags.PRODUCES_TOKENS_ONLY\n        | MetricFlags.LARGER_IS_BETTER\n        | MetricFlags.HIDDEN\n    )\n</code></pre>"},{"location":"api/#aiperf.metrics.types.output_sequence_length_metric.OutputSequenceLengthMetric","title":"<code>OutputSequenceLengthMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Output Sequence Length (OSL) metrics from records.</p> <p>This is the total number of tokens generated by the model for one individual record. It is sometimes referred to as \"Completion Token Count\". It includes both reasoning and output tokens.</p> Formula <p>Output Sequence Length = Output Token Count + Reasoning Token Count</p> Source code in <code>aiperf/metrics/types/output_sequence_length_metric.py</code> <pre><code>class OutputSequenceLengthMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Output Sequence Length (OSL) metrics from records.\n\n    This is the total number of tokens generated by the model for one individual record.\n    It is sometimes referred to as \"Completion Token Count\". It includes both reasoning and\n    output tokens.\n\n    Formula:\n        Output Sequence Length = Output Token Count + Reasoning Token Count\n    \"\"\"\n\n    tag = \"output_sequence_length\"\n    header = \"Output Sequence Length\"\n    short_header = \"OSL\"\n    unit = GenericMetricUnit.TOKENS\n    display_order = 600\n    flags = MetricFlags.PRODUCES_TOKENS_ONLY | MetricFlags.LARGER_IS_BETTER\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the output and reasoning token counts from the record and returns the sum.\n\n        Raises:\n            ValueError: If the record does not have a output or reasoning token count.\n        \"\"\"\n        if record.output_token_count is None and record.reasoning_token_count is None:\n            raise NoMetricValue(\n                \"Output and reasoning token counts are missing in the record.\"\n            )\n\n        return (record.output_token_count or 0) + (record.reasoning_token_count or 0)\n</code></pre>"},{"location":"api/#aiperfmetricstypesoutput_token_count","title":"aiperf.metrics.types.output_token_count","text":""},{"location":"api/#aiperf.metrics.types.output_token_count.OutputTokenCountMetric","title":"<code>OutputTokenCountMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Output Token Count metrics from records.</p> <p>This is the total number of output tokens generated by the model for one individual record.</p> Formula <p>Output Token Count = Sum(Output Token Counts)</p> Source code in <code>aiperf/metrics/types/output_token_count.py</code> <pre><code>class OutputTokenCountMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Output Token Count metrics from records.\n\n    This is the total number of output tokens generated by the model for one\n    individual record.\n\n    Formula:\n        Output Token Count = Sum(Output Token Counts)\n    \"\"\"\n\n    tag = \"output_token_count\"\n    header = \"Output Token Count\"\n    short_header = \"Output Tokens\"\n    short_header_hide_unit = True\n    unit = GenericMetricUnit.TOKENS\n    flags = (\n        MetricFlags.PRODUCES_TOKENS_ONLY\n        | MetricFlags.LARGER_IS_BETTER\n        | MetricFlags.HIDDEN\n    )\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the output token count from the record and returns it.\n\n        Raises:\n            NoMetricValue: If the record does not have an output token count.\n        \"\"\"\n        if not record.output_token_count:\n            raise NoMetricValue(\"Output token count is missing in the record.\")\n\n        return record.output_token_count\n</code></pre>"},{"location":"api/#aiperf.metrics.types.output_token_count.TotalOutputTokensMetric","title":"<code>TotalOutputTokensMetric</code>","text":"<p>               Bases: <code>DerivedSumMetric[int, OutputTokenCountMetric]</code></p> <p>This is the total number of output tokens generated by the model for all records.</p> Formula <pre><code>Total Output Tokens = Sum(Output Token Counts)\n</code></pre> Source code in <code>aiperf/metrics/types/output_token_count.py</code> <pre><code>class TotalOutputTokensMetric(DerivedSumMetric[int, OutputTokenCountMetric]):\n    \"\"\"\n    This is the total number of output tokens generated by the model for\n    all records.\n\n    Formula:\n        ```\n        Total Output Tokens = Sum(Output Token Counts)\n        ```\n    \"\"\"\n\n    tag = \"total_output_tokens\"\n    header = \"Total Output Tokens\"\n    short_header = \"Total Output\"\n</code></pre>"},{"location":"api/#aiperfmetricstypesoutput_token_throughput_metrics","title":"aiperf.metrics.types.output_token_throughput_metrics","text":""},{"location":"api/#aiperf.metrics.types.output_token_throughput_metrics.OutputTokenThroughputMetric","title":"<code>OutputTokenThroughputMetric</code>","text":"<p>               Bases: <code>BaseDerivedMetric[float]</code></p> <p>Post Processor for calculating Output Token Throughput Metric.</p> Formula <p>Output Token Throughput = Benchmark Token Count / Benchmark Duration (seconds)</p> Source code in <code>aiperf/metrics/types/output_token_throughput_metrics.py</code> <pre><code>class OutputTokenThroughputMetric(BaseDerivedMetric[float]):\n    \"\"\"\n    Post Processor for calculating Output Token Throughput Metric.\n\n    Formula:\n        Output Token Throughput = Benchmark Token Count / Benchmark Duration (seconds)\n    \"\"\"\n\n    tag = \"output_token_throughput\"\n    header = \"Output Token Throughput\"\n    short_header = \"Output TPS\"\n    short_header_hide_unit = True\n    unit = MetricOverTimeUnit.TOKENS_PER_SECOND\n    display_order = 800\n    flags = MetricFlags.PRODUCES_TOKENS_ONLY | MetricFlags.LARGER_IS_BETTER\n    required_metrics = {\n        BenchmarkTokenCountMetric.tag,\n        BenchmarkDurationMetric.tag,\n    }\n\n    def _derive_value(\n        self,\n        metric_results: MetricResultsDict,\n    ) -&gt; float:\n        benchmark_token_count = metric_results.get_or_raise(BenchmarkTokenCountMetric)\n        benchmark_duration_converted = metric_results.get_converted_or_raise(\n            BenchmarkDurationMetric,\n            self.unit.time_unit,  # type: ignore\n        )\n        return benchmark_token_count / benchmark_duration_converted  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.metrics.types.output_token_throughput_metrics.OutputTokenThroughputPerUserMetric","title":"<code>OutputTokenThroughputPerUserMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[float]</code></p> <p>Post Processor for calculating Output Token Throughput Per User Metric.</p> Formula <p>Output Token Throughput Per User = 1 / Inter-Token Latency (seconds)</p> Source code in <code>aiperf/metrics/types/output_token_throughput_metrics.py</code> <pre><code>class OutputTokenThroughputPerUserMetric(BaseRecordMetric[float]):\n    \"\"\"\n    Post Processor for calculating Output Token Throughput Per User Metric.\n\n    Formula:\n        Output Token Throughput Per User = 1 / Inter-Token Latency (seconds)\n    \"\"\"\n\n    tag = \"output_token_throughput_per_user\"\n    header = \"Output Token Throughput Per User\"\n    short_header = \"Output TPS/User\"\n    short_header_hide_unit = True\n    unit = MetricOverTimeUnit.TOKENS_PER_SECOND_PER_USER\n    display_order = 500\n    flags = MetricFlags.STREAMING_TOKENS_ONLY | MetricFlags.LARGER_IS_BETTER\n    required_metrics = {\n        InterTokenLatencyMetric.tag,\n    }\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; float:\n        \"\"\"This method calculates the output token throughput per user by computing the inverse of the inter-token latency.\"\"\"\n        converted_itl = record_metrics.get_converted_or_raise(\n            InterTokenLatencyMetric,\n            self.unit.time_unit,  # type: ignore\n        )\n        return 1 / converted_itl\n</code></pre>"},{"location":"api/#aiperfmetricstypesprefill_throughput","title":"aiperf.metrics.types.prefill_throughput","text":""},{"location":"api/#aiperf.metrics.types.prefill_throughput.PrefillThroughputMetric","title":"<code>PrefillThroughputMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[float]</code></p> <p>Post-processor for calculating Prefill Throughput metrics from records. This is only applicable to streaming responses.</p> Formula <p>Prefill Throughput = Prefill Sequence Length / Time to First Token (seconds)</p> Source code in <code>aiperf/metrics/types/prefill_throughput.py</code> <pre><code>class PrefillThroughputMetric(BaseRecordMetric[float]):\n    \"\"\"\n    Post-processor for calculating Prefill Throughput metrics from records. This is only applicable to streaming responses.\n\n    Formula:\n        Prefill Throughput = Prefill Sequence Length / Time to First Token (seconds)\n    \"\"\"\n\n    tag = \"prefill_throughput\"\n    header = \"Prefill Throughput\"\n    short_header = \"Prefill TPS\"\n    short_header_hide_unit = True\n    unit = MetricOverTimeUnit.TOKENS_PER_SECOND\n    flags = (\n        MetricFlags.STREAMING_TOKENS_ONLY\n        | MetricFlags.LARGER_IS_BETTER\n        | MetricFlags.EXPERIMENTAL\n    )\n    required_metrics = {\n        InputSequenceLengthMetric.tag,\n        TTFTMetric.tag,\n    }\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; float:\n        \"\"\"This method calculates the prefill throughput by dividing the input sequence length by the TTFT.\"\"\"\n\n        isl = record_metrics.get_or_raise(InputSequenceLengthMetric)\n        converted_ttft = record_metrics.get_converted_or_raise(\n            TTFTMetric,\n            self.unit.time_unit,  # type: ignore\n        )\n        return isl / converted_ttft  # type: ignore\n</code></pre>"},{"location":"api/#aiperfmetricstypesreasoning_token_count","title":"aiperf.metrics.types.reasoning_token_count","text":""},{"location":"api/#aiperf.metrics.types.reasoning_token_count.ReasoningTokenCountMetric","title":"<code>ReasoningTokenCountMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Reasoning Token Count metrics from records.</p> <p>This is the total number of reasoning tokens generated by the model for one individual record.</p> Formula <pre><code>Reasoning Token Count = Sum(Reasoning Tokens)\n</code></pre> Source code in <code>aiperf/metrics/types/reasoning_token_count.py</code> <pre><code>class ReasoningTokenCountMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Reasoning Token Count metrics from records.\n\n    This is the total number of reasoning tokens generated by the model for\n    one individual record.\n\n    Formula:\n        ```\n        Reasoning Token Count = Sum(Reasoning Tokens)\n        ```\n    \"\"\"\n\n    tag = \"reasoning_token_count\"\n    header = \"Reasoning Token Count\"\n    short_header = \"Reasoning Tokens\"\n    short_header_hide_unit = True\n    unit = GenericMetricUnit.TOKENS\n    flags = (\n        MetricFlags.PRODUCES_TOKENS_ONLY\n        | MetricFlags.LARGER_IS_BETTER\n        | MetricFlags.SUPPORTS_REASONING\n        | MetricFlags.EXPERIMENTAL\n    )\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the reasoning token count from the record and returns it.\n\n        Raises:\n            ValueError: If the record does not have a reasoning token count.\n        \"\"\"\n        if record.reasoning_token_count is None:\n            raise NoMetricValue(\"Reasoning token count is missing in the record.\")\n\n        return record.reasoning_token_count\n</code></pre>"},{"location":"api/#aiperf.metrics.types.reasoning_token_count.TotalReasoningTokensMetric","title":"<code>TotalReasoningTokensMetric</code>","text":"<p>               Bases: <code>DerivedSumMetric[int, ReasoningTokenCountMetric]</code></p> <p>This is the total number of reasoning tokens generated by the model for all records.</p> Formula <pre><code>Total Reasoning Tokens = Sum(Reasoning Tokens)\n</code></pre> Source code in <code>aiperf/metrics/types/reasoning_token_count.py</code> <pre><code>class TotalReasoningTokensMetric(DerivedSumMetric[int, ReasoningTokenCountMetric]):\n    \"\"\"\n    This is the total number of reasoning tokens generated by the model for\n    all records.\n\n    Formula:\n        ```\n        Total Reasoning Tokens = Sum(Reasoning Tokens)\n        ```\n    \"\"\"\n\n    tag = \"total_reasoning_tokens\"\n    header = \"Total Reasoning Tokens\"\n    short_header = \"Total Reasoning\"\n</code></pre>"},{"location":"api/#aiperfmetricstypesrequest_count_metric","title":"aiperf.metrics.types.request_count_metric","text":""},{"location":"api/#aiperf.metrics.types.request_count_metric.RequestCountMetric","title":"<code>RequestCountMetric</code>","text":"<p>               Bases: <code>BaseAggregateCounterMetric[int]</code></p> <p>This is the total number of valid requests processed by the benchmark. It is incremented for each valid request.</p> Formula <pre><code>Request Count = Sum(Valid Requests)\n</code></pre> Source code in <code>aiperf/metrics/types/request_count_metric.py</code> <pre><code>class RequestCountMetric(BaseAggregateCounterMetric[int]):\n    \"\"\"\n    This is the total number of valid requests processed by the benchmark.\n    It is incremented for each valid request.\n\n    Formula:\n        ```\n        Request Count = Sum(Valid Requests)\n        ```\n    \"\"\"\n\n    tag = \"request_count\"\n    header = \"Request Count\"\n    short_header = \"Requests\"\n    short_header_hide_unit = True\n    unit = GenericMetricUnit.REQUESTS\n    display_order = 1000\n    flags = MetricFlags.LARGER_IS_BETTER\n    required_metrics = None\n</code></pre>"},{"location":"api/#aiperfmetricstypesrequest_latency_metric","title":"aiperf.metrics.types.request_latency_metric","text":""},{"location":"api/#aiperf.metrics.types.request_latency_metric.RequestLatencyMetric","title":"<code>RequestLatencyMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Request Latency metrics from records.</p> Formula <p>Request Latency = Final Response Timestamp - Request Start Timestamp</p> Source code in <code>aiperf/metrics/types/request_latency_metric.py</code> <pre><code>class RequestLatencyMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Request Latency metrics from records.\n\n    Formula:\n        Request Latency = Final Response Timestamp - Request Start Timestamp\n    \"\"\"\n\n    tag = \"request_latency\"\n    header = \"Request Latency\"\n    short_header = \"Req Latency\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    display_order = 300\n    flags = MetricFlags.NONE\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the request and last response timestamps, and calculates the differences in time.\n        \"\"\"\n        request_ts: int = record.start_perf_ns\n        final_response_ts: int = record.responses[-1].perf_ns\n        if final_response_ts &lt; request_ts:\n            raise ValueError(\"Final response timestamp is less than request timestamp.\")\n        return final_response_ts - request_ts\n</code></pre>"},{"location":"api/#aiperfmetricstypesrequest_throughput_metric","title":"aiperf.metrics.types.request_throughput_metric","text":""},{"location":"api/#aiperf.metrics.types.request_throughput_metric.RequestThroughputMetric","title":"<code>RequestThroughputMetric</code>","text":"<p>               Bases: <code>BaseDerivedMetric[float]</code></p> <p>Post Processor for calculating Request throughput metrics from records.</p> Formula <p>Request Throughput = Valid Request Count / Benchmark Duration (seconds)</p> Source code in <code>aiperf/metrics/types/request_throughput_metric.py</code> <pre><code>class RequestThroughputMetric(BaseDerivedMetric[float]):\n    \"\"\"\n    Post Processor for calculating Request throughput metrics from records.\n\n    Formula:\n        Request Throughput = Valid Request Count / Benchmark Duration (seconds)\n    \"\"\"\n\n    tag = \"request_throughput\"\n    header = \"Request Throughput\"\n    short_header = \"Req/sec\"\n    short_header_hide_unit = True\n    unit = MetricOverTimeUnit.REQUESTS_PER_SECOND\n    display_order = 900\n    flags = MetricFlags.LARGER_IS_BETTER\n    required_metrics = {\n        RequestCountMetric.tag,\n        BenchmarkDurationMetric.tag,\n    }\n\n    def _derive_value(\n        self,\n        metric_results: MetricResultsDict,\n    ) -&gt; float:\n        request_count = metric_results.get_or_raise(RequestCountMetric)\n        benchmark_duration_converted = metric_results.get_converted_or_raise(\n            BenchmarkDurationMetric,\n            self.unit.time_unit,  # type: ignore\n        )\n        return request_count / benchmark_duration_converted  # type: ignore\n</code></pre>"},{"location":"api/#aiperfmetricstypesstream_latency_metrics","title":"aiperf.metrics.types.stream_latency_metrics","text":""},{"location":"api/#aiperf.metrics.types.stream_latency_metrics.StreamPrefillLatencyMetric","title":"<code>StreamPrefillLatencyMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Stream Prefill Latency metrics from records. This is only applicable to streaming responses.</p> <p>This is the time it takes for the server to process the input prompt and begin streaming content, after the stream has been established (200 OK response received). This is an alternate version of the TTFT metric, which removes some of the connection and stream setup overhead, sometimes referred to as \"Pure TTFT\".</p> <p>Note that not all servers will respond with a 200 OK response as soon as the stream is established. For example, some servers will wait for the first token to be ready before sending the 200 OK response. In these cases, the stream prefill latency will not be meaningful.</p> Formula <p>Stream Prefill Latency = Time to First Token - Stream Setup Latency</p> Source code in <code>aiperf/metrics/types/stream_latency_metrics.py</code> <pre><code>class StreamPrefillLatencyMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Stream Prefill Latency metrics from records. This is only applicable to streaming responses.\n\n    This is the time it takes for the server to process the input prompt and begin streaming content,\n    after the stream has been established (200 OK response received). This is an alternate version of the\n    TTFT metric, which removes some of the connection and stream setup overhead, sometimes referred to as \"Pure TTFT\".\n\n    Note that not all servers will respond with a 200 OK response as soon as the stream is established.\n    For example, some servers will wait for the first token to be ready before sending the 200 OK response.\n    In these cases, the stream prefill latency will not be meaningful.\n\n    Formula:\n        Stream Prefill Latency = Time to First Token - Stream Setup Latency\n    \"\"\"\n\n    tag = \"stream_prefill_latency\"\n    header = \"Stream Prefill Latency\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    flags = MetricFlags.STREAMING_TOKENS_ONLY | MetricFlags.EXPERIMENTAL\n    required_metrics = {\n        StreamSetupLatencyMetric.tag,\n        TTFTMetric.tag,\n    }\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"This method calculates the stream prefill latency by subtracting the stream setup latency from the TTFT.\"\"\"\n\n        stream_setup_latency = record_metrics.get_or_raise(StreamSetupLatencyMetric)\n        ttft = record_metrics.get_or_raise(TTFTMetric)\n\n        return ttft - stream_setup_latency  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.metrics.types.stream_latency_metrics.StreamSetupLatencyMetric","title":"<code>StreamSetupLatencyMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Stream Setup Latency metrics from records. This is only applicable to streaming responses.</p> <p>This is the time it takes for the client to send the request and receive the 200 OK response from the server, before any SSE content is received. It measures the tcp/http connection time, request processing, and stream initialization time.</p> <p>Note that not all servers will respond with a 200 OK response as soon as the stream is established. For example, some servers will wait for the first token to be ready before sending the 200 OK response. In these cases, the stream setup latency will not be meaningful.</p> Formula <p>Stream Setup Latency = Stream Start Timestamp - Request Start Timestamp</p> Source code in <code>aiperf/metrics/types/stream_latency_metrics.py</code> <pre><code>class StreamSetupLatencyMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Stream Setup Latency metrics from records. This is only applicable to streaming responses.\n\n    This is the time it takes for the client to send the request and receive the 200 OK response from the server,\n    before any SSE content is received. It measures the tcp/http connection time, request processing, and stream initialization time.\n\n    Note that not all servers will respond with a 200 OK response as soon as the stream is established.\n    For example, some servers will wait for the first token to be ready before sending the 200 OK response.\n    In these cases, the stream setup latency will not be meaningful.\n\n    Formula:\n        Stream Setup Latency = Stream Start Timestamp - Request Start Timestamp\n    \"\"\"\n\n    tag = \"stream_setup_latency\"\n    header = \"Stream Setup Latency\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    flags = MetricFlags.STREAMING_ONLY | MetricFlags.EXPERIMENTAL\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"This method extracts the request and receive start timestamps, and calculates the stream setup time.\"\"\"\n\n        if not record.request.recv_start_perf_ns or not record.start_perf_ns:\n            raise NoMetricValue(\n                \"Stream setup latency metric requires a recv_start_perf_ns and start_perf_ns\"\n            )\n\n        if record.request.recv_start_perf_ns &lt; record.start_perf_ns:\n            raise ValueError(\"recv_start_perf_ns is less than start_perf_ns\")\n\n        return record.request.recv_start_perf_ns - record.start_perf_ns\n</code></pre>"},{"location":"api/#aiperfmetricstypesthinking_efficiency_metrics","title":"aiperf.metrics.types.thinking_efficiency_metrics","text":""},{"location":"api/#aiperf.metrics.types.thinking_efficiency_metrics.OverallThinkingEfficiencyMetric","title":"<code>OverallThinkingEfficiencyMetric</code>","text":"<p>               Bases: <code>BaseDerivedMetric[float]</code></p> <p>This is the ratio of the total reasoning tokens to the total output tokens across all records.</p> <p>This is different from the individual thinking efficiency metric in that the value gets normalized by the total number of tokens produced.</p> Formula <pre><code>Overall Thinking Efficiency = Total Reasoning Tokens / Total Output Tokens\n</code></pre> References <p>@misc{lrm_token_economy_2025,     title={Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark},     author={TSB},     year={2025},     month={August},     url={https://github.com/cpldcpu/LRMTokenEconomy} }</p> Source code in <code>aiperf/metrics/types/thinking_efficiency_metrics.py</code> <pre><code>class OverallThinkingEfficiencyMetric(BaseDerivedMetric[float]):\n    \"\"\"\n    This is the ratio of the total reasoning tokens to the total output tokens across all records.\n\n    This is different from the individual thinking efficiency metric in that the value gets normalized\n    by the total number of tokens produced.\n\n    Formula:\n        ```\n        Overall Thinking Efficiency = Total Reasoning Tokens / Total Output Tokens\n        ```\n\n    References:\n        @misc{lrm_token_economy_2025,\n            title={Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark},\n            author={TSB},\n            year={2025},\n            month={August},\n            url={https://github.com/cpldcpu/LRMTokenEconomy}\n        }\n    \"\"\"\n\n    tag = \"overall_thinking_efficiency\"\n    header = \"Overall Thinking Efficiency\"\n    short_header = \"Overall Eff.\"\n    short_header_hide_unit = True\n    unit = GenericMetricUnit.RATIO\n    flags = (\n        MetricFlags.PRODUCES_TOKENS_ONLY\n        | MetricFlags.SUPPORTS_REASONING\n        | MetricFlags.EXPERIMENTAL\n    )\n    required_metrics = {\n        TotalOutputTokensMetric.tag,\n        TotalReasoningTokensMetric.tag,\n    }\n\n    def _derive_value(\n        self,\n        metric_results: MetricResultsDict,\n    ) -&gt; float:\n        total_reasoning_tokens = metric_results.get_or_raise(TotalReasoningTokensMetric)\n        total_output_tokens = metric_results.get_or_raise(TotalOutputTokensMetric)\n\n        return total_reasoning_tokens / total_output_tokens  # type: ignore\n</code></pre>"},{"location":"api/#aiperf.metrics.types.thinking_efficiency_metrics.ThinkingEfficiencyMetric","title":"<code>ThinkingEfficiencyMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[float]</code></p> <p>This is the ratio of the reasoning tokens to the output tokens for a single record.</p> Formula <pre><code>Thinking Efficiency = Total Reasoning Tokens / Total Output Tokens\n</code></pre> References <p>@misc{lrm_token_economy_2025,     title={Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark},     author={TSB},     year={2025},     month={August},     url={https://github.com/cpldcpu/LRMTokenEconomy} }</p> Source code in <code>aiperf/metrics/types/thinking_efficiency_metrics.py</code> <pre><code>class ThinkingEfficiencyMetric(BaseRecordMetric[float]):\n    \"\"\"\n    This is the ratio of the reasoning tokens to the output tokens for a single record.\n\n    Formula:\n        ```\n        Thinking Efficiency = Total Reasoning Tokens / Total Output Tokens\n        ```\n\n    References:\n        @misc{lrm_token_economy_2025,\n            title={Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark},\n            author={TSB},\n            year={2025},\n            month={August},\n            url={https://github.com/cpldcpu/LRMTokenEconomy}\n        }\n    \"\"\"\n\n    tag = \"thinking_efficiency\"\n    header = \"Thinking Efficiency\"\n    short_header_hide_unit = True\n    unit = GenericMetricUnit.RATIO\n    flags = (\n        MetricFlags.PRODUCES_TOKENS_ONLY\n        | MetricFlags.SUPPORTS_REASONING\n        | MetricFlags.EXPERIMENTAL\n    )\n    required_metrics = {\n        ReasoningTokenCountMetric.tag,\n        OutputTokenCountMetric.tag,\n    }\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; float:\n        reasoning_token_count = record_metrics.get_or_raise(ReasoningTokenCountMetric)\n        output_token_count = record_metrics.get_or_raise(OutputTokenCountMetric)\n\n        return reasoning_token_count / output_token_count  # type: ignore\n</code></pre>"},{"location":"api/#aiperfmetricstypesttft_metric","title":"aiperf.metrics.types.ttft_metric","text":""},{"location":"api/#aiperf.metrics.types.ttft_metric.TTFTMetric","title":"<code>TTFTMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Time to First Token (TTFT) metrics from records.</p> Formula <p>TTFT = First Response Timestamp - Request Start Timestamp</p> Source code in <code>aiperf/metrics/types/ttft_metric.py</code> <pre><code>class TTFTMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Time to First Token (TTFT) metrics from records.\n\n    Formula:\n        TTFT = First Response Timestamp - Request Start Timestamp\n    \"\"\"\n\n    tag = \"ttft\"\n    header = \"Time to First Token\"\n    short_header = \"TTFT\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    display_order = 100\n    flags = MetricFlags.STREAMING_TOKENS_ONLY\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the timestamps from the request start and the first response in the given\n        RequestRecord object, computes the difference (TTFT), and returns the result.\n\n        Raises:\n            NoMetricValue: If the record does not have at least one response\n            ValueError: If the first response is before the request start timestamp.\n        \"\"\"\n\n        if len(record.responses) &lt; 1:\n            raise NoMetricValue(\n                \"Record must have at least one response to calculate TTFT.\"\n            )\n\n        request_ts: int = record.request.start_perf_ns\n        first_response_ts: int = record.responses[0].perf_ns\n        if first_response_ts &lt; request_ts:\n            raise ValueError(\n                \"First response timestamp is before request start timestamp, cannot compute TTFT.\"\n            )\n\n        return first_response_ts - request_ts\n</code></pre>"},{"location":"api/#aiperfmetricstypesttst_metric","title":"aiperf.metrics.types.ttst_metric","text":""},{"location":"api/#aiperf.metrics.types.ttst_metric.TTSTMetric","title":"<code>TTSTMetric</code>","text":"<p>               Bases: <code>BaseRecordMetric[int]</code></p> <p>Post-processor for calculating Time to Second Token (TTST) metrics from records.</p> Formula <p>TTST = Second Response Timestamp - First Response Timestamp</p> Source code in <code>aiperf/metrics/types/ttst_metric.py</code> <pre><code>class TTSTMetric(BaseRecordMetric[int]):\n    \"\"\"\n    Post-processor for calculating Time to Second Token (TTST) metrics from records.\n\n    Formula:\n        TTST = Second Response Timestamp - First Response Timestamp\n    \"\"\"\n\n    tag = \"ttst\"\n    header = \"Time to Second Token\"\n    short_header = \"TTST\"\n    unit = MetricTimeUnit.NANOSECONDS\n    display_unit = MetricTimeUnit.MILLISECONDS\n    display_order = 200\n    flags = MetricFlags.STREAMING_TOKENS_ONLY\n    required_metrics = None\n\n    def _parse_record(\n        self,\n        record: ParsedResponseRecord,\n        record_metrics: MetricRecordDict,\n    ) -&gt; int:\n        \"\"\"\n        This method extracts the timestamps from the first and second response in the given\n        RequestRecord object, computes the difference (TTST), and returns the result.\n\n        Raises:\n            NoMetricValue: If the record does not have at least two responses\n            ValueError: If the second response is before the first response.\n        \"\"\"\n\n        if len(record.responses) &lt; 2:\n            raise NoMetricValue(\n                \"Record must have at least two responses to calculate TTST.\"\n            )\n\n        first_response_ts: int = record.responses[0].perf_ns\n        second_response_ts: int = record.responses[1].perf_ns\n        if second_response_ts &lt; first_response_ts:\n            raise ValueError(\n                \"Second response timestamp must be greater than or equal to the first response timestamp.\"\n            )\n        return second_response_ts - first_response_ts\n</code></pre>"},{"location":"api/#aiperfmodule_loader","title":"aiperf.module_loader","text":"<p>Module loader for AIPerf.</p> <p>This module is used to load all modules into the system to ensure everything is registered and ready to be used. This is done to avoid the performance penalty of importing all modules during CLI startup, while still ensuring that all implementations are properly registered with their factories.</p>"},{"location":"api/#aiperf.module_loader.ensure_modules_loaded","title":"<code>ensure_modules_loaded()</code>","text":"<p>Ensure all modules are loaded exactly once.</p> Source code in <code>aiperf/module_loader.py</code> <pre><code>def ensure_modules_loaded() -&gt; None:\n    \"\"\"Ensure all modules are loaded exactly once.\"\"\"\n    global _modules_loaded\n    with _modules_loaded_lock:\n        if not _modules_loaded:\n            start_time = time.perf_counter()\n            _logger.debug(\"Loading all modules\")\n            _load_all_modules()\n            _logger.debug(\n                f\"Modules loaded in {time.perf_counter() - start_time:.2f} seconds\"\n            )\n            _modules_loaded = True\n</code></pre>"},{"location":"api/#aiperfparsersinference_result_parser","title":"aiperf.parsers.inference_result_parser","text":""},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser","title":"<code>InferenceResultParser</code>","text":"<p>               Bases: <code>CommunicationMixin</code></p> <p>InferenceResultParser is responsible for parsing the inference results.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>class InferenceResultParser(CommunicationMixin):\n    \"\"\"InferenceResultParser is responsible for parsing the inference results.\"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n        )\n        self.conversation_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n        self.tokenizers: dict[str, Tokenizer] = {}\n        self.user_config: UserConfig = user_config\n        self.tokenizer_lock: asyncio.Lock = asyncio.Lock()\n        self.model_endpoint: ModelEndpointInfo = ModelEndpointInfo.from_user_config(\n            user_config\n        )\n        self.extractor: ResponseExtractorProtocol = (\n            ResponseExtractorFactory.create_instance(\n                self.model_endpoint.endpoint.type,\n                model_endpoint=self.model_endpoint,\n            )\n        )\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize inference result parser-specific components.\"\"\"\n        self.debug(\"Initializing inference result parser\")\n\n        self.extractor = ResponseExtractorFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n            model_endpoint=self.model_endpoint,\n        )\n\n    async def configure(self) -&gt; None:\n        \"\"\"Configure the tokenizers.\"\"\"\n        self.info(\"Configuring tokenizers for inference result parser\")\n        begin = time.perf_counter()\n        async with self.tokenizer_lock:\n            self.tokenizers = {\n                model.name: Tokenizer.from_pretrained(\n                    self.user_config.tokenizer.name or model.name,\n                    trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                    revision=self.user_config.tokenizer.revision,\n                )\n                for model in self.model_endpoint.models.models\n            }\n        duration = time.perf_counter() - begin\n        tokenizer_info = {\n            model: {\n                \"class\": tokenizer._tokenizer.__class__.__name__,\n                \"name_or_path\": getattr(tokenizer._tokenizer, \"name_or_path\", \"\"),\n            }\n            for model, tokenizer in self.tokenizers.items()\n        }\n        self.info(f\"Initialized tokenizers: {tokenizer_info} in {duration:.2f} seconds\")\n\n    async def get_tokenizer(self, model: str) -&gt; Tokenizer:\n        \"\"\"Get the tokenizer for a given model.\"\"\"\n        async with self.tokenizer_lock:\n            if model not in self.tokenizers:\n                self.tokenizers[model] = Tokenizer.from_pretrained(\n                    self.user_config.tokenizer.name or model,\n                    trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                    revision=self.user_config.tokenizer.revision,\n                )\n            return self.tokenizers[model]\n\n    async def parse_request_record(\n        self, request_record: RequestRecord\n    ) -&gt; ParsedResponseRecord:\n        \"\"\"Handle an inference results message.\"\"\"\n        self.trace_or_debug(\n            lambda: f\"Received inference results message: {request_record}\",\n            lambda: \"Received inference results\",\n        )\n\n        if request_record.has_error:\n            return ParsedResponseRecord(\n                request=request_record,\n                responses=[],\n            )\n\n        elif request_record.valid:\n            try:\n                record = await self.process_valid_record(request_record)\n                self.debug(\n                    lambda: f\"Received {len(record.request.responses)} responses, input_token_count: {record.input_token_count}, \"\n                    f\"output_token_count: {record.output_token_count}, reasoning_token_count: {record.reasoning_token_count}\"\n                )\n                return record\n            except Exception as e:\n                # TODO: We should add an ErrorDetails to the response record and not the request record.\n                self.exception(f\"Error processing valid record: {e}\")\n                request_record.error = ErrorDetails.from_exception(e)\n                return ParsedResponseRecord(\n                    request=request_record,\n                    responses=[],\n                )\n        else:\n            self.warning(f\"Received invalid inference results: {request_record}\")\n            # TODO: We should add an ErrorDetails to response record and not the request record.\n            request_record.error = ErrorDetails(\n                code=None,\n                message=\"Invalid inference results\",\n                type=\"InvalidInferenceResults\",\n            )\n            return ParsedResponseRecord(\n                request=request_record,\n                responses=[],\n            )\n\n    async def process_valid_record(\n        self, request_record: RequestRecord\n    ) -&gt; ParsedResponseRecord:\n        \"\"\"Process a valid request record.\"\"\"\n        if request_record.model_name is None:\n            self.warning(\n                lambda: f\"Model name is None, unable to process record: {request_record}\"\n            )\n            return ParsedResponseRecord(\n                request=request_record,\n                responses=[],\n                input_token_count=None,\n                output_token_count=None,\n            )\n\n        tokenizer = await self.get_tokenizer(request_record.model_name)\n        resp = await self.extractor.extract_response_data(request_record)\n        input_token_count = await self.compute_input_token_count(\n            request_record, tokenizer\n        )\n\n        output_texts: list[str] = []\n        reasoning_texts: list[str] = []\n        for response in resp:\n            if isinstance(response.data, ReasoningResponseData):\n                if response.data.reasoning:\n                    reasoning_texts.append(response.data.reasoning)\n                if response.data.content:\n                    output_texts.append(response.data.content)\n            else:\n                output_texts.append(response.data.get_text())\n\n        output_token_count = (\n            len(tokenizer.encode(\"\".join(output_texts))) if output_texts else None\n        )\n        reasoning_token_count = (\n            len(tokenizer.encode(\"\".join(reasoning_texts))) if reasoning_texts else None\n        )\n\n        return ParsedResponseRecord(\n            request=request_record,\n            responses=resp,\n            input_token_count=input_token_count,\n            output_token_count=output_token_count,\n            reasoning_token_count=reasoning_token_count,\n        )\n\n    async def get_turn(self, request_record: RequestRecord) -&gt; Turn | None:\n        \"\"\"Get the turn for a given request record.\"\"\"\n        if request_record.turn is not None:\n            return request_record.turn\n\n        if request_record.conversation_id is None or request_record.turn_index is None:\n            self.warning(\n                lambda: f\"Conversation ID or turn index is None: {request_record.conversation_id=} {request_record.turn_index=}\"\n            )\n            return None\n\n        turn_response: ConversationTurnResponseMessage = (\n            await self.conversation_request_client.request(\n                ConversationTurnRequestMessage(\n                    service_id=self.id,\n                    conversation_id=request_record.conversation_id,\n                    turn_index=request_record.turn_index,\n                )\n            )\n        )\n        if isinstance(turn_response, ErrorMessage):\n            self.error(lambda: f\"Error getting turn response: {turn_response}\")\n            return None\n\n        return turn_response.turn\n\n    async def compute_input_token_count(\n        self, request_record: RequestRecord, tokenizer: Tokenizer\n    ) -&gt; int | None:\n        \"\"\"Compute the number of tokens in the input for a given request record.\"\"\"\n        turn = await self.get_turn(request_record)\n        if turn is None:\n            return None\n\n        input_token_count = 0\n        for text in turn.texts:\n            input_token_count += len(tokenizer.encode(\"\".join(text.contents)))\n        return input_token_count\n</code></pre>"},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser.compute_input_token_count","title":"<code>compute_input_token_count(request_record, tokenizer)</code>  <code>async</code>","text":"<p>Compute the number of tokens in the input for a given request record.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>async def compute_input_token_count(\n    self, request_record: RequestRecord, tokenizer: Tokenizer\n) -&gt; int | None:\n    \"\"\"Compute the number of tokens in the input for a given request record.\"\"\"\n    turn = await self.get_turn(request_record)\n    if turn is None:\n        return None\n\n    input_token_count = 0\n    for text in turn.texts:\n        input_token_count += len(tokenizer.encode(\"\".join(text.contents)))\n    return input_token_count\n</code></pre>"},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser.configure","title":"<code>configure()</code>  <code>async</code>","text":"<p>Configure the tokenizers.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>async def configure(self) -&gt; None:\n    \"\"\"Configure the tokenizers.\"\"\"\n    self.info(\"Configuring tokenizers for inference result parser\")\n    begin = time.perf_counter()\n    async with self.tokenizer_lock:\n        self.tokenizers = {\n            model.name: Tokenizer.from_pretrained(\n                self.user_config.tokenizer.name or model.name,\n                trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                revision=self.user_config.tokenizer.revision,\n            )\n            for model in self.model_endpoint.models.models\n        }\n    duration = time.perf_counter() - begin\n    tokenizer_info = {\n        model: {\n            \"class\": tokenizer._tokenizer.__class__.__name__,\n            \"name_or_path\": getattr(tokenizer._tokenizer, \"name_or_path\", \"\"),\n        }\n        for model, tokenizer in self.tokenizers.items()\n    }\n    self.info(f\"Initialized tokenizers: {tokenizer_info} in {duration:.2f} seconds\")\n</code></pre>"},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser.get_tokenizer","title":"<code>get_tokenizer(model)</code>  <code>async</code>","text":"<p>Get the tokenizer for a given model.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>async def get_tokenizer(self, model: str) -&gt; Tokenizer:\n    \"\"\"Get the tokenizer for a given model.\"\"\"\n    async with self.tokenizer_lock:\n        if model not in self.tokenizers:\n            self.tokenizers[model] = Tokenizer.from_pretrained(\n                self.user_config.tokenizer.name or model,\n                trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                revision=self.user_config.tokenizer.revision,\n            )\n        return self.tokenizers[model]\n</code></pre>"},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser.get_turn","title":"<code>get_turn(request_record)</code>  <code>async</code>","text":"<p>Get the turn for a given request record.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>async def get_turn(self, request_record: RequestRecord) -&gt; Turn | None:\n    \"\"\"Get the turn for a given request record.\"\"\"\n    if request_record.turn is not None:\n        return request_record.turn\n\n    if request_record.conversation_id is None or request_record.turn_index is None:\n        self.warning(\n            lambda: f\"Conversation ID or turn index is None: {request_record.conversation_id=} {request_record.turn_index=}\"\n        )\n        return None\n\n    turn_response: ConversationTurnResponseMessage = (\n        await self.conversation_request_client.request(\n            ConversationTurnRequestMessage(\n                service_id=self.id,\n                conversation_id=request_record.conversation_id,\n                turn_index=request_record.turn_index,\n            )\n        )\n    )\n    if isinstance(turn_response, ErrorMessage):\n        self.error(lambda: f\"Error getting turn response: {turn_response}\")\n        return None\n\n    return turn_response.turn\n</code></pre>"},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser.parse_request_record","title":"<code>parse_request_record(request_record)</code>  <code>async</code>","text":"<p>Handle an inference results message.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>async def parse_request_record(\n    self, request_record: RequestRecord\n) -&gt; ParsedResponseRecord:\n    \"\"\"Handle an inference results message.\"\"\"\n    self.trace_or_debug(\n        lambda: f\"Received inference results message: {request_record}\",\n        lambda: \"Received inference results\",\n    )\n\n    if request_record.has_error:\n        return ParsedResponseRecord(\n            request=request_record,\n            responses=[],\n        )\n\n    elif request_record.valid:\n        try:\n            record = await self.process_valid_record(request_record)\n            self.debug(\n                lambda: f\"Received {len(record.request.responses)} responses, input_token_count: {record.input_token_count}, \"\n                f\"output_token_count: {record.output_token_count}, reasoning_token_count: {record.reasoning_token_count}\"\n            )\n            return record\n        except Exception as e:\n            # TODO: We should add an ErrorDetails to the response record and not the request record.\n            self.exception(f\"Error processing valid record: {e}\")\n            request_record.error = ErrorDetails.from_exception(e)\n            return ParsedResponseRecord(\n                request=request_record,\n                responses=[],\n            )\n    else:\n        self.warning(f\"Received invalid inference results: {request_record}\")\n        # TODO: We should add an ErrorDetails to response record and not the request record.\n        request_record.error = ErrorDetails(\n            code=None,\n            message=\"Invalid inference results\",\n            type=\"InvalidInferenceResults\",\n        )\n        return ParsedResponseRecord(\n            request=request_record,\n            responses=[],\n        )\n</code></pre>"},{"location":"api/#aiperf.parsers.inference_result_parser.InferenceResultParser.process_valid_record","title":"<code>process_valid_record(request_record)</code>  <code>async</code>","text":"<p>Process a valid request record.</p> Source code in <code>aiperf/parsers/inference_result_parser.py</code> <pre><code>async def process_valid_record(\n    self, request_record: RequestRecord\n) -&gt; ParsedResponseRecord:\n    \"\"\"Process a valid request record.\"\"\"\n    if request_record.model_name is None:\n        self.warning(\n            lambda: f\"Model name is None, unable to process record: {request_record}\"\n        )\n        return ParsedResponseRecord(\n            request=request_record,\n            responses=[],\n            input_token_count=None,\n            output_token_count=None,\n        )\n\n    tokenizer = await self.get_tokenizer(request_record.model_name)\n    resp = await self.extractor.extract_response_data(request_record)\n    input_token_count = await self.compute_input_token_count(\n        request_record, tokenizer\n    )\n\n    output_texts: list[str] = []\n    reasoning_texts: list[str] = []\n    for response in resp:\n        if isinstance(response.data, ReasoningResponseData):\n            if response.data.reasoning:\n                reasoning_texts.append(response.data.reasoning)\n            if response.data.content:\n                output_texts.append(response.data.content)\n        else:\n            output_texts.append(response.data.get_text())\n\n    output_token_count = (\n        len(tokenizer.encode(\"\".join(output_texts))) if output_texts else None\n    )\n    reasoning_token_count = (\n        len(tokenizer.encode(\"\".join(reasoning_texts))) if reasoning_texts else None\n    )\n\n    return ParsedResponseRecord(\n        request=request_record,\n        responses=resp,\n        input_token_count=input_token_count,\n        output_token_count=output_token_count,\n        reasoning_token_count=reasoning_token_count,\n    )\n</code></pre>"},{"location":"api/#aiperfparsersopenai_parsers","title":"aiperf.parsers.openai_parsers","text":""},{"location":"api/#aiperf.parsers.openai_parsers.ChatCompletionChunkParser","title":"<code>ChatCompletionChunkParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for ChatCompletionChunk objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.CHAT_COMPLETION_CHUNK)\nclass ChatCompletionChunkParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for ChatCompletionChunk objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a ChatCompletionChunk into a ResponseData object.\"\"\"\n        return _parse_chat_common(obj.get(\"choices\", [{}])[0].get(\"delta\", {}))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ChatCompletionChunkParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a ChatCompletionChunk into a ResponseData object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a ChatCompletionChunk into a ResponseData object.\"\"\"\n    return _parse_chat_common(obj.get(\"choices\", [{}])[0].get(\"delta\", {}))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ChatCompletionParser","title":"<code>ChatCompletionParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for ChatCompletion objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.CHAT_COMPLETION)\nclass ChatCompletionParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for ChatCompletion objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a ChatCompletion into a ResponseData object.\"\"\"\n        return _parse_chat_common(obj.get(\"choices\", [{}])[0].get(\"message\", {}))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ChatCompletionParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a ChatCompletion into a ResponseData object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a ChatCompletion into a ResponseData object.\"\"\"\n    return _parse_chat_common(obj.get(\"choices\", [{}])[0].get(\"message\", {}))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.CompletionParser","title":"<code>CompletionParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for Completion objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.COMPLETION)\nclass CompletionParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for Completion objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a Completion object.\"\"\"\n        return _make_text_response_data(obj.get(\"choices\", [{}])[0].get(\"text\"))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.CompletionParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a Completion object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a Completion object.\"\"\"\n    return _make_text_response_data(obj.get(\"choices\", [{}])[0].get(\"text\"))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ListParser","title":"<code>ListParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for List objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.LIST)\nclass ListParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for List objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a List object.\"\"\"\n        data = obj.get(\"data\", [])\n        if all(\n            isinstance(item, dict) and item.get(\"object\") == OpenAIObjectType.EMBEDDING\n            for item in data\n        ):\n            return _make_embedding_response_data(data)\n        else:\n            raise ValueError(f\"Received invalid list in response: {obj}\")\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ListParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a List object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a List object.\"\"\"\n    data = obj.get(\"data\", [])\n    if all(\n        isinstance(item, dict) and item.get(\"object\") == OpenAIObjectType.EMBEDDING\n        for item in data\n    ):\n        return _make_embedding_response_data(data)\n    else:\n        raise ValueError(f\"Received invalid list in response: {obj}\")\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.OpenAIResponseExtractor","title":"<code>OpenAIResponseExtractor</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code></p> <p>Extractor for OpenAI responses.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@ResponseExtractorFactory.register_all(\n    EndpointType.OPENAI_CHAT_COMPLETIONS,\n    EndpointType.OPENAI_COMPLETIONS,\n    EndpointType.OPENAI_EMBEDDINGS,\n    EndpointType.RANKINGS,\n    EndpointType.OPENAI_RESPONSES,\n)\nclass OpenAIResponseExtractor(AIPerfLoggerMixin):\n    \"\"\"Extractor for OpenAI responses.\"\"\"\n\n    def __init__(self, model_endpoint: ModelEndpointInfo) -&gt; None:\n        \"\"\"Create a new response extractor based on the provided configuration.\"\"\"\n        super().__init__()\n        self.model_endpoint = model_endpoint\n\n    async def extract_response_data(\n        self, record: RequestRecord\n    ) -&gt; list[ParsedResponse]:\n        \"\"\"Extract the text from a server response message.\"\"\"\n        results = []\n        for response in record.responses:\n            response_data = self._parse_response(response)\n            if not response_data:\n                continue\n            results.append(response_data)\n        return results\n\n    def _parse_response(\n        self, response: InferenceServerResponse\n    ) -&gt; ParsedResponse | None:\n        \"\"\"Parse a response into a ParsedResponse object.\"\"\"\n        parsed_data = None\n        # Note, this uses Python 3.10+ pattern matching, no new objects are created\n        match response:\n            case TextResponse():\n                parsed_data = self._parse_raw_text(response.text)\n            case SSEMessage():\n                parsed_data = self._parse_raw_text(response.extract_data_content())\n            case _:\n                self.warning(f\"Unsupported response type: {type(response)}\")\n        if not parsed_data:\n            return None\n\n        return ParsedResponse(\n            perf_ns=response.perf_ns,\n            data=parsed_data,\n        )\n\n    def _parse_raw_text(self, raw_text: str) -&gt; BaseResponseData | None:\n        \"\"\"Parse the raw text of the response using the appropriate parser from OpenAIObjectParserFactory.\n\n        Returns:\n            ParsedResponse | None: The parsed response, or None if the response is not a valid or supported OpenAI object.\n        \"\"\"\n        if raw_text in (\"\", None, \"[DONE]\"):\n            return None\n\n        try:\n            json_str = load_json_str(raw_text)\n        except orjson.JSONDecodeError as e:\n            self.warning(f\"Invalid JSON: {raw_text} - {e!r}\")\n            return None\n\n        if \"object\" in json_str:\n            try:\n                object_type = OpenAIObjectType(json_str[\"object\"])\n            except ValueError:\n                self.warning(\n                    f\"Unsupported OpenAI object type received: {json_str['object']}\"\n                )\n                return None\n        else:\n            object_type = self._infer_object_type(json_str)\n            if object_type is None:\n                return None\n\n        try:\n            parser = OpenAIObjectParserFactory.get_or_create_instance(object_type)\n            return parser.parse(json_str)\n        except FactoryCreationError:\n            self.warning(f\"No parser found for object type: {object_type!r}\")\n            return None\n\n    def _infer_object_type(self, json_obj: dict[str, Any]) -&gt; OpenAIObjectType | None:\n        \"\"\"Infer the object type from the JSON structure for responses without explicit 'object' field.\"\"\"\n        if \"rankings\" in json_obj:\n            return OpenAIObjectType.RANKINGS\n\n        self.warning(f\"Could not infer object type from response: {json_obj}\")\n        return None\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.OpenAIResponseExtractor.__init__","title":"<code>__init__(model_endpoint)</code>","text":"<p>Create a new response extractor based on the provided configuration.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def __init__(self, model_endpoint: ModelEndpointInfo) -&gt; None:\n    \"\"\"Create a new response extractor based on the provided configuration.\"\"\"\n    super().__init__()\n    self.model_endpoint = model_endpoint\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.OpenAIResponseExtractor.extract_response_data","title":"<code>extract_response_data(record)</code>  <code>async</code>","text":"<p>Extract the text from a server response message.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>async def extract_response_data(\n    self, record: RequestRecord\n) -&gt; list[ParsedResponse]:\n    \"\"\"Extract the text from a server response message.\"\"\"\n    results = []\n    for response in record.responses:\n        response_data = self._parse_response(response)\n        if not response_data:\n            continue\n        results.append(response_data)\n    return results\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.RankingsParser","title":"<code>RankingsParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for Rankings objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.RANKINGS)\nclass RankingsParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for Rankings objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a Rankings object.\"\"\"\n        rankings = obj.get(\"rankings\", [])\n        if not rankings:\n            return None\n        return RankingsResponseData(rankings=rankings)\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.RankingsParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a Rankings object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a Rankings object.\"\"\"\n    rankings = obj.get(\"rankings\", [])\n    if not rankings:\n        return None\n    return RankingsResponseData(rankings=rankings)\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ResponseParser","title":"<code>ResponseParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for OpenAI Responses objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.RESPONSE)\nclass ResponseParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for OpenAI Responses objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a Responses object.\"\"\"\n        return _make_text_response_data(obj.get(\"output_text\"))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.ResponseParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a Responses object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a Responses object.\"\"\"\n    return _make_text_response_data(obj.get(\"output_text\"))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.TextCompletionParser","title":"<code>TextCompletionParser</code>","text":"<p>               Bases: <code>OpenAIObjectParserProtocol</code></p> <p>Parser for TextCompletion objects.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>@OpenAIObjectParserFactory.register(OpenAIObjectType.TEXT_COMPLETION)\nclass TextCompletionParser(OpenAIObjectParserProtocol):\n    \"\"\"Parser for TextCompletion objects.\"\"\"\n\n    def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n        \"\"\"Parse a TextCompletion object.\"\"\"\n        return _make_text_response_data(obj.get(\"choices\", [{}])[0].get(\"text\"))\n</code></pre>"},{"location":"api/#aiperf.parsers.openai_parsers.TextCompletionParser.parse","title":"<code>parse(obj)</code>","text":"<p>Parse a TextCompletion object.</p> Source code in <code>aiperf/parsers/openai_parsers.py</code> <pre><code>def parse(self, obj: dict[str, Any]) -&gt; BaseResponseData | None:\n    \"\"\"Parse a TextCompletion object.\"\"\"\n    return _make_text_response_data(obj.get(\"choices\", [{}])[0].get(\"text\"))\n</code></pre>"},{"location":"api/#aiperfpost_processorsbase_metrics_processor","title":"aiperf.post_processors.base_metrics_processor","text":""},{"location":"api/#aiperf.post_processors.base_metrics_processor.BaseMetricsProcessor","title":"<code>BaseMetricsProcessor</code>","text":"<p>               Bases: <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> <p>Base class for all metrics processors. This class is responsible for filtering the metrics based on the user config.</p> Source code in <code>aiperf/post_processors/base_metrics_processor.py</code> <pre><code>class BaseMetricsProcessor(AIPerfLoggerMixin, ABC):\n    \"\"\"Base class for all metrics processors. This class is responsible for filtering the metrics based on the user config.\"\"\"\n\n    def __init__(self, user_config: UserConfig, **kwargs):\n        self.user_config = user_config\n        super().__init__(user_config=user_config, **kwargs)\n\n    def get_filters(self) -&gt; tuple[MetricFlags, MetricFlags]:\n        \"\"\"Get the filters for the metrics based on the user config.\n        Returns:\n            tuple[MetricFlags, MetricFlags]: The required and disallowed flags.\n        \"\"\"\n        # Start with no flags (unfiltered)\n        required_flags, disallowed_flags = MetricFlags.NONE, MetricFlags.NONE\n        # Disable metrics that are not applicable to the endpoint type\n        if not self.user_config.endpoint.type.produces_tokens:\n            disallowed_flags |= MetricFlags.PRODUCES_TOKENS_ONLY\n        if not self.user_config.endpoint.type.supports_audio:\n            disallowed_flags |= MetricFlags.SUPPORTS_AUDIO_ONLY\n        if not self.user_config.endpoint.type.supports_images:\n            disallowed_flags |= MetricFlags.SUPPORTS_IMAGE_ONLY\n        if not self.user_config.endpoint.streaming:\n            disallowed_flags |= MetricFlags.STREAMING_ONLY\n        return required_flags, disallowed_flags\n\n    def _setup_metrics(\n        self,\n        *metric_types: MetricType,\n        error_metrics_only: bool = False,\n        exclude_error_metrics: bool = False,\n    ) -&gt; list[BaseMetric]:\n        \"\"\"Get an ordered list of metrics that are applicable to the endpoint type and user config.\n        The metrics are ordered based on their dependencies, ensuring proper computation order.\n\n        Be sure to compute the metrics sequentially versus in parallel, as some metrics may depend on the results of previous metrics.\n        \"\"\"\n        required_flags, disallowed_flags = self.get_filters()\n        if error_metrics_only:\n            required_flags |= MetricFlags.ERROR_ONLY\n        elif exclude_error_metrics:\n            disallowed_flags |= MetricFlags.ERROR_ONLY\n\n        metrics: list[BaseMetric] = []\n        supported_tags = MetricRegistry.tags_applicable_to(\n            required_flags,\n            disallowed_flags,\n            *metric_types,\n        )\n        ordered_tags = MetricRegistry.create_dependency_order_for(\n            supported_tags,\n        )\n        for metric_tag in ordered_tags:\n            metrics.append(MetricRegistry.get_instance(metric_tag))\n        return metrics\n</code></pre>"},{"location":"api/#aiperf.post_processors.base_metrics_processor.BaseMetricsProcessor.get_filters","title":"<code>get_filters()</code>","text":"<p>Get the filters for the metrics based on the user config. Returns:     tuple[MetricFlags, MetricFlags]: The required and disallowed flags.</p> Source code in <code>aiperf/post_processors/base_metrics_processor.py</code> <pre><code>def get_filters(self) -&gt; tuple[MetricFlags, MetricFlags]:\n    \"\"\"Get the filters for the metrics based on the user config.\n    Returns:\n        tuple[MetricFlags, MetricFlags]: The required and disallowed flags.\n    \"\"\"\n    # Start with no flags (unfiltered)\n    required_flags, disallowed_flags = MetricFlags.NONE, MetricFlags.NONE\n    # Disable metrics that are not applicable to the endpoint type\n    if not self.user_config.endpoint.type.produces_tokens:\n        disallowed_flags |= MetricFlags.PRODUCES_TOKENS_ONLY\n    if not self.user_config.endpoint.type.supports_audio:\n        disallowed_flags |= MetricFlags.SUPPORTS_AUDIO_ONLY\n    if not self.user_config.endpoint.type.supports_images:\n        disallowed_flags |= MetricFlags.SUPPORTS_IMAGE_ONLY\n    if not self.user_config.endpoint.streaming:\n        disallowed_flags |= MetricFlags.STREAMING_ONLY\n    return required_flags, disallowed_flags\n</code></pre>"},{"location":"api/#aiperfpost_processorsmetric_record_processor","title":"aiperf.post_processors.metric_record_processor","text":""},{"location":"api/#aiperf.post_processors.metric_record_processor.MetricRecordProcessor","title":"<code>MetricRecordProcessor</code>","text":"<p>               Bases: <code>BaseMetricsProcessor</code></p> <p>Processor for metric records.</p> <p>This is the first stage of the metrics processing pipeline, and is done is a distributed manner across multiple service instances. It is responsible for streaming the records to the post processor, and computing the metrics from the records. It computes metrics from MetricType.RECORD and MetricType.AGGREGATE types.</p> Source code in <code>aiperf/post_processors/metric_record_processor.py</code> <pre><code>@implements_protocol(RecordProcessorProtocol)\n@RecordProcessorFactory.register(RecordProcessorType.METRIC_RECORD)\nclass MetricRecordProcessor(BaseMetricsProcessor):\n    \"\"\"Processor for metric records.\n\n    This is the first stage of the metrics processing pipeline, and is done is a distributed manner across multiple service instances.\n    It is responsible for streaming the records to the post processor, and computing the metrics from the records.\n    It computes metrics from MetricType.RECORD and MetricType.AGGREGATE types.\"\"\"\n\n    def __init__(\n        self,\n        user_config: UserConfig,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(user_config=user_config, **kwargs)\n\n        # Store a reference to the parse_record function for valid metrics.\n        # This is done to avoid extra attribute lookups.\n        self.valid_parse_funcs: list[\n            tuple[MetricTagT, Callable[[ParsedResponseRecord, MetricRecordDict], Any]]\n        ] = [\n            (metric.tag, metric.parse_record)  # type: ignore\n            for metric in self._setup_metrics(\n                MetricType.RECORD, MetricType.AGGREGATE, exclude_error_metrics=True\n            )\n        ]\n\n        # Store a reference to the parse_record function for error metrics.\n        # This is done to avoid extra attribute lookups.\n        self.error_parse_funcs: list[\n            tuple[MetricTagT, Callable[[ParsedResponseRecord, MetricRecordDict], Any]]\n        ] = [\n            (metric.tag, metric.parse_record)  # type: ignore\n            for metric in self._setup_metrics(\n                MetricType.RECORD, MetricType.AGGREGATE, error_metrics_only=True\n            )\n        ]\n\n    async def process_record(self, record: ParsedResponseRecord) -&gt; MetricRecordDict:\n        \"\"\"Process a response record from the inference results parser.\"\"\"\n        record_metrics: MetricRecordDict = MetricRecordDict()\n        parse_funcs = self.valid_parse_funcs if record.valid else self.error_parse_funcs\n        # NOTE: Need to parse the record in a loop, as the parse_record function may depend on the results of previous metrics.\n        for tag, parse_func in parse_funcs:\n            try:\n                record_metrics[tag] = parse_func(record, record_metrics)\n            except NoMetricValue as e:\n                self.debug(f\"No metric value for metric '{tag}': {e!r}\")\n            except Exception as e:\n                self.warning(f\"Error parsing record for metric '{tag}': {e!r}\")\n        return record_metrics\n</code></pre>"},{"location":"api/#aiperf.post_processors.metric_record_processor.MetricRecordProcessor.process_record","title":"<code>process_record(record)</code>  <code>async</code>","text":"<p>Process a response record from the inference results parser.</p> Source code in <code>aiperf/post_processors/metric_record_processor.py</code> <pre><code>async def process_record(self, record: ParsedResponseRecord) -&gt; MetricRecordDict:\n    \"\"\"Process a response record from the inference results parser.\"\"\"\n    record_metrics: MetricRecordDict = MetricRecordDict()\n    parse_funcs = self.valid_parse_funcs if record.valid else self.error_parse_funcs\n    # NOTE: Need to parse the record in a loop, as the parse_record function may depend on the results of previous metrics.\n    for tag, parse_func in parse_funcs:\n        try:\n            record_metrics[tag] = parse_func(record, record_metrics)\n        except NoMetricValue as e:\n            self.debug(f\"No metric value for metric '{tag}': {e!r}\")\n        except Exception as e:\n            self.warning(f\"Error parsing record for metric '{tag}': {e!r}\")\n    return record_metrics\n</code></pre>"},{"location":"api/#aiperfpost_processorsmetric_results_processor","title":"aiperf.post_processors.metric_results_processor","text":""},{"location":"api/#aiperf.post_processors.metric_results_processor.MetricResultsProcessor","title":"<code>MetricResultsProcessor</code>","text":"<p>               Bases: <code>BaseMetricsProcessor</code></p> <p>Processor for metric results.</p> <p>This is the final stage of the metrics processing pipeline, and is done is a unified manner by the RecordsManager. It is responsible for processing the results and returning them to the RecordsManager, as well as summarizing the results.</p> Source code in <code>aiperf/post_processors/metric_results_processor.py</code> <pre><code>@implements_protocol(ResultsProcessorProtocol)\n@ResultsProcessorFactory.register(ResultsProcessorType.METRIC_RESULTS)\nclass MetricResultsProcessor(BaseMetricsProcessor):\n    \"\"\"Processor for metric results.\n\n    This is the final stage of the metrics processing pipeline, and is done is a unified manner by the RecordsManager.\n    It is responsible for processing the results and returning them to the RecordsManager, as well as summarizing the results.\n    \"\"\"\n\n    def __init__(self, user_config: UserConfig, **kwargs: Any):\n        super().__init__(user_config=user_config, **kwargs)\n        # For derived metrics, we don't care about splitting up the error metrics\n        self.derive_funcs: dict[\n            MetricTagT, Callable[[MetricResultsDict], MetricValueTypeT]\n        ] = {\n            metric.tag: metric.derive_value  # type: ignore\n            for metric in self._setup_metrics(MetricType.DERIVED)\n        }\n\n        # Create the results dict, which will be used to store the results of non-derived metrics,\n        # and then be updated with the derived metrics.\n        self._results: MetricResultsDict = MetricResultsDict()\n\n        # Get all of the metric classes.\n        _all_metric_classes: list[type[BaseMetric]] = MetricRegistry.all_classes()\n\n        # Pre-cache the types for the metrics.\n        self._tags_to_types: dict[MetricTagT, MetricType] = {\n            metric.tag: metric.type for metric in _all_metric_classes\n        }\n\n        # Pre-cache the instances for the metrics.\n        self._instances_map: dict[MetricTagT, BaseMetric] = {\n            tag: MetricRegistry.get_instance(tag) for tag in MetricRegistry.all_tags()\n        }\n\n        # Pre-cache the aggregate functions for the aggregate metrics.\n        self._tags_to_aggregate_funcs: dict[\n            MetricTagT, Callable[[MetricResultsDict], MetricValueTypeT]\n        ] = {\n            metric.tag: MetricRegistry.get_instance(metric.tag).aggregate_value  # type: ignore\n            for metric in _all_metric_classes\n            if metric.type == MetricType.AGGREGATE\n        }\n\n    async def process_result(self, incoming_metrics: MetricRecordDict) -&gt; None:\n        \"\"\"Process a result from the metric record processor.\"\"\"\n        if self.is_trace_enabled:\n            self.trace(f\"Processing incoming metrics: {incoming_metrics}\")\n\n        for tag, value in incoming_metrics.items():\n            try:\n                metric_type = self._tags_to_types[tag]\n                if metric_type == MetricType.RECORD:\n                    if tag not in self._results:\n                        self._results[tag] = MetricArray()\n                    self._results[tag].append(value)  # type: ignore\n\n                elif metric_type == MetricType.AGGREGATE:\n                    metric: BaseAggregateMetric = self._instances_map[tag]  # type: ignore\n                    metric.aggregate_value(value)\n                    self._results[tag] = metric.current_value\n\n                else:\n                    raise ValueError(f\"Metric '{tag}' is not a valid metric type\")\n            except NoMetricValue as e:\n                self.debug(f\"No metric value for metric '{tag}': {e!r}\")\n            except Exception as e:\n                self.warning(f\"Error processing metric '{tag}': {e!r}\")\n\n        if self.is_trace_enabled:\n            self.trace(f\"Results after processing incoming metrics: {self._results}\")\n\n    async def update_derived_metrics(self) -&gt; None:\n        \"\"\"Computes the values for the derived metrics, and stores them in the results dict.\"\"\"\n        for tag, derive_func in self.derive_funcs.items():\n            try:\n                self._results[tag] = derive_func(self._results)\n            except NoMetricValue as e:\n                self.debug(f\"No metric value for derived metric '{tag}': {e!r}\")\n            except Exception as e:\n                self.warning(f\"Error deriving metric '{tag}': {e!r}\")\n\n    async def summarize(self) -&gt; list[MetricResult]:\n        \"\"\"Summarize the results.\n\n        This will compute the values for the derived metrics, and then create the MetricResult objects for each metric.\n        \"\"\"\n        await self.update_derived_metrics()\n\n        # Compute and return the metric results.\n        return [\n            self._create_metric_result(tag, values)\n            for tag, values in self._results.items()\n        ]\n\n    async def full_metrics(self) -&gt; MetricResultsDict:\n        \"\"\"Returns the full metrics dict, including the derived metrics.\"\"\"\n        await self.update_derived_metrics()\n        return self._results\n\n    def _create_metric_result(\n        self, tag: MetricTagT, values: MetricDictValueTypeT\n    ) -&gt; MetricResult:\n        \"\"\"Create a MetricResult from a the current values of a metric.\"\"\"\n\n        metric_class = self._instances_map[tag]\n\n        if isinstance(values, MetricArray):\n            return values.to_result(tag, metric_class.header, str(metric_class.unit))\n\n        if isinstance(values, int | float):\n            return MetricResult(\n                tag=metric_class.tag,\n                header=metric_class.header,\n                unit=str(metric_class.unit),\n                avg=values,\n                count=1,\n            )\n\n        raise ValueError(f\"Unexpected values type: {type(values)}\")\n</code></pre>"},{"location":"api/#aiperf.post_processors.metric_results_processor.MetricResultsProcessor.full_metrics","title":"<code>full_metrics()</code>  <code>async</code>","text":"<p>Returns the full metrics dict, including the derived metrics.</p> Source code in <code>aiperf/post_processors/metric_results_processor.py</code> <pre><code>async def full_metrics(self) -&gt; MetricResultsDict:\n    \"\"\"Returns the full metrics dict, including the derived metrics.\"\"\"\n    await self.update_derived_metrics()\n    return self._results\n</code></pre>"},{"location":"api/#aiperf.post_processors.metric_results_processor.MetricResultsProcessor.process_result","title":"<code>process_result(incoming_metrics)</code>  <code>async</code>","text":"<p>Process a result from the metric record processor.</p> Source code in <code>aiperf/post_processors/metric_results_processor.py</code> <pre><code>async def process_result(self, incoming_metrics: MetricRecordDict) -&gt; None:\n    \"\"\"Process a result from the metric record processor.\"\"\"\n    if self.is_trace_enabled:\n        self.trace(f\"Processing incoming metrics: {incoming_metrics}\")\n\n    for tag, value in incoming_metrics.items():\n        try:\n            metric_type = self._tags_to_types[tag]\n            if metric_type == MetricType.RECORD:\n                if tag not in self._results:\n                    self._results[tag] = MetricArray()\n                self._results[tag].append(value)  # type: ignore\n\n            elif metric_type == MetricType.AGGREGATE:\n                metric: BaseAggregateMetric = self._instances_map[tag]  # type: ignore\n                metric.aggregate_value(value)\n                self._results[tag] = metric.current_value\n\n            else:\n                raise ValueError(f\"Metric '{tag}' is not a valid metric type\")\n        except NoMetricValue as e:\n            self.debug(f\"No metric value for metric '{tag}': {e!r}\")\n        except Exception as e:\n            self.warning(f\"Error processing metric '{tag}': {e!r}\")\n\n    if self.is_trace_enabled:\n        self.trace(f\"Results after processing incoming metrics: {self._results}\")\n</code></pre>"},{"location":"api/#aiperf.post_processors.metric_results_processor.MetricResultsProcessor.summarize","title":"<code>summarize()</code>  <code>async</code>","text":"<p>Summarize the results.</p> <p>This will compute the values for the derived metrics, and then create the MetricResult objects for each metric.</p> Source code in <code>aiperf/post_processors/metric_results_processor.py</code> <pre><code>async def summarize(self) -&gt; list[MetricResult]:\n    \"\"\"Summarize the results.\n\n    This will compute the values for the derived metrics, and then create the MetricResult objects for each metric.\n    \"\"\"\n    await self.update_derived_metrics()\n\n    # Compute and return the metric results.\n    return [\n        self._create_metric_result(tag, values)\n        for tag, values in self._results.items()\n    ]\n</code></pre>"},{"location":"api/#aiperf.post_processors.metric_results_processor.MetricResultsProcessor.update_derived_metrics","title":"<code>update_derived_metrics()</code>  <code>async</code>","text":"<p>Computes the values for the derived metrics, and stores them in the results dict.</p> Source code in <code>aiperf/post_processors/metric_results_processor.py</code> <pre><code>async def update_derived_metrics(self) -&gt; None:\n    \"\"\"Computes the values for the derived metrics, and stores them in the results dict.\"\"\"\n    for tag, derive_func in self.derive_funcs.items():\n        try:\n            self._results[tag] = derive_func(self._results)\n        except NoMetricValue as e:\n            self.debug(f\"No metric value for derived metric '{tag}': {e!r}\")\n        except Exception as e:\n            self.warning(f\"Error deriving metric '{tag}': {e!r}\")\n</code></pre>"},{"location":"api/#aiperfrecordsphase_completion","title":"aiperf.records.phase_completion","text":""},{"location":"api/#aiperf.records.phase_completion.AllRequestsProcessedCondition","title":"<code>AllRequestsProcessedCondition</code>","text":"<p>               Bases: <code>PhaseCompletionCondition</code></p> <p>Completion condition for when all expected requests have been processed.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>class AllRequestsProcessedCondition(PhaseCompletionCondition):\n    \"\"\"Completion condition for when all expected requests have been processed.\"\"\"\n\n    def is_satisfied(self, context: PhaseCompletionContext) -&gt; bool:\n        # Only trigger for request-count-based benchmarks, not duration-based ones\n        is_request_count_based = context.expected_duration_sec is None\n        return (\n            is_request_count_based\n            and context.final_request_count is not None\n            and context.processing_stats.total_records &gt;= context.final_request_count\n        )\n\n    @property\n    def reason(self) -&gt; CompletionReason:\n        return CompletionReason.ALL_REQUESTS_PROCESSED\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.CompletionReason","title":"<code>CompletionReason</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Reasons why a phase completed.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>class CompletionReason(Enum):\n    \"\"\"Reasons why a phase completed.\"\"\"\n\n    ALL_REQUESTS_PROCESSED = \"all_requests_processed\"\n    DURATION_TIMEOUT = \"duration_timeout\"\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.DurationTimeoutCondition","title":"<code>DurationTimeoutCondition</code>","text":"<p>               Bases: <code>PhaseCompletionCondition</code></p> <p>Completion condition for when the benchmark duration has elapsed.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>class DurationTimeoutCondition(PhaseCompletionCondition):\n    \"\"\"Completion condition for when the benchmark duration has elapsed.\"\"\"\n\n    def is_satisfied(self, context: PhaseCompletionContext) -&gt; bool:\n        return context.timeout_triggered and context.final_request_count is not None\n\n    @property\n    def reason(self) -&gt; CompletionReason:\n        return CompletionReason.DURATION_TIMEOUT\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionChecker","title":"<code>PhaseCompletionChecker</code>","text":"<p>Orchestrates checking multiple completion conditions.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>class PhaseCompletionChecker:\n    \"\"\"Orchestrates checking multiple completion conditions.\"\"\"\n\n    def __init__(self):\n        self.conditions: list[PhaseCompletionCondition] = [\n            AllRequestsProcessedCondition(),\n            DurationTimeoutCondition(),\n        ]\n\n    def is_complete(\n        self,\n        processing_stats: ProcessingStats,\n        final_request_count: int | None = None,\n        timeout_triggered: bool = False,\n        expected_duration_sec: float | None = None,\n    ) -&gt; tuple[bool, CompletionReason | None]:\n        \"\"\"Check if the phase is complete based on registered conditions.\n\n        Args:\n            processing_stats: Current processing statistics\n            final_request_count: Expected number of requests to process (None for duration-based)\n            timeout_triggered: Whether a benchmark duration timeout has occurred\n            expected_duration_sec: Duration for duration-based benchmarks (None for request-count-based)\n\n        Returns:\n            Tuple of (is_complete: bool, reason: CompletionReason | None)\n            If is_complete is False, reason will be None.\n        \"\"\"\n        context = PhaseCompletionContext(\n            processing_stats=processing_stats,\n            final_request_count=final_request_count,\n            timeout_triggered=timeout_triggered,\n            expected_duration_sec=expected_duration_sec,\n        )\n\n        for condition in self.conditions:\n            if condition.is_satisfied(context):\n                return True, condition.reason\n\n        return False, None\n\n    def add_condition(self, condition: PhaseCompletionCondition) -&gt; None:\n        \"\"\"Add a custom completion condition.\"\"\"\n        self.conditions.append(condition)\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionChecker.add_condition","title":"<code>add_condition(condition)</code>","text":"<p>Add a custom completion condition.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>def add_condition(self, condition: PhaseCompletionCondition) -&gt; None:\n    \"\"\"Add a custom completion condition.\"\"\"\n    self.conditions.append(condition)\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionChecker.is_complete","title":"<code>is_complete(processing_stats, final_request_count=None, timeout_triggered=False, expected_duration_sec=None)</code>","text":"<p>Check if the phase is complete based on registered conditions.</p> <p>Parameters:</p> Name Type Description Default <code>processing_stats</code> <code>ProcessingStats</code> <p>Current processing statistics</p> required <code>final_request_count</code> <code>int | None</code> <p>Expected number of requests to process (None for duration-based)</p> <code>None</code> <code>timeout_triggered</code> <code>bool</code> <p>Whether a benchmark duration timeout has occurred</p> <code>False</code> <code>expected_duration_sec</code> <code>float | None</code> <p>Duration for duration-based benchmarks (None for request-count-based)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Tuple of (is_complete: bool, reason: CompletionReason | None)</p> <code>CompletionReason | None</code> <p>If is_complete is False, reason will be None.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>def is_complete(\n    self,\n    processing_stats: ProcessingStats,\n    final_request_count: int | None = None,\n    timeout_triggered: bool = False,\n    expected_duration_sec: float | None = None,\n) -&gt; tuple[bool, CompletionReason | None]:\n    \"\"\"Check if the phase is complete based on registered conditions.\n\n    Args:\n        processing_stats: Current processing statistics\n        final_request_count: Expected number of requests to process (None for duration-based)\n        timeout_triggered: Whether a benchmark duration timeout has occurred\n        expected_duration_sec: Duration for duration-based benchmarks (None for request-count-based)\n\n    Returns:\n        Tuple of (is_complete: bool, reason: CompletionReason | None)\n        If is_complete is False, reason will be None.\n    \"\"\"\n    context = PhaseCompletionContext(\n        processing_stats=processing_stats,\n        final_request_count=final_request_count,\n        timeout_triggered=timeout_triggered,\n        expected_duration_sec=expected_duration_sec,\n    )\n\n    for condition in self.conditions:\n        if condition.is_satisfied(context):\n            return True, condition.reason\n\n    return False, None\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionCondition","title":"<code>PhaseCompletionCondition</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for phase completion conditions.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>class PhaseCompletionCondition(ABC):\n    \"\"\"Abstract base class for phase completion conditions.\"\"\"\n\n    @abstractmethod\n    def is_satisfied(self, context: PhaseCompletionContext) -&gt; bool:\n        \"\"\"Check if this completion condition is satisfied.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def reason(self) -&gt; CompletionReason:\n        \"\"\"The completion reason this condition represents.\"\"\"\n        pass\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionCondition.reason","title":"<code>reason</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The completion reason this condition represents.</p>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionCondition.is_satisfied","title":"<code>is_satisfied(context)</code>  <code>abstractmethod</code>","text":"<p>Check if this completion condition is satisfied.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>@abstractmethod\ndef is_satisfied(self, context: PhaseCompletionContext) -&gt; bool:\n    \"\"\"Check if this completion condition is satisfied.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#aiperf.records.phase_completion.PhaseCompletionContext","title":"<code>PhaseCompletionContext</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Context object containing all state needed for completion checking.</p> Source code in <code>aiperf/records/phase_completion.py</code> <pre><code>class PhaseCompletionContext(AIPerfBaseModel):\n    \"\"\"Context object containing all state needed for completion checking.\"\"\"\n\n    processing_stats: ProcessingStats\n    final_request_count: int | None = None\n    timeout_triggered: bool = False\n    expected_duration_sec: float | None = None\n</code></pre>"},{"location":"api/#aiperfrecordsrecord_processor_service","title":"aiperf.records.record_processor_service","text":""},{"location":"api/#aiperf.records.record_processor_service.RecordProcessor","title":"<code>RecordProcessor</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code></p> <p>RecordProcessor is responsible for processing the records and pushing them to the RecordsManager. This service is meant to be run in a distributed fashion, where the amount of record processors can be scaled based on the load of the system.</p> Source code in <code>aiperf/records/record_processor_service.py</code> <pre><code>@ServiceFactory.register(ServiceType.RECORD_PROCESSOR)\nclass RecordProcessor(PullClientMixin, BaseComponentService):\n    \"\"\"RecordProcessor is responsible for processing the records and pushing them to the RecordsManager.\n    This service is meant to be run in a distributed fashion, where the amount of record processors can be scaled\n    based on the load of the system.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.RAW_INFERENCE_PROXY_BACKEND,\n            pull_client_bind=False,\n            pull_client_max_concurrency=DEFAULT_PULL_CLIENT_MAX_CONCURRENCY,\n        )\n        self.records_push_client: PushClientProtocol = self.comms.create_push_client(\n            CommAddress.RECORDS,\n        )\n        self.conversation_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n        self.tokenizers: dict[str, Tokenizer] = {}\n        self.user_config: UserConfig = user_config\n        self.tokenizer_lock: asyncio.Lock = asyncio.Lock()\n        self.model_endpoint: ModelEndpointInfo = ModelEndpointInfo.from_user_config(\n            user_config\n        )\n        self.inference_result_parser = InferenceResultParser(\n            service_config=service_config,\n            user_config=user_config,\n        )\n        self.records_processors: list[RecordProcessorProtocol] = []\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize record processor-specific components.\"\"\"\n        self.debug(\"Initializing record processor\")\n\n        self.extractor = ResponseExtractorFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n            model_endpoint=self.model_endpoint,\n        )\n\n        # Initialize all the records streamers\n        for processor_type in RecordProcessorFactory.get_all_class_types():\n            self.records_processors.append(\n                RecordProcessorFactory.create_instance(\n                    processor_type,\n                    service_config=self.service_config,\n                    user_config=self.user_config,\n                )\n            )\n\n    @on_command(CommandType.PROFILE_CONFIGURE)\n    async def _profile_configure_command(\n        self, message: ProfileConfigureCommand\n    ) -&gt; None:\n        \"\"\"Configure the tokenizers.\"\"\"\n        await self.inference_result_parser.configure()\n\n    async def get_tokenizer(self, model: str) -&gt; Tokenizer:\n        \"\"\"Get the tokenizer for a given model.\"\"\"\n        async with self.tokenizer_lock:\n            if model not in self.tokenizers:\n                self.tokenizers[model] = Tokenizer.from_pretrained(\n                    self.user_config.tokenizer.name or model,\n                    trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                    revision=self.user_config.tokenizer.revision,\n                )\n            return self.tokenizers[model]\n\n    @on_pull_message(MessageType.INFERENCE_RESULTS)\n    async def _on_inference_results(self, message: InferenceResultsMessage) -&gt; None:\n        \"\"\"Handle an inference results message.\"\"\"\n        parsed_record = await self.inference_result_parser.parse_request_record(\n            message.record\n        )\n        raw_results = await self._process_record(parsed_record)\n        results = []\n        for result in raw_results:\n            if isinstance(result, BaseException):\n                self.warning(f\"Error processing record: {result}\")\n            else:\n                results.append(result)\n        await self.records_push_client.push(\n            MetricRecordsMessage(\n                service_id=self.service_id,\n                worker_id=message.service_id,\n                credit_phase=message.record.credit_phase,\n                results=results,\n                error=message.record.error,\n            )\n        )\n\n    async def _process_record(\n        self, record: ParsedResponseRecord\n    ) -&gt; list[MetricRecordDict | BaseException]:\n        \"\"\"Stream a record to the records processors.\"\"\"\n        tasks = [\n            processor.process_record(record) for processor in self.records_processors\n        ]\n        results: list[MetricRecordDict | BaseException] = await asyncio.gather(\n            *tasks, return_exceptions=True\n        )\n        return results\n</code></pre>"},{"location":"api/#aiperf.records.record_processor_service.RecordProcessor.get_tokenizer","title":"<code>get_tokenizer(model)</code>  <code>async</code>","text":"<p>Get the tokenizer for a given model.</p> Source code in <code>aiperf/records/record_processor_service.py</code> <pre><code>async def get_tokenizer(self, model: str) -&gt; Tokenizer:\n    \"\"\"Get the tokenizer for a given model.\"\"\"\n    async with self.tokenizer_lock:\n        if model not in self.tokenizers:\n            self.tokenizers[model] = Tokenizer.from_pretrained(\n                self.user_config.tokenizer.name or model,\n                trust_remote_code=self.user_config.tokenizer.trust_remote_code,\n                revision=self.user_config.tokenizer.revision,\n            )\n        return self.tokenizers[model]\n</code></pre>"},{"location":"api/#aiperfrecordsrecords_manager","title":"aiperf.records.records_manager","text":""},{"location":"api/#aiperf.records.records_manager.RecordsManager","title":"<code>RecordsManager</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code></p> <p>The RecordsManager service is primarily responsible for holding the results returned from the workers.</p> Source code in <code>aiperf/records/records_manager.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ServiceFactory.register(ServiceType.RECORDS_MANAGER)\nclass RecordsManager(PullClientMixin, BaseComponentService):\n    \"\"\"\n    The RecordsManager service is primarily responsible for holding the\n    results returned from the workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.RECORDS,\n            pull_client_bind=True,\n            pull_client_max_concurrency=DEFAULT_PULL_CLIENT_MAX_CONCURRENCY,\n        )\n\n        #########################################################\n        # Protected by processing_status_lock\n        self.processing_status_lock: asyncio.Lock = asyncio.Lock()\n        self.start_time_ns: int | None = None\n        self.processing_stats: ProcessingStats = ProcessingStats()\n        self.final_request_count: int | None = None\n        self.end_time_ns: int | None = None\n        self.sent_all_records_received: bool = False\n        self.profile_cancelled: bool = False\n        self.timeout_triggered: bool = False\n        self.expected_duration_sec: float | None = None\n        #########################################################\n\n        self._completion_checker = PhaseCompletionChecker()\n\n        self.error_summary: dict[ErrorDetails, int] = {}\n        self.error_summary_lock: asyncio.Lock = asyncio.Lock()\n        # Track per-worker statistics\n        self.worker_stats: dict[str, ProcessingStats] = {}\n        self.worker_stats_lock: asyncio.Lock = asyncio.Lock()\n\n        self._previous_realtime_records: int | None = None\n\n        self._results_processors: list[ResultsProcessorProtocol] = []\n        for results_processor_type in ResultsProcessorFactory.get_all_class_types():\n            results_processor = ResultsProcessorFactory.create_instance(\n                class_type=results_processor_type,\n                service_id=self.service_id,\n                service_config=self.service_config,\n                user_config=self.user_config,\n            )\n            self.debug(\n                f\"Created results processor: {results_processor_type}: {results_processor.__class__.__name__}\"\n            )\n            self._results_processors.append(results_processor)\n\n    @on_pull_message(MessageType.METRIC_RECORDS)\n    async def _on_metric_records(self, message: MetricRecordsMessage) -&gt; None:\n        \"\"\"Handle a metric records message.\"\"\"\n        if self.is_trace_enabled:\n            self.trace(f\"Received metric records: {message}\")\n\n        if message.credit_phase != CreditPhase.PROFILING:\n            self.debug(lambda: f\"Skipping non-profiling record: {message.credit_phase}\")\n            return\n\n        should_include_request = self._should_include_request_by_duration(\n            message.results\n        )\n\n        if should_include_request:\n            await self._send_results_to_results_processors(message.results)\n\n        worker_id = message.worker_id\n\n        if message.valid and should_include_request:\n            # Valid record\n            async with self.worker_stats_lock:\n                worker_stats = self.worker_stats.setdefault(\n                    worker_id, ProcessingStats()\n                )\n                worker_stats.processed += 1\n            async with self.processing_status_lock:\n                self.processing_stats.processed += 1\n        elif message.valid and not should_include_request:\n            # Timed out record\n            self.debug(\n                f\"Filtered out record from worker {worker_id} - response received after duration\"\n            )\n        else:\n            # Invalid record\n            async with self.worker_stats_lock:\n                worker_stats = self.worker_stats.setdefault(\n                    worker_id, ProcessingStats()\n                )\n                worker_stats.errors += 1\n            async with self.processing_status_lock:\n                self.processing_stats.errors += 1\n            if message.error:\n                async with self.error_summary_lock:\n                    self.error_summary[message.error] = (\n                        self.error_summary.get(message.error, 0) + 1\n                    )\n\n        await self._check_if_all_records_received()\n\n    def _should_include_request_by_duration(\n        self, results: list[dict[MetricTagT, MetricValueTypeT]]\n    ) -&gt; bool:\n        \"\"\"Determine if the request should be included based on benchmark duration.\n\n        Args:\n            results: List of metric results for a single request\n\n        Returns:\n            True if the request should be included, else False\n        \"\"\"\n        if not self.expected_duration_sec:\n            return True\n\n        duration_end_ns = self.start_time_ns + int(\n            self.expected_duration_sec * NANOS_PER_SECOND\n        )\n\n        # Check if any response in this request was received after the duration\n        # If so, filter out the entire request (all-or-nothing approach)\n        for result_dict in results:\n            request_timestamp = result_dict.get(MinRequestTimestampMetric.tag)\n            request_latency = result_dict.get(RequestLatencyMetric.tag)\n\n            if request_timestamp is not None and request_latency is not None:\n                final_response_timestamp = request_timestamp + request_latency\n\n                if final_response_timestamp &gt; duration_end_ns:\n                    self.debug(\n                        f\"Filtering out timed-out request - response received \"\n                        f\"{final_response_timestamp - duration_end_ns} ns after timeout\"\n                    )\n                    return False\n\n        return True\n\n    async def _check_if_all_records_received(self) -&gt; None:\n        \"\"\"Check if all records have been received, and if so, publish a message and process the records.\"\"\"\n        all_records_received = False\n\n        async with self.processing_status_lock:\n            # Use the Strategy pattern for completion checking\n            is_complete, completion_reason = self._completion_checker.is_complete(\n                processing_stats=self.processing_stats,\n                final_request_count=self.final_request_count,\n                timeout_triggered=self.timeout_triggered,\n                expected_duration_sec=self.expected_duration_sec,\n            )\n            all_records_received = is_complete\n\n            if all_records_received:\n                if (\n                    self.final_request_count is not None\n                    and self.processing_stats.total_records &gt; self.final_request_count\n                ):\n                    self.warning(\n                        f\"Processed {self.processing_stats.total_records:,} records, but only expected {self.final_request_count:,} records\"\n                    )\n\n                if self.sent_all_records_received:\n                    return\n                self.sent_all_records_received = True\n\n        if all_records_received:\n            self.info(\n                lambda: f\"Processed {self.processing_stats.processed} valid requests and {self.processing_stats.errors} errors ({self.processing_stats.total_records} total).\"\n            )\n            # Make sure everyone knows the final stats, including the worker stats\n            await self._publish_processing_stats()\n\n            async with self.processing_status_lock:\n                cancelled = self.profile_cancelled\n                proc_stats = copy.deepcopy(self.processing_stats)\n\n            # Send a message to the event bus to signal that we received all the records\n            await self.publish(\n                AllRecordsReceivedMessage(\n                    service_id=self.service_id,\n                    request_ns=time.time_ns(),\n                    final_processing_stats=proc_stats,\n                )\n            )\n\n            self.debug(\"Received all records, processing now...\")\n            await self._process_results(cancelled=cancelled)\n\n    async def _send_results_to_results_processors(\n        self, results: list[dict[MetricTagT, MetricValueTypeT]]\n    ) -&gt; None:\n        \"\"\"Send the results to each of the results processors.\"\"\"\n        await asyncio.gather(\n            *[\n                results_processor.process_result(result)\n                for results_processor in self._results_processors\n                for result in results\n            ]\n        )\n\n    @on_message(MessageType.CREDIT_PHASE_START)\n    async def _on_credit_phase_start(\n        self, phase_start_msg: CreditPhaseStartMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase start message in order to track the total number of expected requests.\"\"\"\n        if phase_start_msg.phase != CreditPhase.PROFILING:\n            return\n        async with self.processing_status_lock:\n            self.start_time_ns = phase_start_msg.start_ns\n            self.expected_duration_sec = phase_start_msg.expected_duration_sec\n            self.processing_stats.total_expected_requests = (\n                phase_start_msg.total_expected_requests\n            )\n\n    @on_message(MessageType.CREDIT_PHASE_SENDING_COMPLETE)\n    async def _on_credit_phase_sending_complete(\n        self, message: CreditPhaseSendingCompleteMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase sending complete message in order to track the final request count.\"\"\"\n        if message.phase != CreditPhase.PROFILING:\n            return\n        # This will equate to how many records we expect to receive,\n        # and once we receive that many records, we know to stop.\n        async with self.processing_status_lock:\n            self.final_request_count = message.sent\n            self.info(\n                f\"Sent {self.final_request_count:,} requests. Waiting for completion...\"\n            )\n\n    @on_message(MessageType.CREDIT_PHASE_COMPLETE)\n    async def _on_credit_phase_complete(\n        self, message: CreditPhaseCompleteMessage\n    ) -&gt; None:\n        \"\"\"Handle a credit phase complete message in order to track the end time, and check if all records have been received.\"\"\"\n        if message.phase != CreditPhase.PROFILING:\n            return\n        async with self.processing_status_lock:\n            if self.final_request_count is None:\n                # If for whatever reason the final request count was not set, use the number of completed requests.\n                # This would only happen if the credit phase sending complete message was not received by the service.\n                self.warning(\n                    f\"Final request count was not set for profiling phase, using {message.completed:,} as the final request count\"\n                )\n                self.final_request_count = message.completed\n            self.end_time_ns = message.end_ns\n            self.timeout_triggered = message.timeout_triggered\n\n            self.notice(\n                f\"All requests have completed, please wait for the results to be processed \"\n                f\"(currently {self.processing_stats.total_records:,} of {self.final_request_count:,} records processed)...\"\n            )\n        # This check is to prevent a race condition where the timing manager processes\n        # all records before we have the final request count set.\n        await self._check_if_all_records_received()\n\n    @background_task(interval=DEFAULT_RECORDS_PROGRESS_REPORT_INTERVAL, immediate=False)\n    async def _report_records_task(self) -&gt; None:\n        \"\"\"Report the records processing stats.\"\"\"\n        if self.processing_stats.processed &gt; 0 or self.processing_stats.errors &gt; 0:\n            # Only publish stats if there are records to report\n            await self._publish_processing_stats()\n\n    async def _publish_processing_stats(self) -&gt; None:\n        \"\"\"Publish the profile processing stats.\"\"\"\n\n        async with self.processing_status_lock, self.worker_stats_lock:\n            proc_stats = copy.deepcopy(self.processing_stats)\n            worker_stats = copy.deepcopy(self.worker_stats)\n\n        message = RecordsProcessingStatsMessage(\n            service_id=self.service_id,\n            request_ns=time.time_ns(),\n            processing_stats=proc_stats,\n            worker_stats=worker_stats,\n        )\n        await self.publish(message)\n\n    @on_command(CommandType.PROCESS_RECORDS)\n    async def _on_process_records_command(\n        self, message: ProcessRecordsCommand\n    ) -&gt; ProcessRecordsResult:\n        \"\"\"Handle the process records command by forwarding it to all of the results processors, and returning the results.\"\"\"\n        self.debug(lambda: f\"Received process records command: {message}\")\n        return await self._process_results(cancelled=message.cancelled)\n\n    @on_command(CommandType.PROFILE_CANCEL)\n    async def _on_profile_cancel_command(\n        self, message: ProfileCancelCommand\n    ) -&gt; ProcessRecordsResult:\n        \"\"\"Handle the profile cancel command by cancelling the streaming post processors.\"\"\"\n        self.debug(lambda: f\"Received profile cancel command: {message}\")\n        async with self.processing_status_lock:\n            self.profile_cancelled = True\n        return await self._process_results(cancelled=True)\n\n    @background_task(interval=None, immediate=True)\n    async def _report_realtime_metrics_task(self) -&gt; None:\n        \"\"\"Report the real-time metrics at a regular interval (only if the UI type is dashboard).\"\"\"\n        if self.service_config.ui_type != AIPerfUIType.DASHBOARD:\n            return\n        while not self.stop_requested:\n            await asyncio.sleep(DEFAULT_REALTIME_METRICS_INTERVAL)\n            async with self.processing_status_lock:\n                if (\n                    self.processing_stats.total_records\n                    == self._previous_realtime_records\n                ):\n                    continue  # No new records have been processed, so no need to update the metrics\n                self._previous_realtime_records = self.processing_stats.processed\n            await self._report_realtime_metrics()\n\n    @on_command(CommandType.REALTIME_METRICS)\n    async def _on_realtime_metrics_command(\n        self, message: RealtimeMetricsCommand\n    ) -&gt; None:\n        \"\"\"Handle a real-time metrics command.\"\"\"\n        await self._report_realtime_metrics()\n\n    async def _report_realtime_metrics(self) -&gt; None:\n        \"\"\"Report the real-time metrics.\"\"\"\n        metrics = await self._generate_realtime_metrics()\n        if not metrics:\n            return\n        await self.publish(\n            RealtimeMetricsMessage(\n                service_id=self.service_id,\n                metrics=metrics,\n            )\n        )\n\n    async def _generate_realtime_metrics(self) -&gt; list[MetricResult]:\n        \"\"\"Generate the real-time metrics for the profile run.\"\"\"\n        results = await asyncio.gather(\n            *[\n                results_processor.summarize()\n                for results_processor in self._results_processors\n            ],\n            return_exceptions=True,\n        )\n        return [\n            res\n            for result in results\n            if isinstance(result, list)\n            for res in result\n            if isinstance(res, MetricResult)\n        ]\n\n    async def _process_results(self, cancelled: bool) -&gt; ProcessRecordsResult:\n        \"\"\"Process the results.\"\"\"\n        self.debug(lambda: f\"Processing records (cancelled: {cancelled})\")\n\n        self.info(\"Processing records results...\")\n        # Process the records through the results processors.\n        results = await asyncio.gather(\n            *[\n                results_processor.summarize()\n                for results_processor in self._results_processors\n            ],\n            return_exceptions=True,\n        )\n\n        records_results, error_results = [], []\n        for result in results:\n            if isinstance(result, list):\n                records_results.extend(result)\n            elif isinstance(result, ErrorDetails):\n                error_results.append(result)\n            elif isinstance(result, BaseException):\n                error_results.append(ErrorDetails.from_exception(result))\n\n        result = ProcessRecordsResult(\n            results=ProfileResults(\n                records=records_results,\n                completed=len(records_results),\n                start_ns=self.start_time_ns or time.time_ns(),\n                end_ns=self.end_time_ns or time.time_ns(),\n                error_summary=await self.get_error_summary(),\n                was_cancelled=cancelled,\n            ),\n            errors=error_results,\n        )\n        self.debug(lambda: f\"Process records result: {result}\")\n        await self.publish(\n            ProcessRecordsResultMessage(\n                service_id=self.service_id,\n                results=result,\n            )\n        )\n        return result\n\n    async def get_error_summary(self) -&gt; list[ErrorDetailsCount]:\n        \"\"\"Generate a summary of the error records.\"\"\"\n        async with self.error_summary_lock:\n            return [\n                ErrorDetailsCount(error_details=error_details, count=count)\n                for error_details, count in self.error_summary.items()\n            ]\n</code></pre>"},{"location":"api/#aiperf.records.records_manager.RecordsManager.get_error_summary","title":"<code>get_error_summary()</code>  <code>async</code>","text":"<p>Generate a summary of the error records.</p> Source code in <code>aiperf/records/records_manager.py</code> <pre><code>async def get_error_summary(self) -&gt; list[ErrorDetailsCount]:\n    \"\"\"Generate a summary of the error records.\"\"\"\n    async with self.error_summary_lock:\n        return [\n            ErrorDetailsCount(error_details=error_details, count=count)\n            for error_details, count in self.error_summary.items()\n        ]\n</code></pre>"},{"location":"api/#aiperf.records.records_manager.main","title":"<code>main()</code>","text":"<p>Main entry point for the records manager.</p> Source code in <code>aiperf/records/records_manager.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the records manager.\"\"\"\n\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(RecordsManager)\n</code></pre>"},{"location":"api/#aiperftimingconfig","title":"aiperf.timing.config","text":""},{"location":"api/#aiperf.timing.config.TimingManagerConfig","title":"<code>TimingManagerConfig</code>","text":"<p>               Bases: <code>AIPerfBaseModel</code></p> <p>Configuration for the timing manager.</p> Source code in <code>aiperf/timing/config.py</code> <pre><code>class TimingManagerConfig(AIPerfBaseModel):\n    \"\"\"Configuration for the timing manager.\"\"\"\n\n    timing_mode: TimingMode = LoadGeneratorDefaults.TIMING_MODE\n    concurrency: int | None = LoadGeneratorDefaults.CONCURRENCY\n    request_rate: float | None = LoadGeneratorDefaults.REQUEST_RATE\n    request_rate_mode: RequestRateMode = LoadGeneratorDefaults.REQUEST_RATE_MODE\n    request_count: int = LoadGeneratorDefaults.REQUEST_COUNT\n    warmup_request_count: int = LoadGeneratorDefaults.WARMUP_REQUEST_COUNT\n    benchmark_duration: float | None = LoadGeneratorDefaults.BENCHMARK_DURATION\n    random_seed: int | None = None\n    auto_offset_timestamps: bool = InputDefaults.FIXED_SCHEDULE_AUTO_OFFSET\n    fixed_schedule_start_offset: int | None = InputDefaults.FIXED_SCHEDULE_START_OFFSET\n    fixed_schedule_end_offset: int | None = InputDefaults.FIXED_SCHEDULE_END_OFFSET\n\n    @classmethod\n    def from_user_config(cls, user_config: UserConfig) -&gt; \"TimingManagerConfig\":\n        \"\"\"Create a TimingManagerConfig from a UserConfig.\"\"\"\n\n        return cls(\n            timing_mode=user_config.timing_mode,\n            concurrency=user_config.loadgen.concurrency,\n            request_rate=user_config.loadgen.request_rate,\n            request_rate_mode=user_config.loadgen.request_rate_mode,\n            request_count=user_config.loadgen.request_count,\n            warmup_request_count=user_config.loadgen.warmup_request_count,\n            benchmark_duration=user_config.loadgen.benchmark_duration,\n            random_seed=user_config.input.random_seed,\n            auto_offset_timestamps=user_config.input.fixed_schedule_auto_offset,\n            fixed_schedule_start_offset=user_config.input.fixed_schedule_start_offset,\n            fixed_schedule_end_offset=user_config.input.fixed_schedule_end_offset,\n        )\n</code></pre>"},{"location":"api/#aiperf.timing.config.TimingManagerConfig.from_user_config","title":"<code>from_user_config(user_config)</code>  <code>classmethod</code>","text":"<p>Create a TimingManagerConfig from a UserConfig.</p> Source code in <code>aiperf/timing/config.py</code> <pre><code>@classmethod\ndef from_user_config(cls, user_config: UserConfig) -&gt; \"TimingManagerConfig\":\n    \"\"\"Create a TimingManagerConfig from a UserConfig.\"\"\"\n\n    return cls(\n        timing_mode=user_config.timing_mode,\n        concurrency=user_config.loadgen.concurrency,\n        request_rate=user_config.loadgen.request_rate,\n        request_rate_mode=user_config.loadgen.request_rate_mode,\n        request_count=user_config.loadgen.request_count,\n        warmup_request_count=user_config.loadgen.warmup_request_count,\n        benchmark_duration=user_config.loadgen.benchmark_duration,\n        random_seed=user_config.input.random_seed,\n        auto_offset_timestamps=user_config.input.fixed_schedule_auto_offset,\n        fixed_schedule_start_offset=user_config.input.fixed_schedule_start_offset,\n        fixed_schedule_end_offset=user_config.input.fixed_schedule_end_offset,\n    )\n</code></pre>"},{"location":"api/#aiperftimingcredit_issuing_strategy","title":"aiperf.timing.credit_issuing_strategy","text":""},{"location":"api/#aiperf.timing.credit_issuing_strategy.CreditIssuingStrategy","title":"<code>CreditIssuingStrategy</code>","text":"<p>               Bases: <code>TaskManagerMixin</code>, <code>ABC</code></p> <p>Base class for credit issuing strategies.</p> Source code in <code>aiperf/timing/credit_issuing_strategy.py</code> <pre><code>class CreditIssuingStrategy(TaskManagerMixin, ABC):\n    \"\"\"\n    Base class for credit issuing strategies.\n    \"\"\"\n\n    def __init__(\n        self, config: TimingManagerConfig, credit_manager: CreditManagerProtocol\n    ):\n        super().__init__()\n        self.config = config\n        self.credit_manager = credit_manager\n\n        # This event is set when all phases are complete\n        self.all_phases_complete_event = asyncio.Event()\n\n        # This event is set when a single phase is complete\n        self.phase_complete_event = asyncio.Event()\n\n        # The running stats for each phase, keyed by phase type.\n        self.phase_stats: dict[CreditPhase, CreditPhaseStats] = {}\n\n        # The phases to run including their configuration, in order of execution.\n        self.ordered_phase_configs: list[CreditPhaseConfig] = []\n\n        self._setup_phase_configs()\n        self._validate_phase_configs()\n\n    def _setup_phase_configs(self) -&gt; None:\n        \"\"\"Setup the phases for the strategy. This can be overridden in subclasses to modify the phases.\"\"\"\n        self._setup_warmup_phase_config()\n        self._setup_profiling_phase_config()\n        self.info(\n            lambda: f\"Credit issuing strategy {self.__class__.__name__} initialized with {len(self.ordered_phase_configs)} \"\n            f\"phase(s): {self.ordered_phase_configs}\"\n        )\n\n    def _setup_warmup_phase_config(self) -&gt; None:\n        \"\"\"Setup the warmup phase. This can be overridden in subclasses to modify the warmup phase.\"\"\"\n        if self.config.warmup_request_count &gt; 0:\n            self.ordered_phase_configs.append(\n                CreditPhaseConfig(\n                    type=CreditPhase.WARMUP,\n                    total_expected_requests=self.config.warmup_request_count,\n                )\n            )\n\n    def _setup_profiling_phase_config(self) -&gt; None:\n        \"\"\"Setup the profiling phase. This can be overridden in subclasses to modify the profiling phase.\"\"\"\n        if self.config.benchmark_duration is not None:\n            print(\n                f\"DEBUG: Setting up DURATION-based profiling phase: expected_duration_sec={self.config.benchmark_duration}\"\n            )\n            self.ordered_phase_configs.append(\n                CreditPhaseConfig(\n                    type=CreditPhase.PROFILING,\n                    expected_duration_sec=self.config.benchmark_duration,\n                )\n            )\n        else:\n            print(\n                f\"DEBUG: Setting up REQUEST-COUNT-based profiling phase: total_expected_requests={self.config.request_count}\"\n            )\n            self.ordered_phase_configs.append(\n                CreditPhaseConfig(\n                    type=CreditPhase.PROFILING,\n                    total_expected_requests=self.config.request_count,\n                )\n            )\n\n    def _validate_phase_configs(self) -&gt; None:\n        \"\"\"Validate the phase configs.\"\"\"\n        for phase_config in self.ordered_phase_configs:\n            if not phase_config.is_valid:\n                raise ConfigurationError(\n                    f\"Phase {phase_config.type} is not valid. It must have either a valid total_expected_requests or expected_duration_sec set\"\n                )\n\n    async def start(self) -&gt; None:\n        \"\"\"Start the credit issuing strategy. This will launch the progress reporting loop, the\n        warmup phase (if applicable), and the profiling phase, all in the background.\"\"\"\n        self.debug(\n            lambda: f\"Starting credit issuing strategy {self.__class__.__name__}\"\n        )\n        self.all_phases_complete_event.clear()\n\n        # Start the progress reporting loop in the background\n        self.execute_async(self._progress_report_loop())\n\n        # Execute the phases in the background\n        self.execute_async(self._execute_phases())\n\n        self.debug(\n            lambda: f\"Waiting for all credit phases to complete for {self.__class__.__name__}\"\n        )\n        # Wait for all phases to complete before returning\n        await self.all_phases_complete_event.wait()\n        self.debug(lambda: f\"All credit phases completed for {self.__class__.__name__}\")\n\n    async def _execute_phases(self) -&gt; None:\n        \"\"\"Execute the all of the credit phases sequentially. This can be overridden in subclasses to modify the execution of the phases.\"\"\"\n        for phase_config in self.ordered_phase_configs:\n            self.phase_complete_event.clear()\n\n            phase_stats = CreditPhaseStats.from_phase_config(phase_config)\n            phase_stats.start_ns = time.time_ns()\n            self.phase_stats[phase_config.type] = phase_stats\n\n            self.execute_async(\n                self.credit_manager.publish_phase_start(\n                    phase_config.type,\n                    phase_stats.start_ns,\n                    # Only one of the below will be set, this is already validated in the strategy\n                    phase_config.total_expected_requests,\n                    phase_config.expected_duration_sec,\n                )\n            )\n\n            # This is implemented in subclasses\n            await self._execute_single_phase(phase_stats)\n\n            # We have sent all the credits for this phase, but we still will need to wait for the credits to be returned\n            phase_stats.sent_end_ns = time.time_ns()\n            self.execute_async(\n                self.credit_manager.publish_phase_sending_complete(\n                    phase_config.type, phase_stats.sent_end_ns, phase_stats.sent\n                )\n            )\n\n            # Wait for the credits to be returned before continuing to the next phase\n            await self._wait_for_phase_completion(phase_stats)\n\n    async def _wait_for_phase_completion(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Wait for a phase to complete, with timeout for time-based phases.\"\"\"\n        if phase_stats.is_time_based:\n            # For time-based phases, calculate how much time is left from the original duration\n            elapsed_ns = time.time_ns() - phase_stats.start_ns\n            elapsed_sec = elapsed_ns / NANOS_PER_SECOND\n            remaining_sec = max(0, phase_stats.expected_duration_sec - elapsed_sec)\n\n            if remaining_sec == 0:\n                self.info(\n                    f\"Benchmark duration has elapsed for {phase_stats.type} phase, completing immediately\"\n                )\n                await self._force_phase_completion(phase_stats)\n                return\n\n            # Wait for either phase completion or timeout\n            try:\n                await asyncio.wait_for(\n                    self.phase_complete_event.wait(), timeout=remaining_sec\n                )\n            except asyncio.TimeoutError:\n                # Duration has elapsed, force completion\n                self.info(\n                    f\"Benchmark duration has elapsed for {phase_stats.type} phase after {phase_stats.expected_duration_sec}s, completing with {phase_stats.in_flight} in-flight requests\"\n                )\n                await self._force_phase_completion(phase_stats)\n        else:\n            # For request-count-based phases, wait indefinitely\n            await self.phase_complete_event.wait()\n\n    async def _force_phase_completion(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Force completion of a phase when the duration has elapsed.\"\"\"\n        # Defensive check: ensure this phase is listed as an active phase.\n        # In normal operation, this should be true, but it guards against edge\n        # cases like duplicate timeout events or race conditions during shutdown.\n        if phase_stats.type in self.phase_stats:\n            phase_stats.end_ns = time.time_ns()\n            self.notice(f\"Phase force-completed due to duration timeout: {phase_stats}\")\n\n            self.execute_async(\n                self.credit_manager.publish_phase_complete(\n                    phase_stats.type,\n                    phase_stats.completed,\n                    phase_stats.end_ns,\n                    timeout_triggered=True,\n                )\n            )\n\n            self.phase_complete_event.set()\n\n            if phase_stats.type == CreditPhase.PROFILING:\n                await self.credit_manager.publish_credits_complete()\n                self.all_phases_complete_event.set()\n\n            self.phase_stats.pop(phase_stats.type)\n\n    @abstractmethod\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute a single phase. Should not return until the phase sending is complete. Must be implemented in subclasses.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n    async def stop(self) -&gt; None:\n        \"\"\"Stop the credit issuing strategy.\"\"\"\n        await self.cancel_all_tasks()\n\n    async def _on_credit_return(self, message: CreditReturnMessage) -&gt; None:\n        \"\"\"This is called by the credit manager when a credit is returned. It can be\n        overridden in subclasses to handle the credit return.\"\"\"\n        if message.phase not in self.phase_stats:\n            self.warning(\n                f\"Credit return message received for phase {message.phase} but no phase stats found\"\n            )\n            return\n\n        phase_stats = self.phase_stats[message.phase]\n        phase_stats.completed += 1\n\n        # Check if this phase is complete\n        is_phase_complete = False\n        if phase_stats.is_sending_complete:\n            if phase_stats.is_request_count_based:\n                # Request-count-based: complete when all requests are returned\n                is_phase_complete = (\n                    phase_stats.completed &gt;= phase_stats.total_expected_requests\n                )  # type: ignore[operator]\n            else:\n                # Time-based: complete when all in-flight requests complete before the timeout.\n                # Duration timeout is handled separately with force_phase_completion.\n                is_phase_complete = phase_stats.in_flight == 0\n\n        if is_phase_complete:\n            phase_stats.end_ns = time.time_ns()\n            self.notice(f\"Phase completed: {phase_stats}\")\n\n            self.execute_async(\n                self.credit_manager.publish_phase_complete(\n                    message.phase, phase_stats.completed, phase_stats.end_ns\n                )\n            )\n\n            self.phase_complete_event.set()\n\n            if phase_stats.type == CreditPhase.PROFILING:\n                await self.credit_manager.publish_credits_complete()\n                self.all_phases_complete_event.set()\n\n            # We don't need to keep track of the phase stats anymore\n            self.phase_stats.pop(message.phase)\n\n    async def _progress_report_loop(self) -&gt; None:\n        \"\"\"Report the progress at a fixed interval.\"\"\"\n        self.debug(\"Starting progress reporting loop\")\n        while not self.all_phases_complete_event.is_set():\n            await asyncio.sleep(DEFAULT_CREDIT_PROGRESS_REPORT_INTERVAL)\n\n            for phase, stats in self.phase_stats.items():\n                try:\n                    await self.credit_manager.publish_progress(\n                        phase, stats.sent, stats.completed\n                    )\n                except Exception as e:\n                    self.error(f\"Error publishing credit progress: {e}\")\n                except asyncio.CancelledError:\n                    self.debug(\"Credit progress reporting loop cancelled\")\n                    return\n\n        self.debug(\"All credits completed, stopping credit progress reporting loop\")\n</code></pre>"},{"location":"api/#aiperf.timing.credit_issuing_strategy.CreditIssuingStrategy.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the credit issuing strategy. This will launch the progress reporting loop, the warmup phase (if applicable), and the profiling phase, all in the background.</p> Source code in <code>aiperf/timing/credit_issuing_strategy.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the credit issuing strategy. This will launch the progress reporting loop, the\n    warmup phase (if applicable), and the profiling phase, all in the background.\"\"\"\n    self.debug(\n        lambda: f\"Starting credit issuing strategy {self.__class__.__name__}\"\n    )\n    self.all_phases_complete_event.clear()\n\n    # Start the progress reporting loop in the background\n    self.execute_async(self._progress_report_loop())\n\n    # Execute the phases in the background\n    self.execute_async(self._execute_phases())\n\n    self.debug(\n        lambda: f\"Waiting for all credit phases to complete for {self.__class__.__name__}\"\n    )\n    # Wait for all phases to complete before returning\n    await self.all_phases_complete_event.wait()\n    self.debug(lambda: f\"All credit phases completed for {self.__class__.__name__}\")\n</code></pre>"},{"location":"api/#aiperf.timing.credit_issuing_strategy.CreditIssuingStrategy.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the credit issuing strategy.</p> Source code in <code>aiperf/timing/credit_issuing_strategy.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the credit issuing strategy.\"\"\"\n    await self.cancel_all_tasks()\n</code></pre>"},{"location":"api/#aiperf.timing.credit_issuing_strategy.CreditIssuingStrategyFactory","title":"<code>CreditIssuingStrategyFactory</code>","text":"<p>               Bases: <code>AIPerfFactory[TimingMode, CreditIssuingStrategy]</code></p> <p>Factory for creating credit issuing strategies based on the timing mode.</p> Source code in <code>aiperf/timing/credit_issuing_strategy.py</code> <pre><code>class CreditIssuingStrategyFactory(AIPerfFactory[TimingMode, CreditIssuingStrategy]):\n    \"\"\"Factory for creating credit issuing strategies based on the timing mode.\"\"\"\n</code></pre>"},{"location":"api/#aiperftimingcredit_manager","title":"aiperf.timing.credit_manager","text":""},{"location":"api/#aiperf.timing.credit_manager.CreditManagerProtocol","title":"<code>CreditManagerProtocol</code>","text":"<p>               Bases: <code>PubClientProtocol</code>, <code>Protocol</code></p> <p>Defines the interface for a CreditManager.</p> <p>This is used to allow the credit issuing strategy to interact with the TimingManager in a decoupled way.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>@runtime_checkable\nclass CreditManagerProtocol(PubClientProtocol, Protocol):\n    \"\"\"Defines the interface for a CreditManager.\n\n    This is used to allow the credit issuing strategy to interact with the TimingManager\n    in a decoupled way.\n    \"\"\"\n\n    async def drop_credit(\n        self,\n        credit_phase: CreditPhase,\n        conversation_id: str | None = None,\n        credit_drop_ns: int | None = None,\n    ) -&gt; None: ...\n\n    async def publish_progress(\n        self, phase: CreditPhase, sent: int, completed: int\n    ) -&gt; None: ...\n\n    async def publish_credits_complete(self) -&gt; None: ...\n\n    async def publish_phase_start(\n        self,\n        phase: CreditPhase,\n        start_ns: int,\n        total_expected_requests: int | None,\n        expected_duration_sec: float | None,\n    ) -&gt; None: ...\n\n    async def publish_phase_sending_complete(\n        self, phase: CreditPhase, sent_end_ns: int, sent: int\n    ) -&gt; None: ...\n\n    async def publish_phase_complete(\n        self,\n        phase: CreditPhase,\n        completed: int,\n        end_ns: int,\n        timeout_triggered: bool = False,\n    ) -&gt; None: ...\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesMixin","title":"<code>CreditPhaseMessagesMixin</code>","text":"<p>               Bases: <code>MessageBusClientMixin</code>, <code>CreditPhaseMessagesRequirements</code></p> <p>Mixin for services to implement the CreditManagerProtocol.</p> Requirements <p>This mixin must be used with a class that provides: - pub_client: PubClientProtocol - service_id: str</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>class CreditPhaseMessagesMixin(MessageBusClientMixin, CreditPhaseMessagesRequirements):\n    \"\"\"Mixin for services to implement the CreditManagerProtocol.\n\n    Requirements:\n        This mixin must be used with a class that provides:\n        - pub_client: PubClientProtocol\n        - service_id: str\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if not isinstance(self, CreditPhaseMessagesRequirements):\n            raise TypeError(\n                \"CreditPhaseMessagesMixin must be used with a class that provides CreditPhaseMessagesRequirements\"\n            )\n\n    async def publish_phase_start(\n        self,\n        phase: CreditPhase,\n        start_ns: int,\n        total_expected_requests: int | None,\n        expected_duration_sec: float | None,\n    ) -&gt; None:\n        \"\"\"Publish the phase start message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseStartMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    start_ns=start_ns,\n                    # Only one of the below will be set, this is already validated in the strategy\n                    total_expected_requests=total_expected_requests,\n                    expected_duration_sec=expected_duration_sec,\n                )\n            )\n        )\n\n    async def publish_phase_sending_complete(\n        self, phase: CreditPhase, sent_end_ns: int, sent: int\n    ) -&gt; None:\n        \"\"\"Publish the phase sending complete message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseSendingCompleteMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    sent_end_ns=sent_end_ns,\n                    sent=sent,\n                )\n            )\n        )\n\n    async def publish_phase_complete(\n        self,\n        phase: CreditPhase,\n        completed: int,\n        end_ns: int,\n        timeout_triggered: bool = False,\n    ) -&gt; None:\n        \"\"\"Publish the phase complete message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseCompleteMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    completed=completed,\n                    end_ns=end_ns,\n                    timeout_triggered=timeout_triggered,\n                )\n            )\n        )\n\n    async def publish_progress(\n        self, phase: CreditPhase, sent: int, completed: int\n    ) -&gt; None:\n        \"\"\"Publish the progress message.\"\"\"\n        self.execute_async(\n            self.publish(\n                CreditPhaseProgressMessage(\n                    service_id=self.service_id,\n                    phase=phase,\n                    sent=sent,\n                    completed=completed,\n                )\n            )\n        )\n\n    async def publish_credits_complete(self) -&gt; None:\n        \"\"\"Publish the credits complete message.\"\"\"\n        self.debug(\"Publishing credits complete message\")\n        self.execute_async(\n            self.publish(CreditsCompleteMessage(service_id=self.service_id))\n        )\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesMixin.publish_credits_complete","title":"<code>publish_credits_complete()</code>  <code>async</code>","text":"<p>Publish the credits complete message.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>async def publish_credits_complete(self) -&gt; None:\n    \"\"\"Publish the credits complete message.\"\"\"\n    self.debug(\"Publishing credits complete message\")\n    self.execute_async(\n        self.publish(CreditsCompleteMessage(service_id=self.service_id))\n    )\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesMixin.publish_phase_complete","title":"<code>publish_phase_complete(phase, completed, end_ns, timeout_triggered=False)</code>  <code>async</code>","text":"<p>Publish the phase complete message.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>async def publish_phase_complete(\n    self,\n    phase: CreditPhase,\n    completed: int,\n    end_ns: int,\n    timeout_triggered: bool = False,\n) -&gt; None:\n    \"\"\"Publish the phase complete message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseCompleteMessage(\n                service_id=self.service_id,\n                phase=phase,\n                completed=completed,\n                end_ns=end_ns,\n                timeout_triggered=timeout_triggered,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesMixin.publish_phase_sending_complete","title":"<code>publish_phase_sending_complete(phase, sent_end_ns, sent)</code>  <code>async</code>","text":"<p>Publish the phase sending complete message.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>async def publish_phase_sending_complete(\n    self, phase: CreditPhase, sent_end_ns: int, sent: int\n) -&gt; None:\n    \"\"\"Publish the phase sending complete message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseSendingCompleteMessage(\n                service_id=self.service_id,\n                phase=phase,\n                sent_end_ns=sent_end_ns,\n                sent=sent,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesMixin.publish_phase_start","title":"<code>publish_phase_start(phase, start_ns, total_expected_requests, expected_duration_sec)</code>  <code>async</code>","text":"<p>Publish the phase start message.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>async def publish_phase_start(\n    self,\n    phase: CreditPhase,\n    start_ns: int,\n    total_expected_requests: int | None,\n    expected_duration_sec: float | None,\n) -&gt; None:\n    \"\"\"Publish the phase start message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseStartMessage(\n                service_id=self.service_id,\n                phase=phase,\n                start_ns=start_ns,\n                # Only one of the below will be set, this is already validated in the strategy\n                total_expected_requests=total_expected_requests,\n                expected_duration_sec=expected_duration_sec,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesMixin.publish_progress","title":"<code>publish_progress(phase, sent, completed)</code>  <code>async</code>","text":"<p>Publish the progress message.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>async def publish_progress(\n    self, phase: CreditPhase, sent: int, completed: int\n) -&gt; None:\n    \"\"\"Publish the progress message.\"\"\"\n    self.execute_async(\n        self.publish(\n            CreditPhaseProgressMessage(\n                service_id=self.service_id,\n                phase=phase,\n                sent=sent,\n                completed=completed,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.timing.credit_manager.CreditPhaseMessagesRequirements","title":"<code>CreditPhaseMessagesRequirements</code>","text":"<p>               Bases: <code>AIPerfLoggerProtocol</code>, <code>Protocol</code></p> <p>Requirements for the CreditPhaseMessagesMixin. This is the list of attributes that must be provided by the class that uses this mixin.</p> Source code in <code>aiperf/timing/credit_manager.py</code> <pre><code>@runtime_checkable\nclass CreditPhaseMessagesRequirements(AIPerfLoggerProtocol, Protocol):\n    \"\"\"Requirements for the CreditPhaseMessagesMixin. This is the list of attributes that must\n    be provided by the class that uses this mixin.\"\"\"\n\n    service_id: str\n</code></pre>"},{"location":"api/#aiperftimingfixed_schedule_strategy","title":"aiperf.timing.fixed_schedule_strategy","text":""},{"location":"api/#aiperf.timing.fixed_schedule_strategy.FixedScheduleStrategy","title":"<code>FixedScheduleStrategy</code>","text":"<p>               Bases: <code>CreditIssuingStrategy</code></p> <p>Class for fixed schedule credit issuing strategy.</p> Source code in <code>aiperf/timing/fixed_schedule_strategy.py</code> <pre><code>@CreditIssuingStrategyFactory.register(TimingMode.FIXED_SCHEDULE)\nclass FixedScheduleStrategy(CreditIssuingStrategy):\n    \"\"\"\n    Class for fixed schedule credit issuing strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: TimingManagerConfig,\n        credit_manager: CreditManagerProtocol,\n        schedule: list[tuple[int, str]],\n    ):\n        # NOTE: This all needs to be set before the super call, because the base class will call\n        # _setup_profiling_phase_config() which uses it to set the total expected requests.\n        self._schedule: list[tuple[int, str]] = schedule\n        self._num_requests = len(self._schedule)\n        self._auto_offset_timestamps = config.auto_offset_timestamps\n        self._start_offset = config.fixed_schedule_start_offset\n        self._end_offset = config.fixed_schedule_end_offset\n        super().__init__(config=config, credit_manager=credit_manager)\n\n    def _create_timestamp_groups(self) -&gt; None:\n        \"\"\"\n        Create a dictionary of timestamp groups, and filter the schedule to only include the requested subset.\n        \"\"\"\n        if not self._schedule or self._num_requests == 0:\n            raise ValueError(\n                \"No schedule loaded, unable to setup fixed schedule strategy\"\n            )\n        # Group the schedule by timestamp\n        self._timestamp_groups = defaultdict(list)\n        for timestamp, conversation_id in self._schedule:\n            self._timestamp_groups[timestamp].append(conversation_id)\n\n        # Sort the timestamps, so we can drop credits in order\n        self._sorted_timestamp_keys = sorted(self._timestamp_groups.keys())\n\n        # Define the zero reference point for the schedule\n        if self._auto_offset_timestamps:\n            self._schedule_zero_ms = self._sorted_timestamp_keys[0]\n        elif self._start_offset is not None:\n            self._schedule_zero_ms = self._start_offset\n        else:\n            self._schedule_zero_ms = 0\n\n    def _setup_profiling_phase_config(self) -&gt; None:\n        \"\"\"\n        Setup the profiling phase.\n\n        Overrides the base implementation to set the total expected requests based on the number of requests in the schedule.\n        \"\"\"\n        self._create_timestamp_groups()\n\n        self.ordered_phase_configs.append(\n            CreditPhaseConfig(\n                type=CreditPhase.PROFILING,\n                total_expected_requests=self._num_requests,\n            )\n        )\n\n    def _perf_counter_ms(self) -&gt; float:\n        return time.perf_counter() * MILLIS_PER_SECOND\n\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        print(\"DEBUG: FixedScheduleStrategy._execute_single_phase called!\")\n        # This is used as a reference point for the wait duration calculation\n        start_time_ms = self._perf_counter_ms()\n\n        # Drop credits in order of the schedule\n        for timestamp in self._sorted_timestamp_keys:\n            # Lookup/cache the conversation IDs now, so that way they are ready to go\n            conversation_ids = self._timestamp_groups[timestamp]\n\n            # Calculate the wait duration for this timestamp\n            # (timestamp - schedule_zero_ms) is the offset of the conversation(s) from the start of the schedule\n            # (self._perf_counter_ms() - start_time_ms) is how much time has passed since we started dropping credits\n            wait_duration_ms = (timestamp - self._schedule_zero_ms) - (\n                self._perf_counter_ms() - start_time_ms\n            )\n            wait_duration_sec = wait_duration_ms / MILLIS_PER_SECOND\n\n            if wait_duration_sec &gt; 0:\n                await asyncio.sleep(wait_duration_sec)\n\n            # Drop credits asynchronously for all conversations at this timestamp\n            for conversation_id in conversation_ids:\n                await self.credit_manager.drop_credit(\n                    credit_phase=CreditPhase.PROFILING,\n                    conversation_id=conversation_id,\n                    # We already waited, so it can be sent ASAP\n                    credit_drop_ns=None,\n                )\n                phase_stats.sent += 1\n\n        duration_sec = (self._perf_counter_ms() - start_time_ms) / MILLIS_PER_SECOND\n        self.info(\n            f\"Sent all {self._num_requests:,} fixed schedule requests in {duration_sec:,.2f}s. Waiting for responses...\"\n        )\n</code></pre>"},{"location":"api/#aiperftimingrequest_rate_strategy","title":"aiperf.timing.request_rate_strategy","text":""},{"location":"api/#aiperf.timing.request_rate_strategy.ConcurrencyBurstRateGenerator","title":"<code>ConcurrencyBurstRateGenerator</code>","text":"<p>Generator for concurrency-burst rate (no delay between requests).</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>@implements_protocol(RequestRateGeneratorProtocol)\n@RequestRateGeneratorFactory.register(RequestRateMode.CONCURRENCY_BURST)\nclass ConcurrencyBurstRateGenerator:\n    \"\"\"\n    Generator for concurrency-burst rate (no delay between requests).\n    \"\"\"\n\n    def __init__(self, config: TimingManagerConfig) -&gt; None:\n        if config.concurrency is None or config.concurrency &lt; 1:\n            raise ValueError(\n                f\"Concurrency {config.concurrency} must be set and greater than 0 for {config.request_rate_mode!r}\"\n            )\n        if config.request_rate is not None:\n            raise ValueError(\n                f\"Request rate {config.request_rate} should be None for {config.request_rate_mode!r}\"\n            )\n\n    def next_interval(self) -&gt; float:\n        \"\"\"\n        Generate the next inter-arrival time for a concurrency-burst rate.\n\n        This will always return 0, as the requests should be issued as soon as possible.\n        \"\"\"\n        return 0\n</code></pre>"},{"location":"api/#aiperf.timing.request_rate_strategy.ConcurrencyBurstRateGenerator.next_interval","title":"<code>next_interval()</code>","text":"<p>Generate the next inter-arrival time for a concurrency-burst rate.</p> <p>This will always return 0, as the requests should be issued as soon as possible.</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>def next_interval(self) -&gt; float:\n    \"\"\"\n    Generate the next inter-arrival time for a concurrency-burst rate.\n\n    This will always return 0, as the requests should be issued as soon as possible.\n    \"\"\"\n    return 0\n</code></pre>"},{"location":"api/#aiperf.timing.request_rate_strategy.ConstantRateGenerator","title":"<code>ConstantRateGenerator</code>","text":"<p>Generator for constant rate (fixed inter-arrival times).</p> <p>This generates a fixed inter-arrival time for each request.</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>@implements_protocol(RequestRateGeneratorProtocol)\n@RequestRateGeneratorFactory.register(RequestRateMode.CONSTANT)\nclass ConstantRateGenerator:\n    \"\"\"\n    Generator for constant rate (fixed inter-arrival times).\n\n    This generates a fixed inter-arrival time for each request.\n    \"\"\"\n\n    def __init__(self, config: TimingManagerConfig) -&gt; None:\n        if config.request_rate is None or config.request_rate &lt;= 0:\n            raise ValueError(\n                f\"Request rate {config.request_rate} must be set and greater than 0 for {config.request_rate_mode!r}\"\n            )\n        self._period: float = 1.0 / config.request_rate\n\n    def next_interval(self) -&gt; float:\n        \"\"\"\n        Generate the next inter-arrival time for a constant rate.\n        \"\"\"\n        return self._period\n</code></pre>"},{"location":"api/#aiperf.timing.request_rate_strategy.ConstantRateGenerator.next_interval","title":"<code>next_interval()</code>","text":"<p>Generate the next inter-arrival time for a constant rate.</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>def next_interval(self) -&gt; float:\n    \"\"\"\n    Generate the next inter-arrival time for a constant rate.\n    \"\"\"\n    return self._period\n</code></pre>"},{"location":"api/#aiperf.timing.request_rate_strategy.PoissonRateGenerator","title":"<code>PoissonRateGenerator</code>","text":"<p>Generator for Poisson process (exponential inter-arrival times).</p> <p>In a Poisson process with rate \u03bb (requests per second), the inter-arrival times are exponentially distributed with parameter \u03bb. This attempts to model more realistic traffic patterns where requests arrive randomly but at a consistent average rate.</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>@implements_protocol(RequestRateGeneratorProtocol)\n@RequestRateGeneratorFactory.register(RequestRateMode.POISSON)\nclass PoissonRateGenerator:\n    \"\"\"\n    Generator for Poisson process (exponential inter-arrival times).\n\n    In a Poisson process with rate \u03bb (requests per second), the inter-arrival times\n    are exponentially distributed with parameter \u03bb. This attempts to model more\n    realistic traffic patterns where requests arrive randomly but at a consistent\n    average rate.\n    \"\"\"\n\n    def __init__(self, config: TimingManagerConfig) -&gt; None:\n        if config.request_rate is None or config.request_rate &lt;= 0:\n            raise ValueError(\n                f\"Request rate {config.request_rate} must be set and greater than 0 for {config.request_rate_mode!r}\"\n            )\n        # Initialize random number generator for reproducibility\n        self._rng = (\n            random.Random(config.random_seed) if config.random_seed else random.Random()\n        )\n        self._request_rate: float = config.request_rate\n\n    def next_interval(self) -&gt; float:\n        \"\"\"\n        Generate the next inter-arrival time for a Poisson process.\n\n        For Poisson process, inter-arrival times are exponentially distributed.\n        random.expovariate(lambd) generates exponentially distributed random numbers\n        where lambd is the rate parameter (requests per second)\n        \"\"\"\n        return self._rng.expovariate(self._request_rate)\n</code></pre>"},{"location":"api/#aiperf.timing.request_rate_strategy.PoissonRateGenerator.next_interval","title":"<code>next_interval()</code>","text":"<p>Generate the next inter-arrival time for a Poisson process.</p> <p>For Poisson process, inter-arrival times are exponentially distributed. random.expovariate(lambd) generates exponentially distributed random numbers where lambd is the rate parameter (requests per second)</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>def next_interval(self) -&gt; float:\n    \"\"\"\n    Generate the next inter-arrival time for a Poisson process.\n\n    For Poisson process, inter-arrival times are exponentially distributed.\n    random.expovariate(lambd) generates exponentially distributed random numbers\n    where lambd is the rate parameter (requests per second)\n    \"\"\"\n    return self._rng.expovariate(self._request_rate)\n</code></pre>"},{"location":"api/#aiperf.timing.request_rate_strategy.RequestRateStrategy","title":"<code>RequestRateStrategy</code>","text":"<p>               Bases: <code>CreditIssuingStrategy</code></p> <p>Strategy for issuing credits based on a specified request rate. Optionally, a max concurrency limit can be specified.</p> <p>Supports three modes: - CONSTANT: Issues credits at a constant rate with fixed intervals - POISSON: Issues credits using a Poisson process with exponentially distributed intervals - CONCURRENCY_BURST: Issues credits as soon as possible, up to a max concurrency limit. Only allowed when a request rate is not specified.</p> Source code in <code>aiperf/timing/request_rate_strategy.py</code> <pre><code>@CreditIssuingStrategyFactory.register(TimingMode.REQUEST_RATE)\nclass RequestRateStrategy(CreditIssuingStrategy):\n    \"\"\"\n    Strategy for issuing credits based on a specified request rate. Optionally, a max concurrency limit can be specified.\n\n    Supports three modes:\n    - CONSTANT: Issues credits at a constant rate with fixed intervals\n    - POISSON: Issues credits using a Poisson process with exponentially distributed intervals\n    - CONCURRENCY_BURST: Issues credits as soon as possible, up to a max concurrency limit. Only allowed when a request rate is not specified.\n    \"\"\"\n\n    def __init__(\n        self, config: TimingManagerConfig, credit_manager: CreditManagerProtocol\n    ):\n        super().__init__(config=config, credit_manager=credit_manager)\n        self._request_rate_generator = RequestRateGeneratorFactory.create_instance(\n            config\n        )\n        # If the user has provided a concurrency, use a semaphore to limit the maximum number of concurrent requests\n        self._semaphore: asyncio.Semaphore | None = (\n            asyncio.Semaphore(value=config.concurrency) if config.concurrency else None\n        )\n\n    async def _execute_single_phase(self, phase_stats: CreditPhaseStats) -&gt; None:\n        \"\"\"Execute credit drops based on the request rate generator, optionally with a max concurrency limit.\"\"\"\n\n        loop_count = 0\n        while phase_stats.should_send():\n            loop_count += 1\n\n            # Ensure we have an available credit before dropping\n            if self._semaphore:\n                await self._semaphore.acquire()\n                if self.is_trace_enabled:\n                    self.trace(f\"Acquired credit drop semaphore: {self._semaphore!r}\")\n                if not phase_stats.should_send():\n                    # Check one last time to see if we should still send a credit in case the\n                    # time-based phase expired while we were waiting for the semaphore.\n                    self._semaphore.release()\n                    if self.is_trace_enabled:\n                        self.trace(\n                            f\"Released semaphore after should_send returned False: {self._semaphore!r}\"\n                        )\n                    break\n\n            await self.credit_manager.drop_credit(credit_phase=phase_stats.type)\n            phase_stats.sent += 1\n\n            next_interval = self._request_rate_generator.next_interval()\n            if next_interval &gt; 0:\n                await asyncio.sleep(next_interval)\n\n    async def _on_credit_return(self, message: CreditReturnMessage) -&gt; None:\n        \"\"\"Process a credit return message. If concurrency is enabled, release the semaphore to allow another credit to be issued.\"\"\"\n\n        # Release the semaphore to allow another credit to be issued,\n        # then call the superclass to handle the credit return like normal\n        if self._semaphore:\n            self._semaphore.release()\n            if self.is_trace_enabled:\n                self.trace(f\"Credit return released semaphore: {self._semaphore!r}\")\n        await super()._on_credit_return(message)\n</code></pre>"},{"location":"api/#aiperftimingtiming_manager","title":"aiperf.timing.timing_manager","text":""},{"location":"api/#aiperf.timing.timing_manager.TimingManager","title":"<code>TimingManager</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code>, <code>CreditPhaseMessagesMixin</code></p> <p>The TimingManager service is responsible to generate the schedule and issuing timing credits for requests.</p> Source code in <code>aiperf/timing/timing_manager.py</code> <pre><code>@implements_protocol(ServiceProtocol)\n@ServiceFactory.register(ServiceType.TIMING_MANAGER)\nclass TimingManager(PullClientMixin, BaseComponentService, CreditPhaseMessagesMixin):\n    \"\"\"\n    The TimingManager service is responsible to generate the schedule and issuing\n    timing credits for requests.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.CREDIT_RETURN,\n            pull_client_bind=True,\n        )\n        self.debug(\"Timing manager __init__\")\n        self.config = TimingManagerConfig.from_user_config(self.user_config)\n\n        self.dataset_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n        self.credit_drop_push_client: PushClientProtocol = (\n            self.comms.create_push_client(\n                CommAddress.CREDIT_DROP,\n                bind=True,\n            )\n        )\n\n        self._credit_issuing_strategy: CreditIssuingStrategy | None = None\n\n    @on_command(CommandType.PROFILE_CONFIGURE)\n    async def _profile_configure_command(\n        self, message: ProfileConfigureCommand\n    ) -&gt; None:\n        \"\"\"Configure the timing manager.\"\"\"\n        self.debug(f\"Configuring credit issuing strategy for {self.service_id}\")\n\n        if self.config.timing_mode == TimingMode.FIXED_SCHEDULE:\n            # This will block until the dataset is ready and the timing response is received\n            dataset_timing_response: DatasetTimingResponse = (\n                await self.dataset_request_client.request(\n                    message=DatasetTimingRequest(\n                        service_id=self.service_id,\n                    ),\n                )\n            )\n            self.debug(\n                lambda: f\"Received dataset timing response: {dataset_timing_response}\"\n            )\n            self.info(\"Using fixed schedule strategy\")\n            self._credit_issuing_strategy = (\n                CreditIssuingStrategyFactory.create_instance(\n                    TimingMode.FIXED_SCHEDULE,\n                    config=self.config,\n                    credit_manager=self,\n                    schedule=dataset_timing_response.timing_data,\n                )\n            )\n        else:\n            self.info(f\"Using {self.config.timing_mode.title()} strategy\")\n            self._credit_issuing_strategy = (\n                CreditIssuingStrategyFactory.create_instance(\n                    self.config.timing_mode,\n                    config=self.config,\n                    credit_manager=self,\n                )\n            )\n\n        if not self._credit_issuing_strategy:\n            raise InvalidStateError(\"No credit issuing strategy configured\")\n        self.debug(\n            lambda: f\"Timing manager configured with credit issuing strategy: {self._credit_issuing_strategy}\"\n        )\n\n    @on_command(CommandType.PROFILE_START)\n    async def _on_start_profiling(self, message: CommandMessage) -&gt; None:\n        \"\"\"Start the timing manager and issue credit drops according to the configured strategy.\"\"\"\n        self.debug(\"Starting profiling\")\n\n        self.debug(\"Waiting for timing manager to be initialized\")\n        await self.initialized_event.wait()\n        self.debug(\"Timing manager initialized, starting profiling\")\n\n        if not self._credit_issuing_strategy:\n            raise InvalidStateError(\"No credit issuing strategy configured\")\n\n        self.execute_async(self._credit_issuing_strategy.start())\n        self.info(\n            f\"Credit issuing strategy for {self.config.timing_mode.title()} started\"\n        )\n\n    @on_command(CommandType.PROFILE_CANCEL)\n    async def _handle_profile_cancel_command(\n        self, message: ProfileCancelCommand\n    ) -&gt; None:\n        self.debug(lambda: f\"Received profile cancel command: {message}\")\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n        if self._credit_issuing_strategy:\n            await self._credit_issuing_strategy.stop()\n\n    @on_stop\n    async def _timing_manager_stop(self) -&gt; None:\n        \"\"\"Stop the timing manager.\"\"\"\n        self.debug(\"Stopping timing manager\")\n        if self._credit_issuing_strategy:\n            await self._credit_issuing_strategy.stop()\n        await self.cancel_all_tasks()\n\n    @on_pull_message(MessageType.CREDIT_RETURN)\n    async def _on_credit_return(self, message: CreditReturnMessage) -&gt; None:\n        \"\"\"Handle the credit return message.\"\"\"\n        if self.is_debug_enabled:\n            self.debug(f\"Timing manager received credit return message: {message}\")\n        if self._credit_issuing_strategy:\n            await self._credit_issuing_strategy._on_credit_return(message)\n\n    async def drop_credit(\n        self,\n        credit_phase: CreditPhase,\n        conversation_id: str | None = None,\n        credit_drop_ns: int | None = None,\n    ) -&gt; None:\n        \"\"\"Drop a credit.\"\"\"\n        self.execute_async(\n            self.credit_drop_push_client.push(\n                message=CreditDropMessage(\n                    service_id=self.service_id,\n                    phase=credit_phase,\n                    credit_drop_ns=credit_drop_ns,\n                    conversation_id=conversation_id,\n                ),\n            )\n        )\n</code></pre>"},{"location":"api/#aiperf.timing.timing_manager.TimingManager.drop_credit","title":"<code>drop_credit(credit_phase, conversation_id=None, credit_drop_ns=None)</code>  <code>async</code>","text":"<p>Drop a credit.</p> Source code in <code>aiperf/timing/timing_manager.py</code> <pre><code>async def drop_credit(\n    self,\n    credit_phase: CreditPhase,\n    conversation_id: str | None = None,\n    credit_drop_ns: int | None = None,\n) -&gt; None:\n    \"\"\"Drop a credit.\"\"\"\n    self.execute_async(\n        self.credit_drop_push_client.push(\n            message=CreditDropMessage(\n                service_id=self.service_id,\n                phase=credit_phase,\n                credit_drop_ns=credit_drop_ns,\n                conversation_id=conversation_id,\n            ),\n        )\n    )\n</code></pre>"},{"location":"api/#aiperf.timing.timing_manager.main","title":"<code>main()</code>","text":"<p>Main entry point for the timing manager.</p> Source code in <code>aiperf/timing/timing_manager.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for the timing manager.\"\"\"\n    from aiperf.common.bootstrap import bootstrap_and_run_service\n\n    bootstrap_and_run_service(TimingManager)\n</code></pre>"},{"location":"api/#aiperfuibase_ui","title":"aiperf.ui.base_ui","text":""},{"location":"api/#aiperf.ui.base_ui.BaseAIPerfUI","title":"<code>BaseAIPerfUI</code>","text":"<p>               Bases: <code>ProgressTrackerMixin</code>, <code>WorkerTrackerMixin</code>, <code>RealtimeMetricsMixin</code></p> <p>Base class for AIPerf UI implementations.</p> <p>This class provides a simple starting point for a UI for AIPerf components. It inherits from the :class:<code>ProgressTrackerMixin</code>, :class:<code>WorkerTrackerMixin</code>, and :class:<code>RealtimeMetricsMixin</code> to provide a simple starting point for a UI for AIPerf components.</p> <p>Now, you can use the various hooks defined in the :class:<code>ProgressTrackerMixin</code>, :class:<code>WorkerTrackerMixin</code>, and :class:<code>RealtimeMetricsMixin</code> to create a UI for AIPerf components.</p> <p>Example:</p> <pre><code>@AIPerfUIFactory.register(\"custom\")\nclass MyUI(BaseAIPerfUI):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    @on_records_progress\n    def _on_records_progress(self, records_stats: RecordsStats):\n        '''Callback for records progress updates.'''\n        pass\n\n    @on_requests_phase_progress\n    def _on_requests_phase_progress(self, phase: CreditPhase, requests_stats: RequestsStats):\n        '''Callback for requests phase progress updates.'''\n        pass\n\n    @on_worker_update\n    def _on_worker_update(self, worker_id: str, worker_stats: WorkerStats):\n        '''Callback for worker updates.'''\n        pass\n\n    @on_realtime_metrics\n    def _on_realtime_metrics(self, metrics: list[MetricResult]):\n        '''Callback for real-time metrics updates.'''\n        pass\n</code></pre> Source code in <code>aiperf/ui/base_ui.py</code> <pre><code>class BaseAIPerfUI(ProgressTrackerMixin, WorkerTrackerMixin, RealtimeMetricsMixin):\n    \"\"\"Base class for AIPerf UI implementations.\n\n    This class provides a simple starting point for a UI for AIPerf components.\n    It inherits from the :class:`ProgressTrackerMixin`, :class:`WorkerTrackerMixin`, and :class:`RealtimeMetricsMixin`\n    to provide a simple starting point for a UI for AIPerf components.\n\n    Now, you can use the various hooks defined in the :class:`ProgressTrackerMixin`, :class:`WorkerTrackerMixin`, and :class:`RealtimeMetricsMixin`\n    to create a UI for AIPerf components.\n\n    Example:\n    ```python\n    @AIPerfUIFactory.register(\"custom\")\n    class MyUI(BaseAIPerfUI):\n        def __init__(self, **kwargs):\n            super().__init__(**kwargs)\n\n        @on_records_progress\n        def _on_records_progress(self, records_stats: RecordsStats):\n            '''Callback for records progress updates.'''\n            pass\n\n        @on_requests_phase_progress\n        def _on_requests_phase_progress(self, phase: CreditPhase, requests_stats: RequestsStats):\n            '''Callback for requests phase progress updates.'''\n            pass\n\n        @on_worker_update\n        def _on_worker_update(self, worker_id: str, worker_stats: WorkerStats):\n            '''Callback for worker updates.'''\n            pass\n\n        @on_realtime_metrics\n        def _on_realtime_metrics(self, metrics: list[MetricResult]):\n            '''Callback for real-time metrics updates.'''\n            pass\n    ```\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperfuidashboardaiperf_dashboard_ui","title":"aiperf.ui.dashboard.aiperf_dashboard_ui","text":""},{"location":"api/#aiperf.ui.dashboard.aiperf_dashboard_ui.AIPerfDashboardUI","title":"<code>AIPerfDashboardUI</code>","text":"<p>               Bases: <code>BaseAIPerfUI</code></p> <p>AIPerf Dashboard UI.</p> <p>This is the main Dashboard UI class that implements the AIPerfUIProtocol. It is responsible for managing the Textual App, its lifecycle, and passing the progress updates to the Textual App. It also manages the lifecycle of the log consumer, which is responsible for consuming log records from the shared log queue and displaying them in the log viewer.</p> <p>The reason for this wrapper is that the internal lifecycle of the Textual App is handled by Textual, and it is not fully compatible with our AIPerf lifecycle.</p> Source code in <code>aiperf/ui/dashboard/aiperf_dashboard_ui.py</code> <pre><code>@implements_protocol(AIPerfUIProtocol)\n@AIPerfUIFactory.register(AIPerfUIType.DASHBOARD)\nclass AIPerfDashboardUI(BaseAIPerfUI):\n    \"\"\"\n    AIPerf Dashboard UI.\n\n    This is the main Dashboard UI class that implements the AIPerfUIProtocol. It is\n    responsible for managing the Textual App, its lifecycle, and passing the progress\n    updates to the Textual App. It also manages the lifecycle of the log consumer,\n    which is responsible for consuming log records from the shared log queue and\n    displaying them in the log viewer.\n\n    The reason for this wrapper is that the internal lifecycle of the Textual App is\n    handled by Textual, and it is not fully compatible with our AIPerf lifecycle.\n    \"\"\"\n\n    def __init__(\n        self,\n        log_queue: multiprocessing.Queue,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        controller: SystemController,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            controller=controller,\n            **kwargs,\n        )\n        self.controller = controller\n        self.service_config = service_config\n        self.app: AIPerfTextualApp = AIPerfTextualApp(\n            service_config=service_config, controller=controller\n        )\n        # Setup the log consumer to consume log records from the shared log queue\n        self.log_consumer: LogConsumer = LogConsumer(log_queue=log_queue, app=self.app)\n        self.attach_child_lifecycle(self.log_consumer)  # type: ignore\n\n        # Attach the hooks directly to the function on the app, to avoid the extra function call overhead\n        self.attach_hook(AIPerfHook.ON_RECORDS_PROGRESS, self.app.on_records_progress)\n        self.attach_hook(\n            AIPerfHook.ON_PROFILING_PROGRESS, self.app.on_profiling_progress\n        )\n        self.attach_hook(AIPerfHook.ON_WARMUP_PROGRESS, self.app.on_warmup_progress)\n        self.attach_hook(AIPerfHook.ON_WORKER_UPDATE, self.app.on_worker_update)\n        self.attach_hook(\n            AIPerfHook.ON_WORKER_STATUS_SUMMARY, self.app.on_worker_status_summary\n        )\n        self.attach_hook(AIPerfHook.ON_REALTIME_METRICS, self.app.on_realtime_metrics)\n\n    @on_start\n    async def _run_app(self) -&gt; None:\n        \"\"\"Run the enhanced Dashboard application.\"\"\"\n        self.debug(\"Starting AIPerf Dashboard UI...\")\n        # Start the Textual App in the background\n        self.execute_async(self.app.run_async())\n\n    @on_stop\n    async def _on_stop(self) -&gt; None:\n        \"\"\"Stop the Dashboard application gracefully.\"\"\"\n        self.debug(\"Shutting down Dashboard UI\")\n        self.app.exit(return_code=0)\n</code></pre>"},{"location":"api/#aiperfuidashboardaiperf_textual_app","title":"aiperf.ui.dashboard.aiperf_textual_app","text":""},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp","title":"<code>AIPerfTextualApp</code>","text":"<p>               Bases: <code>App</code></p> <p>AIPerf Textual App.</p> <p>This is the main application class for the Textual UI. It is responsible for composing the application layout and handling the application commands.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>class AIPerfTextualApp(App):\n    \"\"\"\n    AIPerf Textual App.\n\n    This is the main application class for the Textual UI. It is responsible for\n    composing the application layout and handling the application commands.\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE = False\n    \"\"\"Disable the command palette that is enabled by default in Textual.\"\"\"\n\n    ALLOW_IN_MAXIMIZED_VIEW = \"ProgressHeader, Footer\"\n    \"\"\"Allow the custom header and footer to be displayed when a panel is maximized.\"\"\"\n\n    NOTIFICATION_TIMEOUT = 3\n    \"\"\"The timeout for notifications in seconds.\"\"\"\n\n    CSS = \"\"\"\n    #main-container {\n        height: 100%;\n    }\n    #dashboard-section {\n        height: 3fr;\n        min-height: 14;\n    }\n    #logs-section {\n        height: 2fr;\n        max-height: 16;\n    }\n    #workers-section {\n        height: 3;\n    }\n    #progress-section {\n        width: 1fr;\n    }\n    #metrics-section {\n        width: 2fr;\n    }\n    .hidden {\n        display: none;\n    }\n    \"\"\"\n\n    BINDINGS = [\n        (\"ctrl+c\", \"quit\", \"Quit\"),\n        (\"1\", \"minimize_all_panels\", \"Overview\"),\n        (\"2\", \"toggle_maximize('progress')\", \"Progress\"),\n        (\"3\", \"toggle_maximize('metrics')\", \"Metrics\"),\n        (\"4\", \"toggle_maximize('workers')\", \"Workers\"),\n        (\"5\", \"toggle_maximize('logs')\", \"Logs\"),\n        (\"escape\", \"restore_all_panels\", \"Restore View\"),\n        Binding(\"ctrl+s\", \"screenshot\", \"Save Screenshot\", show=False),\n        Binding(\"l\", \"toggle_hide_log_viewer\", \"Toggle Logs\", show=False),\n    ]\n\n    def __init__(\n        self, service_config: ServiceConfig, controller: SystemController\n    ) -&gt; None:\n        super().__init__()\n\n        self.title = \"NVIDIA AIPerf\"\n        if AIPERF_DEV_MODE:\n            self.title = \"NVIDIA AIPerf (Developer Mode)\"\n\n        self.log_viewer: RichLogViewer | None = None\n        self.progress_dashboard: ProgressDashboard | None = None\n        self.progress_header: ProgressHeader | None = None\n        self.worker_dashboard: WorkerDashboard | None = None\n        self.realtime_metrics_dashboard: RealtimeMetricsDashboard | None = None\n        self.profile_results: list[RenderableType] = []\n        self.service_config = service_config\n        self.controller: SystemController = controller\n        self._warmup_stats: RequestsStats | None = None\n        self._profiling_stats: RequestsStats | None = None\n        self._records_stats: RecordsStats | None = None\n\n    def on_mount(self) -&gt; None:\n        self.register_theme(AIPERF_THEME)\n        self.theme = AIPERF_THEME.name\n\n    def compose(self) -&gt; ComposeResult:\n        \"\"\"Compose the full application layout.\"\"\"\n        self.progress_header = ProgressHeader(title=self.title, id=\"progress-header\")\n        yield self.progress_header\n\n        # NOTE: SIM117 is disabled because nested with statements are recommended for textual ui layouts\n        with Vertical(id=\"main-container\"):\n            with Container(id=\"dashboard-section\"):  # noqa: SIM117\n                with Horizontal(id=\"overview-section\"):\n                    with Container(id=\"progress-section\"):\n                        self.progress_dashboard = ProgressDashboard(id=\"progress\")\n                        yield self.progress_dashboard\n\n                    with Container(id=\"metrics-section\"):\n                        self.realtime_metrics_dashboard = RealtimeMetricsDashboard(\n                            service_config=self.service_config, id=\"metrics\"\n                        )\n                        yield self.realtime_metrics_dashboard\n\n            with Container(id=\"workers-section\", classes=\"hidden\"):\n                self.worker_dashboard = WorkerDashboard(id=\"workers\")\n                yield self.worker_dashboard\n\n            with Container(id=\"logs-section\"):\n                self.log_viewer = RichLogViewer(id=\"logs\")\n                yield self.log_viewer\n\n        yield Footer()\n\n    async def action_quit(self) -&gt; None:\n        \"\"\"Stop the UI and forward the signal to the main process.\"\"\"\n        self.exit(return_code=0)\n        # Clear the references to the widgets to ensure they do not get updated after the app is stopped\n        self.worker_dashboard = None\n        self.progress_dashboard = None\n        self.progress_header = None\n        self.realtime_metrics_dashboard = None\n        self.log_viewer = None\n        # Forward the signal to the main process\n        os.kill(os.getpid(), signal.SIGINT)\n\n    async def action_toggle_hide_log_viewer(self) -&gt; None:\n        \"\"\"Toggle the visibility of the log viewer section.\"\"\"\n        with suppress(Exception):\n            self.query_one(\"#logs-section\").toggle_class(\"hidden\")\n\n    async def action_restore_all_panels(self) -&gt; None:\n        \"\"\"Restore all panels.\"\"\"\n        self.screen.minimize()\n        with suppress(Exception):\n            self.query_one(\"#logs-section\").remove_class(\"hidden\")\n\n    async def action_minimize_all_panels(self) -&gt; None:\n        \"\"\"Minimize all panels.\"\"\"\n        self.screen.minimize()\n\n    async def action_toggle_maximize(self, panel_id: str) -&gt; None:\n        \"\"\"Toggle the maximize state of the panel with the given id.\"\"\"\n        panel = self.query_one(f\"#{panel_id}\")\n        if panel and panel.is_maximized:\n            self.screen.minimize()\n        else:\n            self.screen.maximize(panel)\n\n    async def on_warmup_progress(self, warmup_stats: RequestsStats) -&gt; None:\n        \"\"\"Forward warmup progress updates to the Textual App.\"\"\"\n        if not self._warmup_stats:\n            self.query_one(\"#progress-section\").remove_class(\"hidden\")\n        self._warmup_stats = warmup_stats\n        if self.progress_dashboard:\n            async with self.progress_dashboard.batch():\n                self.progress_dashboard.on_warmup_progress(warmup_stats)\n        if self.progress_header:\n            self.progress_header.update_progress(\n                header=\"Warmup\",\n                progress=warmup_stats.finished,\n                total=warmup_stats.total_expected_requests,\n            )\n\n    async def on_profiling_progress(self, profiling_stats: RequestsStats) -&gt; None:\n        \"\"\"Forward requests phase progress updates to the Textual App.\"\"\"\n        if not self._profiling_stats:\n            self.query_one(\"#progress-section\").remove_class(\"hidden\")\n        self._profiling_stats = profiling_stats\n        if self.progress_dashboard:\n            async with self.progress_dashboard.batch():\n                self.progress_dashboard.on_profiling_progress(profiling_stats)\n        if self.progress_header:\n            self.progress_header.update_progress(\n                header=\"Profiling\",\n                progress=profiling_stats.finished,\n                total=profiling_stats.total_expected_requests,\n            )\n\n    async def on_records_progress(self, records_stats: RecordsStats) -&gt; None:\n        \"\"\"Forward records progress updates to the Textual App.\"\"\"\n        self._records_stats = records_stats\n        if self.progress_dashboard:\n            async with self.progress_dashboard.batch():\n                self.progress_dashboard.on_records_progress(records_stats)\n\n        if (\n            self._profiling_stats\n            and self._profiling_stats.is_complete\n            and self.progress_header\n        ):\n            self.progress_header.update_progress(\n                header=\"Records\",\n                progress=self._profiling_stats.finished,\n                total=self._profiling_stats.total_expected_requests,\n            )\n\n    async def on_worker_update(self, worker_id: str, worker_stats: WorkerStats):\n        \"\"\"Forward worker updates to the Textual App.\"\"\"\n        if self.worker_dashboard:\n            async with self.worker_dashboard.batch():\n                self.worker_dashboard.on_worker_update(worker_id, worker_stats)\n\n    async def on_worker_status_summary(self, worker_status_summary: dict[str, WorkerStatus]) -&gt; None:  # fmt: skip\n        \"\"\"Forward worker status summary updates to the Textual App.\"\"\"\n        if self.worker_dashboard:\n            async with self.worker_dashboard.batch():\n                self.worker_dashboard.on_worker_status_summary(worker_status_summary)\n\n    async def on_realtime_metrics(self, metrics: list[MetricResult]) -&gt; None:\n        \"\"\"Forward real-time metrics updates to the Textual App.\"\"\"\n        if self.realtime_metrics_dashboard:\n            async with self.realtime_metrics_dashboard.batch():\n                self.realtime_metrics_dashboard.on_realtime_metrics(metrics)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.ALLOW_IN_MAXIMIZED_VIEW","title":"<code>ALLOW_IN_MAXIMIZED_VIEW = 'ProgressHeader, Footer'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Allow the custom header and footer to be displayed when a panel is maximized.</p>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.ENABLE_COMMAND_PALETTE","title":"<code>ENABLE_COMMAND_PALETTE = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Disable the command palette that is enabled by default in Textual.</p>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.NOTIFICATION_TIMEOUT","title":"<code>NOTIFICATION_TIMEOUT = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout for notifications in seconds.</p>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.action_minimize_all_panels","title":"<code>action_minimize_all_panels()</code>  <code>async</code>","text":"<p>Minimize all panels.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def action_minimize_all_panels(self) -&gt; None:\n    \"\"\"Minimize all panels.\"\"\"\n    self.screen.minimize()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.action_quit","title":"<code>action_quit()</code>  <code>async</code>","text":"<p>Stop the UI and forward the signal to the main process.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def action_quit(self) -&gt; None:\n    \"\"\"Stop the UI and forward the signal to the main process.\"\"\"\n    self.exit(return_code=0)\n    # Clear the references to the widgets to ensure they do not get updated after the app is stopped\n    self.worker_dashboard = None\n    self.progress_dashboard = None\n    self.progress_header = None\n    self.realtime_metrics_dashboard = None\n    self.log_viewer = None\n    # Forward the signal to the main process\n    os.kill(os.getpid(), signal.SIGINT)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.action_restore_all_panels","title":"<code>action_restore_all_panels()</code>  <code>async</code>","text":"<p>Restore all panels.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def action_restore_all_panels(self) -&gt; None:\n    \"\"\"Restore all panels.\"\"\"\n    self.screen.minimize()\n    with suppress(Exception):\n        self.query_one(\"#logs-section\").remove_class(\"hidden\")\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.action_toggle_hide_log_viewer","title":"<code>action_toggle_hide_log_viewer()</code>  <code>async</code>","text":"<p>Toggle the visibility of the log viewer section.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def action_toggle_hide_log_viewer(self) -&gt; None:\n    \"\"\"Toggle the visibility of the log viewer section.\"\"\"\n    with suppress(Exception):\n        self.query_one(\"#logs-section\").toggle_class(\"hidden\")\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.action_toggle_maximize","title":"<code>action_toggle_maximize(panel_id)</code>  <code>async</code>","text":"<p>Toggle the maximize state of the panel with the given id.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def action_toggle_maximize(self, panel_id: str) -&gt; None:\n    \"\"\"Toggle the maximize state of the panel with the given id.\"\"\"\n    panel = self.query_one(f\"#{panel_id}\")\n    if panel and panel.is_maximized:\n        self.screen.minimize()\n    else:\n        self.screen.maximize(panel)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.compose","title":"<code>compose()</code>","text":"<p>Compose the full application layout.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>def compose(self) -&gt; ComposeResult:\n    \"\"\"Compose the full application layout.\"\"\"\n    self.progress_header = ProgressHeader(title=self.title, id=\"progress-header\")\n    yield self.progress_header\n\n    # NOTE: SIM117 is disabled because nested with statements are recommended for textual ui layouts\n    with Vertical(id=\"main-container\"):\n        with Container(id=\"dashboard-section\"):  # noqa: SIM117\n            with Horizontal(id=\"overview-section\"):\n                with Container(id=\"progress-section\"):\n                    self.progress_dashboard = ProgressDashboard(id=\"progress\")\n                    yield self.progress_dashboard\n\n                with Container(id=\"metrics-section\"):\n                    self.realtime_metrics_dashboard = RealtimeMetricsDashboard(\n                        service_config=self.service_config, id=\"metrics\"\n                    )\n                    yield self.realtime_metrics_dashboard\n\n        with Container(id=\"workers-section\", classes=\"hidden\"):\n            self.worker_dashboard = WorkerDashboard(id=\"workers\")\n            yield self.worker_dashboard\n\n        with Container(id=\"logs-section\"):\n            self.log_viewer = RichLogViewer(id=\"logs\")\n            yield self.log_viewer\n\n    yield Footer()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.on_profiling_progress","title":"<code>on_profiling_progress(profiling_stats)</code>  <code>async</code>","text":"<p>Forward requests phase progress updates to the Textual App.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def on_profiling_progress(self, profiling_stats: RequestsStats) -&gt; None:\n    \"\"\"Forward requests phase progress updates to the Textual App.\"\"\"\n    if not self._profiling_stats:\n        self.query_one(\"#progress-section\").remove_class(\"hidden\")\n    self._profiling_stats = profiling_stats\n    if self.progress_dashboard:\n        async with self.progress_dashboard.batch():\n            self.progress_dashboard.on_profiling_progress(profiling_stats)\n    if self.progress_header:\n        self.progress_header.update_progress(\n            header=\"Profiling\",\n            progress=profiling_stats.finished,\n            total=profiling_stats.total_expected_requests,\n        )\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.on_realtime_metrics","title":"<code>on_realtime_metrics(metrics)</code>  <code>async</code>","text":"<p>Forward real-time metrics updates to the Textual App.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def on_realtime_metrics(self, metrics: list[MetricResult]) -&gt; None:\n    \"\"\"Forward real-time metrics updates to the Textual App.\"\"\"\n    if self.realtime_metrics_dashboard:\n        async with self.realtime_metrics_dashboard.batch():\n            self.realtime_metrics_dashboard.on_realtime_metrics(metrics)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.on_records_progress","title":"<code>on_records_progress(records_stats)</code>  <code>async</code>","text":"<p>Forward records progress updates to the Textual App.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def on_records_progress(self, records_stats: RecordsStats) -&gt; None:\n    \"\"\"Forward records progress updates to the Textual App.\"\"\"\n    self._records_stats = records_stats\n    if self.progress_dashboard:\n        async with self.progress_dashboard.batch():\n            self.progress_dashboard.on_records_progress(records_stats)\n\n    if (\n        self._profiling_stats\n        and self._profiling_stats.is_complete\n        and self.progress_header\n    ):\n        self.progress_header.update_progress(\n            header=\"Records\",\n            progress=self._profiling_stats.finished,\n            total=self._profiling_stats.total_expected_requests,\n        )\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.on_warmup_progress","title":"<code>on_warmup_progress(warmup_stats)</code>  <code>async</code>","text":"<p>Forward warmup progress updates to the Textual App.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def on_warmup_progress(self, warmup_stats: RequestsStats) -&gt; None:\n    \"\"\"Forward warmup progress updates to the Textual App.\"\"\"\n    if not self._warmup_stats:\n        self.query_one(\"#progress-section\").remove_class(\"hidden\")\n    self._warmup_stats = warmup_stats\n    if self.progress_dashboard:\n        async with self.progress_dashboard.batch():\n            self.progress_dashboard.on_warmup_progress(warmup_stats)\n    if self.progress_header:\n        self.progress_header.update_progress(\n            header=\"Warmup\",\n            progress=warmup_stats.finished,\n            total=warmup_stats.total_expected_requests,\n        )\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.on_worker_status_summary","title":"<code>on_worker_status_summary(worker_status_summary)</code>  <code>async</code>","text":"<p>Forward worker status summary updates to the Textual App.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def on_worker_status_summary(self, worker_status_summary: dict[str, WorkerStatus]) -&gt; None:  # fmt: skip\n    \"\"\"Forward worker status summary updates to the Textual App.\"\"\"\n    if self.worker_dashboard:\n        async with self.worker_dashboard.batch():\n            self.worker_dashboard.on_worker_status_summary(worker_status_summary)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.aiperf_textual_app.AIPerfTextualApp.on_worker_update","title":"<code>on_worker_update(worker_id, worker_stats)</code>  <code>async</code>","text":"<p>Forward worker updates to the Textual App.</p> Source code in <code>aiperf/ui/dashboard/aiperf_textual_app.py</code> <pre><code>async def on_worker_update(self, worker_id: str, worker_stats: WorkerStats):\n    \"\"\"Forward worker updates to the Textual App.\"\"\"\n    if self.worker_dashboard:\n        async with self.worker_dashboard.batch():\n            self.worker_dashboard.on_worker_update(worker_id, worker_stats)\n</code></pre>"},{"location":"api/#aiperfuidashboardaiperf_theme","title":"aiperf.ui.dashboard.aiperf_theme","text":"<p>AIPerf Theme for Textual UI.</p>"},{"location":"api/#aiperfuidashboardcustom_widgets","title":"aiperf.ui.dashboard.custom_widgets","text":""},{"location":"api/#aiperf.ui.dashboard.custom_widgets.MaximizableWidget","title":"<code>MaximizableWidget</code>","text":"<p>               Bases: <code>Widget</code></p> <p>Mixin that allows a widget to be maximized by double-clicking on it.</p> Source code in <code>aiperf/ui/dashboard/custom_widgets.py</code> <pre><code>class MaximizableWidget(Widget):\n    \"\"\"Mixin that allows a widget to be maximized by double-clicking on it.\"\"\"\n\n    ALLOW_MAXIMIZE = True\n    \"\"\"Allow the widget to be maximized.\"\"\"\n\n    def on_click(self, event: Click) -&gt; None:\n        \"\"\"Handle click events to toggle the maximize state of the widget.\"\"\"\n        if event.chain == 2:\n            event.stop()\n            self.toggle_maximize()\n\n    def toggle_maximize(self) -&gt; None:\n        \"\"\"Toggle the maximize state of the widget.\"\"\"\n        if not self.is_maximized:\n            self.screen.maximize(self)\n        else:\n            self.screen.minimize()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.custom_widgets.MaximizableWidget.ALLOW_MAXIMIZE","title":"<code>ALLOW_MAXIMIZE = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Allow the widget to be maximized.</p>"},{"location":"api/#aiperf.ui.dashboard.custom_widgets.MaximizableWidget.on_click","title":"<code>on_click(event)</code>","text":"<p>Handle click events to toggle the maximize state of the widget.</p> Source code in <code>aiperf/ui/dashboard/custom_widgets.py</code> <pre><code>def on_click(self, event: Click) -&gt; None:\n    \"\"\"Handle click events to toggle the maximize state of the widget.\"\"\"\n    if event.chain == 2:\n        event.stop()\n        self.toggle_maximize()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.custom_widgets.MaximizableWidget.toggle_maximize","title":"<code>toggle_maximize()</code>","text":"<p>Toggle the maximize state of the widget.</p> Source code in <code>aiperf/ui/dashboard/custom_widgets.py</code> <pre><code>def toggle_maximize(self) -&gt; None:\n    \"\"\"Toggle the maximize state of the widget.\"\"\"\n    if not self.is_maximized:\n        self.screen.maximize(self)\n    else:\n        self.screen.minimize()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.custom_widgets.NonFocusableDataTable","title":"<code>NonFocusableDataTable</code>","text":"<p>               Bases: <code>DataTable</code></p> <p>DataTable that cannot receive focus. This is done to prevent the table from focusing when the user clicks on it, which would cause the table to darken its background.</p> Source code in <code>aiperf/ui/dashboard/custom_widgets.py</code> <pre><code>class NonFocusableDataTable(DataTable, can_focus=False):\n    \"\"\"DataTable that cannot receive focus.\n    This is done to prevent the table from focusing when the user clicks on it, which would cause the table to darken its background.\"\"\"\n</code></pre>"},{"location":"api/#aiperfuidashboardprogress_dashboard","title":"aiperf.ui.dashboard.progress_dashboard","text":""},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard","title":"<code>ProgressDashboard</code>","text":"<p>               Bases: <code>Container</code>, <code>MaximizableWidget</code></p> <p>Textual widget that displays Rich progress bars for profile execution.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>class ProgressDashboard(Container, MaximizableWidget):\n    \"\"\"Textual widget that displays Rich progress bars for profile execution.\"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    ProgressDashboard {\n        height: 1fr;\n        border: round $primary;\n        border-title-color: $primary;\n        border-title-style: bold;\n        border-title-align: center;\n        padding: 0 1 0 1;\n    }\n    #status-display {\n        height: auto;\n        margin: 0 1 0 1;\n    }\n    #progress-display {\n        height: auto;\n        margin: 0 1 0 1;\n    }\n    #stats-display {\n        height: auto;\n    }\n    #stats-display.no-stats {\n        height: 1fr;\n        content-align: center middle;\n        color: $warning;\n        text-style: italic;\n    }\n    \"\"\"\n\n    SPINNER_REFRESH_RATE = 0.1  # 10 FPS\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.border_title = \"Profile Progress\"\n\n        self.progress = Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            TaskProgressColumn(),\n            expand=False,\n        )\n\n        self.task_ids: dict[str, TaskID] = {}\n        self.progress_widget: Static | None = None\n        self.stats_widget: Static | None = None\n        self.records_stats: RecordsStats | None = None\n        self.profiling_stats: RequestsStats | None = None\n        self.warmup_stats: RequestsStats | None = None\n        self.refresh_timer: Timer | None = None\n\n    def on_mount(self) -&gt; None:\n        \"\"\"Set up the refresh timer when the widget is mounted.\"\"\"\n        self.refresh_timer = self.set_interval(\n            self.SPINNER_REFRESH_RATE, self.refresh_timer_callback\n        )\n\n    def on_unmount(self) -&gt; None:\n        \"\"\"Clean up the timer when the widget is unmounted.\"\"\"\n        if self.refresh_timer:\n            self.refresh_timer.stop()\n\n    def refresh_timer_callback(self) -&gt; None:\n        \"\"\"Callback for the refresh timer to update the progress widget.\"\"\"\n        if self.progress_widget:\n            self.progress_widget.update(self.progress)\n\n    def compose(self) -&gt; ComposeResult:\n        self.progress_widget = Static(self.progress, id=\"progress-display\")\n        yield self.progress_widget\n\n        self.stats_widget = Static(\n            \"Waiting for profile data...\",\n            id=\"stats-display\",\n            classes=\"no-stats\",\n        )\n        yield self.stats_widget\n\n    def create_or_update_progress(self, name: str, stats: StatsProtocol) -&gt; None:\n        \"\"\"Create or update the progress for a given task.\"\"\"\n        task_id = self.task_ids.get(name)\n        if task_id is None and stats.total_expected_requests:\n            self.task_ids[name] = self.progress.add_task(\n                name, total=stats.total_expected_requests\n            )\n        elif task_id is not None:\n            self.progress.update(task_id, completed=stats.finished)\n            if stats.is_complete:\n                self.progress.update(\n                    task_id,\n                    description=f\"[green]{name}[/green]\",\n                )\n\n    def on_warmup_progress(self, warmup_stats: RequestsStats) -&gt; None:\n        \"\"\"Callback for warmup progress updates.\"\"\"\n        if not self.warmup_stats:\n            self.query_one(\"#stats-display\").remove_class(\"no-stats\")\n        self.warmup_stats = warmup_stats\n        self.create_or_update_progress(\"Warmup\", warmup_stats)\n        self.update_display(CreditPhase.WARMUP, self.warmup_stats)\n\n    def on_profiling_progress(self, profiling_stats: RequestsStats) -&gt; None:\n        \"\"\"Callback for profiling progress updates.\"\"\"\n        if not self.profiling_stats:\n            self.query_one(\"#stats-display\").remove_class(\"no-stats\")\n        self.profiling_stats = profiling_stats\n        self.create_or_update_progress(\"Profiling\", profiling_stats)\n        self.update_display(CreditPhase.PROFILING, self.profiling_stats)\n\n    def on_records_progress(self, records_stats: RecordsStats) -&gt; None:\n        \"\"\"Callback for records progress updates.\"\"\"\n        if not self.records_stats:\n            self.query_one(\"#stats-display\").remove_class(\"no-stats\")\n        self.records_stats = records_stats\n        self.create_or_update_progress(\"Records\", records_stats)\n        # NOTE: Send the profiling stats to the display, not the records stats\n        self.update_display(CreditPhase.PROFILING, self.profiling_stats)\n\n    def update_display(\n        self, phase: CreditPhase, stats: StatsProtocol | None = None\n    ) -&gt; None:\n        \"\"\"Update the progress display.\"\"\"\n        if self.progress_widget:\n            self.progress_widget.update(self.progress)\n        if self.stats_widget:\n            self.stats_widget.update(self.create_stats_table(phase, stats))\n\n    def _get_status(self) -&gt; Text:\n        \"\"\"Get the status of the profile.\"\"\"\n        if self.records_stats and self.records_stats.is_complete:\n            return Text(\"Complete\", style=\"bold green\")\n        elif self.profiling_stats and self.profiling_stats.is_complete:\n            return Text(\"Processing\", style=\"bold green\")\n        elif self.profiling_stats:\n            return Text(\"Profiling\", style=\"bold yellow\")\n        elif self.warmup_stats:\n            return Text(\"Warmup\", style=\"bold yellow\")\n        else:\n            return Text(\"Waiting for profile data...\", style=\"dim\")\n\n    def create_stats_table(\n        self, phase: CreditPhase, stats: StatsProtocol | None = None\n    ) -&gt; VisualType:\n        \"\"\"Create a table with the profile status and progress.\"\"\"\n        stats_table = Table.grid(padding=(0, 1, 0, 0))\n        stats_table.add_column(style=\"bold cyan\", justify=\"right\")\n        stats_table.add_column(style=\"bold white\")\n\n        if not stats:\n            return stats_table\n\n        stats_table.add_row(\"Status:\", self._get_status())\n\n        if stats.total_expected_requests:\n            stats_table.add_row(\n                \"Progress:\",\n                f\"{stats.finished or 0:,} / {stats.total_expected_requests:,} requests \"\n                f\"({stats.progress_percent:.1f}%)\",\n            )\n\n        if self.records_stats:\n            error_percent = 0.0\n            if self.records_stats.total_records:\n                error_percent = (\n                    (self.records_stats.errors or 0) / self.records_stats.total_records * 100\n                )  # fmt: skip\n            error_color = (\n                \"green\"\n                if error_percent == 0\n                else \"red\"\n                if error_percent &gt; 10\n                else \"yellow\"\n            )\n            stats_table.add_row(\n                \"Errors:\",\n                f\"[{error_color}]{self.records_stats.errors or 0:,} / {self.records_stats.total_records or 0:,} \"\n                f\"({error_percent:.1f}%)[/{error_color}]\",\n            )\n\n        stats_table.add_row(\"Request Rate:\", f\"{stats.per_second or 0:,.1f} requests/s\")\n\n        if self.records_stats:\n            stats_table.add_row(\n                \"Processing Rate:\",\n                f\"{self.records_stats.per_second or 0:,.1f} records/s\",\n            )\n\n        if not stats.is_complete:\n            # Display request stats while profiling\n            if stats.start_ns:\n                stats_table.add_row(\"Elapsed:\", format_elapsed_time(stats.elapsed_time))\n            if stats.eta:\n                stats_table.add_row(\"ETA:\", format_eta(stats.eta))\n        elif self.records_stats:\n            # Display record processing stats after profiling\n            if self.records_stats.start_ns:\n                stats_table.add_row(\n                    \"Elapsed:\", format_elapsed_time(self.records_stats.elapsed_time)\n                )\n            if self.records_stats.eta:\n                stats_table.add_row(\"Records ETA:\", format_eta(self.records_stats.eta))\n\n        return stats_table\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.create_or_update_progress","title":"<code>create_or_update_progress(name, stats)</code>","text":"<p>Create or update the progress for a given task.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def create_or_update_progress(self, name: str, stats: StatsProtocol) -&gt; None:\n    \"\"\"Create or update the progress for a given task.\"\"\"\n    task_id = self.task_ids.get(name)\n    if task_id is None and stats.total_expected_requests:\n        self.task_ids[name] = self.progress.add_task(\n            name, total=stats.total_expected_requests\n        )\n    elif task_id is not None:\n        self.progress.update(task_id, completed=stats.finished)\n        if stats.is_complete:\n            self.progress.update(\n                task_id,\n                description=f\"[green]{name}[/green]\",\n            )\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.create_stats_table","title":"<code>create_stats_table(phase, stats=None)</code>","text":"<p>Create a table with the profile status and progress.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def create_stats_table(\n    self, phase: CreditPhase, stats: StatsProtocol | None = None\n) -&gt; VisualType:\n    \"\"\"Create a table with the profile status and progress.\"\"\"\n    stats_table = Table.grid(padding=(0, 1, 0, 0))\n    stats_table.add_column(style=\"bold cyan\", justify=\"right\")\n    stats_table.add_column(style=\"bold white\")\n\n    if not stats:\n        return stats_table\n\n    stats_table.add_row(\"Status:\", self._get_status())\n\n    if stats.total_expected_requests:\n        stats_table.add_row(\n            \"Progress:\",\n            f\"{stats.finished or 0:,} / {stats.total_expected_requests:,} requests \"\n            f\"({stats.progress_percent:.1f}%)\",\n        )\n\n    if self.records_stats:\n        error_percent = 0.0\n        if self.records_stats.total_records:\n            error_percent = (\n                (self.records_stats.errors or 0) / self.records_stats.total_records * 100\n            )  # fmt: skip\n        error_color = (\n            \"green\"\n            if error_percent == 0\n            else \"red\"\n            if error_percent &gt; 10\n            else \"yellow\"\n        )\n        stats_table.add_row(\n            \"Errors:\",\n            f\"[{error_color}]{self.records_stats.errors or 0:,} / {self.records_stats.total_records or 0:,} \"\n            f\"({error_percent:.1f}%)[/{error_color}]\",\n        )\n\n    stats_table.add_row(\"Request Rate:\", f\"{stats.per_second or 0:,.1f} requests/s\")\n\n    if self.records_stats:\n        stats_table.add_row(\n            \"Processing Rate:\",\n            f\"{self.records_stats.per_second or 0:,.1f} records/s\",\n        )\n\n    if not stats.is_complete:\n        # Display request stats while profiling\n        if stats.start_ns:\n            stats_table.add_row(\"Elapsed:\", format_elapsed_time(stats.elapsed_time))\n        if stats.eta:\n            stats_table.add_row(\"ETA:\", format_eta(stats.eta))\n    elif self.records_stats:\n        # Display record processing stats after profiling\n        if self.records_stats.start_ns:\n            stats_table.add_row(\n                \"Elapsed:\", format_elapsed_time(self.records_stats.elapsed_time)\n            )\n        if self.records_stats.eta:\n            stats_table.add_row(\"Records ETA:\", format_eta(self.records_stats.eta))\n\n    return stats_table\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.on_mount","title":"<code>on_mount()</code>","text":"<p>Set up the refresh timer when the widget is mounted.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"Set up the refresh timer when the widget is mounted.\"\"\"\n    self.refresh_timer = self.set_interval(\n        self.SPINNER_REFRESH_RATE, self.refresh_timer_callback\n    )\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.on_profiling_progress","title":"<code>on_profiling_progress(profiling_stats)</code>","text":"<p>Callback for profiling progress updates.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def on_profiling_progress(self, profiling_stats: RequestsStats) -&gt; None:\n    \"\"\"Callback for profiling progress updates.\"\"\"\n    if not self.profiling_stats:\n        self.query_one(\"#stats-display\").remove_class(\"no-stats\")\n    self.profiling_stats = profiling_stats\n    self.create_or_update_progress(\"Profiling\", profiling_stats)\n    self.update_display(CreditPhase.PROFILING, self.profiling_stats)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.on_records_progress","title":"<code>on_records_progress(records_stats)</code>","text":"<p>Callback for records progress updates.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def on_records_progress(self, records_stats: RecordsStats) -&gt; None:\n    \"\"\"Callback for records progress updates.\"\"\"\n    if not self.records_stats:\n        self.query_one(\"#stats-display\").remove_class(\"no-stats\")\n    self.records_stats = records_stats\n    self.create_or_update_progress(\"Records\", records_stats)\n    # NOTE: Send the profiling stats to the display, not the records stats\n    self.update_display(CreditPhase.PROFILING, self.profiling_stats)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.on_unmount","title":"<code>on_unmount()</code>","text":"<p>Clean up the timer when the widget is unmounted.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def on_unmount(self) -&gt; None:\n    \"\"\"Clean up the timer when the widget is unmounted.\"\"\"\n    if self.refresh_timer:\n        self.refresh_timer.stop()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.on_warmup_progress","title":"<code>on_warmup_progress(warmup_stats)</code>","text":"<p>Callback for warmup progress updates.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def on_warmup_progress(self, warmup_stats: RequestsStats) -&gt; None:\n    \"\"\"Callback for warmup progress updates.\"\"\"\n    if not self.warmup_stats:\n        self.query_one(\"#stats-display\").remove_class(\"no-stats\")\n    self.warmup_stats = warmup_stats\n    self.create_or_update_progress(\"Warmup\", warmup_stats)\n    self.update_display(CreditPhase.WARMUP, self.warmup_stats)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.refresh_timer_callback","title":"<code>refresh_timer_callback()</code>","text":"<p>Callback for the refresh timer to update the progress widget.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def refresh_timer_callback(self) -&gt; None:\n    \"\"\"Callback for the refresh timer to update the progress widget.\"\"\"\n    if self.progress_widget:\n        self.progress_widget.update(self.progress)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_dashboard.ProgressDashboard.update_display","title":"<code>update_display(phase, stats=None)</code>","text":"<p>Update the progress display.</p> Source code in <code>aiperf/ui/dashboard/progress_dashboard.py</code> <pre><code>def update_display(\n    self, phase: CreditPhase, stats: StatsProtocol | None = None\n) -&gt; None:\n    \"\"\"Update the progress display.\"\"\"\n    if self.progress_widget:\n        self.progress_widget.update(self.progress)\n    if self.stats_widget:\n        self.stats_widget.update(self.create_stats_table(phase, stats))\n</code></pre>"},{"location":"api/#aiperfuidashboardprogress_header","title":"aiperf.ui.dashboard.progress_header","text":""},{"location":"api/#aiperf.ui.dashboard.progress_header.ProgressHeader","title":"<code>ProgressHeader</code>","text":"<p>               Bases: <code>Widget</code></p> <p>Custom header for the progress dashboard.</p> Source code in <code>aiperf/ui/dashboard/progress_header.py</code> <pre><code>class ProgressHeader(Widget):\n    \"\"\"Custom header for the progress dashboard.\"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    ProgressHeader {\n        dock: top;\n        width: 100%;\n        background: $footer-background;\n        color: $warning;\n        text-style: bold;\n        height: 1;\n    }\n    .bar--indeterminate {\n        color: $primary;\n        background: $secondary;\n    }\n    .bar--complete {\n        color: $error;\n    }\n    PercentageStatus {\n        color: $primary;\n    }\n    ETAStatus {\n        color: $primary;\n    }\n    #padding {\n        width: 1fr;\n    }\n    #progress-bar {\n        width: 1fr;\n        background: $footer-background;\n        align: right middle;\n        padding-right: 1;\n    }\n    #header-title {\n        width: 1fr;\n        content-align: center middle;\n        color: $primary;\n    }\n    #progress-name {\n        width: 1fr;\n        content-align: left middle;\n        color: $primary;\n        padding-left: 1;\n    }\n    #progress-name.warmup, #progress-bar.warmup, PercentageStatus.warmup {\n        color: $warning;\n    }\n    #progress-name.profiling, #progress-bar.profiling, PercentageStatus.profiling {\n        color: $primary;\n    }\n    #progress-name.records, #progress-bar.records, PercentageStatus.records{\n        color: $success;\n    }\n    \"\"\"\n\n    def __init__(self, title: str, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.title = title\n        self.progress_name = \"\"\n\n    def compose(self):\n        with Horizontal():\n            yield Static(id=\"progress-name\")\n            yield Static(self.title, id=\"header-title\")\n            yield ProgressBar(\n                id=\"progress-bar\",\n                total=100,\n                show_eta=False,\n                show_percentage=True,\n                classes=\"hidden\",\n            )\n            yield Static(id=\"padding\")\n\n    def update_progress(\n        self, header: str, progress: float, total: float | None = None\n    ) -&gt; None:\n        \"\"\"Update the progress of the progress bar.\"\"\"\n        with contextlib.suppress(Exception):\n            bar = self.query_one(ProgressBar)\n            if self.progress_name != header:\n                bar.remove_class(\"hidden\")\n                self.query_one(\"#padding\").add_class(\"hidden\")\n                self.query_one(\"#progress-name\", Static).remove_class(\n                    \"warmup\", \"profiling\", \"records\"\n                ).add_class(header.lower()).update(header)\n                self.query_one(\"PercentageStatus\").remove_class(\n                    \"warmup\", \"profiling\", \"records\"\n                ).add_class(header.lower())\n                self.progress_name = header\n            bar.update(progress=progress, total=total)\n            self.refresh()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.progress_header.ProgressHeader.update_progress","title":"<code>update_progress(header, progress, total=None)</code>","text":"<p>Update the progress of the progress bar.</p> Source code in <code>aiperf/ui/dashboard/progress_header.py</code> <pre><code>def update_progress(\n    self, header: str, progress: float, total: float | None = None\n) -&gt; None:\n    \"\"\"Update the progress of the progress bar.\"\"\"\n    with contextlib.suppress(Exception):\n        bar = self.query_one(ProgressBar)\n        if self.progress_name != header:\n            bar.remove_class(\"hidden\")\n            self.query_one(\"#padding\").add_class(\"hidden\")\n            self.query_one(\"#progress-name\", Static).remove_class(\n                \"warmup\", \"profiling\", \"records\"\n            ).add_class(header.lower()).update(header)\n            self.query_one(\"PercentageStatus\").remove_class(\n                \"warmup\", \"profiling\", \"records\"\n            ).add_class(header.lower())\n            self.progress_name = header\n        bar.update(progress=progress, total=total)\n        self.refresh()\n</code></pre>"},{"location":"api/#aiperfuidashboardrealtime_metrics_dashboard","title":"aiperf.ui.dashboard.realtime_metrics_dashboard","text":""},{"location":"api/#aiperf.ui.dashboard.realtime_metrics_dashboard.RealtimeMetricsDashboard","title":"<code>RealtimeMetricsDashboard</code>","text":"<p>               Bases: <code>Container</code>, <code>MaximizableWidget</code></p> Source code in <code>aiperf/ui/dashboard/realtime_metrics_dashboard.py</code> <pre><code>class RealtimeMetricsDashboard(Container, MaximizableWidget):\n    DEFAULT_CSS = \"\"\"\n    RealtimeMetricsDashboard {\n        border: round $primary;\n        border-title-color: $primary;\n        border-title-style: bold;\n        border-title-align: center;\n        height: 1fr;\n        layout: vertical;\n        margin: 0 0 0 0;\n        scrollbar-gutter: auto;\n    }\n    #realtime-metrics-status {\n        height: 100%;\n        width: 100%;\n        color: $warning;\n        text-style: italic;\n        content-align: center middle;\n    }\n    .hidden {\n        display: none;\n    }\n    \"\"\"\n\n    def __init__(self, service_config: ServiceConfig, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.service_config = service_config\n        self.metrics_table: RealtimeMetricsTable | None = None\n        self.metrics: list[MetricResult] = []\n        self.border_title = \"Real-Time Metrics\"\n\n    def compose(self) -&gt; ComposeResult:\n        self.metrics_table = RealtimeMetricsTable(\n            service_config=self.service_config, id=\"metrics-table\", classes=\"hidden\"\n        )\n        yield self.metrics_table\n        yield Static(\n            \"No metrics available yet. Please wait...\",\n            id=\"realtime-metrics-status\",\n        )\n\n    def on_realtime_metrics(self, metrics: list[MetricResult]) -&gt; None:\n        \"\"\"Handle metrics updates.\"\"\"\n        if not self.metrics:\n            with suppress(Exception):\n                self.query_one(\"#metrics-table\", RealtimeMetricsTable).remove_class(\"hidden\")  # fmt: skip\n                self.query_one(\"#realtime-metrics-status\", Static).add_class(\"hidden\")\n\n        self.metrics = metrics\n        if self.metrics_table:\n            self.metrics_table.update(metrics)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.realtime_metrics_dashboard.RealtimeMetricsDashboard.on_realtime_metrics","title":"<code>on_realtime_metrics(metrics)</code>","text":"<p>Handle metrics updates.</p> Source code in <code>aiperf/ui/dashboard/realtime_metrics_dashboard.py</code> <pre><code>def on_realtime_metrics(self, metrics: list[MetricResult]) -&gt; None:\n    \"\"\"Handle metrics updates.\"\"\"\n    if not self.metrics:\n        with suppress(Exception):\n            self.query_one(\"#metrics-table\", RealtimeMetricsTable).remove_class(\"hidden\")  # fmt: skip\n            self.query_one(\"#realtime-metrics-status\", Static).add_class(\"hidden\")\n\n    self.metrics = metrics\n    if self.metrics_table:\n        self.metrics_table.update(metrics)\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.realtime_metrics_dashboard.RealtimeMetricsTable","title":"<code>RealtimeMetricsTable</code>","text":"<p>               Bases: <code>Widget</code></p> Source code in <code>aiperf/ui/dashboard/realtime_metrics_dashboard.py</code> <pre><code>class RealtimeMetricsTable(Widget):\n    DEFAULT_CSS = \"\"\"\n    RealtimeMetricsTable {\n        height: 1fr;\n    }\n    NonFocusableDataTable {\n        height: 1fr;\n    }\n    \"\"\"\n\n    STATS_FIELDS = [\"avg\", \"min\", \"max\", \"p99\", \"p90\", \"p75\", \"std\"]\n    COLUMNS = [\"Metric\", *STATS_FIELDS]\n\n    def __init__(self, service_config: ServiceConfig, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.service_config = service_config\n        self.data_table: NonFocusableDataTable | None = None\n        self._columns_initialized = False\n        self._column_keys: dict[str, ColumnKey] = {}\n        self._metric_row_keys: dict[str, RowKey] = {}\n        self.metrics: list[MetricResult] = []\n\n    def compose(self) -&gt; ComposeResult:\n        self.data_table = NonFocusableDataTable(\n            cursor_type=\"row\", show_cursor=False, zebra_stripes=True\n        )\n        yield self.data_table\n\n    def on_mount(self) -&gt; None:\n        if self.data_table and not self._columns_initialized:\n            self._initialize_columns()\n            if self.metrics:\n                self.update(self.metrics)\n\n    def _should_skip(self, metric: MetricResult) -&gt; bool:\n        \"\"\"Determine if a metric should be skipped.\"\"\"\n        metric_class = MetricRegistry.get_class(metric.tag)\n        if metric_class.has_flags(MetricFlags.ERROR_ONLY):\n            return True\n        return (\n            metric_class.has_flags(MetricFlags.HIDDEN)\n            and not self.service_config.developer.show_internal_metrics\n        )\n\n    def _initialize_columns(self) -&gt; None:\n        \"\"\"Initialize table columns.\"\"\"\n        for col in self.COLUMNS:\n            self._column_keys[col] = self.data_table.add_column(  # type: ignore\n                Text(col, justify=\"right\")\n            )\n        self._columns_initialized = True\n\n    def update(self, metrics: list[MetricResult]) -&gt; None:\n        \"\"\"Update the metrics table.\"\"\"\n        self.metrics = metrics\n\n        if not self.data_table or not self.data_table.is_mounted:\n            return\n\n        if not self._columns_initialized:\n            self._initialize_columns()\n\n        metrics = [\n            metric\n            for metric in sorted(\n                metrics,\n                key=lambda m: MetricRegistry.get_class(m.tag).display_order\n                or sys.maxsize,\n            )\n            if not self._should_skip(metric)\n        ]\n        _logger.debug(lambda: f\"Updating metrics table with {len(metrics)} metrics\")\n        for metric in metrics:\n            row_cells = self._format_metric_row(metric)\n            if metric.tag in self._metric_row_keys:\n                row_key = self._metric_row_keys[metric.tag]\n                try:\n                    _ = self.data_table.get_row_index(row_key)\n                    self._update_single_row(row_cells, row_key)\n                    continue\n                except RowDoesNotExist:\n                    # Row doesn't exist, fall through to add as new\n                    pass\n\n            # Add new metric row\n            row_key = self.data_table.add_row(*row_cells)\n            self._metric_row_keys[metric.tag] = row_key\n\n    def _update_single_row(self, row_cells: list[Text], row_key: RowKey) -&gt; None:\n        \"\"\"Update a single row's cells.\"\"\"\n        for col_name, cell_value in zip(self.COLUMNS, row_cells, strict=True):\n            try:\n                self.data_table.update_cell(  # type: ignore\n                    row_key, self._column_keys[col_name], cell_value, update_width=True\n                )\n            except Exception as e:\n                _logger.warning(\n                    f\"Error updating cell {col_name} with value {cell_value}: {e!r}\"\n                )\n\n    def _format_metric_row(self, metric: MetricResult) -&gt; list[Text]:\n        \"\"\"Format worker data into table row cells.\"\"\"\n        metric_class = MetricRegistry.get_class(metric.tag)\n        display_unit = metric_class.display_unit or metric_class.unit\n        short_header = metric_class.short_header or metric_class.header\n        if not metric_class.short_header_hide_unit:\n            short_header = f\"{short_header} ({display_unit})\"\n        return [\n            Text(\n                short_header,\n                style=\"bold cyan\",\n                justify=\"right\",\n            ),\n            *[\n                self._format_metric_value(\n                    getattr(metric, field), display_unit, metric_class\n                )\n                for field in self.STATS_FIELDS\n            ],\n        ]\n\n    def _format_metric_value(\n        self,\n        value: Any | None,\n        display_unit: MetricUnitT,\n        metric_class: type[BaseMetric],\n    ) -&gt; Text:\n        \"\"\"Format a metric value.\"\"\"\n        if value is None:\n            return Text(\"N/A\", justify=\"right\", style=\"dim\")\n\n        if display_unit != metric_class.unit:\n            try:\n                value = metric_class.unit.convert_to(display_unit, value)\n            except MetricUnitError as e:\n                _logger.warning(f\"Error during unit conversion: {e}\")\n\n        if isinstance(value, datetime):\n            value_str = value.strftime(\"%Y-%m-%d %H:%M:%S\")\n        elif isinstance(value, int | float):\n            value_str = f\"{value:,.2f}\"\n        else:\n            value_str = str(value)\n        return Text(value_str, justify=\"right\", style=\"green\")\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.realtime_metrics_dashboard.RealtimeMetricsTable.update","title":"<code>update(metrics)</code>","text":"<p>Update the metrics table.</p> Source code in <code>aiperf/ui/dashboard/realtime_metrics_dashboard.py</code> <pre><code>def update(self, metrics: list[MetricResult]) -&gt; None:\n    \"\"\"Update the metrics table.\"\"\"\n    self.metrics = metrics\n\n    if not self.data_table or not self.data_table.is_mounted:\n        return\n\n    if not self._columns_initialized:\n        self._initialize_columns()\n\n    metrics = [\n        metric\n        for metric in sorted(\n            metrics,\n            key=lambda m: MetricRegistry.get_class(m.tag).display_order\n            or sys.maxsize,\n        )\n        if not self._should_skip(metric)\n    ]\n    _logger.debug(lambda: f\"Updating metrics table with {len(metrics)} metrics\")\n    for metric in metrics:\n        row_cells = self._format_metric_row(metric)\n        if metric.tag in self._metric_row_keys:\n            row_key = self._metric_row_keys[metric.tag]\n            try:\n                _ = self.data_table.get_row_index(row_key)\n                self._update_single_row(row_cells, row_key)\n                continue\n            except RowDoesNotExist:\n                # Row doesn't exist, fall through to add as new\n                pass\n\n        # Add new metric row\n        row_key = self.data_table.add_row(*row_cells)\n        self._metric_row_keys[metric.tag] = row_key\n</code></pre>"},{"location":"api/#aiperfuidashboardrich_log_viewer","title":"aiperf.ui.dashboard.rich_log_viewer","text":""},{"location":"api/#aiperf.ui.dashboard.rich_log_viewer.LogConsumer","title":"<code>LogConsumer</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code></p> <p>LogConsumer is a class that consumes log records from the shared log queue and displays them in the RichLogViewer.</p> Source code in <code>aiperf/ui/dashboard/rich_log_viewer.py</code> <pre><code>class LogConsumer(AIPerfLifecycleMixin):\n    \"\"\"LogConsumer is a class that consumes log records from the shared log queue\n    and displays them in the RichLogViewer.\"\"\"\n\n    def __init__(\n        self, log_queue: multiprocessing.Queue, app: \"AIPerfTextualApp\", **kwargs\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        self.log_queue = log_queue\n        self.app = app\n\n    LOG_REFRESH_INTERVAL = 0.1\n\n    @background_task(immediate=True, interval=LOG_REFRESH_INTERVAL)\n    async def _consume_logs(self) -&gt; None:\n        \"\"\"Consume log records from the queue and display them.\n\n        This is a background task that runs every LOG_REFRESH_INTERVAL seconds\n        to consume log records from the queue and display them in the log viewer.\n        \"\"\"\n        if self.app.log_viewer is None:\n            return\n\n        # Process all pending log records\n        while not self.log_queue.empty():\n            try:\n                log_data = self.log_queue.get_nowait()\n                self.app.log_viewer.display_log_record(log_data)\n                await yield_to_event_loop()\n            except Exception:\n                # Silently ignore queue errors to avoid recursion\n                break\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.rich_log_viewer.RichLogViewer","title":"<code>RichLogViewer</code>","text":"<p>               Bases: <code>RichLog</code></p> <p>RichLogViewer is a widget that displays log records in a rich format.</p> Source code in <code>aiperf/ui/dashboard/rich_log_viewer.py</code> <pre><code>class RichLogViewer(RichLog):\n    \"\"\"RichLogViewer is a widget that displays log records in a rich format.\"\"\"\n\n    # NOTE: MaximizableWidget is not used here because the RichLog widget is not compatible with it.\n    ALLOW_MAXIMIZE = True\n    \"\"\"Allow the widget to be maximized.\"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    RichLogViewer {\n        border: round $primary;\n        border-title-color: $primary;\n        border-title-style: bold;\n        border-title-align: center;\n        layout: vertical;\n        scrollbar-gutter: stable;\n        &amp;:focus {\n            background-tint: $primary 0%;\n        }\n    }\n    \"\"\"\n\n    MAX_LOG_LINES = 2000\n    MAX_LOG_MESSAGE_LENGTH = 500\n\n    LOG_LEVEL_STYLES = {\n        \"TRACE\": \"dim\",\n        \"DEBUG\": \"dim\",\n        \"INFO\": \"cyan\",\n        \"NOTICE\": \"blue\",\n        \"WARNING\": \"yellow\",\n        \"SUCCESS\": \"green\",\n        \"ERROR\": \"red\",\n        \"CRITICAL\": \"red\",\n    }\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(\n            highlight=True,\n            markup=True,\n            wrap=True,\n            auto_scroll=True,\n            max_lines=self.MAX_LOG_LINES,\n            **kwargs,\n        )\n        self.border_title = \"Application Logs\"\n        self.highlighter: Highlighter = ReprHighlighter()\n\n    def display_log_record(self, log_data: dict) -&gt; None:\n        timestamp = datetime.fromtimestamp(log_data[\"created\"]).strftime(\"%H:%M:%S.%f\")[:-3]  # fmt: skip\n        level_style = self.LOG_LEVEL_STYLES.get(log_data[\"levelname\"], \"white\")\n\n        formatted_log = Text.assemble(\n            Text.from_markup(f\"[dim]{timestamp}[/dim] \"),\n            Text.from_markup(\n                f\"[bold][{level_style}]{log_data['levelname']}[/{level_style}][/bold] \"\n            ),\n            Text.from_markup(f\"[bold]{log_data['name']}[/bold] \"),\n            self.highlighter(\n                Text.from_markup(log_data[\"msg\"][: self.MAX_LOG_MESSAGE_LENGTH])\n            ),\n        )\n        self.write(formatted_log)\n\n    def on_click(self, event: Click) -&gt; None:\n        \"\"\"Handle click events to toggle the maximize state of the widget.\"\"\"\n        if event.chain == 2:\n            event.stop()\n            self.toggle_maximize()\n\n    def toggle_maximize(self) -&gt; None:\n        \"\"\"Toggle the maximize state of the widget.\"\"\"\n        if not self.is_maximized:\n            self.screen.maximize(self)\n        else:\n            self.screen.minimize()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.rich_log_viewer.RichLogViewer.ALLOW_MAXIMIZE","title":"<code>ALLOW_MAXIMIZE = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Allow the widget to be maximized.</p>"},{"location":"api/#aiperf.ui.dashboard.rich_log_viewer.RichLogViewer.on_click","title":"<code>on_click(event)</code>","text":"<p>Handle click events to toggle the maximize state of the widget.</p> Source code in <code>aiperf/ui/dashboard/rich_log_viewer.py</code> <pre><code>def on_click(self, event: Click) -&gt; None:\n    \"\"\"Handle click events to toggle the maximize state of the widget.\"\"\"\n    if event.chain == 2:\n        event.stop()\n        self.toggle_maximize()\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.rich_log_viewer.RichLogViewer.toggle_maximize","title":"<code>toggle_maximize()</code>","text":"<p>Toggle the maximize state of the widget.</p> Source code in <code>aiperf/ui/dashboard/rich_log_viewer.py</code> <pre><code>def toggle_maximize(self) -&gt; None:\n    \"\"\"Toggle the maximize state of the widget.\"\"\"\n    if not self.is_maximized:\n        self.screen.maximize(self)\n    else:\n        self.screen.minimize()\n</code></pre>"},{"location":"api/#aiperfuidashboardworker_dashboard","title":"aiperf.ui.dashboard.worker_dashboard","text":""},{"location":"api/#aiperf.ui.dashboard.worker_dashboard.WorkerDashboard","title":"<code>WorkerDashboard</code>","text":"<p>               Bases: <code>Container</code>, <code>MaximizableWidget</code></p> Source code in <code>aiperf/ui/dashboard/worker_dashboard.py</code> <pre><code>class WorkerDashboard(Container, MaximizableWidget):\n    DEFAULT_CSS = \"\"\"\n    WorkerDashboard {\n        border: round $primary;\n        border-title-color: $primary;\n        border-title-style: bold;\n        border-title-align: center;\n        height: 1fr;\n        layout: vertical;\n    }\n\n    #summary-content {\n        height: 1;\n        layout: horizontal;\n        align: left middle;\n        margin: 0 1 0 1;\n    }\n\n    .summary-item { margin: 0 1; }\n    .summary-title { text-style: bold; }\n    .summary-healthy { color: $success; text-style: bold; }\n    .summary-high-load { color: $warning; text-style: bold; }\n    .summary-error { color: $error; text-style: bold; }\n    .summary-idle { color: $text-muted; }\n    .summary-stale { color: $surface-darken-1; }\n\n    #table-section {\n        height: 1fr;\n        margin: 1 0 0 1;\n    }\n    \"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.worker_stats: dict[str, WorkerStats] = {}\n        self.table_widget: WorkerStatusTable | None = None\n        self.border_title = \"Worker Status\"\n\n    def compose(self) -&gt; ComposeResult:\n        with Vertical(id=\"worker-dashboard-content\"):\n            with Horizontal(id=\"summary-content\"):\n                yield Label(\"Summary: \", classes=\"summary-item summary-title\")\n                for status in WorkerStatus:\n                    # Create a label for each possible status type\n                    yield Label(\n                        f\"0 {status.replace('_', ' ')}\",\n                        id=f\"{status.replace('_', '-').lower()}-count\",\n                        classes=f\"summary-item summary-{status.replace('_', '-').lower()}\",\n                    )\n                    yield Label(\"\u2022\", classes=\"summary-item\")\n\n            with Container(id=\"table-section\"):\n                self.table_widget = WorkerStatusTable()\n                yield self.table_widget\n\n    def on_worker_update(self, worker_id: str, worker_stats: WorkerStats) -&gt; None:\n        \"\"\"Handle individual worker updates.\"\"\"\n        self.worker_stats[worker_id] = worker_stats\n        if self.table_widget:\n            self.table_widget.update_single_worker(worker_stats)\n\n    def on_worker_status_summary(\n        self, worker_status_summary: dict[str, WorkerStatus]\n    ) -&gt; None:\n        \"\"\"Handle worker status summary updates.\"\"\"\n        summary = Counter(worker_status_summary.values())\n\n        # For each status type, update the label with the count of workers in that status\n        for status in WorkerStatus:\n            with contextlib.suppress(WrongType, NoMatches):\n                self.query_one(\n                    f\"#{status.replace('_', '-').lower()}-count\", Label\n                ).update(f\"{summary[status]} {status.replace('_', ' ')}\")\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.worker_dashboard.WorkerDashboard.on_worker_status_summary","title":"<code>on_worker_status_summary(worker_status_summary)</code>","text":"<p>Handle worker status summary updates.</p> Source code in <code>aiperf/ui/dashboard/worker_dashboard.py</code> <pre><code>def on_worker_status_summary(\n    self, worker_status_summary: dict[str, WorkerStatus]\n) -&gt; None:\n    \"\"\"Handle worker status summary updates.\"\"\"\n    summary = Counter(worker_status_summary.values())\n\n    # For each status type, update the label with the count of workers in that status\n    for status in WorkerStatus:\n        with contextlib.suppress(WrongType, NoMatches):\n            self.query_one(\n                f\"#{status.replace('_', '-').lower()}-count\", Label\n            ).update(f\"{summary[status]} {status.replace('_', ' ')}\")\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.worker_dashboard.WorkerDashboard.on_worker_update","title":"<code>on_worker_update(worker_id, worker_stats)</code>","text":"<p>Handle individual worker updates.</p> Source code in <code>aiperf/ui/dashboard/worker_dashboard.py</code> <pre><code>def on_worker_update(self, worker_id: str, worker_stats: WorkerStats) -&gt; None:\n    \"\"\"Handle individual worker updates.\"\"\"\n    self.worker_stats[worker_id] = worker_stats\n    if self.table_widget:\n        self.table_widget.update_single_worker(worker_stats)\n</code></pre>"},{"location":"api/#aiperfuidashboardworker_status_table","title":"aiperf.ui.dashboard.worker_status_table","text":""},{"location":"api/#aiperf.ui.dashboard.worker_status_table.WorkerStatusTable","title":"<code>WorkerStatusTable</code>","text":"<p>               Bases: <code>Widget</code></p> Source code in <code>aiperf/ui/dashboard/worker_status_table.py</code> <pre><code>class WorkerStatusTable(Widget):\n    DEFAULT_CSS = \"\"\"\n    WorkerStatusTable {\n        height: 1fr;\n    }\n    NonFocusableDataTable {\n        height: 1fr;\n    }\n    \"\"\"\n\n    COLUMNS = [\"Worker ID\", \"Status\", \"In-flight\", \"Completed\", \"Failed\", \"CPU\", \"Memory\", \"Total Read\", \"Total Write\"]  # fmt: skip\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.data_table: NonFocusableDataTable | None = None\n        self._worker_row_keys: dict[str, RowKey] = {}\n        self._columns_initialized = False\n        self._column_keys: dict[str, ColumnKey] = {}\n\n    def compose(self) -&gt; ComposeResult:\n        self.data_table = NonFocusableDataTable(\n            cursor_type=\"row\", show_cursor=False, zebra_stripes=True\n        )\n        yield self.data_table\n\n    def on_mount(self) -&gt; None:\n        if self.data_table and not self._columns_initialized:\n            self._initialize_columns()\n\n    def _initialize_columns(self) -&gt; None:\n        \"\"\"Initialize table columns.\"\"\"\n        for col in self.COLUMNS:\n            self._column_keys[col] = self.data_table.add_column(  # type: ignore\n                Text(col, justify=\"right\")\n            )\n        self._columns_initialized = True\n\n    def update_single_worker(self, worker_stats: WorkerStats) -&gt; None:\n        \"\"\"Update a single worker's row.\"\"\"\n        if not self.data_table or not self.data_table.is_mounted:\n            return\n\n        if not self._columns_initialized:\n            self._initialize_columns()\n\n        row_cells = self._format_worker_row(worker_stats)\n\n        if worker_stats.worker_id in self._worker_row_keys:\n            row_key = self._worker_row_keys[worker_stats.worker_id]\n            try:\n                _ = self.data_table.get_row_index(row_key)\n                self._update_single_row(row_cells, row_key)\n                return\n            except RowDoesNotExist:\n                # Row doesn't exist, fall through to add as new\n                pass\n\n        # Add new worker row\n        row_key = self.data_table.add_row(*row_cells)\n        self._worker_row_keys[worker_stats.worker_id] = row_key\n\n    def _update_single_row(self, row_cells: list[Text], row_key: RowKey) -&gt; None:\n        \"\"\"Update a single row's cells.\"\"\"\n        for col_name, cell_value in zip(self.COLUMNS, row_cells, strict=True):\n            try:\n                self.data_table.update_cell(  # type: ignore\n                    row_key, self._column_keys[col_name], cell_value, update_width=True\n                )\n            except Exception as e:\n                _logger.warning(\n                    f\"Error updating cell {col_name} with value {cell_value}: {e!r}\"\n                )\n\n    @staticmethod\n    def _format_memory(memory_bytes: int | None) -&gt; str:\n        \"\"\"Format memory usage.\"\"\"\n        return format_bytes(memory_bytes) if memory_bytes is not None else \"N/A\"\n\n    @staticmethod\n    def _format_cpu(cpu_usage: float | None) -&gt; str:\n        \"\"\"Format CPU usage percentage.\"\"\"\n        return f\"{cpu_usage:5.01f}%\" if cpu_usage is not None else \"N/A\"\n\n    def _format_worker_row(self, worker_stats: WorkerStats) -&gt; list[Text]:\n        \"\"\"Format worker data into table row cells.\"\"\"\n        row_data = [\n            Text(worker_stats.worker_id, style=\"bold cyan\", justify=\"right\"),\n            Text(\n                worker_stats.status.replace(\"_\", \" \").title(),\n                style=WORKER_STATUS_STYLES[worker_stats.status],\n                justify=\"right\",\n            ),\n            Text(f\"{worker_stats.task_stats.in_progress:,}\", justify=\"right\"),\n            Text(f\"{worker_stats.task_stats.completed:,}\", justify=\"right\"),\n            Text(f\"{worker_stats.task_stats.failed:,}\", justify=\"right\"),\n        ]\n\n        health = worker_stats.health\n\n        if health:\n            row_data.extend([\n                Text(self._format_cpu(health.cpu_usage), justify=\"right\"),\n                Text(self._format_memory(health.memory_usage), justify=\"right\"),\n            ])  # fmt: skip\n        else:\n            row_data.extend([\n                Text(\"N/A\", justify=\"right\"),\n                Text(\"N/A\", justify=\"right\"),\n            ])  # fmt: skip\n\n        if health and health.io_counters:\n            row_data.extend([\n                Text(format_bytes(health.io_counters.read_chars), justify=\"right\"),\n                Text(format_bytes(health.io_counters.write_chars), justify=\"right\"),\n            ])  # fmt: skip\n        else:\n            row_data.extend([\n                Text(\"N/A\", justify=\"right\"),\n                Text(\"N/A\", justify=\"right\"),\n            ])  # fmt: skip\n        return row_data\n</code></pre>"},{"location":"api/#aiperf.ui.dashboard.worker_status_table.WorkerStatusTable.update_single_worker","title":"<code>update_single_worker(worker_stats)</code>","text":"<p>Update a single worker's row.</p> Source code in <code>aiperf/ui/dashboard/worker_status_table.py</code> <pre><code>def update_single_worker(self, worker_stats: WorkerStats) -&gt; None:\n    \"\"\"Update a single worker's row.\"\"\"\n    if not self.data_table or not self.data_table.is_mounted:\n        return\n\n    if not self._columns_initialized:\n        self._initialize_columns()\n\n    row_cells = self._format_worker_row(worker_stats)\n\n    if worker_stats.worker_id in self._worker_row_keys:\n        row_key = self._worker_row_keys[worker_stats.worker_id]\n        try:\n            _ = self.data_table.get_row_index(row_key)\n            self._update_single_row(row_cells, row_key)\n            return\n        except RowDoesNotExist:\n            # Row doesn't exist, fall through to add as new\n            pass\n\n    # Add new worker row\n    row_key = self.data_table.add_row(*row_cells)\n    self._worker_row_keys[worker_stats.worker_id] = row_key\n</code></pre>"},{"location":"api/#aiperfuino_ui","title":"aiperf.ui.no_ui","text":""},{"location":"api/#aiperf.ui.no_ui.NoUI","title":"<code>NoUI</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code></p> <p>A UI that does nothing.</p> <p>Implements the :class:<code>AIPerfUIProtocol</code> to allow it to be used as a UI, but provides no functionality.</p> <p>NOTE: Not inheriting from :class:<code>BaseAIPerfUI</code> because it does not need to track progress or workers.</p> Source code in <code>aiperf/ui/no_ui.py</code> <pre><code>@implements_protocol(AIPerfUIProtocol)\n@AIPerfUIFactory.register(AIPerfUIType.NONE)\nclass NoUI(AIPerfLifecycleMixin):\n    \"\"\"\n    A UI that does nothing.\n\n    Implements the :class:`AIPerfUIProtocol` to allow it to be used as a UI, but provides no functionality.\n\n    NOTE: Not inheriting from :class:`BaseAIPerfUI` because it does not need to track progress or workers.\n    \"\"\"\n</code></pre>"},{"location":"api/#aiperfuitqdm_ui","title":"aiperf.ui.tqdm_ui","text":""},{"location":"api/#aiperf.ui.tqdm_ui.ProgressBar","title":"<code>ProgressBar</code>","text":"<p>A progress bar that can be updated with a progress percentage.</p> Source code in <code>aiperf/ui/tqdm_ui.py</code> <pre><code>class ProgressBar:\n    \"\"\"A progress bar that can be updated with a progress percentage.\"\"\"\n\n    def __init__(\n        self,\n        desc: str,\n        color: str,\n        position: int,\n        total: int,\n        **kwargs,\n    ):\n        self.bar = tqdm(\n            total=total,\n            desc=desc,\n            colour=color,\n            position=position,\n            leave=False,\n            dynamic_ncols=False,\n            bar_format=\"{desc}: {n:,.0f}/{total:,} |{bar}| {percentage:3.0f}% [{elapsed}&lt;{remaining}]\",\n            **kwargs,\n        )\n        self.total = total\n        self.update_threshold = DEFAULT_UI_MIN_UPDATE_PERCENT\n        self.last_percent = 0.0\n        self.last_value = 0.0\n\n    def update(self, progress: int):\n        \"\"\"Update the progress bar with a new progress percentage.\"\"\"\n        pct = (progress / self.total) * 100.0\n        if pct &gt;= self.last_percent + self.update_threshold:\n            self.bar.update(progress - self.last_value)\n            self.last_percent = pct\n            self.last_value = progress\n\n    def close(self):\n        \"\"\"Close the progress bar.\"\"\"\n        self.bar.close()\n</code></pre>"},{"location":"api/#aiperf.ui.tqdm_ui.ProgressBar.close","title":"<code>close()</code>","text":"<p>Close the progress bar.</p> Source code in <code>aiperf/ui/tqdm_ui.py</code> <pre><code>def close(self):\n    \"\"\"Close the progress bar.\"\"\"\n    self.bar.close()\n</code></pre>"},{"location":"api/#aiperf.ui.tqdm_ui.ProgressBar.update","title":"<code>update(progress)</code>","text":"<p>Update the progress bar with a new progress percentage.</p> Source code in <code>aiperf/ui/tqdm_ui.py</code> <pre><code>def update(self, progress: int):\n    \"\"\"Update the progress bar with a new progress percentage.\"\"\"\n    pct = (progress / self.total) * 100.0\n    if pct &gt;= self.last_percent + self.update_threshold:\n        self.bar.update(progress - self.last_value)\n        self.last_percent = pct\n        self.last_value = progress\n</code></pre>"},{"location":"api/#aiperf.ui.tqdm_ui.TQDMProgressUI","title":"<code>TQDMProgressUI</code>","text":"<p>               Bases: <code>BaseAIPerfUI</code></p> <p>A UI that shows progress bars for the records and requests phases.</p> Source code in <code>aiperf/ui/tqdm_ui.py</code> <pre><code>@implements_protocol(AIPerfUIProtocol)\n@AIPerfUIFactory.register(AIPerfUIType.SIMPLE)\nclass TQDMProgressUI(BaseAIPerfUI):\n    \"\"\"A UI that shows progress bars for the records and requests phases.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._warmup_bar: ProgressBar | None = None\n        self._requests_bar: ProgressBar | None = None\n        self._records_bar: ProgressBar | None = None\n\n    @on_stop\n    def _close_all_bars(self):\n        \"\"\"Close all progress bars.\"\"\"\n        for bar in [self._records_bar, self._requests_bar, self._warmup_bar]:\n            if bar is not None:\n                bar.close()\n\n    @on_warmup_progress\n    def _on_warmup_progress(self, warmup_stats: RequestsStats):\n        \"\"\"Callback for warmup progress updates.\"\"\"\n        if self._warmup_bar is None and warmup_stats.total_expected_requests:\n            self._warmup_bar = ProgressBar(\n                desc=\"Warmup\",\n                color=\"yellow\",\n                position=0,\n                total=warmup_stats.total_expected_requests,\n            )\n        if self._warmup_bar:\n            self._warmup_bar.update(warmup_stats.finished)\n\n    @on_profiling_progress\n    def _on_profiling_progress(self, profiling_stats: RequestsStats):\n        \"\"\"Callback for profiling progress updates.\"\"\"\n        if self._requests_bar is None and profiling_stats.total_expected_requests:\n            self._requests_bar = ProgressBar(\n                desc=\"Requests (Profiling)\",\n                color=\"green\",\n                position=1,\n                total=profiling_stats.total_expected_requests,\n            )\n        if self._requests_bar:\n            self._requests_bar.update(profiling_stats.finished)\n\n    @on_records_progress\n    def _on_records_progress(self, records_stats: RecordsStats):\n        \"\"\"Callback for records progress updates.\"\"\"\n        if self._records_bar is None and records_stats.total_expected_requests:\n            self._records_bar = ProgressBar(\n                desc=\"Records (Processing)\",\n                color=\"blue\",\n                position=2,\n                total=records_stats.total_expected_requests,\n            )\n        if self._records_bar:\n            self._records_bar.update(records_stats.finished)\n</code></pre>"},{"location":"api/#aiperfuiutils","title":"aiperf.ui.utils","text":""},{"location":"api/#aiperf.ui.utils.format_bytes","title":"<code>format_bytes(bytes, none_str='--')</code>","text":"<p>Format bytes to human-readable format.</p> Source code in <code>aiperf/ui/utils.py</code> <pre><code>def format_bytes(bytes: int | None, none_str: str = \"--\") -&gt; str:\n    \"\"\"Format bytes to human-readable format.\"\"\"\n    if bytes is None:\n        return none_str\n    if bytes &lt; 1000:\n        return f\"{bytes} B\"\n\n    for factor, suffix in zip(_byte_factors, _byte_suffixes, strict=True):\n        if bytes / factor &lt; 100:\n            return f\"{bytes / factor:.1f} {suffix}\"\n        if bytes / factor &lt; 1000:\n            return f\"{bytes / factor:.0f} {suffix}\"\n\n    raise ValueError(f\"Bytes value is too large to format: {bytes}\")\n</code></pre>"},{"location":"api/#aiperf.ui.utils.format_elapsed_time","title":"<code>format_elapsed_time(seconds, none_str='--')</code>","text":"<p>Format elapsed time in seconds to human-readable format.</p> Source code in <code>aiperf/ui/utils.py</code> <pre><code>def format_elapsed_time(seconds: float | None, none_str: str = \"--\") -&gt; str:\n    \"\"\"Format elapsed time in seconds to human-readable format.\"\"\"\n    if seconds is None:\n        return none_str\n\n    _logger.debug(f\"format_elapsed_time: seconds: {seconds}\")\n    if seconds &lt; 60:\n        return f\"{seconds:.1f}s\"\n\n    parts = []\n    _logger.debug(\n        f\"format_elapsed_time: _time_suffixes: {_time_suffixes}, _time_factors: {_time_factors}\"\n    )\n    for suffix, factor in zip(_time_suffixes, _time_factors, strict=True):\n        _logger.debug(f\"format_elapsed_time: seconds: {seconds}, factor: {factor}\")\n        if seconds &gt;= factor:\n            amount = int(seconds // factor)\n            if amount &gt; 0:\n                parts.append(f\"{amount}{suffix}\")\n            seconds -= amount * factor\n\n    if seconds &gt; 0:\n        parts.append(f\"{seconds:.0f}s\")\n\n    return \" \".join(parts)\n</code></pre>"},{"location":"api/#aiperf.ui.utils.format_eta","title":"<code>format_eta(seconds, none_str='--')</code>","text":"<p>Format an ETA in seconds to human-readable format.</p> Source code in <code>aiperf/ui/utils.py</code> <pre><code>def format_eta(seconds: float | None, none_str: str = \"--\") -&gt; str:\n    \"\"\"Format an ETA in seconds to human-readable format.\"\"\"\n    if seconds is None:\n        return none_str\n\n    if seconds &lt; 60:\n        return f\"{seconds:.1f}s\"\n\n    minutes = int(seconds // 60)\n    remaining_seconds = seconds % 60\n\n    if minutes &lt; 60:\n        if remaining_seconds &lt; 1:\n            return f\"{minutes}m\"\n        return f\"{minutes}m {remaining_seconds:.0f}s\"\n\n    hours = minutes // 60\n    minutes = minutes % 60\n\n    if hours &lt; 24:\n        if minutes == 0:\n            return f\"{hours}h\"\n        return f\"{hours}h {minutes}m\"\n\n    days = hours // 24\n    hours = hours % 24\n\n    if hours == 0:\n        return f\"{days}d\"\n    return f\"{days}d {hours}h\"\n</code></pre>"},{"location":"api/#aiperfworkerscredit_processor_mixin","title":"aiperf.workers.credit_processor_mixin","text":""},{"location":"api/#aiperf.workers.credit_processor_mixin.CreditProcessorMixin","title":"<code>CreditProcessorMixin</code>","text":"<p>               Bases: <code>CreditProcessorMixinRequirements</code></p> <p>CreditProcessorMixin is a mixin that provides a method to process credit drops.</p> Source code in <code>aiperf/workers/credit_processor_mixin.py</code> <pre><code>class CreditProcessorMixin(CreditProcessorMixinRequirements):\n    \"\"\"CreditProcessorMixin is a mixin that provides a method to process credit drops.\"\"\"\n\n    def __init__(self, **kwargs):\n        if not isinstance(self, CreditProcessorMixinRequirements):\n            raise ValueError(\n                \"CreditProcessorMixin must be used with CreditProcessorMixinRequirements\"\n            )\n\n    async def _process_credit_drop_internal(self, message: CreditDropMessage) -&gt; None:\n        \"\"\"Process a credit drop message. Make sure to return the credit as soon as possible.\n\n        - Every credit must be returned after processing\n        - All results or errors should be converted to a RequestRecord and pushed to the inference results client.\n\n        NOTE: This function MUST NOT return until the credit drop is fully processed.\n        This is to ensure that the max concurrency is respected via the semaphore of the pull client.\n        The way this is enforced is by requiring that this method returns a CreditReturnMessage.\n        \"\"\"\n        # TODO: Add tests to ensure that the above note is never violated in the future\n\n        if self.is_trace_enabled:\n            self.trace(f\"Processing credit drop: {message}\")\n        drop_perf_ns = time.perf_counter_ns()  # The time the credit was received\n\n        self.task_stats.total += 1\n\n        record: RequestRecord = RequestRecord()\n        try:\n            record = await self._execute_single_credit_internal(message)\n\n        except Exception as e:\n            self.exception(f\"Error processing credit drop: {e}\")\n            record.error = ErrorDetails.from_exception(e)\n            record.end_perf_ns = time.perf_counter_ns()\n\n        finally:\n            return_message = CreditReturnMessage(\n                service_id=self.service_id,\n                phase=message.phase,\n                delayed_ns=record.delayed_ns,\n            )\n            if self.is_trace_enabled:\n                self.trace(f\"Returning credit {return_message}\")\n            # NOTE: Do not do this execute_async, as we want to give the credit back as soon as possible.\n            await self.credit_return_push_client.push(return_message)\n\n            self.task_stats.task_finished(record.valid)\n\n            record.credit_phase = message.phase\n            # Calculate the latency of the credit drop (from when the credit drop was first received to when the request was sent)\n            record.credit_drop_latency = record.start_perf_ns - drop_perf_ns\n            msg = InferenceResultsMessage(\n                service_id=self.service_id,\n                record=record,\n            )\n            self.execute_async(self.inference_results_push_client.push(msg))\n\n    async def _execute_single_credit_internal(\n        self, message: CreditDropMessage\n    ) -&gt; RequestRecord:\n        \"\"\"Run a credit task for a single credit.\"\"\"\n\n        if not self.inference_client:\n            raise NotInitializedError(\"Inference server client not initialized.\")\n\n        # retrieve the prompt from the dataset\n        conversation_response: ConversationResponseMessage = (\n            await self.conversation_request_client.request(\n                ConversationRequestMessage(\n                    service_id=self.service_id,\n                    conversation_id=message.conversation_id,\n                    credit_phase=message.phase,\n                )\n            )\n        )\n        if self.is_trace_enabled:\n            self.trace(f\"Received response message: {conversation_response}\")\n\n        turn_index = 0\n        turn = conversation_response.conversation.turns[turn_index]\n\n        if isinstance(conversation_response, ErrorMessage):\n            return RequestRecord(\n                model_name=turn.model or self.model_endpoint.primary_model_name,\n                conversation_id=message.conversation_id,\n                turn_index=turn_index,\n                turn=turn,\n                timestamp_ns=time.time_ns(),\n                start_perf_ns=time.perf_counter_ns(),\n                end_perf_ns=time.perf_counter_ns(),\n                error=conversation_response.error,\n            )\n\n        record = await self._call_inference_api_internal(message, turn)\n        record.model_name = turn.model or self.model_endpoint.primary_model_name\n        record.conversation_id = conversation_response.conversation.session_id\n        record.turn_index = turn_index\n        return record\n\n    async def _call_inference_api_internal(\n        self,\n        message: CreditDropMessage,\n        turn: Turn,\n    ) -&gt; RequestRecord:\n        \"\"\"Make a single call to the inference API. Will return an error record if the call fails.\"\"\"\n        if self.is_trace_enabled:\n            self.trace(f\"Calling inference API for turn: {turn}\")\n        formatted_payload = None\n        pre_send_perf_ns = None\n        timestamp_ns = None\n        try:\n            # Format payload for the API request\n            formatted_payload = await self.request_converter.format_payload(\n                model_endpoint=self.model_endpoint,\n                turn=turn,\n            )\n\n            # NOTE: Current implementation of the TimingManager bypasses this, it is for future use.\n            # Wait for the credit drop time if it is in the future.\n            # Note that we check this after we have retrieved the data from the dataset, to ensure\n            # that we are fully ready to go.\n            delayed_ns = None\n            drop_ns = message.credit_drop_ns\n            now_ns = time.time_ns()\n            if drop_ns and drop_ns &gt; now_ns:\n                if self.is_trace_enabled:\n                    self.trace(\n                        f\"Waiting for credit drop expected time: {(drop_ns - now_ns) / NANOS_PER_SECOND:.2f} s\"\n                    )\n                await asyncio.sleep((drop_ns - now_ns) / NANOS_PER_SECOND)\n            elif drop_ns and drop_ns &lt; now_ns:\n                delayed_ns = now_ns - drop_ns\n\n            # Save the current perf_ns before sending the request so it can be used to calculate\n            # the start_perf_ns of the request in case of an exception.\n            pre_send_perf_ns = time.perf_counter_ns()\n            timestamp_ns = time.time_ns()\n\n            # Send the request to the Inference Server API and wait for the response\n            result: RequestRecord = await self.inference_client.send_request(\n                model_endpoint=self.model_endpoint,\n                payload=formatted_payload,\n            )\n\n            if self.is_debug_enabled:\n                self.debug(\n                    f\"pre_send_perf_ns to start_perf_ns latency: {result.start_perf_ns - pre_send_perf_ns} ns\"\n                )\n\n            result.delayed_ns = delayed_ns\n            result.turn = turn\n            return result\n\n        except Exception as e:\n            self.error(\n                f\"Error calling inference server API at {self.model_endpoint.url}: {e!r}\"\n            )\n            return RequestRecord(\n                turn=turn,\n                timestamp_ns=timestamp_ns or time.time_ns(),\n                # Try and use the pre_send_perf_ns if it is available, otherwise use the current time.\n                start_perf_ns=pre_send_perf_ns or time.perf_counter_ns(),\n                end_perf_ns=time.perf_counter_ns(),\n                error=ErrorDetails.from_exception(e),\n            )\n</code></pre>"},{"location":"api/#aiperf.workers.credit_processor_mixin.CreditProcessorMixinRequirements","title":"<code>CreditProcessorMixinRequirements</code>","text":"<p>               Bases: <code>TaskManagerProtocol</code>, <code>Protocol</code></p> <p>CreditProcessorMixinRequirements is a protocol that provides the requirements needed for the CreditProcessorMixin.</p> Source code in <code>aiperf/workers/credit_processor_mixin.py</code> <pre><code>@runtime_checkable\nclass CreditProcessorMixinRequirements(TaskManagerProtocol, Protocol):\n    \"\"\"CreditProcessorMixinRequirements is a protocol that provides the requirements needed for the CreditProcessorMixin.\"\"\"\n\n    service_id: str\n    inference_client: InferenceClientProtocol\n    conversation_request_client: RequestClientProtocol\n    inference_results_push_client: PushClientProtocol\n    credit_return_push_client: PushClientProtocol\n    request_converter: RequestConverterProtocol\n    model_endpoint: ModelEndpointInfo\n    task_stats: WorkerTaskStats\n\n    async def _process_credit_drop_internal(self, message: CreditDropMessage) -&gt; None:\n        \"\"\"Process a credit drop message. Return the credit as soon as possible.\"\"\"\n        ...\n\n    async def _execute_single_credit_internal(\n        self, message: CreditDropMessage\n    ) -&gt; RequestRecord:\n        \"\"\"Execute a single credit drop. Return the RequestRecord.\"\"\"\n        ...\n\n    async def _call_inference_api_internal(\n        self,\n        message: CreditDropMessage,\n        turn: Turn,\n    ) -&gt; RequestRecord:\n        \"\"\"Make a single call to the inference API. Will return an error record if the call fails.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperf.workers.credit_processor_mixin.CreditProcessorProtocol","title":"<code>CreditProcessorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>CreditProcessorProtocol is a protocol that provides a method to process credit drops.</p> Source code in <code>aiperf/workers/credit_processor_mixin.py</code> <pre><code>@runtime_checkable\nclass CreditProcessorProtocol(Protocol):\n    \"\"\"CreditProcessorProtocol is a protocol that provides a method to process credit drops.\"\"\"\n\n    async def _process_credit_drop_internal(\n        self, message: CreditDropMessage\n    ) -&gt; CreditReturnMessage:\n        \"\"\"Process a credit drop message. Return the CreditReturnMessage.\"\"\"\n        ...\n</code></pre>"},{"location":"api/#aiperfworkersworker","title":"aiperf.workers.worker","text":""},{"location":"api/#aiperf.workers.worker.Worker","title":"<code>Worker</code>","text":"<p>               Bases: <code>PullClientMixin</code>, <code>BaseComponentService</code>, <code>ProcessHealthMixin</code>, <code>CreditProcessorMixin</code></p> <p>Worker is primarily responsible for making API calls to the inference server. It also manages the conversation between turns and returns the results to the Inference Results Parsers.</p> Source code in <code>aiperf/workers/worker.py</code> <pre><code>@ServiceFactory.register(ServiceType.WORKER)\nclass Worker(\n    PullClientMixin, BaseComponentService, ProcessHealthMixin, CreditProcessorMixin\n):\n    \"\"\"Worker is primarily responsible for making API calls to the inference server.\n    It also manages the conversation between turns and returns the results to the Inference Results Parsers.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ):\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            pull_client_address=CommAddress.CREDIT_DROP,\n            pull_client_bind=False,\n            **kwargs,\n        )\n\n        self.debug(lambda: f\"Worker process __init__ (pid: {self._process.pid})\")\n\n        self.health_check_interval = DEFAULT_WORKER_HEALTH_CHECK_INTERVAL\n\n        self.task_stats: WorkerTaskStats = WorkerTaskStats()\n\n        self.credit_return_push_client: PushClientProtocol = (\n            self.comms.create_push_client(\n                CommAddress.CREDIT_RETURN,\n            )\n        )\n        self.inference_results_push_client: PushClientProtocol = (\n            self.comms.create_push_client(\n                CommAddress.RAW_INFERENCE_PROXY_FRONTEND,\n            )\n        )\n        self.conversation_request_client: RequestClientProtocol = (\n            self.comms.create_request_client(\n                CommAddress.DATASET_MANAGER_PROXY_FRONTEND,\n            )\n        )\n\n        self.model_endpoint = ModelEndpointInfo.from_user_config(self.user_config)\n\n        self.debug(\n            lambda: f\"Creating inference client for {self.model_endpoint.endpoint.type}, \"\n            f\"class: {InferenceClientFactory.get_class_from_type(self.model_endpoint.endpoint.type).__name__}\",\n        )\n        self.request_converter = RequestConverterFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n        )\n        self.inference_client = InferenceClientFactory.create_instance(\n            self.model_endpoint.endpoint.type,\n            model_endpoint=self.model_endpoint,\n        )\n\n    @on_pull_message(MessageType.CREDIT_DROP)\n    async def _credit_drop_callback(self, message: CreditDropMessage) -&gt; None:\n        \"\"\"Handle an incoming credit drop message from the timing manager. Every credit must be returned after processing.\"\"\"\n\n        try:\n            # NOTE: This must be awaited to ensure that the max concurrency is respected\n            await self._process_credit_drop_internal(message)\n        except Exception as e:\n            self.error(f\"Error processing credit drop: {e!r}\")\n            await self.credit_return_push_client.push(\n                CreditReturnMessage(\n                    service_id=self.service_id,\n                    phase=message.phase,\n                )\n            )\n\n    @on_stop\n    async def _shutdown_worker(self) -&gt; None:\n        self.debug(\"Shutting down worker\")\n        if self.inference_client:\n            await self.inference_client.close()\n\n    @background_task(\n        immediate=False,\n        interval=lambda self: self.health_check_interval,\n    )\n    async def _health_check_task(self) -&gt; None:\n        \"\"\"Task to report the health of the worker to the worker manager.\"\"\"\n        await self.publish(self.create_health_message())\n\n    def create_health_message(self) -&gt; WorkerHealthMessage:\n        return WorkerHealthMessage(\n            service_id=self.service_id,\n            health=self.get_process_health(),\n            task_stats=self.task_stats,\n        )\n\n    @on_command(CommandType.PROFILE_CANCEL)\n    async def _handle_profile_cancel_command(\n        self, message: ProfileCancelCommand\n    ) -&gt; None:\n        self.debug(lambda: f\"Received profile cancel command: {message}\")\n        await self.publish(\n            CommandAcknowledgedResponse.from_command_message(message, self.service_id)\n        )\n        await self.stop()\n</code></pre>"},{"location":"api/#aiperfworkersworker_manager","title":"aiperf.workers.worker_manager","text":""},{"location":"api/#aiperf.workers.worker_manager.WorkerManager","title":"<code>WorkerManager</code>","text":"<p>               Bases: <code>BaseComponentService</code></p> <p>The WorkerManager service is primary responsibility to manage the worker processes. It will spawn the workers, monitor their health, and stop them when the service is stopped. In the future it will also be responsible for the auto-scaling of the workers.</p> Source code in <code>aiperf/workers/worker_manager.py</code> <pre><code>@ServiceFactory.register(ServiceType.WORKER_MANAGER)\nclass WorkerManager(BaseComponentService):\n    \"\"\"\n    The WorkerManager service is primary responsibility to manage the worker processes.\n    It will spawn the workers, monitor their health, and stop them when the service is stopped.\n    In the future it will also be responsible for the auto-scaling of the workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_config: ServiceConfig,\n        user_config: UserConfig,\n        service_id: str | None = None,\n        **kwargs,\n    ):\n        super().__init__(\n            service_config=service_config,\n            user_config=user_config,\n            service_id=service_id,\n            **kwargs,\n        )\n\n        self.trace(\"WorkerManager.__init__\")\n        self.worker_infos: dict[str, WorkerStatusInfo] = {}\n\n        self.cpu_count = multiprocessing.cpu_count()\n        self.debug(lambda: f\"Detected {self.cpu_count} CPU cores/threads\")\n\n        self.max_concurrency = self.user_config.loadgen.concurrency\n        self.max_workers = self.service_config.workers.max\n        if self.max_workers is None:\n            # Default to the number of CPU cores - 1\n            self.max_workers = self.cpu_count - 1\n\n        # Cap the worker count to the max concurrency, but only if the user is in concurrency mode.\n        if self.max_concurrency:\n            self.max_workers = min(\n                self.max_concurrency,\n                self.max_workers,\n            )\n\n        # Ensure we have at least the min workers\n        self.max_workers = max(\n            self.max_workers,\n            self.service_config.workers.min or 0,\n        )\n        self.initial_workers = self.max_workers\n\n    @on_start\n    async def _start(self) -&gt; None:\n        \"\"\"Start worker manager-specific components.\"\"\"\n        self.debug(\"WorkerManager starting\")\n\n        await self.send_command_and_wait_for_response(\n            SpawnWorkersCommand(\n                service_id=self.service_id,\n                num_workers=self.initial_workers,\n                # Target the system controller directly to avoid broadcasting to all services.\n                target_service_type=ServiceType.SYSTEM_CONTROLLER,\n            )\n        )\n        self.debug(\"WorkerManager started\")\n\n    @on_stop\n    async def _stop(self) -&gt; None:\n        self.debug(\"WorkerManager stopping\")\n\n        await self.publish(\n            ShutdownWorkersCommand(\n                service_id=self.service_id,\n                all_workers=True,\n                # Target the system controller directly to avoid broadcasting to all services.\n                target_service_type=ServiceType.SYSTEM_CONTROLLER,\n            )\n        )\n\n    @on_message(MessageType.WORKER_HEALTH)\n    async def _on_worker_health(self, message: WorkerHealthMessage) -&gt; None:\n        worker_id = message.service_id\n        info = self.worker_infos.get(worker_id)\n        if not info:\n            info = WorkerStatusInfo(\n                worker_id=worker_id,\n                last_update_ns=time.time_ns(),\n                status=WorkerStatus.HEALTHY,\n                health=message.health,\n                task_stats=message.task_stats,\n            )\n            self.worker_infos[worker_id] = info\n        self._update_worker_status(info, message)\n\n    def _update_worker_status(\n        self, info: WorkerStatusInfo, message: WorkerHealthMessage\n    ) -&gt; None:\n        \"\"\"Check the status of a worker.\"\"\"\n        info.last_update_ns = time.time_ns()\n        # Error Status\n        if message.task_stats.failed &gt; info.task_stats.failed:\n            info.last_error_ns = time.time_ns()\n            info.status = WorkerStatus.ERROR\n        elif (time.time_ns() - (info.last_error_ns or 0)) / NANOS_PER_SECOND &lt; DEFAULT_WORKER_ERROR_RECOVERY_TIME:  # fmt: skip\n            info.status = WorkerStatus.ERROR\n\n        # High Load Status\n        elif message.health.cpu_usage &gt; DEFAULT_WORKER_HIGH_LOAD_CPU_USAGE:\n            info.last_high_load_ns = time.time_ns()\n            info.status = WorkerStatus.HIGH_LOAD\n        elif (time.time_ns() - (info.last_high_load_ns or 0)) / NANOS_PER_SECOND &lt; DEFAULT_WORKER_HIGH_LOAD_RECOVERY_TIME:  # fmt: skip\n            info.status = WorkerStatus.HIGH_LOAD\n\n        # Idle Status\n        elif message.task_stats.total == 0 or message.task_stats.in_progress == 0:\n            info.status = WorkerStatus.IDLE\n\n        # Healthy Status\n        else:\n            info.status = WorkerStatus.HEALTHY\n\n        info.health = message.health\n        info.task_stats = message.task_stats\n\n    @background_task(immediate=False, interval=DEFAULT_WORKER_CHECK_INTERVAL)\n    async def _worker_status_loop(self) -&gt; None:\n        \"\"\"Check the status of all workers.\"\"\"\n        self.debug(\"Checking worker status\")\n\n        for _, info in self.worker_infos.items():\n            if (time.time_ns() - (info.last_update_ns or 0)) / NANOS_PER_SECOND &gt; DEFAULT_WORKER_STALE_TIME:  # fmt: skip\n                info.status = WorkerStatus.STALE\n\n    @background_task(immediate=False, interval=DEFAULT_WORKER_STATUS_SUMMARY_INTERVAL)\n    async def _worker_summary_loop(self) -&gt; None:\n        \"\"\"Generate a summary of the worker status.\"\"\"\n        summary = WorkerStatusSummaryMessage(\n            service_id=self.service_id,\n            worker_statuses={\n                worker_id: info.status for worker_id, info in self.worker_infos.items()\n            },\n        )\n        self.debug(lambda: f\"Publishing worker status summary: {summary}\")\n        await self.publish(summary)\n</code></pre>"},{"location":"api/#aiperf.workers.worker_manager.WorkerStatusInfo","title":"<code>WorkerStatusInfo</code>","text":"<p>               Bases: <code>WorkerStats</code></p> <p>Information about a worker's status.</p> Source code in <code>aiperf/workers/worker_manager.py</code> <pre><code>class WorkerStatusInfo(WorkerStats):\n    \"\"\"Information about a worker's status.\"\"\"\n\n    worker_id: str = Field(..., description=\"The ID of the worker\")\n    last_error_ns: int | None = Field(\n        default=None,\n        description=\"The last time the worker had an error\",\n    )\n    last_high_load_ns: int | None = Field(\n        default=None,\n        description=\"The last time the worker was in high load\",\n    )\n</code></pre>"},{"location":"api/#aiperfzmqdealer_request_client","title":"aiperf.zmq.dealer_request_client","text":""},{"location":"api/#aiperf.zmq.dealer_request_client.ZMQDealerRequestClient","title":"<code>ZMQDealerRequestClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code>, <code>TaskManagerMixin</code></p> <p>ZMQ DEALER socket client for asynchronous request-response communication.</p> <p>The DEALER socket connects to ROUTER sockets and can send requests asynchronously, receiving responses through callbacks or awaitable futures.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502    ROUTER    \u2502 \u2502   (Client)   \u2502                    \u2502  (Service)   \u2502 \u2502              \u2502&lt;\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - DEALER Clients send requests to ROUTER Services - Responses are routed back to the originating DEALER</p> <p>DEALER/ROUTER is a Many-to-One communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQDealerRouterProxy</code> for more details.</p> Source code in <code>aiperf/zmq/dealer_request_client.py</code> <pre><code>@implements_protocol(RequestClientProtocol)\n@CommunicationClientFactory.register(CommClientType.REQUEST)\nclass ZMQDealerRequestClient(BaseZMQClient, TaskManagerMixin):\n    \"\"\"\n    ZMQ DEALER socket client for asynchronous request-response communication.\n\n    The DEALER socket connects to ROUTER sockets and can send requests asynchronously,\n    receiving responses through callbacks or awaitable futures.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502    ROUTER    \u2502\n    \u2502   (Client)   \u2502                    \u2502  (Service)   \u2502\n    \u2502              \u2502&lt;\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - DEALER Clients send requests to ROUTER Services\n    - Responses are routed back to the originating DEALER\n\n    DEALER/ROUTER is a Many-to-One communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQDealerRouterProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Dealer (Req) client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.DEALER, address, bind, socket_ops, **kwargs)\n\n        self.request_callbacks: dict[\n            str, Callable[[Message], Coroutine[Any, Any, None]]\n        ] = {}\n\n    @background_task(immediate=True, interval=None)\n    async def _request_async_task(self) -&gt; None:\n        \"\"\"Task to handle incoming requests.\"\"\"\n        while not self.stop_requested:\n            try:\n                message = await self.socket.recv_string()\n                self.trace(lambda msg=message: f\"Received response: {msg}\")\n                response_message = Message.from_json(message)\n\n                # Call the callback if it exists\n                if response_message.request_id in self.request_callbacks:\n                    callback = self.request_callbacks.pop(response_message.request_id)\n                    self.execute_async(callback(response_message))\n\n            except zmq.Again:\n                self.debug(\"No data on dealer socket received, yielding to event loop\")\n                await yield_to_event_loop()\n            except Exception as e:\n                self.exception(f\"Exception receiving responses: {e}\")\n                await yield_to_event_loop()\n            except asyncio.CancelledError:\n                self.debug(\"Dealer request client receiver task cancelled\")\n                raise  # re-raise the cancelled error\n\n    @on_stop\n    async def _stop_remaining_tasks(self) -&gt; None:\n        \"\"\"Wait for all tasks to complete.\"\"\"\n        await self.cancel_all_tasks()\n\n    async def request_async(\n        self,\n        message: Message,\n        callback: Callable[[Message], Coroutine[Any, Any, None]],\n    ) -&gt; None:\n        \"\"\"Send a request and be notified when the response is received.\"\"\"\n        await self._check_initialized()\n\n        if not isinstance(message, Message):\n            raise TypeError(\n                f\"message must be an instance of Message, got {type(message).__name__}\"\n            )\n\n        # Generate request ID if not provided so that responses can be matched\n        if not message.request_id:\n            message.request_id = str(uuid.uuid4())\n\n        self.request_callbacks[message.request_id] = callback\n\n        request_json = message.model_dump_json()\n        self.trace(lambda msg=request_json: f\"Sending request: {msg}\")\n\n        try:\n            await self.socket.send_string(request_json)\n\n        except Exception as e:\n            raise CommunicationError(\n                f\"Exception sending request: {e.__class__.__qualname__} {e}\",\n            ) from e\n\n    async def request(\n        self,\n        message: Message,\n        timeout: float = DEFAULT_COMMS_REQUEST_TIMEOUT,\n    ) -&gt; Message:\n        \"\"\"Send a request and wait for a response up to timeout seconds.\n\n        Args:\n            message (Message): The request message to send.\n            timeout (float): Maximum time to wait for a response in seconds.\n\n        Returns:\n            Message: The response message received.\n\n        Raises:\n            CommunicationError: if the request fails, or\n            asyncio.TimeoutError: if the response is not received in time.\n        \"\"\"\n        future = asyncio.Future[Message]()\n\n        async def callback(response_message: Message) -&gt; None:\n            future.set_result(response_message)\n\n        await self.request_async(message, callback)\n        return await asyncio.wait_for(future, timeout=timeout)\n</code></pre>"},{"location":"api/#aiperf.zmq.dealer_request_client.ZMQDealerRequestClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Dealer (Req) client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/zmq/dealer_request_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Dealer (Req) client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.DEALER, address, bind, socket_ops, **kwargs)\n\n    self.request_callbacks: dict[\n        str, Callable[[Message], Coroutine[Any, Any, None]]\n    ] = {}\n</code></pre>"},{"location":"api/#aiperf.zmq.dealer_request_client.ZMQDealerRequestClient.request","title":"<code>request(message, timeout=DEFAULT_COMMS_REQUEST_TIMEOUT)</code>  <code>async</code>","text":"<p>Send a request and wait for a response up to timeout seconds.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The request message to send.</p> required <code>timeout</code> <code>float</code> <p>Maximum time to wait for a response in seconds.</p> <code>DEFAULT_COMMS_REQUEST_TIMEOUT</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Message</code> <p>The response message received.</p> <p>Raises:</p> Type Description <code>CommunicationError</code> <p>if the request fails, or</p> <code>TimeoutError</code> <p>if the response is not received in time.</p> Source code in <code>aiperf/zmq/dealer_request_client.py</code> <pre><code>async def request(\n    self,\n    message: Message,\n    timeout: float = DEFAULT_COMMS_REQUEST_TIMEOUT,\n) -&gt; Message:\n    \"\"\"Send a request and wait for a response up to timeout seconds.\n\n    Args:\n        message (Message): The request message to send.\n        timeout (float): Maximum time to wait for a response in seconds.\n\n    Returns:\n        Message: The response message received.\n\n    Raises:\n        CommunicationError: if the request fails, or\n        asyncio.TimeoutError: if the response is not received in time.\n    \"\"\"\n    future = asyncio.Future[Message]()\n\n    async def callback(response_message: Message) -&gt; None:\n        future.set_result(response_message)\n\n    await self.request_async(message, callback)\n    return await asyncio.wait_for(future, timeout=timeout)\n</code></pre>"},{"location":"api/#aiperf.zmq.dealer_request_client.ZMQDealerRequestClient.request_async","title":"<code>request_async(message, callback)</code>  <code>async</code>","text":"<p>Send a request and be notified when the response is received.</p> Source code in <code>aiperf/zmq/dealer_request_client.py</code> <pre><code>async def request_async(\n    self,\n    message: Message,\n    callback: Callable[[Message], Coroutine[Any, Any, None]],\n) -&gt; None:\n    \"\"\"Send a request and be notified when the response is received.\"\"\"\n    await self._check_initialized()\n\n    if not isinstance(message, Message):\n        raise TypeError(\n            f\"message must be an instance of Message, got {type(message).__name__}\"\n        )\n\n    # Generate request ID if not provided so that responses can be matched\n    if not message.request_id:\n        message.request_id = str(uuid.uuid4())\n\n    self.request_callbacks[message.request_id] = callback\n\n    request_json = message.model_dump_json()\n    self.trace(lambda msg=request_json: f\"Sending request: {msg}\")\n\n    try:\n        await self.socket.send_string(request_json)\n\n    except Exception as e:\n        raise CommunicationError(\n            f\"Exception sending request: {e.__class__.__qualname__} {e}\",\n        ) from e\n</code></pre>"},{"location":"api/#aiperfzmqpub_client","title":"aiperf.zmq.pub_client","text":""},{"location":"api/#aiperf.zmq.pub_client.ZMQPubClient","title":"<code>ZMQPubClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>The PUB socket broadcasts messages to all connected SUB sockets that have subscribed to the message topic/type.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 OR \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - Single PUB socket broadcasts messages to all subscribers (One-to-Many) OR - Multiple PUB sockets broadcast messages to a single SUB socket (Many-to-One)</p> <ul> <li>SUB sockets filter messages by topic/message_type</li> <li>Fire-and-forget messaging (no acknowledgments)</li> </ul> <p>PUB/SUB is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQXPubXSubProxy</code> for more details.</p> Source code in <code>aiperf/zmq/pub_client.py</code> <pre><code>@implements_protocol(PubClientProtocol)\n@CommunicationClientFactory.register(CommClientType.PUB)\nclass ZMQPubClient(BaseZMQClient):\n    \"\"\"\n    The PUB socket broadcasts messages to all connected SUB sockets that have\n    subscribed to the message topic/type.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    OR\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - Single PUB socket broadcasts messages to all subscribers (One-to-Many)\n    OR\n    - Multiple PUB sockets broadcast messages to a single SUB socket (Many-to-One)\n\n    - SUB sockets filter messages by topic/message_type\n    - Fire-and-forget messaging (no acknowledgments)\n\n    PUB/SUB is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQXPubXSubProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Publisher client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.PUB, address, bind, socket_ops, **kwargs)\n\n    async def publish(self, message: Message) -&gt; None:\n        \"\"\"Publish a message. The topic will be set automatically based on the message type.\n\n        Args:\n            message: Message to publish (must be a Message object)\n        \"\"\"\n        await self._check_initialized()\n\n        try:\n            topic = self._determine_topic(message)\n            message_json = message.model_dump_json()\n            # Publish message\n            self.trace(lambda: f\"Publishing message {topic=} {message_json=}\")\n            await self.socket.send_multipart([topic.encode(), message_json.encode()])\n\n        except (asyncio.CancelledError, zmq.ContextTerminated):\n            self.debug(\n                lambda: f\"Pub client {self.client_id} cancelled or context terminated\"\n            )\n            return\n\n        except Exception as e:\n            raise CommunicationError(\n                f\"Failed to publish message {message.message_type}: {e}\",\n            ) from e\n\n    def _determine_topic(self, message: Message) -&gt; str:\n        \"\"\"Determine the topic based on the message.\"\"\"\n        # For targeted messages such as commands, we can set the topic to a specific service by id or type\n        # Note that target_service_id always takes precedence over target_service_type\n\n        # NOTE: Keep in mind that subscriptions in ZMQ are prefix based wildcards, so the unique portion has to come first.\n        if isinstance(message, TargetedServiceMessage):\n            if message.target_service_id:\n                return f\"{message.message_type}{TOPIC_DELIMITER}{message.target_service_id}{TOPIC_END}\"\n            if message.target_service_type:\n                return f\"{message.message_type}{TOPIC_DELIMITER}{message.target_service_type}{TOPIC_END}\"\n        return f\"{message.message_type}{TOPIC_END}\"\n</code></pre>"},{"location":"api/#aiperf.zmq.pub_client.ZMQPubClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Publisher client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/zmq/pub_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Publisher client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.PUB, address, bind, socket_ops, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.zmq.pub_client.ZMQPubClient.publish","title":"<code>publish(message)</code>  <code>async</code>","text":"<p>Publish a message. The topic will be set automatically based on the message type.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to publish (must be a Message object)</p> required Source code in <code>aiperf/zmq/pub_client.py</code> <pre><code>async def publish(self, message: Message) -&gt; None:\n    \"\"\"Publish a message. The topic will be set automatically based on the message type.\n\n    Args:\n        message: Message to publish (must be a Message object)\n    \"\"\"\n    await self._check_initialized()\n\n    try:\n        topic = self._determine_topic(message)\n        message_json = message.model_dump_json()\n        # Publish message\n        self.trace(lambda: f\"Publishing message {topic=} {message_json=}\")\n        await self.socket.send_multipart([topic.encode(), message_json.encode()])\n\n    except (asyncio.CancelledError, zmq.ContextTerminated):\n        self.debug(\n            lambda: f\"Pub client {self.client_id} cancelled or context terminated\"\n        )\n        return\n\n    except Exception as e:\n        raise CommunicationError(\n            f\"Failed to publish message {message.message_type}: {e}\",\n        ) from e\n</code></pre>"},{"location":"api/#aiperfzmqpull_client","title":"aiperf.zmq.pull_client","text":""},{"location":"api/#aiperf.zmq.pull_client.ZMQPullClient","title":"<code>ZMQPullClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ PULL socket client for receiving work from PUSH sockets.</p> <p>The PULL socket receives messages from PUSH sockets in a pipeline pattern, distributing work fairly among multiple PULL workers.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502 \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502 \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   Tasks:    \u2502             \u25b2                     \u25b2 \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502 \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   - Task D  \u2502             \u25bc \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502    PULL     \u2502                      \u2502 (Worker N)  \u2502                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - PULL receives work from multiple PUSH producers - Work is fairly distributed among PULL workers - Pipeline pattern for distributed processing - Each message is delivered to exactly one PULL socket</p> <p>PULL/PUSH is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQPushPullProxy</code> for more details.</p> Source code in <code>aiperf/zmq/pull_client.py</code> <pre><code>@implements_protocol(PullClientProtocol)\n@CommunicationClientFactory.register(CommClientType.PULL)\nclass ZMQPullClient(BaseZMQClient):\n    \"\"\"\n    ZMQ PULL socket client for receiving work from PUSH sockets.\n\n    The PULL socket receives messages from PUSH sockets in a pipeline pattern,\n    distributing work fairly among multiple PULL workers.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502\n    \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502\n    \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   Tasks:    \u2502             \u25b2                     \u25b2\n    \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n    \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   - Task D  \u2502             \u25bc\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502    PULL     \u2502\n                         \u2502 (Worker N)  \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - PULL receives work from multiple PUSH producers\n    - Work is fairly distributed among PULL workers\n    - Pipeline pattern for distributed processing\n    - Each message is delivered to exactly one PULL socket\n\n    PULL/PUSH is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQPushPullProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Puller class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n            max_pull_concurrency (int, optional): The maximum number of concurrent requests to allow.\n        \"\"\"\n        super().__init__(zmq.SocketType.PULL, address, bind, socket_ops, **kwargs)\n        self._pull_callbacks: dict[\n            MessageTypeT, Callable[[Message], Coroutine[Any, Any, None]]\n        ] = {}\n\n        if max_pull_concurrency is not None:\n            self.semaphore = asyncio.Semaphore(value=max_pull_concurrency)\n        else:\n            self.semaphore = asyncio.Semaphore(\n                value=DEFAULT_PULL_CLIENT_MAX_CONCURRENCY\n            )\n\n    @background_task(immediate=True, interval=None)\n    async def _pull_receiver(self) -&gt; None:\n        \"\"\"Background task for receiving data from the pull socket.\n\n        This method is a coroutine that will run indefinitely until the client is\n        shutdown. It will wait for messages from the socket and handle them.\n        \"\"\"\n        while not self.stop_requested:\n            try:\n                # acquire the semaphore to limit the number of concurrent requests\n                # NOTE: This MUST be done BEFORE calling recv_string() to allow the zmq push/pull\n                # logic to properly load balance the requests.\n                await self.semaphore.acquire()\n\n                message_json = await self.socket.recv_string()\n                self.trace(\n                    lambda msg=message_json: f\"Received message from pull socket: {msg}\"\n                )\n                self.execute_async(self._process_message(message_json))\n\n            except zmq.Again:\n                self.debug(\"Pull client receiver task timed out\")\n                self.semaphore.release()  # release the semaphore as it was not used\n                await yield_to_event_loop()\n            except Exception as e:\n                self.exception(f\"Exception receiving data from pull socket: {e}\")\n                self.semaphore.release()  # release the semaphore as it was not used\n                await yield_to_event_loop()\n            except (asyncio.CancelledError, zmq.ContextTerminated):\n                self.debug(\"Pull client receiver task cancelled\")\n                self.semaphore.release()  # release the semaphore as it was not used\n                break\n\n    @on_stop\n    async def _stop(self) -&gt; None:\n        \"\"\"Wait for all tasks to complete.\"\"\"\n        await self.cancel_all_tasks()\n\n    async def _process_message(self, message_json: str) -&gt; None:\n        \"\"\"Process a message from the pull socket.\n\n        This method is called by the background task when a message is received from\n        the pull socket. It will deserialize the message and call the appropriate\n        callback function.\n        \"\"\"\n        try:\n            message = Message.from_json(message_json)\n\n            # Call callbacks with Message object\n            if message.message_type in self._pull_callbacks:\n                await self._pull_callbacks[message.message_type](message)\n            else:\n                self.warning(\n                    lambda message_type=message.message_type: f\"Pull message received for message type {message_type} without callback\"\n                )\n        finally:\n            # always release the semaphore to allow receiving more messages\n            self.semaphore.release()\n\n    def register_pull_callback(\n        self,\n        message_type: MessageTypeT,\n        callback: Callable[[Message], Coroutine[Any, Any, None]],\n    ) -&gt; None:\n        \"\"\"Register a ZMQ Pull data callback for a given message type.\n\n        Note that only one callback can be registered for a given message type.\n\n        Args:\n            message_type: The message type to register the callback for.\n            callback: The function to call when data is received.\n        Raises:\n            CommunicationError: If the client is not initialized\n        \"\"\"\n        # Register callback\n        if message_type not in self._pull_callbacks:\n            self._pull_callbacks[message_type] = callback\n        else:\n            raise ValueError(\n                f\"Callback already registered for message type {message_type}\"\n            )\n</code></pre>"},{"location":"api/#aiperf.zmq.pull_client.ZMQPullClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, max_pull_concurrency=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Puller class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> <code>max_pull_concurrency</code> <code>int</code> <p>The maximum number of concurrent requests to allow.</p> <code>None</code> Source code in <code>aiperf/zmq/pull_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Puller class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n        max_pull_concurrency (int, optional): The maximum number of concurrent requests to allow.\n    \"\"\"\n    super().__init__(zmq.SocketType.PULL, address, bind, socket_ops, **kwargs)\n    self._pull_callbacks: dict[\n        MessageTypeT, Callable[[Message], Coroutine[Any, Any, None]]\n    ] = {}\n\n    if max_pull_concurrency is not None:\n        self.semaphore = asyncio.Semaphore(value=max_pull_concurrency)\n    else:\n        self.semaphore = asyncio.Semaphore(\n            value=DEFAULT_PULL_CLIENT_MAX_CONCURRENCY\n        )\n</code></pre>"},{"location":"api/#aiperf.zmq.pull_client.ZMQPullClient.register_pull_callback","title":"<code>register_pull_callback(message_type, callback)</code>","text":"<p>Register a ZMQ Pull data callback for a given message type.</p> <p>Note that only one callback can be registered for a given message type.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>MessageTypeT</code> <p>The message type to register the callback for.</p> required <code>callback</code> <code>Callable[[Message], Coroutine[Any, Any, None]]</code> <p>The function to call when data is received.</p> required <p>Raises:     CommunicationError: If the client is not initialized</p> Source code in <code>aiperf/zmq/pull_client.py</code> <pre><code>def register_pull_callback(\n    self,\n    message_type: MessageTypeT,\n    callback: Callable[[Message], Coroutine[Any, Any, None]],\n) -&gt; None:\n    \"\"\"Register a ZMQ Pull data callback for a given message type.\n\n    Note that only one callback can be registered for a given message type.\n\n    Args:\n        message_type: The message type to register the callback for.\n        callback: The function to call when data is received.\n    Raises:\n        CommunicationError: If the client is not initialized\n    \"\"\"\n    # Register callback\n    if message_type not in self._pull_callbacks:\n        self._pull_callbacks[message_type] = callback\n    else:\n        raise ValueError(\n            f\"Callback already registered for message type {message_type}\"\n        )\n</code></pre>"},{"location":"api/#aiperfzmqpush_client","title":"aiperf.zmq.push_client","text":""},{"location":"api/#aiperf.zmq.push_client.MAX_PUSH_RETRIES","title":"<code>MAX_PUSH_RETRIES = 2</code>  <code>module-attribute</code>","text":"<p>Maximum number of retries for pushing a message.</p>"},{"location":"api/#aiperf.zmq.push_client.RETRY_DELAY_INTERVAL_SEC","title":"<code>RETRY_DELAY_INTERVAL_SEC = 0.1</code>  <code>module-attribute</code>","text":"<p>The interval to wait before retrying to push a message.</p>"},{"location":"api/#aiperf.zmq.push_client.ZMQPushClient","title":"<code>ZMQPushClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ PUSH socket client for sending work to PULL sockets.</p> <p>The PUSH socket sends messages to PULL sockets in a pipeline pattern, distributing work fairly among available PULL workers.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502 \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502 \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   Tasks:    \u2502             \u25b2                     \u25b2 \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502 \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   - Task D  \u2502             \u25bc \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502    PULL     \u2502                      \u2502 (Worker 3)  \u2502                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - Round-robin distribution of work tasks (One-to-Many) - Each message delivered to exactly one worker - Pipeline pattern for distributed processing - Automatic load balancing across available workers</p> <p>PUSH/PULL is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQPushPullProxy</code> for more details.</p> Source code in <code>aiperf/zmq/push_client.py</code> <pre><code>@implements_protocol(PushClientProtocol)\n@CommunicationClientFactory.register(CommClientType.PUSH)\nclass ZMQPushClient(BaseZMQClient):\n    \"\"\"\n    ZMQ PUSH socket client for sending work to PULL sockets.\n\n    The PUSH socket sends messages to PULL sockets in a pipeline pattern,\n    distributing work fairly among available PULL workers.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    PUSH     \u2502      \u2502    PULL     \u2502      \u2502    PULL     \u2502\n    \u2502 (Producer)  \u2502      \u2502 (Worker 1)  \u2502      \u2502 (Worker 2)  \u2502\n    \u2502             \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   Tasks:    \u2502             \u25b2                     \u25b2\n    \u2502   - Task A  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n    \u2502   - Task B  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502   - Task C  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   - Task D  \u2502             \u25bc\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502    PULL     \u2502\n                         \u2502 (Worker 3)  \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - Round-robin distribution of work tasks (One-to-Many)\n    - Each message delivered to exactly one worker\n    - Pipeline pattern for distributed processing\n    - Automatic load balancing across available workers\n\n    PUSH/PULL is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQPushPullProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Push client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.PUSH, address, bind, socket_ops, **kwargs)\n\n    async def _push_message(\n        self,\n        message: Message,\n        retry_count: int = 0,\n        max_retries: int = MAX_PUSH_RETRIES,\n    ) -&gt; None:\n        \"\"\"Push a message to the socket. Will retry up to max_retries times.\n\n        Args:\n            message: Message to be sent must be a Message object\n            retry_count: Current retry count\n            max_retries: Maximum number of times to retry pushing the message\n        \"\"\"\n        try:\n            data_json = message.model_dump_json()\n            await self.socket.send_string(data_json)\n            self.trace(lambda msg=data_json: f\"Pushed json data: {msg}\")\n        except (asyncio.CancelledError, zmq.ContextTerminated):\n            self.debug(\"Push client cancelled or context terminated\")\n            return\n        except zmq.Again as e:\n            self.debug(\"Push client timed out\")\n            if retry_count &gt;= max_retries:\n                raise CommunicationError(\n                    f\"Failed to push data after {retry_count} retries: {e}\",\n                ) from e\n\n            await asyncio.sleep(RETRY_DELAY_INTERVAL_SEC)\n            return await self._push_message(message, retry_count + 1, max_retries)\n        except Exception as e:\n            raise CommunicationError(f\"Failed to push data: {e}\") from e\n\n    async def push(self, message: Message) -&gt; None:\n        \"\"\"Push data to a target. The message will be routed automatically\n        based on the message type.\n\n        Args:\n            message: Message to be sent must be a Message object\n        \"\"\"\n        await self._check_initialized()\n\n        await self._push_message(message)\n</code></pre>"},{"location":"api/#aiperf.zmq.push_client.ZMQPushClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Push client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/zmq/push_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Push client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.PUSH, address, bind, socket_ops, **kwargs)\n</code></pre>"},{"location":"api/#aiperf.zmq.push_client.ZMQPushClient.push","title":"<code>push(message)</code>  <code>async</code>","text":"<p>Push data to a target. The message will be routed automatically based on the message type.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to be sent must be a Message object</p> required Source code in <code>aiperf/zmq/push_client.py</code> <pre><code>async def push(self, message: Message) -&gt; None:\n    \"\"\"Push data to a target. The message will be routed automatically\n    based on the message type.\n\n    Args:\n        message: Message to be sent must be a Message object\n    \"\"\"\n    await self._check_initialized()\n\n    await self._push_message(message)\n</code></pre>"},{"location":"api/#aiperfzmqrouter_reply_client","title":"aiperf.zmq.router_reply_client","text":""},{"location":"api/#aiperf.zmq.router_reply_client.ZMQRouterReplyClient","title":"<code>ZMQRouterReplyClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ ROUTER socket client for handling requests from DEALER clients.</p> <p>The ROUTER socket receives requests from DEALER clients and sends responses back to the originating DEALER client using routing envelopes.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502    ROUTER    \u2502 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502  (Service)   \u2502 \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502              \u2502 \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - ROUTER handles requests from multiple DEALER clients - Maintains routing envelopes to send responses back - Many-to-one request handling pattern - Supports concurrent request processing</p> <p>ROUTER/DEALER is a Many-to-One communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQDealerRouterProxy</code> for more details.</p> Source code in <code>aiperf/zmq/router_reply_client.py</code> <pre><code>@implements_protocol(ReplyClientProtocol)\n@CommunicationClientFactory.register(CommClientType.REPLY)\nclass ZMQRouterReplyClient(BaseZMQClient):\n    \"\"\"\n    ZMQ ROUTER socket client for handling requests from DEALER clients.\n\n    The ROUTER socket receives requests from DEALER clients and sends responses\n    back to the originating DEALER client using routing envelopes.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502    ROUTER    \u2502\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502  (Service)   \u2502\n    \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502              \u2502\n    \u2502    DEALER    \u2502\u2500\u2500\u2500\u2500\u2500 Request \u2500\u2500\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502   (Client)   \u2502&lt;\u2500\u2500\u2500\u2500 Response \u2500\u2500\u2500\u2500\u2500\u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Usage Pattern:\n    - ROUTER handles requests from multiple DEALER clients\n    - Maintains routing envelopes to send responses back\n    - Many-to-one request handling pattern\n    - Supports concurrent request processing\n\n    ROUTER/DEALER is a Many-to-One communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQDealerRouterProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Router (Rep) client class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.ROUTER, address, bind, socket_ops, **kwargs)\n\n        self._request_handlers: dict[\n            MessageTypeT,\n            tuple[str, Callable[[Message], Coroutine[Any, Any, Message | None]]],\n        ] = {}\n        self._response_futures: dict[str, asyncio.Future[Message | None]] = {}\n\n    @on_stop\n    async def _clear_request_handlers(self) -&gt; None:\n        self._request_handlers.clear()\n\n    def register_request_handler(\n        self,\n        service_id: str,\n        message_type: MessageTypeT,\n        handler: Callable[[Message], Coroutine[Any, Any, Message | None]],\n    ) -&gt; None:\n        \"\"\"Register a request handler. Anytime a request is received that matches the\n        message type, the handler will be called. The handler should return a response\n        message. If the handler returns None, the request will be ignored.\n\n        Note that there is a limit of 1 to 1 mapping between message type and handler.\n\n        Args:\n            service_id: The service ID to register the handler for\n            message_type: The message type to register the handler for\n            handler: The handler to register\n        \"\"\"\n        if message_type in self._request_handlers:\n            raise ValueError(\n                f\"Handler already registered for message type {message_type}\"\n            )\n\n        self.debug(\n            lambda service_id=service_id,\n            type=message_type: f\"Registering request handler for {service_id} with message type {type}\"\n        )\n        self._request_handlers[message_type] = (service_id, handler)\n\n    async def _handle_request(self, request_id: str, request: Message) -&gt; None:\n        \"\"\"Handle a request.\n\n        This method will:\n        - Parse the request JSON to create a Message object\n        - Call the handler for the message type\n        - Set the response future\n        \"\"\"\n        message_type = request.message_type\n\n        try:\n            _, handler = self._request_handlers[message_type]\n            response = await handler(request)\n\n        except Exception as e:\n            self.exception(f\"Exception calling handler for {message_type}: {e}\")\n            response = ErrorMessage(\n                request_id=request_id,\n                error=ErrorDetails.from_exception(e),\n            )\n\n        try:\n            self._response_futures[request_id].set_result(response)\n        except Exception as e:\n            self.exception(\n                f\"Exception setting response future for request {request_id}: {e}\"\n            )\n\n    async def _wait_for_response(\n        self, request_id: str, routing_envelope: tuple[bytes, ...]\n    ) -&gt; None:\n        \"\"\"Wait for a response to a request.\n\n        This method will wait for the response future to be set and then send the response\n        back to the client.\n        \"\"\"\n        try:\n            # Wait for the response asynchronously.\n            response = await self._response_futures[request_id]\n\n            if response is None:\n                self.warning(\n                    lambda req_id=request_id: f\"Got None as response for request {req_id}\"\n                )\n                response = ErrorMessage(\n                    request_id=request_id,\n                    error=ErrorDetails(\n                        type=\"NO_RESPONSE\",\n                        message=\"No response was generated for the request.\",\n                    ),\n                )\n\n            self._response_futures.pop(request_id, None)\n\n            # Send the response back to the client.\n            await self.socket.send_multipart(\n                [*routing_envelope, response.model_dump_json().encode()]\n            )\n        except Exception as e:\n            self.exception(\n                f\"Exception waiting for response for request {request_id}: {e}\"\n            )\n\n    @background_task(immediate=True, interval=None)\n    async def _rep_router_receiver(self) -&gt; None:\n        \"\"\"Background task for receiving requests and sending responses.\n\n        This method is a coroutine that will run indefinitely until the client is\n        shutdown. It will wait for requests from the socket and send responses in\n        an asynchronous manner.\n        \"\"\"\n        self.debug(\"Router reply client background task initialized\")\n\n        while not self.stop_requested:\n            try:\n                # Receive request\n                try:\n                    data = await self.socket.recv_multipart()\n                    self.trace(lambda msg=data: f\"Received request: {msg}\")\n\n                    request = Message.from_json(data[-1])\n                    if not request.request_id:\n                        self.exception(f\"Request ID is missing from request: {data}\")\n                        continue\n\n                    routing_envelope: tuple[bytes, ...] = (\n                        tuple(data[:-1])\n                        if len(data) &gt; 1\n                        else (request.request_id.encode(),)\n                    )\n                except zmq.Again:\n                    # This means we timed out waiting for a request.\n                    # We can continue to the next iteration of the loop.\n                    self.debug(\"Router reply client receiver task timed out\")\n                    await yield_to_event_loop()\n                    continue\n\n                # Create a new response future for this request that will be resolved\n                # when the handler returns a response.\n                self._response_futures[request.request_id] = asyncio.Future()\n                # Handle the request in a new task.\n                self.execute_async(self._handle_request(request.request_id, request))\n                self.execute_async(\n                    self._wait_for_response(request.request_id, routing_envelope)\n                )\n\n            except Exception as e:\n                self.exception(f\"Exception receiving request: {e}\")\n                await yield_to_event_loop()\n            except asyncio.CancelledError:\n                self.debug(\"Router reply client receiver task cancelled\")\n                break\n</code></pre>"},{"location":"api/#aiperf.zmq.router_reply_client.ZMQRouterReplyClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Router (Rep) client class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/zmq/router_reply_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Router (Rep) client class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.ROUTER, address, bind, socket_ops, **kwargs)\n\n    self._request_handlers: dict[\n        MessageTypeT,\n        tuple[str, Callable[[Message], Coroutine[Any, Any, Message | None]]],\n    ] = {}\n    self._response_futures: dict[str, asyncio.Future[Message | None]] = {}\n</code></pre>"},{"location":"api/#aiperf.zmq.router_reply_client.ZMQRouterReplyClient.register_request_handler","title":"<code>register_request_handler(service_id, message_type, handler)</code>","text":"<p>Register a request handler. Anytime a request is received that matches the message type, the handler will be called. The handler should return a response message. If the handler returns None, the request will be ignored.</p> <p>Note that there is a limit of 1 to 1 mapping between message type and handler.</p> <p>Parameters:</p> Name Type Description Default <code>service_id</code> <code>str</code> <p>The service ID to register the handler for</p> required <code>message_type</code> <code>MessageTypeT</code> <p>The message type to register the handler for</p> required <code>handler</code> <code>Callable[[Message], Coroutine[Any, Any, Message | None]]</code> <p>The handler to register</p> required Source code in <code>aiperf/zmq/router_reply_client.py</code> <pre><code>def register_request_handler(\n    self,\n    service_id: str,\n    message_type: MessageTypeT,\n    handler: Callable[[Message], Coroutine[Any, Any, Message | None]],\n) -&gt; None:\n    \"\"\"Register a request handler. Anytime a request is received that matches the\n    message type, the handler will be called. The handler should return a response\n    message. If the handler returns None, the request will be ignored.\n\n    Note that there is a limit of 1 to 1 mapping between message type and handler.\n\n    Args:\n        service_id: The service ID to register the handler for\n        message_type: The message type to register the handler for\n        handler: The handler to register\n    \"\"\"\n    if message_type in self._request_handlers:\n        raise ValueError(\n            f\"Handler already registered for message type {message_type}\"\n        )\n\n    self.debug(\n        lambda service_id=service_id,\n        type=message_type: f\"Registering request handler for {service_id} with message type {type}\"\n    )\n    self._request_handlers[message_type] = (service_id, handler)\n</code></pre>"},{"location":"api/#aiperfzmqsub_client","title":"aiperf.zmq.sub_client","text":""},{"location":"api/#aiperf.zmq.sub_client.ZMQSubClient","title":"<code>ZMQSubClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>ZMQ SUB socket client for subscribing to messages from PUB sockets. One-to-Many or Many-to-One communication pattern.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502 \u2502 (Publisher)  \u2502    \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 OR \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502 \u2502              \u2502    \u2502 (Subscriber) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>Usage Pattern: - Single SUB socket subscribes to multiple PUB publishers (One-to-Many) OR - Multiple SUB sockets subscribe to a single PUB publisher (Many-to-One)</p> <ul> <li>Subscribes to specific message topics/types</li> <li>Receives all messages matching subscriptions</li> </ul> <p>SUB/PUB is a One-to-Many communication pattern. If you need Many-to-Many, use a ZMQ Proxy as well. see :class:<code>ZMQXPubXSubProxy</code> for more details.</p> Source code in <code>aiperf/zmq/sub_client.py</code> <pre><code>@implements_protocol(SubClientProtocol)\n@CommunicationClientFactory.register(CommClientType.SUB)\nclass ZMQSubClient(BaseZMQClient):\n    \"\"\"\n    ZMQ SUB socket client for subscribing to messages from PUB sockets.\n    One-to-Many or Many-to-One communication pattern.\n\n    ASCII Diagram:\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     SUB      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502\u2500\u2500\u2500&gt;\u2502              \u2502\n    \u2502 (Publisher)  \u2502    \u2502              \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    OR\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2502     PUB      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 (Publisher)  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502              \u2502\u2500\u2500\u2500&gt;\u2502     SUB      \u2502\n    \u2502              \u2502    \u2502 (Subscriber) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n    Usage Pattern:\n    - Single SUB socket subscribes to multiple PUB publishers (One-to-Many)\n    OR\n    - Multiple SUB sockets subscribe to a single PUB publisher (Many-to-One)\n\n    - Subscribes to specific message topics/types\n    - Receives all messages matching subscriptions\n\n    SUB/PUB is a One-to-Many communication pattern. If you need Many-to-Many,\n    use a ZMQ Proxy as well. see :class:`ZMQXPubXSubProxy` for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Subscriber class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to bind or connect the socket.\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        super().__init__(zmq.SocketType.SUB, address, bind, socket_ops, **kwargs)\n\n        self._subscribers: dict[MessageTypeT, list[Callable[[Message], Any]]] = {}\n\n    async def subscribe_all(\n        self,\n        message_callback_map: dict[\n            MessageTypeT,\n            Callable[[Message], Any] | list[Callable[[Message], Any]],\n        ],\n    ) -&gt; None:\n        \"\"\"Subscribe to all message_types in the map. For each MessageType, a single\n        callback or a list of callbacks can be provided.\"\"\"\n        await self._check_initialized()\n        for message_type, callbacks in message_callback_map.items():\n            if isinstance(callbacks, list):\n                for callback in callbacks:\n                    await self._subscribe_internal(message_type, callback)\n            else:\n                await self._subscribe_internal(message_type, callbacks)\n\n    async def subscribe(\n        self, message_type: MessageTypeT, callback: Callable[[Message], Any]\n    ) -&gt; None:\n        \"\"\"Subscribe to a message_type.\n\n        Args:\n            message_type: MessageTypeT to subscribe to\n            callback: Function to call when a message is received (receives Message object)\n\n        Raises:\n            Exception if subscription was not successful, None otherwise\n        \"\"\"\n        await self._check_initialized()\n        await self._subscribe_internal(message_type, callback)\n\n    async def _subscribe_internal(\n        self, topic: str, callback: Callable[[Message], Any]\n    ) -&gt; None:\n        \"\"\"Subscribe to a message_type.\n\n        Args:\n            message_type: MessageTypeT to subscribe to\n            callback: Function to call when a message is received (receives Message object)\n        \"\"\"\n        try:\n            # Only subscribe to topic if this is the first callback for this type\n            if topic not in self._subscribers:\n                self.debug(lambda: f\"Subscribed to topic: {topic}\")\n                self.socket.setsockopt(\n                    zmq.SUBSCRIBE, topic.encode() + TOPIC_END_ENCODED\n                )\n            else:\n                self.debug(\n                    lambda: f\"Adding callback to existing subscription for topic: {topic}\"\n                )\n\n            self._subscribers.setdefault(topic, []).append(callback)\n\n        except Exception as e:\n            self.exception(f\"Exception subscribing to topic {topic}: {e}\")\n            raise CommunicationError(\n                f\"Failed to subscribe to topic {topic}: {e}\",\n            ) from e\n\n    async def _handle_message(self, topic_bytes: bytes, message_bytes: bytes) -&gt; None:\n        \"\"\"Handle a message from a subscribed message_type.\"\"\"\n\n        # strip the final TOPIC_END chars from the topic\n        topic = topic_bytes.decode()[: -len(TOPIC_END)]\n        message_json = message_bytes.decode()\n        self.trace(\n            lambda: f\"Received message from topic: '{topic}', message: {message_json}\"\n        )\n\n        # Targeted messages are in the format \"&lt;message_type&gt;.&lt;target_service_id&gt;\"\n        # or \"&lt;message_type&gt;.&lt;target_service_type&gt;\"\n        # grab the first part which is the message type\n        message_type = (\n            topic.split(TOPIC_DELIMITER)[0] if TOPIC_DELIMITER in topic else topic\n        )\n\n        if message_type == MessageType.COMMAND:\n            message = CommandMessage.from_json(message_json)\n        elif message_type == MessageType.COMMAND_RESPONSE:\n            message = CommandResponse.from_json(message_json)\n        else:\n            message = Message.from_json_with_type(message_type, message_json)\n\n        self.debug(\n            lambda: f\"Calling callbacks for message: {message}, {self._subscribers.get(topic)}\"\n        )\n\n        # Call callbacks with the parsed message object\n        if topic in self._subscribers:\n            with contextlib.suppress(Exception):  # Ignore errors, they will get logged\n                await call_all_functions(self._subscribers[topic], message)\n\n    @background_task(immediate=True, interval=None)\n    async def _sub_receiver(self) -&gt; None:\n        \"\"\"Background task for receiving messages from subscribed topics.\n\n        This method is a coroutine that will run indefinitely until the client is\n        shutdown. It will wait for messages from the socket and handle them.\n        \"\"\"\n        while not self.stop_requested:\n            try:\n                topic_bytes, message_bytes = await self.socket.recv_multipart()\n                if self.is_trace_enabled:\n                    self.trace(\n                        f\"Socket received message: {topic_bytes} {message_bytes}\"\n                    )\n                self.execute_async(self._handle_message(topic_bytes, message_bytes))\n\n            except zmq.Again:\n                self.debug(f\"Sub client {self.client_id} receiver task timed out\")\n                await yield_to_event_loop()\n            except Exception as e:\n                self.exception(\n                    f\"Exception receiving message from subscription: {e}, {type(e)}\"\n                )\n                await yield_to_event_loop()\n            except (asyncio.CancelledError, zmq.ContextTerminated):\n                self.debug(f\"Sub client {self.client_id} receiver task cancelled\")\n                break\n</code></pre>"},{"location":"api/#aiperf.zmq.sub_client.ZMQSubClient.__init__","title":"<code>__init__(address, bind, socket_ops=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Subscriber class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/zmq/sub_client.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Subscriber class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to bind or connect the socket.\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    super().__init__(zmq.SocketType.SUB, address, bind, socket_ops, **kwargs)\n\n    self._subscribers: dict[MessageTypeT, list[Callable[[Message], Any]]] = {}\n</code></pre>"},{"location":"api/#aiperf.zmq.sub_client.ZMQSubClient.subscribe","title":"<code>subscribe(message_type, callback)</code>  <code>async</code>","text":"<p>Subscribe to a message_type.</p> <p>Parameters:</p> Name Type Description Default <code>message_type</code> <code>MessageTypeT</code> <p>MessageTypeT to subscribe to</p> required <code>callback</code> <code>Callable[[Message], Any]</code> <p>Function to call when a message is received (receives Message object)</p> required Source code in <code>aiperf/zmq/sub_client.py</code> <pre><code>async def subscribe(\n    self, message_type: MessageTypeT, callback: Callable[[Message], Any]\n) -&gt; None:\n    \"\"\"Subscribe to a message_type.\n\n    Args:\n        message_type: MessageTypeT to subscribe to\n        callback: Function to call when a message is received (receives Message object)\n\n    Raises:\n        Exception if subscription was not successful, None otherwise\n    \"\"\"\n    await self._check_initialized()\n    await self._subscribe_internal(message_type, callback)\n</code></pre>"},{"location":"api/#aiperf.zmq.sub_client.ZMQSubClient.subscribe_all","title":"<code>subscribe_all(message_callback_map)</code>  <code>async</code>","text":"<p>Subscribe to all message_types in the map. For each MessageType, a single callback or a list of callbacks can be provided.</p> Source code in <code>aiperf/zmq/sub_client.py</code> <pre><code>async def subscribe_all(\n    self,\n    message_callback_map: dict[\n        MessageTypeT,\n        Callable[[Message], Any] | list[Callable[[Message], Any]],\n    ],\n) -&gt; None:\n    \"\"\"Subscribe to all message_types in the map. For each MessageType, a single\n    callback or a list of callbacks can be provided.\"\"\"\n    await self._check_initialized()\n    for message_type, callbacks in message_callback_map.items():\n        if isinstance(callbacks, list):\n            for callback in callbacks:\n                await self._subscribe_internal(message_type, callback)\n        else:\n            await self._subscribe_internal(message_type, callbacks)\n</code></pre>"},{"location":"api/#aiperfzmqzmq_base_client","title":"aiperf.zmq.zmq_base_client","text":""},{"location":"api/#aiperf.zmq.zmq_base_client.BaseZMQClient","title":"<code>BaseZMQClient</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code></p> <p>Base class for all ZMQ clients. It can be used as-is to create a new ZMQ client, or it can be subclassed to create specific ZMQ client functionality.</p> <p>It inherits from the :class:<code>AIPerfLifecycleMixin</code>, allowing derived classes to implement specific hooks.</p> Source code in <code>aiperf/zmq/zmq_base_client.py</code> <pre><code>class BaseZMQClient(AIPerfLifecycleMixin):\n    \"\"\"Base class for all ZMQ clients. It can be used as-is to create a new ZMQ client,\n    or it can be subclassed to create specific ZMQ client functionality.\n\n    It inherits from the :class:`AIPerfLifecycleMixin`, allowing derived\n    classes to implement specific hooks.\n    \"\"\"\n\n    def __init__(\n        self,\n        socket_type: zmq.SocketType,\n        address: str,\n        bind: bool,\n        socket_ops: dict | None = None,\n        client_id: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ZMQ Base class.\n\n        Args:\n            address (str): The address to bind or connect to.\n            bind (bool): Whether to BIND or CONNECT the socket.\n            socket_type (SocketType): The type of ZMQ socket (eg. PUB, SUB, ROUTER, DEALER, etc.).\n            socket_ops (dict, optional): Additional socket options to set.\n        \"\"\"\n        self.context: zmq.asyncio.Context = zmq.asyncio.Context.instance()\n        self.socket_type: zmq.SocketType = socket_type\n        self.socket: zmq.asyncio.Socket = self.context.socket(self.socket_type)\n        self.address: str = address\n        self.bind: bool = bind\n        self.socket_ops: dict = socket_ops or {}\n        self.client_id: str = (\n            client_id\n            or f\"{self.socket_type.name.lower()}_client_{uuid.uuid4().hex[:8]}\"\n        )\n        super().__init__(id=self.client_id, **kwargs)\n        self.trace(lambda: f\"ZMQ client __init__: {self.client_id}\")\n\n    async def _check_initialized(self) -&gt; None:\n        \"\"\"Raise an exception if the socket is not initialized or closed.\"\"\"\n        if self.stop_requested:\n            raise asyncio.CancelledError(\"Socket was stopped\")\n        if not self.socket:\n            raise NotInitializedError(\"Socket not initialized or closed\")\n\n    @property\n    def socket_type_name(self) -&gt; str:\n        \"\"\"Get the name of the socket type.\"\"\"\n        return self.socket_type.name\n\n    @on_init\n    async def _initialize_socket(self) -&gt; None:\n        \"\"\"Initialize the communication.\n\n        This method will:\n        - Create the zmq socket\n        - Bind or connect the socket to the address\n        - Set the socket options\n        - Run the AIPerfHook.ON_INIT hooks\n        \"\"\"\n        try:\n            self.debug(\n                lambda: f\"ZMQ {self.socket_type_name} socket initialized, try {'BIND' if self.bind else 'CONNECT'} to {self.address} ({self.client_id})\"\n            )\n\n            if self.bind:\n                self.socket.bind(self.address)\n            else:\n                self.socket.connect(self.address)\n\n            # Set default timeouts\n            self.socket.setsockopt(zmq.RCVTIMEO, ZMQSocketDefaults.RCVTIMEO)\n            self.socket.setsockopt(zmq.SNDTIMEO, ZMQSocketDefaults.SNDTIMEO)\n\n            # Set high water mark\n            self.socket.setsockopt(zmq.SNDHWM, ZMQSocketDefaults.SNDHWM)\n            self.socket.setsockopt(zmq.RCVHWM, ZMQSocketDefaults.RCVHWM)\n\n            # Set performance-oriented socket options\n            self.socket.setsockopt(zmq.TCP_KEEPALIVE, ZMQSocketDefaults.TCP_KEEPALIVE)\n            self.socket.setsockopt(\n                zmq.TCP_KEEPALIVE_IDLE, ZMQSocketDefaults.TCP_KEEPALIVE_IDLE\n            )\n            self.socket.setsockopt(\n                zmq.TCP_KEEPALIVE_INTVL, ZMQSocketDefaults.TCP_KEEPALIVE_INTVL\n            )\n            self.socket.setsockopt(\n                zmq.TCP_KEEPALIVE_CNT, ZMQSocketDefaults.TCP_KEEPALIVE_CNT\n            )\n            self.socket.setsockopt(zmq.IMMEDIATE, ZMQSocketDefaults.IMMEDIATE)\n            self.socket.setsockopt(zmq.LINGER, ZMQSocketDefaults.LINGER)\n\n            # Set additional socket options requested by the caller\n            for key, val in self.socket_ops.items():\n                self.socket.setsockopt(key, val)\n\n            self.debug(\n                lambda: f\"ZMQ {self.socket_type_name} socket {'BOUND' if self.bind else 'CONNECTED'} to {self.address} ({self.client_id})\"\n            )\n\n        except Exception as e:\n            raise InitializationError(f\"Failed to initialize ZMQ socket: {e}\") from e\n\n    @on_stop\n    async def _shutdown_socket(self) -&gt; None:\n        \"\"\"Shutdown the socket.\"\"\"\n        try:\n            if self.socket:\n                self.socket.close()\n        except zmq.ContextTerminated:\n            self.debug(\n                lambda: f\"ZMQ context already terminated, skipping socket close ({self.client_id})\"\n            )\n            return\n        except Exception as e:\n            self.exception(\n                f\"Uncaught exception shutting down ZMQ socket: {e} ({self.client_id})\"\n            )\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_base_client.BaseZMQClient.socket_type_name","title":"<code>socket_type_name</code>  <code>property</code>","text":"<p>Get the name of the socket type.</p>"},{"location":"api/#aiperf.zmq.zmq_base_client.BaseZMQClient.__init__","title":"<code>__init__(socket_type, address, bind, socket_ops=None, client_id=None, **kwargs)</code>","text":"<p>Initialize the ZMQ Base class.</p> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>The address to bind or connect to.</p> required <code>bind</code> <code>bool</code> <p>Whether to BIND or CONNECT the socket.</p> required <code>socket_type</code> <code>SocketType</code> <p>The type of ZMQ socket (eg. PUB, SUB, ROUTER, DEALER, etc.).</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> Source code in <code>aiperf/zmq/zmq_base_client.py</code> <pre><code>def __init__(\n    self,\n    socket_type: zmq.SocketType,\n    address: str,\n    bind: bool,\n    socket_ops: dict | None = None,\n    client_id: str | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the ZMQ Base class.\n\n    Args:\n        address (str): The address to bind or connect to.\n        bind (bool): Whether to BIND or CONNECT the socket.\n        socket_type (SocketType): The type of ZMQ socket (eg. PUB, SUB, ROUTER, DEALER, etc.).\n        socket_ops (dict, optional): Additional socket options to set.\n    \"\"\"\n    self.context: zmq.asyncio.Context = zmq.asyncio.Context.instance()\n    self.socket_type: zmq.SocketType = socket_type\n    self.socket: zmq.asyncio.Socket = self.context.socket(self.socket_type)\n    self.address: str = address\n    self.bind: bool = bind\n    self.socket_ops: dict = socket_ops or {}\n    self.client_id: str = (\n        client_id\n        or f\"{self.socket_type.name.lower()}_client_{uuid.uuid4().hex[:8]}\"\n    )\n    super().__init__(id=self.client_id, **kwargs)\n    self.trace(lambda: f\"ZMQ client __init__: {self.client_id}\")\n</code></pre>"},{"location":"api/#aiperfzmqzmq_comms","title":"aiperf.zmq.zmq_comms","text":""},{"location":"api/#aiperf.zmq.zmq_comms.BaseZMQCommunication","title":"<code>BaseZMQCommunication</code>","text":"<p>               Bases: <code>BaseCommunication</code>, <code>AIPerfLoggerMixin</code>, <code>ABC</code></p> <p>ZeroMQ-based implementation of the CommunicationProtocol.</p> <p>Uses ZeroMQ for publish/subscribe, request/reply, and pull/push patterns to facilitate communication between AIPerf components.</p> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>@implements_protocol(CommunicationProtocol)\nclass BaseZMQCommunication(BaseCommunication, AIPerfLoggerMixin, ABC):\n    \"\"\"ZeroMQ-based implementation of the CommunicationProtocol.\n\n    Uses ZeroMQ for publish/subscribe, request/reply, and pull/push patterns to\n    facilitate communication between AIPerf components.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: BaseZMQCommunicationConfig,\n    ) -&gt; None:\n        super().__init__()\n        self.config = config\n\n        self.context = zmq.asyncio.Context.instance()\n        self._clients_cache: dict[\n            tuple[CommClientType, CommAddressType, bool], CommunicationClientProtocol\n        ] = {}\n\n        self.debug(f\"ZMQ communication using protocol: {type(self.config).__name__}\")\n\n    def get_address(self, address_type: CommAddressType) -&gt; str:\n        \"\"\"Get the actual address based on the address type from the config.\"\"\"\n        if isinstance(address_type, CommAddress):\n            return self.config.get_address(address_type)\n        return address_type\n\n    def create_client(\n        self,\n        client_type: CommClientType,\n        address: CommAddressType,\n        bind: bool = False,\n        socket_ops: dict | None = None,\n        max_pull_concurrency: int | None = None,\n        **kwargs,\n    ) -&gt; CommunicationClientProtocol:\n        \"\"\"Create a communication client for a given client type and address.\n\n        Args:\n            client_type: The type of client to create.\n            address: The type of address to use when looking up in the communication config, or the address itself.\n            bind: Whether to bind or connect the socket.\n            socket_ops: Additional socket options to set.\n            max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n        \"\"\"\n        if (client_type, address, bind) in self._clients_cache:\n            return self._clients_cache[(client_type, address, bind)]\n\n        if self.state != LifecycleState.CREATED:\n            # We require the clients to be created before the communication class is initialized.\n            # This is because this class manages the lifecycle of the clients of as well.\n            raise InvalidStateError(\n                f\"Communication clients must be created before the {self.__class__.__name__} \"\n                f\"class is initialized: {self.state!r}\"\n            )\n\n        client = CommunicationClientFactory.create_instance(\n            client_type,\n            address=self.get_address(address),\n            bind=bind,\n            socket_ops=socket_ops,\n            max_pull_concurrency=max_pull_concurrency,\n            **kwargs,\n        )\n\n        self._clients_cache[(client_type, address, bind)] = client\n        self.attach_child_lifecycle(client)\n        return client\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_comms.BaseZMQCommunication.create_client","title":"<code>create_client(client_type, address, bind=False, socket_ops=None, max_pull_concurrency=None, **kwargs)</code>","text":"<p>Create a communication client for a given client type and address.</p> <p>Parameters:</p> Name Type Description Default <code>client_type</code> <code>CommClientType</code> <p>The type of client to create.</p> required <code>address</code> <code>CommAddressType</code> <p>The type of address to use when looking up in the communication config, or the address itself.</p> required <code>bind</code> <code>bool</code> <p>Whether to bind or connect the socket.</p> <code>False</code> <code>socket_ops</code> <code>dict | None</code> <p>Additional socket options to set.</p> <code>None</code> <code>max_pull_concurrency</code> <code>int | None</code> <p>The maximum number of concurrent pull requests to allow. (Only used for pull clients)</p> <code>None</code> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>def create_client(\n    self,\n    client_type: CommClientType,\n    address: CommAddressType,\n    bind: bool = False,\n    socket_ops: dict | None = None,\n    max_pull_concurrency: int | None = None,\n    **kwargs,\n) -&gt; CommunicationClientProtocol:\n    \"\"\"Create a communication client for a given client type and address.\n\n    Args:\n        client_type: The type of client to create.\n        address: The type of address to use when looking up in the communication config, or the address itself.\n        bind: Whether to bind or connect the socket.\n        socket_ops: Additional socket options to set.\n        max_pull_concurrency: The maximum number of concurrent pull requests to allow. (Only used for pull clients)\n    \"\"\"\n    if (client_type, address, bind) in self._clients_cache:\n        return self._clients_cache[(client_type, address, bind)]\n\n    if self.state != LifecycleState.CREATED:\n        # We require the clients to be created before the communication class is initialized.\n        # This is because this class manages the lifecycle of the clients of as well.\n        raise InvalidStateError(\n            f\"Communication clients must be created before the {self.__class__.__name__} \"\n            f\"class is initialized: {self.state!r}\"\n        )\n\n    client = CommunicationClientFactory.create_instance(\n        client_type,\n        address=self.get_address(address),\n        bind=bind,\n        socket_ops=socket_ops,\n        max_pull_concurrency=max_pull_concurrency,\n        **kwargs,\n    )\n\n    self._clients_cache[(client_type, address, bind)] = client\n    self.attach_child_lifecycle(client)\n    return client\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_comms.BaseZMQCommunication.get_address","title":"<code>get_address(address_type)</code>","text":"<p>Get the actual address based on the address type from the config.</p> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>def get_address(self, address_type: CommAddressType) -&gt; str:\n    \"\"\"Get the actual address based on the address type from the config.\"\"\"\n    if isinstance(address_type, CommAddress):\n        return self.config.get_address(address_type)\n    return address_type\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_comms.ZMQIPCCommunication","title":"<code>ZMQIPCCommunication</code>","text":"<p>               Bases: <code>BaseZMQCommunication</code></p> <p>ZeroMQ-based implementation of the Communication interface using IPC transport.</p> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>@CommunicationFactory.register(CommunicationBackend.ZMQ_IPC)\n@implements_protocol(CommunicationProtocol)\nclass ZMQIPCCommunication(BaseZMQCommunication):\n    \"\"\"ZeroMQ-based implementation of the Communication interface using IPC transport.\"\"\"\n\n    def __init__(self, config: ZMQIPCConfig | None = None) -&gt; None:\n        \"\"\"Initialize ZMQ IPC communication.\n\n        Args:\n            config: ZMQIPCConfig object with configuration parameters\n        \"\"\"\n        super().__init__(config or ZMQIPCConfig())\n        # call after super init so that way self.config is set\n        self._setup_ipc_directory()\n\n    def _setup_ipc_directory(self) -&gt; None:\n        \"\"\"Create IPC socket directory if using IPC transport.\"\"\"\n        self._ipc_socket_dir = Path(self.config.path)\n        if not self._ipc_socket_dir.exists():\n            self.debug(\n                f\"IPC socket directory does not exist, creating: {self._ipc_socket_dir}\"\n            )\n            self._ipc_socket_dir.mkdir(parents=True, exist_ok=True)\n            self.debug(f\"Created IPC socket directory: {self._ipc_socket_dir}\")\n        else:\n            self.debug(f\"IPC socket directory already exists: {self._ipc_socket_dir}\")\n\n    @on_stop\n    def _cleanup_ipc_sockets(self) -&gt; None:\n        \"\"\"Clean up IPC socket files.\"\"\"\n        if self._ipc_socket_dir and self._ipc_socket_dir.exists():\n            # Remove all .ipc files in the directory\n            ipc_files = glob.glob(str(self._ipc_socket_dir / \"*.ipc\"))\n            for ipc_file in ipc_files:\n                try:\n                    if os.path.exists(ipc_file):\n                        os.unlink(ipc_file)\n                        self.debug(f\"Removed IPC socket file: {ipc_file}\")\n                except OSError as e:\n                    if e.errno != errno.ENOENT:\n                        self.warning(\n                            f\"Failed to remove IPC socket file {ipc_file}: {e}\"\n                        )\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_comms.ZMQIPCCommunication.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize ZMQ IPC communication.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ZMQIPCConfig | None</code> <p>ZMQIPCConfig object with configuration parameters</p> <code>None</code> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>def __init__(self, config: ZMQIPCConfig | None = None) -&gt; None:\n    \"\"\"Initialize ZMQ IPC communication.\n\n    Args:\n        config: ZMQIPCConfig object with configuration parameters\n    \"\"\"\n    super().__init__(config or ZMQIPCConfig())\n    # call after super init so that way self.config is set\n    self._setup_ipc_directory()\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_comms.ZMQTCPCommunication","title":"<code>ZMQTCPCommunication</code>","text":"<p>               Bases: <code>BaseZMQCommunication</code></p> <p>ZeroMQ-based implementation of the Communication interface using TCP transport.</p> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>@CommunicationFactory.register(CommunicationBackend.ZMQ_TCP)\n@implements_protocol(CommunicationProtocol)\nclass ZMQTCPCommunication(BaseZMQCommunication):\n    \"\"\"ZeroMQ-based implementation of the Communication interface using TCP transport.\"\"\"\n\n    def __init__(self, config: ZMQTCPConfig | None = None) -&gt; None:\n        \"\"\"Initialize ZMQ TCP communication.\n\n        Args:\n            config: ZMQTCPTransportConfig object with configuration parameters\n        \"\"\"\n        super().__init__(config or ZMQTCPConfig())\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_comms.ZMQTCPCommunication.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize ZMQ TCP communication.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ZMQTCPConfig | None</code> <p>ZMQTCPTransportConfig object with configuration parameters</p> <code>None</code> Source code in <code>aiperf/zmq/zmq_comms.py</code> <pre><code>def __init__(self, config: ZMQTCPConfig | None = None) -&gt; None:\n    \"\"\"Initialize ZMQ TCP communication.\n\n    Args:\n        config: ZMQTCPTransportConfig object with configuration parameters\n    \"\"\"\n    super().__init__(config or ZMQTCPConfig())\n</code></pre>"},{"location":"api/#aiperfzmqzmq_defaults","title":"aiperf.zmq.zmq_defaults","text":""},{"location":"api/#aiperf.zmq.zmq_defaults.TOPIC_DELIMITER","title":"<code>TOPIC_DELIMITER = '.'</code>  <code>module-attribute</code>","text":"<p>The delimiter between topic parts. This is used to create an inverted hierarchy of topics for filtering by service type or service id.</p> <p>For example: - \"command\" - \"system_controller.command\" - \"timing_manager_eff34565.command\"</p>"},{"location":"api/#aiperf.zmq.zmq_defaults.TOPIC_END","title":"<code>TOPIC_END = '$'</code>  <code>module-attribute</code>","text":"<p>This is used to add to the end of each topic to prevent the topic from being a prefix of another topic. This is required for the PUB/SUB pattern to work correctly, otherwise topics like \"command_response\" will be received by the \"command\" subscriber as well.</p> <p>For example: - \"command$\" - \"command_response$\"</p>"},{"location":"api/#aiperf.zmq.zmq_defaults.TOPIC_END_ENCODED","title":"<code>TOPIC_END_ENCODED = TOPIC_END.encode()</code>  <code>module-attribute</code>","text":"<p>The encoded version of TOPIC_END.</p>"},{"location":"api/#aiperf.zmq.zmq_defaults.ZMQSocketDefaults","title":"<code>ZMQSocketDefaults</code>","text":"<p>Default values for ZMQ sockets.</p> Source code in <code>aiperf/zmq/zmq_defaults.py</code> <pre><code>class ZMQSocketDefaults:\n    \"\"\"Default values for ZMQ sockets.\"\"\"\n\n    # Socket Options\n    RCVTIMEO = 300000  # 5 minutes\n    SNDTIMEO = 300000  # 5 minutes\n    TCP_KEEPALIVE = 1\n    TCP_KEEPALIVE_IDLE = 60\n    TCP_KEEPALIVE_INTVL = 10\n    TCP_KEEPALIVE_CNT = 3\n    IMMEDIATE = 1  # Don't queue messages\n    LINGER = 0  # Don't wait on close\n\n    # High Water Mark\n    # TODO: Investigate better ways to handle this\n    # https://zeromq.org/socket-api/#high-water-mark\n    # NOTE: We set these to 0 to allow for unlimited messages to be queued. This is important to\n    #       ensure that the system does not lose messages. It does however mean that the system\n    #       could run out of memory if too many messages are queued.\n    SNDHWM = 0  # No send high water mark (unlimited)\n    RCVHWM = 0  # No receive high water mark (unlimited)\n</code></pre>"},{"location":"api/#aiperfzmqzmq_proxy_base","title":"aiperf.zmq.zmq_proxy_base","text":""},{"location":"api/#aiperf.zmq.zmq_proxy_base.BaseZMQProxy","title":"<code>BaseZMQProxy</code>","text":"<p>               Bases: <code>AIPerfLifecycleMixin</code>, <code>ABC</code></p> <p>A Base ZMQ Proxy class.</p> <ul> <li>Frontend and backend sockets forward messages bidirectionally<ul> <li>Frontend and Backend sockets both BIND</li> </ul> </li> <li>Multiple clients CONNECT to <code>frontend_address</code></li> <li>Multiple services CONNECT to <code>backend_address</code></li> <li>Control: Optional REP socket for proxy commands (start/stop/pause) - not implemented yet</li> <li>Monitoring: Optional PUB socket that broadcasts copies of all forwarded messages - not implemented yet</li> <li>Proxy runs in separate thread to avoid blocking main event loop</li> </ul> Source code in <code>aiperf/zmq/zmq_proxy_base.py</code> <pre><code>class BaseZMQProxy(AIPerfLifecycleMixin, ABC):\n    \"\"\"\n    A Base ZMQ Proxy class.\n\n    - Frontend and backend sockets forward messages bidirectionally\n        - Frontend and Backend sockets both BIND\n    - Multiple clients CONNECT to `frontend_address`\n    - Multiple services CONNECT to `backend_address`\n    - Control: Optional REP socket for proxy commands (start/stop/pause) - not implemented yet\n    - Monitoring: Optional PUB socket that broadcasts copies of all forwarded messages - not implemented yet\n    - Proxy runs in separate thread to avoid blocking main event loop\n    \"\"\"\n\n    def __init__(\n        self,\n        frontend_socket_class: type[BaseZMQClient],\n        backend_socket_class: type[BaseZMQClient],\n        zmq_proxy_config: BaseZMQProxyConfig,\n        socket_ops: dict | None = None,\n        proxy_uuid: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the ZMQ Proxy. This is a base class for all ZMQ Proxies.\n\n        Args:\n            frontend_socket_class (type[BaseZMQClient]): The frontend socket class.\n            backend_socket_class (type[BaseZMQClient]): The backend socket class.\n            zmq_proxy_config (BaseZMQProxyConfig): The ZMQ proxy configuration.\n            socket_ops (dict, optional): Additional socket options to set.\n            proxy_uuid (str, optional): An optional UUID for the proxy instance. If not provided,\n                a new UUID will be generated. This is useful for tracing and debugging purposes.\n        \"\"\"\n\n        self.proxy_uuid = proxy_uuid or uuid.uuid4().hex[:8]\n        self.proxy_id = f\"{self.__class__.__name__.lower()}_{self.proxy_uuid}\"\n        super().__init__()\n        self.context = zmq.asyncio.Context.instance()\n        self.socket_ops = socket_ops\n\n        self.monitor_task: asyncio.Task | None = None\n        self.proxy_task: asyncio.Task | None = None\n        self.control_client: ProxySocketClient | None = None\n        self.capture_client: ProxySocketClient | None = None\n\n        self.frontend_address = zmq_proxy_config.frontend_address\n        self.backend_address = zmq_proxy_config.backend_address\n        self.control_address = zmq_proxy_config.control_address\n        self.capture_address = zmq_proxy_config.capture_address\n\n        self.debug(\n            lambda: f\"Proxy Initializing - Frontend: {self.frontend_address}, Backend: {self.backend_address}\"\n        )\n\n        self.backend_socket = backend_socket_class(\n            address=self.backend_address,\n            socket_ops=self.socket_ops,\n            proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n        )  # type: ignore\n\n        self.frontend_socket = frontend_socket_class(\n            address=self.frontend_address,\n            socket_ops=self.socket_ops,\n            proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n        )  # type: ignore\n\n        if self.control_address:\n            self.debug(lambda: f\"Proxy Control - Address: {self.control_address}\")\n            self.control_client = ProxySocketClient(\n                socket_type=SocketType.REP,\n                address=self.control_address,\n                socket_ops=self.socket_ops,\n                end_type=ProxyEndType.Control,\n                proxy_uuid=self.proxy_uuid,\n            )\n\n        if self.capture_address:\n            self.debug(lambda: f\"Proxy Capture - Address: {self.capture_address}\")\n            self.capture_client = ProxySocketClient(\n                socket_type=SocketType.PUB,\n                address=self.capture_address,\n                socket_ops=self.socket_ops,\n                end_type=ProxyEndType.Capture,\n                proxy_uuid=self.proxy_uuid,\n            )\n\n    @classmethod\n    @abstractmethod\n    def from_config(\n        cls,\n        config: BaseZMQProxyConfig | None,\n        socket_ops: dict | None = None,\n    ) -&gt; \"BaseZMQProxy | None\":\n        \"\"\"Create a BaseZMQProxy from a BaseZMQProxyConfig, or None if not provided.\"\"\"\n        ...\n\n    @on_init\n    async def _initialize(self) -&gt; None:\n        \"\"\"Initialize and start the BaseZMQProxy.\"\"\"\n        self.debug(\"Proxy Initializing Sockets...\")\n        self.debug(\n            lambda: f\"Frontend {self.frontend_socket.socket_type.name} socket binding to: {self.frontend_address} (for {self.backend_socket.socket_type.name} clients)\"\n        )\n        self.debug(\n            lambda: f\"Backend {self.backend_socket.socket_type.name} socket binding to: {self.backend_address} (for {self.frontend_socket.socket_type.name} services)\"\n        )\n\n        try:\n            exceptions = await asyncio.gather(\n                self.backend_socket.initialize(),\n                self.frontend_socket.initialize(),\n                *[\n                    client.initialize()\n                    for client in [self.control_client, self.capture_client]\n                    if client\n                ],\n                return_exceptions=True,\n            )\n            if any(exceptions):\n                self.exception(f\"Proxy Socket Initialization Failed: {exceptions}\")\n                raise\n\n            self.debug(\"Proxy Sockets Initialized Successfully\")\n\n            if self.control_client:\n                self.debug(lambda: f\"Control socket bound to: {self.control_address}\")\n            if self.capture_client:\n                self.debug(lambda: f\"Capture socket bound to: {self.capture_address}\")\n\n        except Exception as e:\n            self.exception(f\"Proxy Socket Initialization Failed: {e}\")\n            raise\n\n    @on_start\n    async def _start_proxy(self) -&gt; None:\n        \"\"\"Start the Base ZMQ Proxy.\n\n        This method starts the proxy and waits for it to complete asynchronously.\n        The proxy forwards messages between the frontend and backend sockets.\n\n        Raises:\n            ProxyError: If the proxy produces an error.\n        \"\"\"\n        self.debug(\"Starting Proxy...\")\n\n        self.proxy_task = asyncio.create_task(\n            asyncio.to_thread(\n                zmq.proxy_steerable,\n                self.frontend_socket.socket,\n                self.backend_socket.socket,\n                capture=self.capture_client.socket if self.capture_client else None,\n                control=self.control_client.socket if self.control_client else None,\n            )\n        )\n\n    @background_task(immediate=True, interval=None)\n    async def _monitor_messages(self) -&gt; None:\n        \"\"\"Monitor messages flowing through the proxy via the capture socket.\"\"\"\n        if not self.capture_client or not self.capture_address:\n            self.debug(\"Proxy Monitor Not Enabled\")\n            return\n\n        self.debug(\n            lambda: f\"Proxy Monitor Starting - Capture Address: {self.capture_address}\"\n        )\n\n        capture_socket = self.context.socket(SocketType.SUB)\n        capture_socket.connect(self.capture_address)\n        self.debug(\n            lambda: f\"Proxy Monitor Connected to Capture Address: {self.capture_address}\"\n        )\n        capture_socket.setsockopt(zmq.SUBSCRIBE, b\"\")  # Subscribe to all messages\n        self.debug(\"Proxy Monitor Subscribed to all messages\")\n\n        try:\n            while not self.stop_requested:\n                recv_msg = await capture_socket.recv_multipart()\n                self.debug(lambda msg=recv_msg: f\"Proxy Monitor Received: {msg}\")\n        except Exception as e:\n            self.exception(f\"Proxy Monitor Error - {e}\")\n            raise\n        except asyncio.CancelledError:\n            return\n        finally:\n            capture_socket.close()\n\n    @on_stop\n    async def _stop_proxy(self) -&gt; None:\n        \"\"\"Shutdown the BaseZMQProxy.\"\"\"\n        self.debug(\"Proxy Stopping...\")\n        if self.proxy_task:\n            self.proxy_task.cancel()\n            self.proxy_task = None\n        await self.frontend_socket.stop()\n        await self.backend_socket.stop()\n        if self.control_client:\n            await self.control_client.stop()\n        if self.capture_client:\n            await self.capture_client.stop()\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_proxy_base.BaseZMQProxy.__init__","title":"<code>__init__(frontend_socket_class, backend_socket_class, zmq_proxy_config, socket_ops=None, proxy_uuid=None)</code>","text":"<p>Initialize the ZMQ Proxy. This is a base class for all ZMQ Proxies.</p> <p>Parameters:</p> Name Type Description Default <code>frontend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The frontend socket class.</p> required <code>backend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The backend socket class.</p> required <code>zmq_proxy_config</code> <code>BaseZMQProxyConfig</code> <p>The ZMQ proxy configuration.</p> required <code>socket_ops</code> <code>dict</code> <p>Additional socket options to set.</p> <code>None</code> <code>proxy_uuid</code> <code>str</code> <p>An optional UUID for the proxy instance. If not provided, a new UUID will be generated. This is useful for tracing and debugging purposes.</p> <code>None</code> Source code in <code>aiperf/zmq/zmq_proxy_base.py</code> <pre><code>def __init__(\n    self,\n    frontend_socket_class: type[BaseZMQClient],\n    backend_socket_class: type[BaseZMQClient],\n    zmq_proxy_config: BaseZMQProxyConfig,\n    socket_ops: dict | None = None,\n    proxy_uuid: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the ZMQ Proxy. This is a base class for all ZMQ Proxies.\n\n    Args:\n        frontend_socket_class (type[BaseZMQClient]): The frontend socket class.\n        backend_socket_class (type[BaseZMQClient]): The backend socket class.\n        zmq_proxy_config (BaseZMQProxyConfig): The ZMQ proxy configuration.\n        socket_ops (dict, optional): Additional socket options to set.\n        proxy_uuid (str, optional): An optional UUID for the proxy instance. If not provided,\n            a new UUID will be generated. This is useful for tracing and debugging purposes.\n    \"\"\"\n\n    self.proxy_uuid = proxy_uuid or uuid.uuid4().hex[:8]\n    self.proxy_id = f\"{self.__class__.__name__.lower()}_{self.proxy_uuid}\"\n    super().__init__()\n    self.context = zmq.asyncio.Context.instance()\n    self.socket_ops = socket_ops\n\n    self.monitor_task: asyncio.Task | None = None\n    self.proxy_task: asyncio.Task | None = None\n    self.control_client: ProxySocketClient | None = None\n    self.capture_client: ProxySocketClient | None = None\n\n    self.frontend_address = zmq_proxy_config.frontend_address\n    self.backend_address = zmq_proxy_config.backend_address\n    self.control_address = zmq_proxy_config.control_address\n    self.capture_address = zmq_proxy_config.capture_address\n\n    self.debug(\n        lambda: f\"Proxy Initializing - Frontend: {self.frontend_address}, Backend: {self.backend_address}\"\n    )\n\n    self.backend_socket = backend_socket_class(\n        address=self.backend_address,\n        socket_ops=self.socket_ops,\n        proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n    )  # type: ignore\n\n    self.frontend_socket = frontend_socket_class(\n        address=self.frontend_address,\n        socket_ops=self.socket_ops,\n        proxy_uuid=self.proxy_uuid,  # Pass the proxy UUID for tracing\n    )  # type: ignore\n\n    if self.control_address:\n        self.debug(lambda: f\"Proxy Control - Address: {self.control_address}\")\n        self.control_client = ProxySocketClient(\n            socket_type=SocketType.REP,\n            address=self.control_address,\n            socket_ops=self.socket_ops,\n            end_type=ProxyEndType.Control,\n            proxy_uuid=self.proxy_uuid,\n        )\n\n    if self.capture_address:\n        self.debug(lambda: f\"Proxy Capture - Address: {self.capture_address}\")\n        self.capture_client = ProxySocketClient(\n            socket_type=SocketType.PUB,\n            address=self.capture_address,\n            socket_ops=self.socket_ops,\n            end_type=ProxyEndType.Capture,\n            proxy_uuid=self.proxy_uuid,\n        )\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_proxy_base.BaseZMQProxy.from_config","title":"<code>from_config(config, socket_ops=None)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create a BaseZMQProxy from a BaseZMQProxyConfig, or None if not provided.</p> Source code in <code>aiperf/zmq/zmq_proxy_base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(\n    cls,\n    config: BaseZMQProxyConfig | None,\n    socket_ops: dict | None = None,\n) -&gt; \"BaseZMQProxy | None\":\n    \"\"\"Create a BaseZMQProxy from a BaseZMQProxyConfig, or None if not provided.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_proxy_base.ProxySocketClient","title":"<code>ProxySocketClient</code>","text":"<p>               Bases: <code>BaseZMQClient</code></p> <p>A ZMQ Proxy socket client class that extends BaseZMQClient.</p> <p>This class is used to create proxy sockets for the frontend, backend, capture, and control endpoint types of a ZMQ Proxy.</p> Source code in <code>aiperf/zmq/zmq_proxy_base.py</code> <pre><code>class ProxySocketClient(BaseZMQClient):\n    \"\"\"A ZMQ Proxy socket client class that extends BaseZMQClient.\n\n    This class is used to create proxy sockets for the frontend, backend, capture, and control\n    endpoint types of a ZMQ Proxy.\n    \"\"\"\n\n    def __init__(\n        self,\n        socket_type: SocketType,\n        address: str,\n        end_type: ProxyEndType,\n        socket_ops: dict | None = None,\n        proxy_uuid: str | None = None,\n    ) -&gt; None:\n        self.client_id = f\"proxy_{end_type}_{socket_type.name.lower()}_{proxy_uuid or uuid.uuid4().hex[:8]}\"\n        super().__init__(\n            socket_type,\n            address,\n            bind=True,\n            socket_ops=socket_ops,\n            client_id=self.client_id,\n        )\n        self.debug(\n            lambda: f\"ZMQ Proxy {end_type.name} {socket_type.name} - Address: {address}\"\n        )\n</code></pre>"},{"location":"api/#aiperfzmqzmq_proxy_sockets","title":"aiperf.zmq.zmq_proxy_sockets","text":""},{"location":"api/#aiperf.zmq.zmq_proxy_sockets.ZMQDealerRouterProxy","title":"<code>ZMQDealerRouterProxy = define_proxy_class(ZMQProxyType.DEALER_ROUTER, create_proxy_socket_class(SocketType.ROUTER, ProxyEndType.Frontend), create_proxy_socket_class(SocketType.DEALER, ProxyEndType.Backend))</code>  <code>module-attribute</code>","text":"<p>A ROUTER socket for the proxy's frontend and a DEALER socket for the proxy's backend.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  DEALER   \u2502&lt;\u2500\u2500\u2500&gt;\u2502              PROXY               \u2502&lt;\u2500\u2500\u2500\u2500&gt;\u2502  ROUTER   \u2502 \u2502  Client 1 \u2502     \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502 Service 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 \u2502  ROUTER  \u2502&lt;\u2500\u2500\u2500\u2500\u2500&gt; \u2502  DEALER  \u2502 \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502 \u2502 Frontend \u2502        \u2502 Backend  \u2502 \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  DEALER   \u2502&lt;\u2500\u2500\u2500&gt;\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502&lt;\u2500\u2500\u2500\u2500&gt;\u2502  ROUTER   \u2502 \u2502  Client N \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 Service N \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>The ROUTER frontend socket receives messages from DEALER clients and forwards them through the proxy to ROUTER services. The ZMQ proxy handles the message routing automatically.</p> <p>The DEALER backend socket receives messages from ROUTER services and forwards them through the proxy to DEALER clients. The ZMQ proxy handles the message routing automatically.</p> <p>CRITICAL: This socket must NOT have an identity when used in a proxy configuration, as it needs to be transparent to preserve routing envelopes for proper response forwarding back to original DEALER clients.</p>"},{"location":"api/#aiperf.zmq.zmq_proxy_sockets.ZMQPushPullProxy","title":"<code>ZMQPushPullProxy = define_proxy_class(ZMQProxyType.PUSH_PULL, create_proxy_socket_class(SocketType.PULL, ProxyEndType.Frontend), create_proxy_socket_class(SocketType.PUSH, ProxyEndType.Backend))</code>  <code>module-attribute</code>","text":"<p>A PULL socket for the proxy's frontend and a PUSH socket for the proxy's backend.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   PUSH    \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502              PROXY              \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502   PULL    \u2502 \u2502  Client 1 \u2502      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502      \u2502 Service 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 \u2502   PULL   \u2502\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502   PUSH   \u2502 \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 \u2502 Frontend \u2502       \u2502 Backend  \u2502 \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   PUSH    \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2500\u2500\u2500\u2500\u2500&gt;\u2502   PULL    \u2502 \u2502  Client N \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 Service N \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>The PULL frontend socket receives messages from PUSH clients and forwards them through the proxy to PUSH services. The ZMQ proxy handles the message routing automatically.</p> <p>The PUSH backend socket forwards messages from the proxy to PULL services. The ZMQ proxy handles the message routing automatically.</p>"},{"location":"api/#aiperf.zmq.zmq_proxy_sockets.ZMQXPubXSubProxy","title":"<code>ZMQXPubXSubProxy = define_proxy_class(ZMQProxyType.XPUB_XSUB, create_proxy_socket_class(SocketType.XSUB, ProxyEndType.Frontend), create_proxy_socket_class(SocketType.XPUB, ProxyEndType.Backend))</code>  <code>module-attribute</code>","text":"<p>An XSUB socket for the proxy's frontend and an XPUB socket for the proxy's backend.</p> <p>ASCII Diagram: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUB    \u2502\u2500\u2500\u2500&gt;\u2502              PROXY              \u2502\u2500\u2500\u2500&gt;\u2502    SUB    \u2502 \u2502  Client 1 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 Service 1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u2502   XSUB   \u2502\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502   XPUB   \u2502 \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 \u2502 Frontend \u2502       \u2502 Backend  \u2502 \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    PUB    \u2502\u2500\u2500\u2500&gt;\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\u2500\u2500\u2500&gt;\u2502    SUB    \u2502 \u2502  Client N \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Service N \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>The XSUB frontend socket receives messages from PUB clients and forwards them through the proxy to XPUB services. The ZMQ proxy handles the message routing automatically.</p> <p>The XPUB backend socket forwards messages from the proxy to SUB services. The ZMQ proxy handles the message routing automatically.</p>"},{"location":"api/#aiperf.zmq.zmq_proxy_sockets.create_proxy_socket_class","title":"<code>create_proxy_socket_class(socket_type, end_type)</code>","text":"<p>Create a proxy socket class using the specified socket type. This is used to reduce the boilerplate code required to create a ZMQ Proxy class.</p> Source code in <code>aiperf/zmq/zmq_proxy_sockets.py</code> <pre><code>def create_proxy_socket_class(\n    socket_type: SocketType,\n    end_type: ProxyEndType,\n) -&gt; type[BaseZMQClient]:\n    \"\"\"Create a proxy socket class using the specified socket type. This is used to\n    reduce the boilerplate code required to create a ZMQ Proxy class.\n    \"\"\"\n\n    class_name = f\"ZMQProxy{end_type.name}Socket{socket_type.name}\"\n\n    class ProxySocket(ProxySocketClient):\n        \"\"\"A ZMQ Proxy socket class with a specific socket type.\"\"\"\n\n        def __init__(\n            self,\n            address: str,\n            socket_ops: dict | None = None,\n            proxy_uuid: str | None = None,\n        ):\n            \"\"\"Initialize the ZMQ Proxy socket class.\"\"\"\n\n            super().__init__(\n                socket_type,\n                address,\n                end_type=end_type,\n                socket_ops=socket_ops,\n                proxy_uuid=proxy_uuid,\n            )\n\n        @on_init\n        async def _initialize_socket(self) -&gt; None:\n            \"\"\"Initialize the socket with proper configuration for XPUB/XSUB proxy.\"\"\"\n            if self.socket_type == SocketType.XPUB:\n                self.socket.setsockopt(zmq.XPUB_VERBOSE, 1)\n                self.debug(\n                    lambda: \"XPUB socket configured with XPUB_VERBOSE=1 for subscription forwarding\"\n                )\n\n    # Dynamically set the class name and qualname based on the socket and end type\n    ProxySocket.__name__ = class_name\n    ProxySocket.__qualname__ = class_name\n    ProxySocket.__doc__ = f\"A ZMQ Proxy {end_type.name} socket implementation.\"\n    return ProxySocket\n</code></pre>"},{"location":"api/#aiperf.zmq.zmq_proxy_sockets.define_proxy_class","title":"<code>define_proxy_class(proxy_type, frontend_socket_class, backend_socket_class)</code>","text":"<p>This function reduces the boilerplate code required to create a ZMQ Proxy class. It will generate a ZMQ Proxy class and register it with the ZMQProxyFactory.</p> <p>Parameters:</p> Name Type Description Default <code>proxy_type</code> <code>ZMQProxyType</code> <p>The type of proxy to generate.</p> required <code>frontend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The class of the frontend socket.</p> required <code>backend_socket_class</code> <code>type[BaseZMQClient]</code> <p>The class of the backend socket.</p> required Source code in <code>aiperf/zmq/zmq_proxy_sockets.py</code> <pre><code>def define_proxy_class(\n    proxy_type: ZMQProxyType,\n    frontend_socket_class: type[BaseZMQClient],\n    backend_socket_class: type[BaseZMQClient],\n) -&gt; type[BaseZMQProxy]:\n    \"\"\"This function reduces the boilerplate code required to create a ZMQ Proxy class.\n    It will generate a ZMQ Proxy class and register it with the ZMQProxyFactory.\n\n    Args:\n        proxy_type: The type of proxy to generate.\n        frontend_socket_class: The class of the frontend socket.\n        backend_socket_class: The class of the backend socket.\n    \"\"\"\n\n    class ZMQProxy(BaseZMQProxy):\n        \"\"\"\n        A Generated ZMQ Proxy class.\n\n        This class is responsible for creating the ZMQ proxy that forwards messages\n        between frontend and backend sockets.\n        \"\"\"\n\n        def __init__(\n            self,\n            zmq_proxy_config: BaseZMQProxyConfig,\n            socket_ops: dict | None = None,\n        ) -&gt; None:\n            super().__init__(\n                frontend_socket_class=frontend_socket_class,\n                backend_socket_class=backend_socket_class,\n                zmq_proxy_config=zmq_proxy_config,\n                socket_ops=socket_ops,\n            )\n\n        @classmethod\n        def from_config(\n            cls,\n            config: BaseZMQProxyConfig | None,\n            socket_ops: dict | None = None,\n        ) -&gt; \"ZMQProxy | None\":\n            if config is None:\n                return None\n            return cls(\n                zmq_proxy_config=config,\n                socket_ops=socket_ops,\n            )\n\n    # Dynamically set the class name and qualname based on the proxy type\n    ZMQProxy.__name__ = f\"ZMQ_{proxy_type.name}_Proxy\"\n    ZMQProxy.__qualname__ = ZMQProxy.__name__\n    ZMQProxy.__doc__ = f\"A ZMQ Proxy for {proxy_type.name} communication.\"\n    ZMQProxyFactory.register(proxy_type)(ZMQProxy)\n    return ZMQProxy\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#architecture-of-aiperf","title":"Architecture of AIPerf","text":""},{"location":"architecture/#aiperf-architecture-overview","title":"AIPerf Architecture Overview","text":"<p>AIPerf is designed as a modular, extensible benchmarking framework for generative AI models. Its architecture separates concerns across several core components, enabling flexible configuration, scalable load generation, and robust metric collection. The system supports both local and distributed (Kubernetes) execution, and can be easily extended for new models, endpoints, and benchmarking scenarios.</p>"},{"location":"architecture/#system-controller","title":"System Controller","text":"<p>The System Controller is the central orchestrator that manages the lifecycle and coordination of all major modules involved in a benchmarking run. Its main functions include:</p> <ul> <li>Registering and initializing core components.</li> <li>Orchestrating the start, execution, and shutdown of benchmarking tasks.</li> <li>Handling configuration, resource allocation, and inter-module communication.</li> <li>Monitoring the overall progress and health of the benchmarking process.</li> <li>Managing error handling, cleanup, and graceful termination of all modules.</li> </ul>"},{"location":"architecture/#dataset-manager","title":"Dataset Manager","text":"<p>This is responsible for handling all aspects of input data management during benchmarking runs. Its main functions include:</p> <ul> <li>Loading datasets from various sources, such as files (JSONL, CSV), synthetic generators, or trace replay formats.</li> <li>Parsing and validating input data to ensure it matches the expected format for benchmarking.</li> <li>Providing batches or individual samples to the benchmarking workers according to the configured load pattern (e.g., concurrency, request-rate, trace replay).</li> <li>Supporting custom dataset types, such as MoonCake traces, for advanced benchmarking scenarios.</li> <li>Managing the lifecycle of datasets, including initialization, iteration, and cleanup.</li> </ul>"},{"location":"architecture/#timing-manager","title":"Timing Manager","text":"<p>This is responsible for controlling and coordinating the timing of requests during benchmarking runs. Its main functions include:</p> <ul> <li>Scheduling when each request should be sent, based on the selected benchmarking mode (e.g., fixed concurrency, request-rate, or trace replay).</li> <li>Managing precise timing to accurately reproduce real-world or synthetic load patterns.</li> <li>Supporting advanced timing scenarios, such as replaying traces with specific inter-arrival times or simulating bursty traffic.</li> <li>Ensuring that requests are dispatched to workers at the correct intervals, enabling reliable measurement of latency and throughput.</li> <li>Providing timing data and statistics for analysis and reporting.</li> </ul>"},{"location":"architecture/#worker-manager","title":"Worker Manager","text":"<p>This is responsible for orchestrating and managing the pool of worker processes that execute benchmarking tasks. Its main functions include:</p> <ul> <li>Coordinating with the system controller to spawn and shut down workers that send requests to the inference server.</li> <li>Monitoring worker status, progress, and resource usage.</li> <li>Handling worker lifecycle events, such as startup, shutdown, and error recovery.</li> </ul>"},{"location":"architecture/#worker","title":"Worker","text":"<p>This is responsible for executing individual benchmarking tasks. Each worker operates as a process that sends requests to the inference server, collects responses, and records performance metrics. Its main functions include:</p> <ul> <li>Pulling timing credits from the timing manager.</li> <li>Pulling data from the dataset manager for a request.</li> <li>Formatting the data for the endpoint.</li> <li>Sending requests to the target endpoint according to the specified schedule.</li> <li>Recording request and response timestamps.</li> <li>Reporting results to the record processors for aggregation and analysis.</li> </ul>"},{"location":"architecture/#record-processor","title":"Record Processor","text":"<p>This is responsible for processing and interpreting the responses received from the inference server during benchmarking. Its main functions include:</p> <ul> <li>Parsing raw inference results to extract relevant metrics, such as latency, output tokens, and correctness.</li> <li>Handling different response formats from various model endpoints (e.g., OpenAI, vLLM, Triton, custom APIs).</li> <li>Validating and normalizing results to ensure consistency across benchmarking runs.</li> <li>Preparing parsed data for further analysis, aggregation, and reporting by other modules (such as the records manager).</li> <li>Computing the metrics derived from individual requests.</li> <li>Supporting error detection and handling for malformed or unexpected responses.</li> </ul>"},{"location":"architecture/#records-manager","title":"Records Manager","text":"<p>This is responsible for managing the collection, organization, and storage of benchmarking records and results. It acts as a central component for handling the data generated during benchmarking runs, such as inference results, timing information, and other metrics. Its main functions include:</p> <ul> <li>Aggregating data from the records processors, such as inference results, timing information, and metrics.</li> <li>Storing records in memory and/or exporting them to files (e.g., CSV, JSON) for later analysis.</li> <li>Providing interfaces for querying, filtering, and summarizing benchmarking results.</li> <li>Supporting the generation of reports and artifacts for performance evaluation.</li> </ul>"},{"location":"architecture/#gpu-telemetry-coming-soon","title":"GPU Telemetry (Coming soon)","text":"<p>This connects to a metrics endpoint to gather GPU metrics. This requires DCGM Exporter installed on the server machine.</p>"},{"location":"architecture/#inference-server","title":"Inference Server","text":"<p>This is the endpoint that AIPerf targets to generate benchmarking load.</p>"},{"location":"benchmark_datasets/","title":"Benchmark datasets","text":"<p>This document describes datasets that AIPerf can use to generate stimulus. Additional support is under development, so check back often.</p>"},{"location":"benchmark_datasets/#dataset-options","title":"Dataset Options","text":"Dataset Support Data Source Synthetic Text \u2705 Synthetically generated text prompts pulled from Shakespeare Synthetic Audio \u2705 Synthetically generated audio samples Synthetic Images \u2705 Synthetically generated image samples Custom Data \u2705 --input-file your_file.jsonl --custom-dataset-type single-turn Mooncake \u2705 Mooncake trace file <code>--input-file your_trace_file.jsonl --custom-dataset-type mooncake_trace</code> ShareGPT \u2705 Conversations from <code>--public-dataset sharegpt</code>"},{"location":"cli_options/","title":"Cli options","text":""},{"location":"cli_options/#cli-options","title":"CLI Options","text":"<p>Use these options to profile with AIPerf.</p> <pre><code>\u256d\u2500 Endpoint \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  MODEL-NAMES --model-names --model                    -m  Model name(s) to be benchmarked. Can be a comma-separated list or a single model name. [required]                         \u2502\n\u2502    MODEL-SELECTION-STRATEGY --model-selection-strategy      When multiple models are specified, this is how a specific model should be assigned to a prompt. round_robin: nth prompt  \u2502\n\u2502                                                             in the list gets assigned to n-mod len(models). random: assignment is uniformly random [choices: round-robin, random]     \u2502\n\u2502                                                             [default: round-robin]                                                                                                    \u2502\n\u2502    CUSTOM-ENDPOINT --custom-endpoint --endpoint             Set a custom endpoint that differs from the OpenAI defaults.                                                              \u2502\n\u2502    ENDPOINT-TYPE --endpoint-type                            The endpoint type to send requests to on the server. [choices: openai-chat-completions, openai-completions,               \u2502\n\u2502                                                             openai-embeddings, openai-responses] [default: openai-chat-completions]                                                   \u2502\n\u2502    STREAMING --streaming                                    An option to enable the use of the streaming API. [default: False]                                                        \u2502\n\u2502    URL --url                                            -u  URL of the endpoint to target for benchmarking. [default: localhost:8000]                                                 \u2502\n\u2502    REQUEST-TIMEOUT-SECONDS --request-timeout-seconds        The timeout in floating-point seconds for each request to the endpoint. [default: 600.0]                                 \u2502\n\u2502    API-KEY --api-key                                        The API key to use for the endpoint. If provided, it will be sent with every request as a header: Authorization: Bearer   \u2502\n\u2502                                                             &lt;api_key&gt;.                                                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 EXTRA-INPUTS --extra-inputs                                    Provide additional inputs to include with every request. Inputs should be in an 'input_name:value' format.             \u2502\n\u2502                                                                Alternatively, a string representing a json formatted dictionary can be provided. [default: []]                              \u2502\n\u2502 HEADER --header                                            -H  Adds a custom header to the requests. Headers must be specified as 'Header:Value' pairs. Alternatively, a string       \u2502\n\u2502                                                                representing a json formatted dictionary can be provided. [default: []]                                                      \u2502\n\u2502 INPUT-FILE --input-file                                        The file or directory path that contains the dataset to use for profiling. This parameter is used in conjunction with  \u2502\n\u2502                                                                the custom_dataset_type parameter to support different types of user provided datasets.                                \u2502\n\u2502 FIXED-SCHEDULE --fixed-schedule                                Specifies to run a fixed schedule of requests. This is normally inferred from the --input-file parameter, but can be   \u2502\n\u2502                                                                set manually here. [default: False]                                                                                    \u2502\n\u2502 FIXED-SCHEDULE-AUTO-OFFSET --fixed-schedule-auto-offset        Specifies to automatically offset the timestamps in the fixed schedule, such that the first timestamp is considered 0, \u2502\n\u2502                                                                and the rest are shifted accordingly. If disabled, the timestamps will be assumed to be relative to 0. [default:       \u2502\n\u2502                                                                False]                                                                                                                 \u2502\n\u2502 FIXED-SCHEDULE-START-OFFSET --fixed-schedule-start-offset      Specifies the offset in milliseconds to start the fixed schedule at. By default, the schedule starts at 0, but this    \u2502\n\u2502                                                                option can be used to start at a reference point further in the schedule. This option cannot be used in conjunction    \u2502\n\u2502                                                                with the --fixed-schedule-auto-offset. The schedule will include any requests at the start offset.                     \u2502\n\u2502 FIXED-SCHEDULE-END-OFFSET --fixed-schedule-end-offset          Specifies the offset in milliseconds to end the fixed schedule at. By default, the schedule ends at the last timestamp \u2502\n\u2502                                                                in the trace dataset, but this option can be used to only run a subset of the trace. The schedule will include any     \u2502\n\u2502                                                                requests at the end offset.                                                                                            \u2502\n\u2502 CUSTOM-DATASET-TYPE --custom-dataset-type                      The type of custom dataset to use. This parameter is used in conjunction with the --file parameter. [choices:          \u2502\n\u2502                                                                single-turn, multi-turn, random-pool, mooncake-trace] [default: mooncake-trace]                                        \u2502\n\u2502 RANDOM-SEED --random-seed                                      The seed used to generate random values. Set to some value to make the synthetic data generation deterministic. It     \u2502\n\u2502                                                                will use system default if not provided.                                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 OUTPUT-ARTIFACT-DIR --output-artifact-dir --artifact-dir  The directory to store all the (output) artifacts generated by AIPerf. [default: artifacts]                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Tokenizer \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 TOKENIZER --tokenizer                                      The Hugging Face tokenizer to use to interpret token metrics from prompts and responses. The value can be the name of a     \u2502\n\u2502                                                            tokenizer or the filepath of the tokenizer. The default value is the model name.                                           \u2502\n\u2502 TOKENIZER-REVISION --tokenizer-revision                    The specific model version to use. It can be a branch name, tag name, or commit ID. [default: main]                        \u2502\n\u2502 TOKENIZER-TRUST-REMOTE-CODE --tokenizer-trust-remote-code  Allows custom tokenizer to be downloaded and executed. This carries security risks and should only be used for             \u2502\n\u2502                                                            repositories you trust. This is only necessary for custom tokenizers stored in Hugging Face Hub. [default: False]           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Load Generator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 CONCURRENCY --concurrency                                          The concurrency value to benchmark.                                                                                \u2502\n\u2502 REQUEST-RATE --request-rate                                        Sets the request rate for the load generated by AIPerf. Unit: requests/second                                      \u2502\n\u2502 REQUEST-RATE-MODE --request-rate-mode                              Sets the request rate mode for the load generated by AIPerf. Valid values: constant, poisson. constant: Generate   \u2502\n\u2502                                                                    requests at a fixed rate. poisson: Generate requests using a poisson distribution. [default: poisson]              \u2502\n\u2502 REQUEST-COUNT --request-count --num-requests                       The number of requests to use for measurement. [default: 10]                                                       \u2502\n\u2502 WARMUP-REQUEST-COUNT --warmup-request-count --num-warmup-requests  The number of warmup requests to send before benchmarking. [default: 0]                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Conversation Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 CONVERSATION-NUM --conversation-num --num-conversations          The total number of unique conversations to generate. Each conversation represents a single request session between  \u2502\n\u2502   --num-sessions --num-dataset-entries                           client and server. Supported on synthetic mode and the custom random_pool dataset. The number of conversations will  \u2502\n\u2502                                                                  be used to determine the number of entries in both the custom random_pool and synthetic datasets and will be reused  \u2502\n\u2502                                                                  until benchmarking is complete. [default: 100]                                                                       \u2502\n\u2502 CONVERSATION-TURN-MEAN --conversation-turn-mean                  The mean number of turns within a conversation. [default: 1]                                                         \u2502\n\u2502   --session-turns-mean                                                                                                                                                                \u2502\n\u2502 CONVERSATION-TURN-STDDEV --conversation-turn-stddev              The standard deviation of the number of turns within a conversation. [default: 0]                                    \u2502\n\u2502   --session-turns-stddev                                                                                                                                                              \u2502\n\u2502 CONVERSATION-TURN-DELAY-MEAN --conversation-turn-delay-mean      The mean delay between turns within a conversation in milliseconds. [default: 0.0]                                   \u2502\n\u2502   --session-turn-delay-mean                                                                                                                                                           \u2502\n\u2502 CONVERSATION-TURN-DELAY-STDDEV --conversation-turn-delay-stddev  The standard deviation of the delay between turns within a conversation in milliseconds. [default: 0.0]              \u2502\n\u2502   --session-turn-delay-stddev                                                                                                                                                         \u2502\n\u2502 CONVERSATION-TURN-DELAY-RATIO --conversation-turn-delay-ratio    A ratio to scale multi-turn delays. [default: 1.0]                                                                   \u2502\n\u2502   --session-delay-ratio                                                                                                                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Input Sequence Length (ISL) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 PROMPT-INPUT-TOKENS-MEAN --prompt-input-tokens-mean              The mean of number of tokens in the generated prompts when using synthetic data. [default: 550]                      \u2502\n\u2502   --synthetic-input-tokens-mean --isl                                                                                                                                                 \u2502\n\u2502 PROMPT-INPUT-TOKENS-STDDEV --prompt-input-tokens-stddev          The standard deviation of number of tokens in the generated prompts when using synthetic data. [default: 0.0]        \u2502\n\u2502   --synthetic-input-tokens-stddev --isl-stddev                                                                                                                                        \u2502\n\u2502 PROMPT-INPUT-TOKENS-BLOCK-SIZE --prompt-input-tokens-block-size  The block size of the prompt. [default: 512]                                                                         \u2502\n\u2502   --synthetic-input-tokens-block-size --isl-block-size                                                                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Output Sequence Length (OSL) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 PROMPT-OUTPUT-TOKENS-MEAN --prompt-output-tokens-mean      The mean number of tokens in each output.                                                                                  \u2502\n\u2502   --output-tokens-mean --osl                                                                                                                                                          \u2502\n\u2502 PROMPT-OUTPUT-TOKENS-STDDEV --prompt-output-tokens-stddev  The standard deviation of the number of tokens in each output. [default: 0]                                                \u2502\n\u2502   --output-tokens-stddev --osl-stddev                                                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Prompt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 PROMPT-BATCH-SIZE --prompt-batch-size --batch-size-text  -b  The batch size of text requests AIPerf should send. This is currently supported with the embeddings and rankings         \u2502\n\u2502   --batch-size                                               endpoint types [default: 1]                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Prefix Prompt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 PROMPT-PREFIX-POOL-SIZE --prompt-prefix-pool-size  The total size of the prefix prompt pool to select prefixes from. If this value is not zero, these are prompts that are prepended  \u2502\n\u2502   --prefix-prompt-pool-size --num-prefix-prompts   to input prompts. This is useful for benchmarking models that use a K-V cache. [default: 0]                                        \u2502\n\u2502 PROMPT-PREFIX-LENGTH --prompt-prefix-length        The number of tokens in each prefix prompt. This is only used if \"num\" is greater than zero. Note that due to the prefix and user  \u2502\n\u2502   --prefix-prompt-length                           prompts being concatenated, the number of tokens in the final prompt may be off by one. [default: 0]                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Audio Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 AUDIO-BATCH-SIZE --audio-batch-size --batch-size-audio  The batch size of audio requests AIPerf should send. This is currently supported with the OpenAI chat endpoint type [default: \u2502\n\u2502                                                         1]                                                                                                                            \u2502\n\u2502 AUDIO-LENGTH-MEAN --audio-length-mean                   The mean length of the audio in seconds. [default: 0.0]                                                                       \u2502\n\u2502 AUDIO-LENGTH-STDDEV --audio-length-stddev               The standard deviation of the length of the audio in seconds. [default: 0.0]                                                  \u2502\n\u2502 AUDIO-FORMAT --audio-format                             The format of the audio files (wav or mp3). [choices: wav, mp3] [default: wav]                                                \u2502\n\u2502 AUDIO-DEPTHS --audio-depths                             A list of audio bit depths to randomly select from in bits. [default: [16]]                                                   \u2502\n\u2502 AUDIO-SAMPLE-RATES --audio-sample-rates                 A list of audio sample rates to randomly select from in kHz. Common sample rates are 16, 44.1, 48, 96, etc. [default: [16.0]] \u2502\n\u2502 AUDIO-NUM-CHANNELS --audio-num-channels                 The number of audio channels to use for the audio data generation. [default: 1]                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Image Input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 IMAGE-WIDTH-MEAN --image-width-mean                     The mean width of images when generating synthetic image data. [default: 0.0]                                                 \u2502\n\u2502 IMAGE-WIDTH-STDDEV --image-width-stddev                 The standard deviation of width of images when generating synthetic image data. [default: 0.0]                                \u2502\n\u2502 IMAGE-HEIGHT-MEAN --image-height-mean                   The mean height of images when generating synthetic image data. [default: 0.0]                                                \u2502\n\u2502 IMAGE-HEIGHT-STDDEV --image-height-stddev               The standard deviation of height of images when generating synthetic image data. [default: 0.0]                               \u2502\n\u2502 IMAGE-BATCH-SIZE --image-batch-size --batch-size-image  The image batch size of the requests AIPerf should send. This is currently supported with the image retrieval endpoint type.  \u2502\n\u2502                                                         [default: 1]                                                                                                                  \u2502\n\u2502 IMAGE-FORMAT --image-format                             The compression format of the images. [choices: png, jpeg, random] [default: png]                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Service \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 LOG-LEVEL --log-level                                                 Logging level [choices: trace, debug, info, notice, warning, success, error, critical] [default: info]          \u2502\n\u2502 VERBOSE --verbose                                                -v   Equivalent to --log-level DEBUG. Enables more verbose logging output, but lacks some raw message logging.       \u2502\n\u2502                                                                       [default: False]                                                                                                \u2502\n\u2502 EXTRA-VERBOSE --extra-verbose                                    -vv  Equivalent to --log-level TRACE. Enables the most verbose logging output possible. [default: False]             \u2502\n\u2502 RECORD-PROCESSOR-SERVICE-COUNT --record-processor-service-count       Number of services to spawn for processing records. The higher the request rate, the more services should be    \u2502\n\u2502   --record-processors                                                 spawned in order to keep up with the incoming records. If not specified, the number of services will be         \u2502\n\u2502                                                                       automatically determined based on the worker count.                                                             \u2502\n\u2502 UI-TYPE --ui-type --ui                                                Type of UI to use [choices: dashboard, simple, none] [default: dashboard]                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>\u256d\u2500 Workers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 WORKERS-MAX --workers-max --max-workers  Maximum number of workers to create. If not specified, the number of workers will be determined by the smaller of (concurrency + 1) and (num \u2502\n\u2502                                          CPUs - 1).                                                                                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"migrating/","title":"Migrating","text":""},{"location":"migrating/#migrating-from-genai-perf","title":"Migrating from GenAI-Perf","text":"<p>AIPerf is designed to be a drop-in replacement for GenAI-Perf. Most options from GenAI-Perf map directly to AIPerf options. Options that don't are noted below. Some options, primarily for the <code>analyze</code> subcommand, are not yet supported; they're planned for future releases. </p>"},{"location":"migrating/#known-cli-argument-differences","title":"Known CLI Argument Differences","text":"<ul> <li><code>--max-threads</code>: You no longer need to set a max-thread option. Previously, this was a global setting to control GenAI-Perf total thread count. AIPerf provides more-fine grained control of the number of workers issuing requests to the endpoint by using the <code>--workers-max</code> option.</li> <li><code>--</code>: The passthrough args flag is no longer required. All options are now natively supported by AIPerf.</li> </ul> <p>To migrate your previous GenAI-Perf commands to AIPerf commands, remove the above options.</p> <p></p> <p>With these simple updates to your previous scripts, AIPerf can replace your usage of GenAI-Perf.</p>"},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#profiling-with-aiperf","title":"Profiling with AIPerf","text":"<p>This tutorial shows how to measure model performance across different inference solutions using AIPerf.</p>"},{"location":"tutorial/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Profile Qwen3-0.6B Using Dynamo</li> <li>Profile Qwen3-0.6B Using vllm</li> </ul>"},{"location":"tutorial/#profile-qwen3-06b-using-dynamo","title":"Profile Qwen3-0.6B Using Dynamo   <p>[!NOTE] The latest installation instructions for Dynamo are available on Github</p>  <pre><code># Set environment variables\nexport AIPERF_REPO_TAG=\"main\"\nexport DYNAMO_PREBUILT_IMAGE_TAG=\"nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.4.0\"\nexport MODEL=\"Qwen/Qwen3-0.6B\"\n\n# Download the Dyanmo container\ndocker pull ${DYNAMO_PREBUILT_IMAGE_TAG}\n\nexport DYNAMO_REPO_TAG=$(docker run --rm --entrypoint \"\" ${DYNAMO_PREBUILT_IMAGE_TAG} cat /workspace/version.txt | cut -d'+' -f2)\n\n\n# Start up required services\ncurl -O https://raw.githubusercontent.com/ai-dynamo/dynamo/${DYNAMO_REPO_TAG}/deploy/docker-compose.yml\ndocker compose -f docker-compose.yml down || true\ndocker compose -f docker-compose.yml up -d\n\n# Launch Dynamo in the background\ndocker run \\\n  --rm \\\n  --gpus all \\\n  --network host \\\n  ${DYNAMO_PREBUILT_IMAGE_TAG} \\\n    /bin/bash -c \"python3 -m dynamo.frontend &amp; python3 -m dynamo.vllm --model ${MODEL} --enforce-eager --no-enable-prefix-caching\" &gt; server.log 2&gt;&amp;1 &amp;\n\n# Set up AIPerf\ndocker run \\\n  -it \\\n  --rm \\\n  --gpus all \\\n  --network host \\\n  -e AIPERF_REPO_TAG=${AIPERF_REPO_TAG} \\\n  -e MODEL=${MODEL} \\\n  ubuntu:24.04\n\napt update &amp;&amp; apt install -y curl git\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\nsource $HOME/.local/bin/env\n\nuv venv --python 3.10\n\nsource .venv/bin/activate\n\ngit clone -b ${AIPERF_REPO_TAG} --depth 1 https://github.com/ai-dynamo/aiperf.git\n\nuv pip install ./aiperf\n\n# It can take some time for Dynamo to become ready.\n# The following command returns when Dynamo is ready to accept requests.\nwhile [ \"$(curl -s -o /dev/null -w '%{http_code}' localhost:8000/v1/chat/completions -H 'Content-Type: application/json' -d '{\"model\":\"'\"${MODEL}\"'\",\"messages\":[{\"role\":\"user\",\"content\":\"a\"}],\"max_completion_tokens\":1}')\" != \"200\" ]; do sleep 1; done\n\n# Profile the model\naiperf profile \\\n    --model Qwen/Qwen3-0.6B \\\n    --endpoint-type chat \\\n    --endpoint /v1/chat/completions \\\n    --streaming \\\n    --url localhost:8000 \\\n    --synthetic-input-tokens-mean 1000 \\\n    --synthetic-input-tokens-stddev 0 \\\n    --output-tokens-mean 2000 \\\n    --output-tokens-stddev 0 \\\n    --extra-inputs min_tokens:2000 \\\n    --extra-inputs ignore_eos:true \\\n    --concurrency 2048 \\\n    --request-count 6144 \\\n    --warmup-request-count 1000 \\\n    --conversation-num 8000 \\\n    --random-seed 100 \\\n    -v \\\n    -H 'Authorization: Bearer NOT USED' \\\n    -H 'Accept: text/event-stream'\n</code></pre>","text":""},{"location":"tutorial/#profile-qwen3-06b-using-vllm","title":"Profile Qwen3-0.6B Using vLLM  <pre><code># Install vLLM from pip:\npip install vllm\n\n# Load and run the model:\nvllm serve \"Qwen/Qwen3-0.6B\"\n\nuv venv\nsource .venv/bin/activate\nuv pip install git+https://github.com/ai-dynamo/aiperf.git\n\naiperf profile \\\n    --model Qwen/Qwen3-0.6B \\\n    --endpoint-type chat \\\n    --endpoint /v1/chat/completions \\\n    --streaming \\\n    --request-rate 1000 \\\n    --request-count 6500\n</code></pre>","text":""},{"location":"tutorial/#profile-qwen3-06b-using-vllm-and-docker","title":"Profile Qwen3-0.6B Using vLLM and Docker  <pre><code># Install the latest vLLM docker container:\ndocker run \\\n  -it \\\n  --rm \\\n  --gpus all \\\n  --network host \\\n  vllm/vllm-openai:latest \\\n  --model Qwen/Qwen3-0.6B\n\n# In a separate terminal, ensure dependencies are installed\napt update &amp;&amp; apt install -y curl git\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv venv --python 3.10\nsource .venv/bin/activate\n\n# Install and Run AIPerf\nuv pip install git+https://github.com/ai-dynamo/aiperf.git\n\naiperf profile \\\n    --model Qwen/Qwen3-0.6B \\\n    --endpoint-type chat \\\n    --endpoint /v1/chat/completions \\\n    --streaming \\\n    --request-rate 100 \\\n    --request-count 650\n</code></pre>","text":""},{"location":"benchmark_modes/trace_replay/","title":"Trace replay","text":""},{"location":"benchmark_modes/trace_replay/#trace-replay","title":"Trace Replay","text":"<p>This tutorial takes you through an example trace replay profile. Trace Replay benchmarking helps reproduce performance benchmarking results for validation or testing your system under a specific load pattern.</p>"},{"location":"benchmark_modes/trace_replay/#table-of-contents","title":"Table of Contents","text":"<ul> <li>MoonCake Traces</li> <li>Profiling using a MoonCake Trace</li> </ul>"},{"location":"benchmark_modes/trace_replay/#mooncake-traces","title":"MoonCake Traces","text":"<p>Mooncake provides a specification and sample datasets for traces that can be replayed for performance benchmarking.</p> <p>In AIPerf, the trace must be defined in a jsonl file.</p> <p>4 keys are available: - \"timestamp\": the timing of request arrivals - \"input_length\": the number of input tokens - \"output_length\": the number of output tokens - \"hash_ids\": [list of block hashes]</p> <pre><code>{\n    \"timestamp\": 0,\n    \"input_length\": 655,\n    \"output_length\": 52,\n    \"hash_ids\": [46, 47]\n}\n</code></pre>"},{"location":"benchmark_modes/trace_replay/#profiling-using-a-mooncake-trace","title":"Profiling using a MoonCake Trace","text":"<pre><code>echo \\\n'{\"timestamp\": 0, \"input_length\": 655, \"output_length\": 52, \"hash_ids\": [46, 47]}\n{\"timestamp\": 10535, \"input_length\": 672, \"output_length\": 26, \"hash_ids\": [46, 47]}\n{\"timestamp\": 27482, \"input_length\": 655, \"output_length\": 52, \"hash_ids\": [46, 47]}' \\\n&gt; example_trace.jsonl\n\naiperf profile \\\n    -m deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n    --input-file example_trace.jsonl \\\n    --custom-dataset-type mooncake_trace\n</code></pre> <p>The code above will create an example trace file formatted in jsonl. AIPerf will use it to define the dataset and timing to replay the trace.</p>"},{"location":"diagrams/metrics-flow/","title":"Metrics flow","text":"<pre><code>flowchart TD\n    %% Input Data\n    A[\"Parsed Inference Results&lt;br/&gt;&lt;em&gt;(Load&amp;nbsp;Balanced)&lt;/em&gt;\"]\n\n    %% Stage 1: Distributed Record Processing\n    A --&gt; B1[\"MetricRecordProcessor&lt;br/&gt;&lt;em&gt;(Distributed&amp;nbsp;instance&amp;nbsp;1)&lt;/em&gt;\"]\n    A --&gt; B2[\"MetricRecordProcessor&lt;br/&gt;&lt;em&gt;(Distributed&amp;nbsp;instance&amp;nbsp;2)&lt;/em&gt;\"]\n    A --&gt; B3[\"MetricRecordProcessor&lt;br/&gt;&lt;em&gt;(Distributed&amp;nbsp;instance&amp;nbsp;N)&lt;/em&gt;\"]\n\n    %% RECORD Metric Path\n    B1 --&gt; C1[\"RECORD: RequestLatencyMetric&lt;br/&gt;parse_record() \u2192 125ms&lt;br/&gt;&lt;em&gt;(Individual&amp;nbsp;value&amp;nbsp;per&amp;nbsp;request)&lt;/em&gt;\"]\n    B2 --&gt; C2[\"RECORD: RequestLatencyMetric&lt;br/&gt;parse_record() \u2192 87ms&lt;br/&gt;&lt;em&gt;(Individual&amp;nbsp;value&amp;nbsp;per&amp;nbsp;request)&lt;/em&gt;\"]\n    B3 --&gt; C3[\"RECORD: RequestLatencyMetric&lt;br/&gt;parse_record() \u2192 203ms&lt;br/&gt;&lt;em&gt;(Individual&amp;nbsp;value&amp;nbsp;per&amp;nbsp;request)&lt;/em&gt;\"]\n\n    %% AGGREGATE Metric Path\n    B1 --&gt; D1[\"AGGREGATE: TotalRequestsMetric&lt;br/&gt;parse_record() \u2192 +1&lt;br/&gt;&lt;em&gt;(Individual&amp;nbsp;contribution)&lt;/em&gt;\"]\n    B2 --&gt; D2[\"AGGREGATE: TotalRequestsMetric&lt;br/&gt;parse_record() \u2192 +1&lt;br/&gt;&lt;em&gt;(Individual&amp;nbsp;contribution)&lt;/em&gt;\"]\n    B3 --&gt; D3[\"AGGREGATE: TotalRequestsMetric&lt;br/&gt;parse_record() \u2192 +1&lt;br/&gt;&lt;em&gt;(Individual&amp;nbsp;contribution)&lt;/em&gt;\"]\n\n    %% MetricRecordDict Collection\n    C1 --&gt; E1[\"MetricRecordDict&lt;br/&gt;&lt;em&gt;(Per-record&amp;nbsp;results)&lt;/em&gt;\"]\n    D1 --&gt; E1\n    C2 --&gt; E2[\"MetricRecordDict&lt;br/&gt;&lt;em&gt;(Per-record&amp;nbsp;results)&lt;/em&gt;\"]\n    D2 --&gt; E2\n    C3 --&gt; E3[\"MetricRecordDict&lt;br/&gt;&lt;em&gt;(Per-record&amp;nbsp;results)&lt;/em&gt;\"]\n    D3 --&gt; E3\n\n    %% Stage 2: Centralized Results Processing\n    E1 --&gt; G[\"RecordsManager \u2192 MetricResultsProcessor&lt;br/&gt;&lt;em&gt;(Single&amp;nbsp;centralized&amp;nbsp;instance)&lt;/em&gt;\"]\n    E2 --&gt; G\n    E3 --&gt; G\n\n    %% RECORD Processing in Central\n    G --&gt; H1[\"RECORD Collection&lt;br/&gt;append(125ms)&lt;br/&gt;append(87ms)&lt;br/&gt;append(203ms)&lt;br/&gt;&lt;em&gt;(Collect&amp;nbsp;all&amp;nbsp;individual&amp;nbsp;values)&lt;/em&gt;\"]\n\n    %% AGGREGATE Processing in Central\n    G --&gt; H2[\"AGGREGATE Accumulation&lt;br/&gt;aggregate_value(+1) \u2192 total=1&lt;br/&gt;aggregate_value(+1) \u2192 total=2&lt;br/&gt;aggregate_value(+1) \u2192 total=3&lt;br/&gt;&lt;em&gt;(Accumulate&amp;nbsp;across&amp;nbsp;processors)&lt;/em&gt;\"]\n\n    H1 --&gt; L[\"MetricResultsDict&lt;br/&gt;&lt;em&gt;(Full&amp;nbsp;profile&amp;nbsp;run&amp;nbsp;results)&lt;/em&gt;\"]\n    H2 --&gt; L\n\n    %% Stage 4: Summarize Function Processing\n    L --&gt; I2[\"Summarize Function&lt;br/&gt;summarize()&lt;br/&gt;&lt;em&gt;(Process&amp;nbsp;all&amp;nbsp;collected&amp;nbsp;results)&lt;/em&gt;\"]\n\n    %% Three outputs from Summarize Function\n    I2 --&gt; J1[\"RECORD Statistics&lt;br/&gt;p50=125ms, p95=203ms&lt;br/&gt;mean=138ms, std=58ms&lt;br/&gt;min=87ms, max=203ms&lt;br/&gt;&lt;em&gt;(Full&amp;nbsp;statistical&amp;nbsp;analysis)&lt;/em&gt;\"]\n\n    I2 --&gt; J2[\"AGGREGATE Results&lt;br/&gt;final_value=3&lt;br/&gt;count=1&lt;br/&gt;&lt;em&gt;(Single&amp;nbsp;accumulated&amp;nbsp;total)&lt;/em&gt;\"]\n\n    I2 --&gt; J3[\"DERIVED: ThroughputMetric&lt;br/&gt;derive_value(results)&lt;br/&gt;= total_requests / duration&lt;br/&gt;= 3 / 5.2s = 0.58 req/s&lt;br/&gt;&lt;em&gt;(Computed&amp;nbsp;from&amp;nbsp;other&amp;nbsp;metrics)&lt;/em&gt;\"]\n\n    %% Final Output\n    J1 --&gt; K[\"MetricResult List&lt;br/&gt;&lt;em&gt;Complete&amp;nbsp;performance&amp;nbsp;analysis&lt;/em&gt;\"]\n    J2 --&gt; K\n    J3 --&gt; K\n\n    %% Styling\n    classDef input fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000000,font-weight:bold\n    classDef distributed fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000000,font-weight:bold\n    classDef recordMetric fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000000,font-weight:bold\n    classDef aggregateMetric fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000000,font-weight:bold\n    classDef derivedMetric fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000000,font-weight:bold\n    classDef transport fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000000,font-weight:bold\n    classDef central fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000000,font-weight:bold\n    classDef collection fill:#fff8e1,stroke:#ffa000,stroke-width:2px,color:#000000,font-weight:bold\n    classDef statistics fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000000,font-weight:bold\n    classDef output fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000000,font-weight:bold\n\n    %% Apply styles\n    class A input\n    class B1,B2,B3 distributed\n    class C1,C2,C3,J1,H1 recordMetric\n    class D1,D2,D3,J2,H2 aggregateMetric\n    class J3 derivedMetric\n    class I2 central\n    class I1,G statistics\n    class E1,E2,E3,F,L transport\n    class K output\n</code></pre>"},{"location":"diagrams/mixins/","title":"Mixins","text":"<pre><code>flowchart TD\n    %% Core Mixins Hierarchy\n    A[\"BaseMixin&lt;br/&gt;&lt;em&gt;Ensures proper inheritance chain&lt;/em&gt;\"] --&gt; B[\"AIPerfLoggerMixin&lt;br/&gt;&lt;em&gt;Lazy-evaluated logging with f-strings&lt;/em&gt;\"]\n    B --&gt; C[\"HooksMixin&lt;br/&gt;&lt;em&gt;Extensible hook system for behavior&lt;/em&gt;\"]\n    B --&gt; D[\"TaskManagerMixin&lt;br/&gt;&lt;em&gt;Async task and background operations&lt;/em&gt;\"]\n\n    C --&gt; E[\"AIPerfLifecycleMixin&lt;br/&gt;&lt;em&gt;Component lifecycle state management&lt;/em&gt;\"]\n    D --&gt; E\n\n    E --&gt; F[\"MessageBusClientMixin&lt;br/&gt;&lt;em&gt;Message bus communication capabilities&lt;/em&gt;\"]\n\n    %% Service Base Classes\n    F --&gt; G[\"BaseService&lt;br/&gt;&lt;em&gt;Foundation for AIPerf services&lt;/em&gt;\"]\n    G --&gt; H[\"BaseComponentService&lt;br/&gt;&lt;em&gt;Component services with status reporting&lt;/em&gt;\"]\n\n    %% Special SystemController path\n    G --&gt; I[SystemController]\n\n    %% Main Component Services\n    H --&gt; J[DatasetManager]\n    H --&gt; K[TimingManager]\n    H --&gt; L[RecordsManager]\n    H --&gt; M[RecordProcessor]\n    H --&gt; N[WorkerManager]\n    H --&gt; O[Worker]\n\n    %% Modern styling with better colors and shapes\n    classDef baseMixin fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000000,font-weight:bold\n    classDef loggerMixin fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000000,font-weight:bold\n    classDef hooksMixin fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000000,font-weight:bold\n    classDef taskMixin fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000000,font-weight:bold\n    classDef lifecycleMixin fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000000,font-weight:bold\n    classDef messageMixin fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000000,font-weight:bold\n    classDef baseService fill:#fff8e1,stroke:#ffa000,stroke-width:2px,color:#000000,font-weight:bold\n    classDef componentService fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000000,font-weight:bold\n    classDef services fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000000\n    classDef systemController fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000000,font-weight:bold\n\n    %% Apply styles\n    class A baseMixin\n    class B loggerMixin\n    class C hooksMixin\n    class D taskMixin\n    class E lifecycleMixin\n    class F messageMixin\n    class G baseService\n    class H componentService\n    class I systemController\n    class J,K,L,M,N,O services\n</code></pre>"}]}